{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0022182d",
   "metadata": {},
   "source": [
    "### LLM-assisted enrichment (Including Add/Complete fields/keywords, scoring, Contribution Summary(Optional))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3095c2",
   "metadata": {},
   "source": [
    "#### Complete fields of study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38410251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:1000: The handshake operation timed out'))': /simple/backoff/\n"
     ]
    }
   ],
   "source": [
    "!pip install backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ce20c",
   "metadata": {},
   "source": [
    "This enhance_fields_of_study.py script enhances academic paper datasets by extracting relevant fields of study using DeepSeek's LLM API. It processes paper metadata (title and abstract) to generate 3-6 academic discipline labels per paper through structured prompts. The implementation features concurrent processing with rate limiting, exponential backoff for error handling, and checkpointing for resumable operation on large datasets. The enhanced data includes extracted fields and timestamp metadata for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb69a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance_fields_of_study.py\n",
    "\"\"\"\n",
    "Enhanced paper fields extraction using DeepSeek API with concurrent processing.\n",
    "\n",
    "This script processes academic papers from a JSONL file, calls the DeepSeek API\n",
    "to extract relevant academic fields of study for each paper, and saves the \n",
    "enhanced data to a new JSONL file with progress tracking and error handling.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import random\n",
    "from datetime import datetime\n",
    "import backoff\n",
    "\n",
    "# Configure logging for better monitoring\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('api_processing.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = \"sk-79990d599cd74bc0a56f6ca2f200a621\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "REQUEST_TIMEOUT = 30\n",
    "MAX_WORKERS = 10  # Increased but with better rate limiting\n",
    "BASE_DELAY = 1.0  # Base delay between requests\n",
    "MAX_RETRIES = 5\n",
    "BATCH_SIZE = 100  # Save progress every N papers\n",
    "\n",
    "# Rate limiting and thread safety\n",
    "rate_limit_lock = Lock()\n",
    "last_request_time = 0\n",
    "total_requests = 0\n",
    "failed_requests = 0\n",
    "\n",
    "def rate_limited_request():\n",
    "    \"\"\"Ensure minimum delay between API requests across all threads with jitter.\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    with rate_limit_lock:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < BASE_DELAY:\n",
    "            sleep_time = BASE_DELAY - elapsed + random.uniform(0, 0.1)  # Add jitter\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (requests.exceptions.RequestException, KeyError, json.JSONDecodeError),\n",
    "    max_tries=MAX_RETRIES,\n",
    "    max_time=300\n",
    ")\n",
    "def call_deepseek_api_with_backoff(prompt: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Call DeepSeek API with exponential backoff and better error handling.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    global total_requests, failed_requests\n",
    "    \n",
    "    # Apply rate limiting\n",
    "    rate_limited_request()\n",
    "    \n",
    "    total_requests += 1\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"deepseek-chat\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_tokens\": 200\n",
    "            },\n",
    "            timeout=REQUEST_TIMEOUT\n",
    "        )\n",
    "        \n",
    "        # Check for rate limiting\n",
    "        if response.status_code == 429:\n",
    "            retry_after = response.headers.get('Retry-After', 60)\n",
    "            logger.warning(f\"Rate limited. Waiting {retry_after} seconds...\")\n",
    "            time.sleep(float(retry_after))\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Validate response structure\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"Response is not a dictionary: {type(data)}\")\n",
    "        \n",
    "        if \"choices\" not in data or not data[\"choices\"]:\n",
    "            raise ValueError(f\"No choices in response: {data}\")\n",
    "        \n",
    "        message = data[\"choices\"][0].get(\"message\", {})\n",
    "        if not message or \"content\" not in message:\n",
    "            raise ValueError(f\"No content in message: {message}\")\n",
    "        \n",
    "        content = message[\"content\"].strip()\n",
    "        \n",
    "        # Validate that content is not empty\n",
    "        if not content:\n",
    "            raise ValueError(\"Empty response content\")\n",
    "        \n",
    "        # Basic validation - should start with [ for JSON array\n",
    "        if not content.startswith('[') or not content.endswith(']'):\n",
    "            logger.warning(f\"Response doesn't look like JSON array: {content[:100]}...\")\n",
    "            # Try to extract JSON array if it's wrapped in other text\n",
    "            import re\n",
    "            json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                content = json_match.group(0)\n",
    "            else:\n",
    "                raise ValueError(f\"Response not a valid JSON array: {content[:100]}...\")\n",
    "        \n",
    "        return content\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(\"API request timeout\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        failed_requests += 1\n",
    "        if e.response.status_code >= 500:\n",
    "            logger.error(f\"Server error {e.response.status_code}: {e.response.text[:200]}\")\n",
    "        else:\n",
    "            logger.error(f\"HTTP error {e.response.status_code}: {e.response.text[:200]}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        failed_requests += 1\n",
    "        logger.error(f\"Unexpected API error: {str(e)[:200]}\")\n",
    "        raise\n",
    "\n",
    "def call_deepseek_api(prompt: str, max_retries: int = MAX_RETRIES) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Wrapper for API call with fallback to simpler retry logic.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        max_retries: Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return call_deepseek_api_with_backoff(prompt)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"All retries failed for API call: {str(e)[:200]}\")\n",
    "        return None\n",
    "\n",
    "def extract_academic_fields(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract academic fields of study from paper metadata using DeepSeek API.\n",
    "    \n",
    "    Args:\n",
    "        paper: Dictionary containing paper metadata with 'title' and 'abstract'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with original paper data plus extracted fields\n",
    "    \"\"\"\n",
    "    paper_id = paper.get(\"paper_id\", paper.get(\"id\", \"unknown\"))\n",
    "    title = paper.get(\"title\", \"\")[:200]  # Truncate for logging\n",
    "    \n",
    "    # Clean and prepare abstract\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    if abstract and len(abstract) > 1000:\n",
    "        abstract = abstract[:1000] + \"...\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an academic classification expert.\n",
    "\n",
    "Given the following paper information, infer the most appropriate academic fields of study.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY a JSON array of strings.\n",
    "- Each item must be a broad academic field (e.g., \"Computer Vision\", \"Machine Learning\").\n",
    "- Include 3 to 6 fields only.\n",
    "- Do NOT include model names or datasets.\n",
    "\n",
    "Title:\n",
    "{title}\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\"\"\"\n",
    "    fields = []\n",
    "    \n",
    "    try:\n",
    "        api_response = call_deepseek_api(prompt)\n",
    "        \n",
    "        if api_response is None:\n",
    "            logger.debug(f\"No API response for paper {paper_id}\")\n",
    "        else:\n",
    "            try:\n",
    "                # Try to parse as JSON\n",
    "                parsed = json.loads(api_response)\n",
    "                \n",
    "                if isinstance(parsed, list):\n",
    "                    # Validate and clean each field\n",
    "                    for field in parsed:\n",
    "                        if isinstance(field, str):\n",
    "                            cleaned = field.strip()\n",
    "                            if cleaned and len(cleaned) < 100:  # Reasonable length check\n",
    "                                fields.append(cleaned)\n",
    "                \n",
    "                # Limit to 6 fields\n",
    "                fields = fields[:6]\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.warning(f\"Failed to parse JSON for paper {paper_id}: {str(e)[:100]}\")\n",
    "                logger.debug(f\"Raw response: {api_response[:500]}\")\n",
    "                \n",
    "                # Fallback: try to extract fields from text response\n",
    "                if api_response:\n",
    "                    import re\n",
    "                    # Look for field-like patterns\n",
    "                    potential_fields = re.findall(r'\"([^\"]+)\"', api_response)\n",
    "                    if not potential_fields:\n",
    "                        potential_fields = re.findall(r'[\\w\\s]+(?=,|\\.|$)', api_response)\n",
    "                    \n",
    "                    fields = [f.strip() for f in potential_fields if len(f.strip()) > 3 and len(f.strip()) < 50]\n",
    "                    fields = list(set(fields))[:6]  # Deduplicate and limit\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error extracting fields for paper {paper_id}: {str(e)[:200]}\")\n",
    "    \n",
    "    # Create a copy of the paper with extracted fields\n",
    "    enhanced_paper = paper.copy()\n",
    "    enhanced_paper[\"fields_of_study\"] = fields\n",
    "    enhanced_paper[\"fields_extraction_time\"] = datetime.now().isoformat()\n",
    "    \n",
    "    return enhanced_paper\n",
    "\n",
    "def load_papers(input_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load papers from JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input JSONL file\n",
    "    \n",
    "    Returns:\n",
    "        List of paper dictionaries\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If input file doesn't exist\n",
    "    \"\"\"\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "    \n",
    "    papers = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                papers.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.warning(f\"Invalid JSON on line {line_num}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Loaded {len(papers)} papers from {input_path}\")\n",
    "    return papers\n",
    "\n",
    "def save_checkpoint(papers: List[Dict[str, Any]], checkpoint_path: Path):\n",
    "    \"\"\"\n",
    "    Save intermediate results to checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        papers: List of enhanced paper dictionaries\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "    \"\"\"\n",
    "    with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "        for paper in papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    logger.info(f\"Checkpoint saved: {len(papers)} papers to {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path: Path) -> Tuple[List[Dict[str, Any]], set]:\n",
    "    \"\"\"\n",
    "    Load papers from checkpoint file and get processed paper IDs.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (papers list, set of processed paper IDs)\n",
    "    \"\"\"\n",
    "    if not checkpoint_path.exists():\n",
    "        return [], set()\n",
    "    \n",
    "    papers = []\n",
    "    processed_ids = set()\n",
    "    \n",
    "    with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                paper = json.loads(line.strip())\n",
    "                papers.append(paper)\n",
    "                paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                if paper_id:\n",
    "                    processed_ids.add(paper_id)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    logger.info(f\"Loaded {len(papers)} papers from checkpoint\")\n",
    "    return papers, processed_ids\n",
    "\n",
    "def process_paper_batch(papers: List[Dict[str, Any]], \n",
    "                       max_workers: int = MAX_WORKERS,\n",
    "                       checkpoint_path: Optional[Path] = None) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Process multiple papers concurrently using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        papers: List of paper dictionaries to process\n",
    "        max_workers: Maximum number of concurrent threads\n",
    "        checkpoint_path: Optional path for checkpointing\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (enhanced papers list, statistics dictionary)\n",
    "    \"\"\"\n",
    "    enhanced_papers = []\n",
    "    stats = {\n",
    "        \"successful\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"empty_fields\": 0,\n",
    "        \"total\": len(papers)\n",
    "    }\n",
    "    \n",
    "    # Adjust workers based on rate limits\n",
    "    effective_workers = min(max_workers, 5)  # Conservative start\n",
    "    \n",
    "    # Use ThreadPoolExecutor for concurrent processing\n",
    "    with ThreadPoolExecutor(max_workers=effective_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_paper = {\n",
    "            executor.submit(extract_academic_fields, paper): paper \n",
    "            for paper in papers\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        logger.info(f\"Processing {len(papers)} papers with {effective_workers} concurrent workers...\")\n",
    "        \n",
    "        completed = 0\n",
    "        for future in tqdm(as_completed(future_to_paper), total=len(papers), \n",
    "                          desc=\"Extracting academic fields\", unit=\"paper\"):\n",
    "            try:\n",
    "                enhanced_paper = future.result(timeout=REQUEST_TIMEOUT + 10)\n",
    "                enhanced_papers.append(enhanced_paper)\n",
    "                \n",
    "                fields = enhanced_paper.get(\"fields_of_study\", [])\n",
    "                if fields:\n",
    "                    stats[\"successful\"] += 1\n",
    "                else:\n",
    "                    stats[\"empty_fields\"] += 1\n",
    "                    stats[\"failed\"] += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing paper: {str(e)[:200]}\")\n",
    "                stats[\"failed\"] += 1\n",
    "                # Add original paper with empty fields as fallback\n",
    "                original_paper = future_to_paper[future].copy()\n",
    "                original_paper[\"fields_of_study\"] = []\n",
    "                enhanced_papers.append(original_paper)\n",
    "            \n",
    "            completed += 1\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if checkpoint_path and completed % BATCH_SIZE == 0:\n",
    "                save_checkpoint(enhanced_papers, checkpoint_path)\n",
    "    \n",
    "    return enhanced_papers, stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function with concurrent processing and checkpointing.\"\"\"\n",
    "    \n",
    "    # Define file paths\n",
    "    input_file = Path(\"../Data_Cleaning/papers_final_aligned.jsonl\")\n",
    "    output_file = Path(\"papers_enhanced_fields.jsonl\")\n",
    "    checkpoint_file = Path(\"papers_enhanced_checkpoint.jsonl\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load papers\n",
    "        papers = load_papers(input_file)\n",
    "        \n",
    "        if not papers:\n",
    "            logger.error(\"No papers loaded from input file\")\n",
    "            return\n",
    "        \n",
    "        # Load checkpoint if exists\n",
    "        checkpoint_papers, processed_ids = load_checkpoint(checkpoint_file)\n",
    "        \n",
    "        # Filter out already processed papers\n",
    "        if processed_ids:\n",
    "            papers_to_process = []\n",
    "            for paper in papers:\n",
    "                paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                if paper_id not in processed_ids:\n",
    "                    papers_to_process.append(paper)\n",
    "            \n",
    "            logger.info(f\"Already processed {len(checkpoint_papers)} papers. \"\n",
    "                       f\"Processing {len(papers_to_process)} remaining papers.\")\n",
    "            \n",
    "            # Process remaining papers\n",
    "            enhanced_new_papers, stats = process_paper_batch(\n",
    "                papers_to_process, \n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "            \n",
    "            # Combine checkpoint and new results\n",
    "            enhanced_papers = checkpoint_papers + enhanced_new_papers\n",
    "        else:\n",
    "            # Process all papers\n",
    "            enhanced_papers, stats = process_paper_batch(\n",
    "                papers, \n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "        \n",
    "        # Final save\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for paper in enhanced_papers:\n",
    "                f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Print summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"PROCESSING COMPLETE!\")\n",
    "        logger.info(f\"Total papers processed: {stats['total']}\")\n",
    "        logger.info(f\"Successfully extracted fields: {stats['successful']}\")\n",
    "        logger.info(f\"Papers with empty fields: {stats['empty_fields']}\")\n",
    "        logger.info(f\"Failed extractions: {stats['failed']}\")\n",
    "        logger.info(f\"Total API requests: {total_requests}\")\n",
    "        logger.info(f\"Failed API requests: {failed_requests}\")\n",
    "        logger.info(f\"Success rate: {(total_requests - failed_requests) / total_requests * 100:.1f}%\")\n",
    "        logger.info(f\"Total time: {elapsed_time:.2f} seconds\")\n",
    "        logger.info(f\"Processing rate: {stats['total'] / elapsed_time:.2f} papers/second\")\n",
    "        logger.info(f\"Results saved to: {output_file}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Clean up checkpoint file\n",
    "        if checkpoint_file.exists():\n",
    "            checkpoint_file.unlink()\n",
    "            logger.info(f\"Checkpoint file removed: {checkpoint_file}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File error: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Processing interrupted by user.\")\n",
    "        logger.info(f\"Checkpoint saved to {checkpoint_file}. Resume by running again.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e653f3",
   "metadata": {},
   "source": [
    "#### Generate key words for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance_keywords.py\n",
    "\"\"\"\n",
    "Enhanced academic keywords extraction using DeepSeek API with concurrent processing.\n",
    "\n",
    "This script processes academic papers from a JSONL file, calls the DeepSeek API\n",
    "to extract relevant academic keywords for each paper, and saves the enhanced data\n",
    "to a new JSONL file with progress tracking, error handling, and checkpointing.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import backoff\n",
    "from threading import Lock\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('keywords_extraction.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = \"sk-79990d599cd74bc0a56f6ca2f200a621\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "REQUEST_TIMEOUT = 30\n",
    "MAX_WORKERS = 8  # Adjust based on API rate limits\n",
    "BASE_DELAY = 0.5  # Base delay between requests\n",
    "MAX_RETRIES = 5\n",
    "BATCH_SIZE = 50  # Save progress every N papers\n",
    "\n",
    "# Rate limiting\n",
    "rate_limit_lock = Lock()\n",
    "last_request_time = 0\n",
    "request_counter = 0\n",
    "error_counter = 0\n",
    "\n",
    "def rate_limited_request():\n",
    "    \"\"\"Ensure minimum delay between API requests with jitter.\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    with rate_limit_lock:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < BASE_DELAY:\n",
    "            sleep_time = BASE_DELAY - elapsed + random.uniform(0, 0.1)\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (requests.exceptions.RequestException, KeyError, json.JSONDecodeError),\n",
    "    max_tries=MAX_RETRIES,\n",
    "    max_time=300,\n",
    "    jitter=backoff.full_jitter\n",
    ")\n",
    "def call_deepseek_with_backoff(prompt: str, paper_id: str = \"unknown\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Call DeepSeek API with exponential backoff and comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        paper_id: Paper identifier for logging\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    global request_counter, error_counter\n",
    "    \n",
    "    rate_limited_request()\n",
    "    request_counter += 1\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"deepseek-chat\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_tokens\": 300\n",
    "            },\n",
    "            timeout=REQUEST_TIMEOUT\n",
    "        )\n",
    "        \n",
    "        # Handle rate limiting\n",
    "        if response.status_code == 429:\n",
    "            retry_after = int(response.headers.get('Retry-After', 60))\n",
    "            logger.warning(f\"Rate limited for paper {paper_id}. Waiting {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Validate response structure\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"Response is not a dictionary: {type(data)}\")\n",
    "        \n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            raise ValueError(f\"No choices in response: {data}\")\n",
    "        \n",
    "        message = choices[0].get(\"message\", {})\n",
    "        content = message.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            raise ValueError(\"Empty response content\")\n",
    "        \n",
    "        # Log success periodically\n",
    "        if request_counter % 100 == 0:\n",
    "            logger.info(f\"Completed {request_counter} API requests\")\n",
    "        \n",
    "        return content\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"Timeout for paper {paper_id}\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        error_counter += 1\n",
    "        status_code = e.response.status_code if e.response else \"unknown\"\n",
    "        logger.error(f\"HTTP error {status_code} for paper {paper_id}\")\n",
    "        if status_code >= 500:\n",
    "            # Server error, use longer backoff\n",
    "            time.sleep(10)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        error_counter += 1\n",
    "        logger.error(f\"API error for paper {paper_id}: {str(e)[:100]}\")\n",
    "        raise\n",
    "\n",
    "def call_deepseek(prompt: str, paper_id: str = \"unknown\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Wrapper for API call with fallback handling.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        paper_id: Paper identifier for logging\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return call_deepseek_with_backoff(prompt, paper_id)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"All retries failed for paper {paper_id}: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "def extract_keywords(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract academic keywords from paper metadata using DeepSeek API.\n",
    "    \n",
    "    Args:\n",
    "        paper: Dictionary containing paper metadata\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with original paper data plus extracted keywords\n",
    "    \"\"\"\n",
    "    paper_id = paper.get(\"paper_id\", paper.get(\"id\", \"unknown\"))\n",
    "    \n",
    "    # Prepare title and abstract\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    \n",
    "    # Truncate if too long (API has token limits)\n",
    "    if len(abstract) > 3000:\n",
    "        abstract = abstract[:3000] + \"...\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Extract high-quality academic keywords from the following paper.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY a JSON array of strings.\n",
    "- 5 to 8 keywords.\n",
    "- Keywords should describe tasks, methods, or research problems.\n",
    "- Avoid generic words like \"model\", \"method\", \"framework\".\n",
    "- Each keyword should be specific and meaningful.\n",
    "\n",
    "Title:\n",
    "{title}\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\"\"\"\n",
    "    keywords = []\n",
    "    \n",
    "    try:\n",
    "        api_response = call_deepseek(prompt, paper_id)\n",
    "        \n",
    "        if api_response is None:\n",
    "            logger.debug(f\"No API response for paper {paper_id}\")\n",
    "        else:\n",
    "            # Try to parse as JSON\n",
    "            try:\n",
    "                parsed = json.loads(api_response)\n",
    "                \n",
    "                if isinstance(parsed, list):\n",
    "                    # Clean and validate keywords\n",
    "                    for keyword in parsed:\n",
    "                        if isinstance(keyword, str):\n",
    "                            cleaned = keyword.strip()\n",
    "                            # Filter out generic keywords and validate length\n",
    "                            if (cleaned and \n",
    "                                len(cleaned) >= 3 and \n",
    "                                len(cleaned) <= 50 and\n",
    "                                cleaned.lower() not in ['model', 'method', 'framework', \n",
    "                                                       'approach', 'system', 'algorithm']):\n",
    "                                keywords.append(cleaned)\n",
    "                    \n",
    "                    # Deduplicate and limit\n",
    "                    seen = set()\n",
    "                    unique_keywords = []\n",
    "                    for k in keywords:\n",
    "                        if k not in seen:\n",
    "                            seen.add(k)\n",
    "                            unique_keywords.append(k)\n",
    "                    keywords = unique_keywords[:8]  # Max 8 as per requirements\n",
    "                    \n",
    "                    # Ensure minimum of 5 if we have some\n",
    "                    if 0 < len(keywords) < 5:\n",
    "                        logger.debug(f\"Paper {paper_id} has only {len(keywords)} keywords\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback: extract keywords from text response\n",
    "                logger.debug(f\"Failed to parse JSON for paper {paper_id}, extracting from text\")\n",
    "                \n",
    "                # Try to find JSON array pattern\n",
    "                json_pattern = r'\\[.*?\\]'\n",
    "                match = re.search(json_pattern, api_response, re.DOTALL)\n",
    "                if match:\n",
    "                    try:\n",
    "                        parsed = json.loads(match.group(0))\n",
    "                        if isinstance(parsed, list):\n",
    "                            keywords = [str(k).strip() for k in parsed if str(k).strip()]\n",
    "                            keywords = list(set(keywords))[:8]\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # If still no keywords, use text-based extraction\n",
    "                if not keywords:\n",
    "                    # Split by common delimiters and filter\n",
    "                    lines = api_response.strip().split('\\n')\n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        # Remove quotes, brackets, etc.\n",
    "                        line = re.sub(r'^[\\s\\[\\]\\'\"]+|[\\s\\[\\]\\'\\\"]+$', '', line)\n",
    "                        if (line and \n",
    "                            len(line) >= 3 and \n",
    "                            len(line) <= 50 and\n",
    "                            line.lower() not in ['model', 'method', 'framework'] and\n",
    "                            ',' not in line and ';' not in line):\n",
    "                            keywords.append(line)\n",
    "                    \n",
    "                    keywords = list(set(keywords))[:8]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error extracting keywords for paper {paper_id}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Create enhanced paper\n",
    "    enhanced_paper = paper.copy()\n",
    "    enhanced_paper[\"keywords\"] = keywords\n",
    "    enhanced_paper[\"keywords_extraction_time\"] = datetime.now().isoformat()\n",
    "    \n",
    "    return enhanced_paper\n",
    "\n",
    "def load_papers(input_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load papers from JSONL file.\"\"\"\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    papers.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Invalid JSON on line {line_num}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(papers)} papers from {input_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input file not found: {input_path}\")\n",
    "        raise\n",
    "    \n",
    "    return papers\n",
    "\n",
    "def save_checkpoint(papers: List[Dict[str, Any]], checkpoint_path: Path):\n",
    "    \"\"\"Save intermediate results to checkpoint file.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "            for paper in papers:\n",
    "                f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        logger.debug(f\"Checkpoint saved: {len(papers)} papers\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path: Path) -> Tuple[List[Dict[str, Any]], set]:\n",
    "    \"\"\"Load papers from checkpoint file and get processed paper IDs.\"\"\"\n",
    "    if not checkpoint_path.exists():\n",
    "        return [], set()\n",
    "    \n",
    "    papers = []\n",
    "    processed_ids = set()\n",
    "    \n",
    "    try:\n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers.append(paper)\n",
    "                    paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                    if paper_id:\n",
    "                        processed_ids.add(paper_id)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        logger.info(f\"Loaded {len(papers)} papers from checkpoint\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load checkpoint: {e}\")\n",
    "    \n",
    "    return papers, processed_ids\n",
    "\n",
    "def process_concurrently(papers: List[Dict[str, Any]], \n",
    "                        max_workers: int = MAX_WORKERS,\n",
    "                        checkpoint_path: Optional[Path] = None) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Process papers concurrently using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        papers: List of papers to process\n",
    "        max_workers: Maximum concurrent threads\n",
    "        checkpoint_path: Optional checkpoint file path\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (enhanced papers, statistics)\n",
    "    \"\"\"\n",
    "    enhanced_papers = []\n",
    "    stats = {\n",
    "        \"total\": len(papers),\n",
    "        \"successful\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"with_keywords\": 0,\n",
    "        \"empty_keywords\": 0\n",
    "    }\n",
    "    \n",
    "    # Adjust workers based on rate limits\n",
    "    effective_workers = min(max_workers, 10)\n",
    "    logger.info(f\"Processing {len(papers)} papers with {effective_workers} workers\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=effective_workers) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_index = {\n",
    "            executor.submit(extract_keywords, paper): i \n",
    "            for i, paper in enumerate(papers)\n",
    "        }\n",
    "        \n",
    "        # Track completion order for proper ordering\n",
    "        results = [None] * len(papers)\n",
    "        \n",
    "        # Process with progress bar\n",
    "        with tqdm(total=len(papers), desc=\"Extracting keywords\", unit=\"paper\") as pbar:\n",
    "            for future in as_completed(future_to_index):\n",
    "                idx = future_to_index[future]\n",
    "                \n",
    "                try:\n",
    "                    enhanced_paper = future.result(timeout=REQUEST_TIMEOUT + 30)\n",
    "                    results[idx] = enhanced_paper\n",
    "                    \n",
    "                    keywords = enhanced_paper.get(\"keywords\", [])\n",
    "                    if keywords:\n",
    "                        stats[\"with_keywords\"] += 1\n",
    "                        stats[\"successful\"] += 1\n",
    "                    else:\n",
    "                        stats[\"empty_keywords\"] += 1\n",
    "                        stats[\"failed\"] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Processing failed for paper at index {idx}: {str(e)[:100]}\")\n",
    "                    stats[\"failed\"] += 1\n",
    "                    # Add fallback\n",
    "                    fallback_paper = papers[idx].copy()\n",
    "                    fallback_paper[\"keywords\"] = []\n",
    "                    results[idx] = fallback_paper\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Periodic checkpoint\n",
    "                if checkpoint_path and pbar.n % BATCH_SIZE == 0:\n",
    "                    # Collect completed results\n",
    "                    completed = [r for r in results if r is not None]\n",
    "                    save_checkpoint(completed, checkpoint_path)\n",
    "    \n",
    "    # Filter out None results (shouldn't happen with proper error handling)\n",
    "    enhanced_papers = [r for r in results if r is not None]\n",
    "    \n",
    "    return enhanced_papers, stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_file = Path(\"papers_final_aligned.jsonl\")\n",
    "    output_file = Path(\"papers_enhanced_keywords.jsonl\")\n",
    "    checkpoint_file = Path(\"keywords_extraction_checkpoint.jsonl\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load papers\n",
    "        papers = load_papers(input_file)\n",
    "        \n",
    "        if not papers:\n",
    "            logger.error(\"No papers loaded. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint_papers, processed_ids = load_checkpoint(checkpoint_file)\n",
    "        \n",
    "        # Determine which papers need processing\n",
    "        if processed_ids:\n",
    "            papers_to_process = []\n",
    "            for paper in papers:\n",
    "                paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                if paper_id not in processed_ids:\n",
    "                    papers_to_process.append(paper)\n",
    "            \n",
    "            logger.info(f\"Resuming: {len(checkpoint_papers)} already processed, \"\n",
    "                       f\"{len(papers_to_process)} remaining\")\n",
    "            \n",
    "            # Process remaining papers\n",
    "            enhanced_new_papers, stats = process_concurrently(\n",
    "                papers_to_process,\n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "            \n",
    "            # Combine results\n",
    "            enhanced_papers = checkpoint_papers + enhanced_new_papers\n",
    "        else:\n",
    "            # Process all papers\n",
    "            enhanced_papers, stats = process_concurrently(\n",
    "                papers,\n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "        \n",
    "        # Save final results\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for paper in enhanced_papers:\n",
    "                    f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "            logger.info(f\"Saved {len(enhanced_papers)} papers to {output_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save output: {e}\")\n",
    "            # Save to backup file\n",
    "            backup_file = Path(f\"{output_file.stem}_backup_{int(time.time())}.jsonl\")\n",
    "            with open(backup_file, 'w', encoding='utf-8') as f:\n",
    "                for paper in enhanced_papers:\n",
    "                    f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "            logger.info(f\"Saved backup to {backup_file}\")\n",
    "        \n",
    "        # Print summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"KEYWORDS EXTRACTION COMPLETE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Total papers: {stats['total']}\")\n",
    "        logger.info(f\"Successfully processed: {stats['successful']}\")\n",
    "        logger.info(f\"Papers with keywords: {stats['with_keywords']}\")\n",
    "        logger.info(f\"Papers with empty keywords: {stats['empty_keywords']}\")\n",
    "        logger.info(f\"Failed: {stats['failed']}\")\n",
    "        logger.info(f\"Total API requests: {request_counter}\")\n",
    "        logger.info(f\"API errors: {error_counter}\")\n",
    "        if request_counter > 0:\n",
    "            logger.info(f\"API success rate: {(request_counter - error_counter) / request_counter * 100:.1f}%\")\n",
    "        logger.info(f\"Total time: {elapsed_time:.1f} seconds\")\n",
    "        logger.info(f\"Processing rate: {stats['total'] / elapsed_time:.2f} papers/sec\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Clean up checkpoint\n",
    "        if checkpoint_file.exists():\n",
    "            try:\n",
    "                checkpoint_file.unlink()\n",
    "                logger.info(f\"Removed checkpoint file: {checkpoint_file}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to remove checkpoint: {e}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcessing interrupted by user.\")\n",
    "        logger.info(f\"Checkpoint saved. Resume by running the script again.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abca0a",
   "metadata": {},
   "source": [
    "#### Score the article based on the title and abstract of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78b844",
   "metadata": {},
   "source": [
    "This script (enhance_scoring.py) enhances academic paper datasets by adding automated quality assessments using the DeepSeek LLM API. It processes papers from a JSONL file, constructs structured prompts for AI evaluation, and appends quality scores to each paper before saving the enhanced data to a new file. The implementation includes basic API integration with rate limiting to prevent service overload.\n",
    "\n",
    "Scoring System\n",
    "\n",
    "The scoring model evaluates papers across four primary dimensions (0-10 integer scale) plus two comprehensive metrics:\n",
    "• Novelty: Originality and innovative contribution\n",
    "\n",
    "• Technical Depth: Methodological rigor and complexity\n",
    "\n",
    "• Clarity: Presentation quality and logical structure\n",
    "\n",
    "• Impact Potential: Field contribution and application value\n",
    "\n",
    "• Overall Score: Weighted composite (float)\n",
    "\n",
    "• Confidence: Model's assessment certainty (0-1 float)\n",
    "\n",
    "The AI model generates scores by analyzing paper titles and abstracts, simulating peer-review assessment standards with structured JSON output for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance_scoring.py\n",
    "\"\"\"\n",
    "Enhanced academic paper scoring using DeepSeek API with concurrent processing.\n",
    "\n",
    "This script processes academic papers from a JSONL file, calls the DeepSeek API\n",
    "to evaluate paper quality across multiple dimensions, and saves the enhanced data\n",
    "to a new JSONL file with progress tracking, error handling, and checkpointing.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import backoff\n",
    "from threading import Lock\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('paper_scoring.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = \"sk-79990d599cd74bc0a56f6ca2f200a621\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "REQUEST_TIMEOUT = 40  # Increased for scoring task\n",
    "MAX_WORKERS = 6  # Conservative for scoring task\n",
    "BASE_DELAY = 0.8  # Increased delay for scoring\n",
    "MAX_RETRIES = 5\n",
    "BATCH_SIZE = 30  # Save progress every N papers\n",
    "MAX_TOKENS = 150\n",
    "\n",
    "# Rate limiting and tracking\n",
    "rate_limit_lock = Lock()\n",
    "last_request_time = 0\n",
    "request_counter = 0\n",
    "error_counter = 0\n",
    "\n",
    "# Score validation ranges\n",
    "SCORE_RANGES = {\n",
    "    \"novelty\": (0, 10),\n",
    "    \"technical_depth\": (0, 10),\n",
    "    \"clarity\": (0, 10),\n",
    "    \"impact_potential\": (0, 10),\n",
    "    \"overall_score\": (0.0, 10.0),\n",
    "    \"confidence\": (0.0, 1.0)\n",
    "}\n",
    "\n",
    "def rate_limited_request():\n",
    "    \"\"\"Ensure minimum delay between API requests with jitter.\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    with rate_limit_lock:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < BASE_DELAY:\n",
    "            sleep_time = BASE_DELAY - elapsed + random.uniform(0, 0.2)\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()\n",
    "\n",
    "def validate_scores(scores: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Validate and normalize score values.\"\"\"\n",
    "    if not isinstance(scores, dict):\n",
    "        return {}\n",
    "    \n",
    "    validated = {}\n",
    "    \n",
    "    for key, (min_val, max_val) in SCORE_RANGES.items():\n",
    "        if key in scores:\n",
    "            try:\n",
    "                value = scores[key]\n",
    "                # Convert to appropriate type\n",
    "                if key in [\"overall_score\", \"confidence\"]:\n",
    "                    value = float(value)\n",
    "                else:\n",
    "                    value = int(round(float(value)))\n",
    "                \n",
    "                # Clamp to valid range\n",
    "                if key in [\"overall_score\", \"confidence\"]:\n",
    "                    value = max(min_val, min(max_val, value))\n",
    "                else:\n",
    "                    value = max(min_val, min(max_val, value))\n",
    "                \n",
    "                validated[key] = value\n",
    "            except (ValueError, TypeError):\n",
    "                logger.debug(f\"Invalid value for {key}: {scores[key]}\")\n",
    "                # Set default values\n",
    "                if key in [\"overall_score\", \"confidence\"]:\n",
    "                    validated[key] = 0.0\n",
    "                else:\n",
    "                    validated[key] = 0\n",
    "    \n",
    "    # Calculate overall score if not provided but other scores exist\n",
    "    if \"overall_score\" not in validated and all(k in validated for k in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\"]):\n",
    "        validated[\"overall_score\"] = round(\n",
    "            (validated[\"novelty\"] + validated[\"technical_depth\"] + \n",
    "             validated[\"clarity\"] + validated[\"impact_potential\"]) / 4.0, 1\n",
    "        )\n",
    "    \n",
    "    # Add confidence if missing\n",
    "    if \"confidence\" not in validated:\n",
    "        validated[\"confidence\"] = 0.5\n",
    "    \n",
    "    return validated\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (requests.exceptions.RequestException, KeyError, json.JSONDecodeError),\n",
    "    max_tries=MAX_RETRIES,\n",
    "    max_time=300,\n",
    "    jitter=backoff.full_jitter\n",
    ")\n",
    "def call_deepseek_with_backoff(prompt: str, paper_id: str = \"unknown\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Call DeepSeek API with exponential backoff for scoring task.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        paper_id: Paper identifier for logging\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    global request_counter, error_counter\n",
    "    \n",
    "    rate_limited_request()\n",
    "    request_counter += 1\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"deepseek-chat\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": MAX_TOKENS\n",
    "            },\n",
    "            timeout=REQUEST_TIMEOUT\n",
    "        )\n",
    "        \n",
    "        # Handle rate limiting\n",
    "        if response.status_code == 429:\n",
    "            retry_after = int(response.headers.get('Retry-After', 90))\n",
    "            logger.warning(f\"Rate limited for paper {paper_id}. Waiting {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Validate response structure\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"Response is not a dictionary: {type(data)}\")\n",
    "        \n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            raise ValueError(f\"No choices in response: {data}\")\n",
    "        \n",
    "        message = choices[0].get(\"message\", {})\n",
    "        content = message.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            raise ValueError(\"Empty response content\")\n",
    "        \n",
    "        # Log progress periodically\n",
    "        if request_counter % 50 == 0:\n",
    "            logger.info(f\"Completed {request_counter} scoring requests\")\n",
    "        \n",
    "        return content\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"Timeout for paper {paper_id}\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        error_counter += 1\n",
    "        status_code = e.response.status_code if e.response else \"unknown\"\n",
    "        logger.error(f\"HTTP error {status_code} for paper {paper_id}\")\n",
    "        if status_code >= 500:\n",
    "            time.sleep(15)  # Longer sleep for server errors\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        error_counter += 1\n",
    "        logger.error(f\"API error for paper {paper_id}: {str(e)[:100]}\")\n",
    "        raise\n",
    "\n",
    "def call_deepseek(prompt: str, paper_id: str = \"unknown\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Wrapper for API call with fallback handling.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        paper_id: Paper identifier for logging\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return call_deepseek_with_backoff(prompt, paper_id)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"All retries failed for paper {paper_id}: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "def extract_paper_scores(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract quality scores from paper metadata using DeepSeek API.\n",
    "    \n",
    "    Args:\n",
    "        paper: Dictionary containing paper metadata\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with original paper data plus quality scores\n",
    "    \"\"\"\n",
    "    paper_id = paper.get(\"paper_id\", paper.get(\"id\", \"unknown\"))\n",
    "    \n",
    "    # Prepare content\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(abstract) > 2500:\n",
    "        abstract = abstract[:2500] + \"...\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a senior conference reviewer with expertise in academic paper evaluation.\n",
    "\n",
    "Evaluate the following paper strictly and objectively.\n",
    "\n",
    "Score each dimension from 0 to 10 (integer only for novelty, technical_depth, clarity, impact_potential).\n",
    "Overall score should be a float between 0.0 and 10.0 with one decimal place.\n",
    "Confidence should be a float between 0.0 and 1.0 with two decimal places.\n",
    "\n",
    "IMPORTANT: Return ONLY valid JSON in the following format, no additional text:\n",
    "{{\n",
    "  \"novelty\": int,\n",
    "  \"technical_depth\": int,\n",
    "  \"clarity\": int,\n",
    "  \"impact_potential\": int,\n",
    "  \"overall_score\": float,\n",
    "  \"confidence\": float\n",
    "}}\n",
    "\n",
    "Evaluation Guidelines:\n",
    "1. Novelty: Originality and new contributions (0=no novelty, 10=highly novel)\n",
    "2. Technical Depth: Sophistication of methods and analysis (0=superficial, 10=very deep)\n",
    "3. Clarity: Presentation quality and writing (0=unclear, 10=very clear)\n",
    "4. Impact Potential: Potential influence on field (0=no impact, 10=high impact)\n",
    "5. Overall Score: Weighted average considering all factors\n",
    "6. Confidence: Your confidence in the evaluation (0.0=low, 1.0=high)\n",
    "\n",
    "Title:\n",
    "{title}\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    try:\n",
    "        api_response = call_deepseek(prompt, paper_id)\n",
    "        \n",
    "        if api_response is None:\n",
    "            logger.debug(f\"No API response for paper {paper_id}\")\n",
    "        else:\n",
    "            # Clean response - remove markdown code blocks if present\n",
    "            clean_response = api_response.strip()\n",
    "            clean_response = re.sub(r'^```json\\s*', '', clean_response)\n",
    "            clean_response = re.sub(r'\\s*```$', '', clean_response)\n",
    "            \n",
    "            # Try to parse as JSON\n",
    "            try:\n",
    "                parsed = json.loads(clean_response)\n",
    "                \n",
    "                if isinstance(parsed, dict):\n",
    "                    scores = validate_scores(parsed)\n",
    "                else:\n",
    "                    logger.warning(f\"Response is not a dictionary for paper {paper_id}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.debug(f\"Failed to parse JSON for paper {paper_id}: {str(e)[:100]}\")\n",
    "                \n",
    "                # Advanced JSON extraction\n",
    "                json_patterns = [\n",
    "                    r'\\{[^{}]*\\}',  # Simple object\n",
    "                    r'\\{.*\\}',      # Nested object (greedy)\n",
    "                ]\n",
    "                \n",
    "                for pattern in json_patterns:\n",
    "                    matches = re.findall(pattern, clean_response, re.DOTALL)\n",
    "                    for match in matches:\n",
    "                        try:\n",
    "                            parsed = json.loads(match)\n",
    "                            if isinstance(parsed, dict) and any(k in parsed for k in SCORE_RANGES.keys()):\n",
    "                                scores = validate_scores(parsed)\n",
    "                                if scores:\n",
    "                                    break\n",
    "                        except:\n",
    "                            continue\n",
    "                    if scores:\n",
    "                        break\n",
    "                \n",
    "                # Fallback: extract scores from text\n",
    "                if not scores:\n",
    "                    logger.debug(f\"Using text extraction fallback for paper {paper_id}\")\n",
    "                    \n",
    "                    # Look for key-value patterns\n",
    "                    for key in SCORE_RANGES.keys():\n",
    "                        pattern = rf'\"{key}\"\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)'\n",
    "                        match = re.search(pattern, clean_response, re.IGNORECASE)\n",
    "                        if not match:\n",
    "                            # Try without quotes\n",
    "                            pattern = rf'{key}\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)'\n",
    "                            match = re.search(pattern, clean_response, re.IGNORECASE)\n",
    "                        \n",
    "                        if match:\n",
    "                            try:\n",
    "                                value = match.group(1)\n",
    "                                if key in [\"overall_score\", \"confidence\"]:\n",
    "                                    scores[key] = float(value)\n",
    "                                else:\n",
    "                                    scores[key] = int(round(float(value)))\n",
    "                            except:\n",
    "                                pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error scoring paper {paper_id}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Validate and add default values for missing scores\n",
    "    scores = validate_scores(scores)\n",
    "    \n",
    "    # Create enhanced paper\n",
    "    enhanced_paper = paper.copy()\n",
    "    enhanced_paper[\"quality_scores\"] = scores\n",
    "    enhanced_paper[\"scoring_time\"] = datetime.now().isoformat()\n",
    "    \n",
    "    return enhanced_paper\n",
    "\n",
    "def load_papers(input_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load papers from JSONL file.\"\"\"\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    papers.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Invalid JSON on line {line_num}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(papers)} papers from {input_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input file not found: {input_path}\")\n",
    "        raise\n",
    "    \n",
    "    return papers\n",
    "\n",
    "def save_checkpoint(papers: List[Dict[str, Any]], checkpoint_path: Path):\n",
    "    \"\"\"Save intermediate results to checkpoint file.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "            for paper in papers:\n",
    "                f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        logger.debug(f\"Checkpoint saved: {len(papers)} papers\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path: Path) -> Tuple[List[Dict[str, Any]], set]:\n",
    "    \"\"\"Load papers from checkpoint file and get processed paper IDs.\"\"\"\n",
    "    if not checkpoint_path.exists():\n",
    "        return [], set()\n",
    "    \n",
    "    papers = []\n",
    "    processed_ids = set()\n",
    "    \n",
    "    try:\n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers.append(paper)\n",
    "                    paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                    if paper_id:\n",
    "                        processed_ids.add(paper_id)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        logger.info(f\"Loaded {len(papers)} papers from checkpoint\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load checkpoint: {e}\")\n",
    "    \n",
    "    return papers, processed_ids\n",
    "\n",
    "def process_concurrently(papers: List[Dict[str, Any]], \n",
    "                        max_workers: int = MAX_WORKERS,\n",
    "                        checkpoint_path: Optional[Path] = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process papers concurrently using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        papers: List of papers to process\n",
    "        max_workers: Maximum concurrent threads\n",
    "        checkpoint_path: Optional checkpoint file path\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (enhanced papers, statistics)\n",
    "    \"\"\"\n",
    "    enhanced_papers = []\n",
    "    stats = {\n",
    "        \"total\": len(papers),\n",
    "        \"successful\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"complete_scores\": 0,\n",
    "        \"partial_scores\": 0,\n",
    "        \"no_scores\": 0\n",
    "    }\n",
    "    \n",
    "    # Adjust workers for scoring task\n",
    "    effective_workers = min(max_workers, 8)\n",
    "    logger.info(f\"Scoring {len(papers)} papers with {effective_workers} workers\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=effective_workers) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_index = {\n",
    "            executor.submit(extract_paper_scores, paper): i \n",
    "            for i, paper in enumerate(papers)\n",
    "        }\n",
    "        \n",
    "        # Track completion order for proper ordering\n",
    "        results = [None] * len(papers)\n",
    "        \n",
    "        # Process with progress bar\n",
    "        with tqdm(total=len(papers), desc=\"Evaluating papers\", unit=\"paper\") as pbar:\n",
    "            for future in as_completed(future_to_index):\n",
    "                idx = future_to_index[future]\n",
    "                \n",
    "                try:\n",
    "                    enhanced_paper = future.result(timeout=REQUEST_TIMEOUT + 45)\n",
    "                    results[idx] = enhanced_paper\n",
    "                    \n",
    "                    scores = enhanced_paper.get(\"quality_scores\", {})\n",
    "                    if scores:\n",
    "                        required_keys = [\"novelty\", \"technical_depth\", \"clarity\", \n",
    "                                        \"impact_potential\", \"overall_score\", \"confidence\"]\n",
    "                        present_keys = [k for k in required_keys if k in scores]\n",
    "                        \n",
    "                        if len(present_keys) >= 4:  # At least 4 of 6 scores\n",
    "                            stats[\"complete_scores\"] += 1\n",
    "                            stats[\"successful\"] += 1\n",
    "                        elif len(present_keys) >= 1:\n",
    "                            stats[\"partial_scores\"] += 1\n",
    "                            stats[\"successful\"] += 1\n",
    "                        else:\n",
    "                            stats[\"no_scores\"] += 1\n",
    "                            stats[\"failed\"] += 1\n",
    "                    else:\n",
    "                        stats[\"no_scores\"] += 1\n",
    "                        stats[\"failed\"] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Processing failed for paper at index {idx}: {str(e)[:100]}\")\n",
    "                    stats[\"failed\"] += 1\n",
    "                    # Add fallback\n",
    "                    fallback_paper = papers[idx].copy()\n",
    "                    fallback_paper[\"quality_scores\"] = {}\n",
    "                    results[idx] = fallback_paper\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Periodic checkpoint\n",
    "                if checkpoint_path and pbar.n % BATCH_SIZE == 0:\n",
    "                    # Collect completed results\n",
    "                    completed = [r for r in results if r is not None]\n",
    "                    save_checkpoint(completed, checkpoint_path)\n",
    "                    \n",
    "                    # Update progress in log\n",
    "                    logger.info(f\"Progress: {pbar.n}/{len(papers)} papers scored \"\n",
    "                               f\"({pbar.n/len(papers)*100:.1f}%)\")\n",
    "    \n",
    "    # Filter out None results\n",
    "    enhanced_papers = [r for r in results if r is not None]\n",
    "    \n",
    "    return enhanced_papers, stats\n",
    "\n",
    "def calculate_statistics(papers: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate statistics on the collected scores.\"\"\"\n",
    "    scores_data = []\n",
    "    for paper in papers:\n",
    "        scores = paper.get(\"quality_scores\", {})\n",
    "        if scores:\n",
    "            scores_data.append(scores)\n",
    "    \n",
    "    if not scores_data:\n",
    "        return {\"total_papers_with_scores\": 0}\n",
    "    \n",
    "    stats = {\n",
    "        \"total_papers_with_scores\": len(scores_data),\n",
    "        \"average_scores\": {},\n",
    "        \"score_distributions\": {}\n",
    "    }\n",
    "    \n",
    "    # Calculate averages\n",
    "    for key in SCORE_RANGES.keys():\n",
    "        if any(key in s for s in scores_data):\n",
    "            values = [s[key] for s in scores_data if key in s]\n",
    "            if values:\n",
    "                stats[\"average_scores\"][key] = {\n",
    "                    \"mean\": sum(values) / len(values),\n",
    "                    \"min\": min(values),\n",
    "                    \"max\": max(values),\n",
    "                    \"count\": len(values)\n",
    "                }\n",
    "    \n",
    "    # Calculate distributions for integer scores\n",
    "    for key in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\"]:\n",
    "        if key in [k for s in scores_data for k in s]:\n",
    "            values = [s[key] for s in scores_data if key in s]\n",
    "            distribution = {i: values.count(i) for i in range(11)}\n",
    "            stats[\"score_distributions\"][key] = distribution\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_file = Path(\"papers_final_aligned.jsonl\")\n",
    "    output_file = Path(\"papers_enhanced_scores.jsonl\")\n",
    "    checkpoint_file = Path(\"paper_scoring_checkpoint.jsonl\")\n",
    "    stats_file = Path(\"scoring_statistics.json\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load papers\n",
    "        papers = load_papers(input_file)\n",
    "        \n",
    "        if not papers:\n",
    "            logger.error(\"No papers loaded. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint_papers, processed_ids = load_checkpoint(checkpoint_file)\n",
    "        \n",
    "        # Determine which papers need processing\n",
    "        if processed_ids:\n",
    "            papers_to_process = []\n",
    "            for paper in papers:\n",
    "                paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                if paper_id not in processed_ids:\n",
    "                    papers_to_process.append(paper)\n",
    "            \n",
    "            logger.info(f\"Resuming: {len(checkpoint_papers)} already processed, \"\n",
    "                       f\"{len(papers_to_process)} remaining\")\n",
    "            \n",
    "            # Process remaining papers\n",
    "            enhanced_new_papers, stats = process_concurrently(\n",
    "                papers_to_process,\n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "            \n",
    "            # Combine results\n",
    "            enhanced_papers = checkpoint_papers + enhanced_new_papers\n",
    "        else:\n",
    "            # Process all papers\n",
    "            enhanced_papers, stats = process_concurrently(\n",
    "                papers,\n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "        \n",
    "        # Save final results\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for paper in enhanced_papers:\n",
    "                    f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "            logger.info(f\"Saved {len(enhanced_papers)} papers to {output_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save output: {e}\")\n",
    "            # Save to backup file\n",
    "            backup_file = Path(f\"{output_file.stem}_backup_{int(time.time())}.jsonl\")\n",
    "            with open(backup_file, 'w', encoding='utf-8') as f:\n",
    "                for paper in enhanced_papers:\n",
    "                    f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "            logger.info(f\"Saved backup to {backup_file}\")\n",
    "        \n",
    "        # Calculate and save statistics\n",
    "        scoring_stats = calculate_statistics(enhanced_papers)\n",
    "        try:\n",
    "            with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(scoring_stats, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"Saved scoring statistics to {stats_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save statistics: {e}\")\n",
    "        \n",
    "        # Print summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        hours, remainder = divmod(elapsed_time, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        \n",
    "        logger.info(\"=\" * 70)\n",
    "        logger.info(\"PAPER SCORING COMPLETE\")\n",
    "        logger.info(\"=\" * 70)\n",
    "        logger.info(f\"Total papers: {stats['total']}\")\n",
    "        logger.info(f\"Successfully processed: {stats['successful']}\")\n",
    "        logger.info(f\"Complete scores (4+ dimensions): {stats['complete_scores']}\")\n",
    "        logger.info(f\"Partial scores (1-3 dimensions): {stats['partial_scores']}\")\n",
    "        logger.info(f\"No scores: {stats['no_scores']}\")\n",
    "        logger.info(f\"Failed: {stats['failed']}\")\n",
    "        logger.info(f\"Total API requests: {request_counter}\")\n",
    "        logger.info(f\"API errors: {error_counter}\")\n",
    "        if request_counter > 0:\n",
    "            success_rate = (request_counter - error_counter) / request_counter * 100\n",
    "            logger.info(f\"API success rate: {success_rate:.1f}%\")\n",
    "        logger.info(f\"Total time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "        logger.info(f\"Processing rate: {stats['total'] / elapsed_time:.2f} papers/sec\")\n",
    "        \n",
    "        # Show score statistics if available\n",
    "        if scoring_stats[\"total_papers_with_scores\"] > 0:\n",
    "            logger.info(\"-\" * 70)\n",
    "            logger.info(\"SCORING STATISTICS:\")\n",
    "            for key, val in scoring_stats[\"average_scores\"].items():\n",
    "                logger.info(f\"  {key:20s}: {val['mean']:.2f} (min: {val['min']}, max: {val['max']})\")\n",
    "        \n",
    "        logger.info(\"=\" * 70)\n",
    "        \n",
    "        # Clean up checkpoint\n",
    "        if checkpoint_file.exists():\n",
    "            try:\n",
    "                checkpoint_file.unlink()\n",
    "                logger.info(f\"Removed checkpoint file: {checkpoint_file}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to remove checkpoint: {e}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcessing interrupted by user.\")\n",
    "        logger.info(f\"Checkpoint saved. Resume by running the script again.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b36984",
   "metadata": {},
   "source": [
    "#### Generates Structured Research Contribution Summaries\n",
    "This script (`enhance_optional_summary.py`) processes papers from a JSONL file, extracts the title and abstract, and creates a formatted summary containing four key components: problem statement, methodology, key contributions, and potential application scenarios. The enhanced data is saved to a new JSONL file with structured summaries appended to each paper record.\n",
    "\n",
    "The script employs a single-threaded approach with 1-second delays between API calls to prevent rate limiting. It uses a structured prompt template to ensure consistent JSON output format from the LLM, with temperature set to 0.25 for balanced creativity and consistency. The output includes four structured fields: problem description, methodological approach, a list of 3-5 key contributions, and potential application scenarios, all extracted automatically from paper metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance_summary.py\n",
    "\"\"\"\n",
    "Enhanced paper contribution summarization using DeepSeek API with concurrent processing.\n",
    "\n",
    "This script processes academic papers from a JSONL file, calls the DeepSeek API\n",
    "to extract structured research contributions for each paper, and saves the enhanced data\n",
    "to a new JSONL file with progress tracking, error handling, and checkpointing.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import backoff\n",
    "from threading import Lock\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('contribution_extraction.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = \"sk-79990d599cd74bc0a56f6ca2f200a621\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "REQUEST_TIMEOUT = 45  # Increased for summarization task\n",
    "MAX_WORKERS = 5  # Conservative for summarization\n",
    "BASE_DELAY = 0.7  # Base delay between requests\n",
    "MAX_RETRIES = 5\n",
    "BATCH_SIZE = 25  # Save progress every N papers\n",
    "MAX_TOKENS = 400  # Increased for longer summaries\n",
    "\n",
    "# Rate limiting and tracking\n",
    "rate_limit_lock = Lock()\n",
    "last_request_time = 0\n",
    "request_counter = 0\n",
    "error_counter = 0\n",
    "\n",
    "# Expected contribution summary structure\n",
    "CONTRIBUTION_STRUCTURE = {\n",
    "    \"problem\": str,\n",
    "    \"method\": str,\n",
    "    \"key_contributions\": list,\n",
    "    \"application_scenarios\": list\n",
    "}\n",
    "\n",
    "def rate_limited_request():\n",
    "    \"\"\"Ensure minimum delay between API requests with jitter.\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    with rate_limit_lock:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < BASE_DELAY:\n",
    "            sleep_time = BASE_DELAY - elapsed + random.uniform(0, 0.15)\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (requests.exceptions.RequestException, KeyError, json.JSONDecodeError),\n",
    "    max_tries=MAX_RETRIES,\n",
    "    max_time=300,\n",
    "    jitter=backoff.full_jitter\n",
    ")\n",
    "def call_deepseek_with_backoff(prompt: str, paper_id: str = \"unknown\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Call DeepSeek API with exponential backoff for contribution extraction.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        paper_id: Paper identifier for logging\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    global request_counter, error_counter\n",
    "    \n",
    "    rate_limited_request()\n",
    "    request_counter += 1\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"deepseek-chat\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0.25,\n",
    "                \"max_tokens\": MAX_TOKENS\n",
    "            },\n",
    "            timeout=REQUEST_TIMEOUT\n",
    "        )\n",
    "        \n",
    "        # Handle rate limiting\n",
    "        if response.status_code == 429:\n",
    "            retry_after = int(response.headers.get('Retry-After', 75))\n",
    "            logger.warning(f\"Rate limited for paper {paper_id}. Waiting {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Validate response structure\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"Response is not a dictionary: {type(data)}\")\n",
    "        \n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            raise ValueError(f\"No choices in response: {data}\")\n",
    "        \n",
    "        message = choices[0].get(\"message\", {})\n",
    "        content = message.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            raise ValueError(\"Empty response content\")\n",
    "        \n",
    "        # Log progress periodically\n",
    "        if request_counter % 40 == 0:\n",
    "            logger.info(f\"Completed {request_counter} summarization requests\")\n",
    "        \n",
    "        return content\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"Timeout for paper {paper_id}\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        error_counter += 1\n",
    "        status_code = e.response.status_code if e.response else \"unknown\"\n",
    "        logger.error(f\"HTTP error {status_code} for paper {paper_id}\")\n",
    "        if status_code >= 500:\n",
    "            time.sleep(12)  # Longer sleep for server errors\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        error_counter += 1\n",
    "        logger.error(f\"API error for paper {paper_id}: {str(e)[:100]}\")\n",
    "        raise\n",
    "\n",
    "def call_deepseek(prompt: str, paper_id: str = \"unknown\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Wrapper for API call with fallback handling.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        paper_id: Paper identifier for logging\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if all retries fail\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return call_deepseek_with_backoff(prompt, paper_id)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"All retries failed for paper {paper_id}: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "def validate_contribution_summary(summary: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate and clean the contribution summary structure.\n",
    "    \n",
    "    Args:\n",
    "        summary: Raw summary dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Validated and cleaned summary dictionary\n",
    "    \"\"\"\n",
    "    if not isinstance(summary, dict):\n",
    "        return {}\n",
    "    \n",
    "    validated = {}\n",
    "    \n",
    "    # Validate each expected field\n",
    "    for field, expected_type in CONTRIBUTION_STRUCTURE.items():\n",
    "        if field in summary:\n",
    "            value = summary[field]\n",
    "            \n",
    "            # Convert to appropriate type\n",
    "            if expected_type == str:\n",
    "                if isinstance(value, str):\n",
    "                    validated[field] = value.strip()\n",
    "                elif value is not None:\n",
    "                    validated[field] = str(value).strip()\n",
    "                else:\n",
    "                    validated[field] = \"\"\n",
    "            \n",
    "            elif expected_type == list:\n",
    "                if isinstance(value, list):\n",
    "                    # Clean list items\n",
    "                    cleaned_list = []\n",
    "                    for item in value:\n",
    "                        if isinstance(item, str):\n",
    "                            cleaned = item.strip()\n",
    "                            if cleaned:\n",
    "                                cleaned_list.append(cleaned)\n",
    "                        elif item is not None:\n",
    "                            cleaned = str(item).strip()\n",
    "                            if cleaned:\n",
    "                                cleaned_list.append(cleaned)\n",
    "                    validated[field] = cleaned_list\n",
    "                else:\n",
    "                    validated[field] = []\n",
    "        \n",
    "        else:\n",
    "            # Set default values for missing fields\n",
    "            if expected_type == str:\n",
    "                validated[field] = \"\"\n",
    "            else:\n",
    "                validated[field] = []\n",
    "    \n",
    "    # Additional validation\n",
    "    if validated.get(\"problem\") and len(validated[\"problem\"]) > 500:\n",
    "        validated[\"problem\"] = validated[\"problem\"][:500] + \"...\"\n",
    "    \n",
    "    if validated.get(\"method\") and len(validated[\"method\"]) > 500:\n",
    "        validated[\"method\"] = validated[\"method\"][:500] + \"...\"\n",
    "    \n",
    "    # Limit list lengths\n",
    "    for list_field in [\"key_contributions\", \"application_scenarios\"]:\n",
    "        if list_field in validated:\n",
    "            validated[list_field] = validated[list_field][:10]  # Max 10 items\n",
    "    \n",
    "    return validated\n",
    "\n",
    "def extract_contribution_summary(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured contribution summary from paper metadata using DeepSeek API.\n",
    "    \n",
    "    Args:\n",
    "        paper: Dictionary containing paper metadata\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with original paper data plus contribution summary\n",
    "    \"\"\"\n",
    "    paper_id = paper.get(\"paper_id\", paper.get(\"id\", \"unknown\"))\n",
    "    \n",
    "    # Prepare content\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(abstract) > 3500:\n",
    "        abstract = abstract[:3500] + \"...\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert research analyst specializing in academic paper analysis.\n",
    "\n",
    "Summarize the paper into structured research contributions based on the title and abstract.\n",
    "\n",
    "IMPORTANT: Return ONLY a valid JSON object with the following keys:\n",
    "- \"problem\": A concise description of the research problem addressed (1-2 sentences)\n",
    "- \"method\": A brief overview of the methodology or approach used (1-2 sentences)\n",
    "- \"key_contributions\": A list of 3-5 key contributions or innovations\n",
    "- \"application_scenarios\": A list of 2-4 potential application scenarios\n",
    "\n",
    "Guidelines:\n",
    "- Be specific and technical, not generic\n",
    "- Focus on what is novel or significant\n",
    "- For contributions: use bullet-point style but return as JSON list\n",
    "- For application scenarios: be concrete about where this research could be applied\n",
    "- Keep each contribution and scenario item concise (1 sentence each)\n",
    "\n",
    "Title:\n",
    "{title}\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\"\"\"\n",
    "    summary = {}\n",
    "    \n",
    "    try:\n",
    "        api_response = call_deepseek(prompt, paper_id)\n",
    "        \n",
    "        if api_response is None:\n",
    "            logger.debug(f\"No API response for paper {paper_id}\")\n",
    "        else:\n",
    "            # Clean response - remove markdown code blocks and whitespace\n",
    "            clean_response = api_response.strip()\n",
    "            clean_response = re.sub(r'^```(?:json)?\\s*', '', clean_response, flags=re.IGNORECASE)\n",
    "            clean_response = re.sub(r'\\s*```$', '', clean_response)\n",
    "            \n",
    "            # Try to parse as JSON\n",
    "            try:\n",
    "                parsed = json.loads(clean_response)\n",
    "                summary = validate_contribution_summary(parsed)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.debug(f\"Failed to parse JSON for paper {paper_id}: {str(e)[:100]}\")\n",
    "                \n",
    "                # Advanced JSON extraction\n",
    "                json_patterns = [\n",
    "                    r'\\{[^{}]*\\}',  # Simple object\n",
    "                    r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}',  # Nested object\n",
    "                ]\n",
    "                \n",
    "                for pattern in json_patterns:\n",
    "                    matches = re.finditer(pattern, clean_response, re.DOTALL)\n",
    "                    for match in matches:\n",
    "                        try:\n",
    "                            parsed = json.loads(match.group(0))\n",
    "                            if isinstance(parsed, dict):\n",
    "                                temp_summary = validate_contribution_summary(parsed)\n",
    "                                # Check if we have at least some valid structure\n",
    "                                if (temp_summary.get(\"problem\") or \n",
    "                                    temp_summary.get(\"method\") or \n",
    "                                    temp_summary.get(\"key_contributions\")):\n",
    "                                    summary = temp_summary\n",
    "                                    break\n",
    "                        except:\n",
    "                            continue\n",
    "                    if summary:\n",
    "                        break\n",
    "                \n",
    "                # Fallback: extract structured information from text\n",
    "                if not summary:\n",
    "                    logger.debug(f\"Using advanced text extraction for paper {paper_id}\")\n",
    "                    summary = extract_summary_from_text(clean_response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error summarizing paper {paper_id}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Final validation\n",
    "    summary = validate_contribution_summary(summary)\n",
    "    \n",
    "    # Create enhanced paper\n",
    "    enhanced_paper = paper.copy()\n",
    "    enhanced_paper[\"contribution_summary\"] = summary\n",
    "    enhanced_paper[\"summary_extraction_time\"] = datetime.now().isoformat()\n",
    "    \n",
    "    return enhanced_paper\n",
    "\n",
    "def extract_summary_from_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured summary from text response when JSON parsing fails.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text response from API\n",
    "    \n",
    "    Returns:\n",
    "        Structured summary dictionary\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        \"problem\": \"\",\n",
    "        \"method\": \"\",\n",
    "        \"key_contributions\": [],\n",
    "        \"application_scenarios\": []\n",
    "    }\n",
    "    \n",
    "    # Extract problem and method using keyword patterns\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Look for problem description\n",
    "    problem_patterns = [\n",
    "        r'[Pp]roblem[:：]?\\s*(.+)',\n",
    "        r'[Rr]esearch\\s+[Pp]roblem[:：]?\\s*(.+)',\n",
    "        r'[Tt]he\\s+problem\\s+is\\s+(.+)',\n",
    "    ]\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Check for problem\n",
    "        for pattern in problem_patterns:\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                summary[\"problem\"] = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        # Check for method\n",
    "        method_patterns = [\n",
    "            r'[Mm]ethod(?:ology)?[:：]?\\s*(.+)',\n",
    "            r'[Aa]pproach[:：]?\\s*(.+)',\n",
    "            r'[Tt]echnique[:：]?\\s*(.+)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in method_patterns:\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                summary[\"method\"] = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        # Look for contributions (bullet points, numbered lists, or after headings)\n",
    "        if re.match(r'^[Cc]ontributions?[:：]?', line) or re.match(r'^[Kk]ey\\s+[Cc]ontributions?[:：]?', line):\n",
    "            # Collect next lines as contributions\n",
    "            for j in range(i + 1, min(i + 10, len(lines))):\n",
    "                next_line = lines[j].strip()\n",
    "                if next_line and not re.match(r'^[A-Za-z\\s]+[:：]?$', next_line):\n",
    "                    # Clean bullet points or numbers\n",
    "                    clean_line = re.sub(r'^[•\\-\\*\\d\\.\\s]+', '', next_line)\n",
    "                    if clean_line and len(clean_line) > 5:\n",
    "                        summary[\"key_contributions\"].append(clean_line)\n",
    "            \n",
    "            # Limit to 5 contributions\n",
    "            summary[\"key_contributions\"] = summary[\"key_contributions\"][:5]\n",
    "        \n",
    "        # Look for applications\n",
    "        if re.match(r'^[Aa]pplications?[:：]?', line) or re.match(r'^[Uu]ses?[:：]?', line):\n",
    "            # Collect next lines as applications\n",
    "            for j in range(i + 1, min(i + 8, len(lines))):\n",
    "                next_line = lines[j].strip()\n",
    "                if next_line and not re.match(r'^[A-Za-z\\s]+[:：]?$', next_line):\n",
    "                    clean_line = re.sub(r'^[•\\-\\*\\d\\.\\s]+', '', next_line)\n",
    "                    if clean_line and len(clean_line) > 5:\n",
    "                        summary[\"application_scenarios\"].append(clean_line)\n",
    "            \n",
    "            # Limit to 4 applications\n",
    "            summary[\"application_scenarios\"] = summary[\"application_scenarios\"][:4]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def load_papers(input_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load papers from JSONL file.\"\"\"\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    papers.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Invalid JSON on line {line_num}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(papers)} papers from {input_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input file not found: {input_path}\")\n",
    "        raise\n",
    "    \n",
    "    return papers\n",
    "\n",
    "def save_checkpoint(papers: List[Dict[str, Any]], checkpoint_path: Path):\n",
    "    \"\"\"Save intermediate results to checkpoint file.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "            for paper in papers:\n",
    "                f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        logger.debug(f\"Checkpoint saved: {len(papers)} papers\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path: Path) -> Tuple[List[Dict[str, Any]], set]:\n",
    "    \"\"\"Load papers from checkpoint file and get processed paper IDs.\"\"\"\n",
    "    if not checkpoint_path.exists():\n",
    "        return [], set()\n",
    "    \n",
    "    papers = []\n",
    "    processed_ids = set()\n",
    "    \n",
    "    try:\n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers.append(paper)\n",
    "                    paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                    if paper_id:\n",
    "                        processed_ids.add(paper_id)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        logger.info(f\"Loaded {len(papers)} papers from checkpoint\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load checkpoint: {e}\")\n",
    "    \n",
    "    return papers, processed_ids\n",
    "\n",
    "def process_concurrently(papers: List[Dict[str, Any]], \n",
    "                        max_workers: int = MAX_WORKERS,\n",
    "                        checkpoint_path: Optional[Path] = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process papers concurrently using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        papers: List of papers to process\n",
    "        max_workers: Maximum concurrent threads\n",
    "        checkpoint_path: Optional checkpoint file path\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (enhanced papers, statistics)\n",
    "    \"\"\"\n",
    "    enhanced_papers = []\n",
    "    stats = {\n",
    "        \"total\": len(papers),\n",
    "        \"successful\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"complete_summaries\": 0,\n",
    "        \"partial_summaries\": 0,\n",
    "        \"empty_summaries\": 0\n",
    "    }\n",
    "    \n",
    "    # Adjust workers for summarization task\n",
    "    effective_workers = min(max_workers, 6)\n",
    "    logger.info(f\"Summarizing {len(papers)} papers with {effective_workers} workers\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=effective_workers) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_index = {\n",
    "            executor.submit(extract_contribution_summary, paper): i \n",
    "            for i, paper in enumerate(papers)\n",
    "        }\n",
    "        \n",
    "        # Track completion order for proper ordering\n",
    "        results = [None] * len(papers)\n",
    "        \n",
    "        # Process with progress bar\n",
    "        with tqdm(total=len(papers), desc=\"Extracting contributions\", unit=\"paper\") as pbar:\n",
    "            for future in as_completed(future_to_index):\n",
    "                idx = future_to_index[future]\n",
    "                \n",
    "                try:\n",
    "                    enhanced_paper = future.result(timeout=REQUEST_TIMEOUT + 50)\n",
    "                    results[idx] = enhanced_paper\n",
    "                    \n",
    "                    summary = enhanced_paper.get(\"contribution_summary\", {})\n",
    "                    \n",
    "                    if summary:\n",
    "                        # Check completeness\n",
    "                        has_problem = bool(summary.get(\"problem\"))\n",
    "                        has_method = bool(summary.get(\"method\"))\n",
    "                        has_contributions = bool(summary.get(\"key_contributions\"))\n",
    "                        has_applications = bool(summary.get(\"application_scenarios\"))\n",
    "                        \n",
    "                        complete_fields = sum([has_problem, has_method, has_contributions, has_applications])\n",
    "                        \n",
    "                        if complete_fields >= 3:\n",
    "                            stats[\"complete_summaries\"] += 1\n",
    "                            stats[\"successful\"] += 1\n",
    "                        elif complete_fields >= 1:\n",
    "                            stats[\"partial_summaries\"] += 1\n",
    "                            stats[\"successful\"] += 1\n",
    "                        else:\n",
    "                            stats[\"empty_summaries\"] += 1\n",
    "                            stats[\"failed\"] += 1\n",
    "                    else:\n",
    "                        stats[\"empty_summaries\"] += 1\n",
    "                        stats[\"failed\"] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Processing failed for paper at index {idx}: {str(e)[:100]}\")\n",
    "                    stats[\"failed\"] += 1\n",
    "                    # Add fallback\n",
    "                    fallback_paper = papers[idx].copy()\n",
    "                    fallback_paper[\"contribution_summary\"] = {}\n",
    "                    results[idx] = fallback_paper\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Periodic checkpoint\n",
    "                if checkpoint_path and pbar.n % BATCH_SIZE == 0:\n",
    "                    # Collect completed results\n",
    "                    completed = [r for r in results if r is not None]\n",
    "                    save_checkpoint(completed, checkpoint_path)\n",
    "                    \n",
    "                    # Update progress in log\n",
    "                    progress_pct = pbar.n / len(papers) * 100\n",
    "                    logger.info(f\"Progress: {pbar.n}/{len(papers)} papers summarized \"\n",
    "                               f\"({progress_pct:.1f}%)\")\n",
    "    \n",
    "    # Filter out None results\n",
    "    enhanced_papers = [r for r in results if r is not None]\n",
    "    \n",
    "    return enhanced_papers, stats\n",
    "\n",
    "def calculate_summary_statistics(papers: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate statistics on the collected summaries.\"\"\"\n",
    "    summaries = [p.get(\"contribution_summary\", {}) for p in papers]\n",
    "    \n",
    "    stats = {\n",
    "        \"total_papers\": len(papers),\n",
    "        \"papers_with_summary\": 0,\n",
    "        \"field_completeness\": {},\n",
    "        \"average_lengths\": {},\n",
    "        \"summary_quality\": {}\n",
    "    }\n",
    "    \n",
    "    valid_summaries = [s for s in summaries if s]\n",
    "    stats[\"papers_with_summary\"] = len(valid_summaries)\n",
    "    \n",
    "    if not valid_summaries:\n",
    "        return stats\n",
    "    \n",
    "    # Field completeness\n",
    "    for field in CONTRIBUTION_STRUCTURE.keys():\n",
    "        has_field = sum(1 for s in valid_summaries if s.get(field))\n",
    "        stats[\"field_completeness\"][field] = {\n",
    "            \"count\": has_field,\n",
    "            \"percentage\": has_field / len(valid_summaries) * 100\n",
    "        }\n",
    "    \n",
    "    # Average lengths\n",
    "    for field in [\"problem\", \"method\"]:\n",
    "        lengths = [len(s.get(field, \"\")) for s in valid_summaries if s.get(field)]\n",
    "        if lengths:\n",
    "            stats[\"average_lengths\"][field] = {\n",
    "                \"mean\": sum(lengths) / len(lengths),\n",
    "                \"min\": min(lengths),\n",
    "                \"max\": max(lengths)\n",
    "            }\n",
    "    \n",
    "    for list_field in [\"key_contributions\", \"application_scenarios\"]:\n",
    "        counts = [len(s.get(list_field, [])) for s in valid_summaries]\n",
    "        if counts:\n",
    "            stats[\"average_lengths\"][list_field] = {\n",
    "                \"mean\": sum(counts) / len(counts),\n",
    "                \"min\": min(counts),\n",
    "                \"max\": max(counts)\n",
    "            }\n",
    "    \n",
    "    # Summary quality scoring\n",
    "    quality_scores = []\n",
    "    for summary in valid_summaries:\n",
    "        score = 0\n",
    "        if summary.get(\"problem\"):\n",
    "            score += 1\n",
    "        if summary.get(\"method\"):\n",
    "            score += 1\n",
    "        if summary.get(\"key_contributions\"):\n",
    "            score += 1\n",
    "        if summary.get(\"application_scenarios\"):\n",
    "            score += 1\n",
    "        quality_scores.append(score)\n",
    "    \n",
    "    if quality_scores:\n",
    "        stats[\"summary_quality\"] = {\n",
    "            \"average_score\": sum(quality_scores) / len(quality_scores),\n",
    "            \"distribution\": {\n",
    "                \"0_fields\": quality_scores.count(0),\n",
    "                \"1_field\": quality_scores.count(1),\n",
    "                \"2_fields\": quality_scores.count(2),\n",
    "                \"3_fields\": quality_scores.count(3),\n",
    "                \"4_fields\": quality_scores.count(4)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_file = Path(\"papers_final_aligned.jsonl\")\n",
    "    output_file = Path(\"papers_enhanced_contributions.jsonl\")\n",
    "    checkpoint_file = Path(\"contribution_summary_checkpoint.jsonl\")\n",
    "    stats_file = Path(\"contribution_statistics.json\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load papers\n",
    "        papers = load_papers(input_file)\n",
    "        \n",
    "        if not papers:\n",
    "            logger.error(\"No papers loaded. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint_papers, processed_ids = load_checkpoint(checkpoint_file)\n",
    "        \n",
    "        # Determine which papers need processing\n",
    "        if processed_ids:\n",
    "            papers_to_process = []\n",
    "            for paper in papers:\n",
    "                paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                if paper_id not in processed_ids:\n",
    "                    papers_to_process.append(paper)\n",
    "            \n",
    "            logger.info(f\"Resuming: {len(checkpoint_papers)} already processed, \"\n",
    "                       f\"{len(papers_to_process)} remaining\")\n",
    "            \n",
    "            # Process remaining papers\n",
    "            enhanced_new_papers, stats = process_concurrently(\n",
    "                papers_to_process,\n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "            \n",
    "            # Combine results\n",
    "            enhanced_papers = checkpoint_papers + enhanced_new_papers\n",
    "        else:\n",
    "            # Process all papers\n",
    "            enhanced_papers, stats = process_concurrently(\n",
    "                papers,\n",
    "                checkpoint_path=checkpoint_file\n",
    "            )\n",
    "        \n",
    "        # Save final results\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for paper in enhanced_papers:\n",
    "                    f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "            logger.info(f\"Saved {len(enhanced_papers)} papers to {output_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save output: {e}\")\n",
    "            # Save to backup file\n",
    "            backup_file = Path(f\"{output_file.stem}_backup_{int(time.time())}.jsonl\")\n",
    "            with open(backup_file, 'w', encoding='utf-8') as f:\n",
    "                for paper in enhanced_papers:\n",
    "                    f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "            logger.info(f\"Saved backup to {backup_file}\")\n",
    "        \n",
    "        # Calculate and save statistics\n",
    "        summary_stats = calculate_summary_statistics(enhanced_papers)\n",
    "        try:\n",
    "            with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(summary_stats, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"Saved contribution statistics to {stats_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save statistics: {e}\")\n",
    "        \n",
    "        # Print summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        hours, remainder = divmod(elapsed_time, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        \n",
    "        logger.info(\"=\" * 70)\n",
    "        logger.info(\"CONTRIBUTION SUMMARIZATION COMPLETE\")\n",
    "        logger.info(\"=\" * 70)\n",
    "        logger.info(f\"Total papers: {stats['total']}\")\n",
    "        logger.info(f\"Successfully processed: {stats['successful']}\")\n",
    "        logger.info(f\"Complete summaries (3+ fields): {stats['complete_summaries']}\")\n",
    "        logger.info(f\"Partial summaries (1-2 fields): {stats['partial_summaries']}\")\n",
    "        logger.info(f\"Empty summaries: {stats['empty_summaries']}\")\n",
    "        logger.info(f\"Failed: {stats['failed']}\")\n",
    "        logger.info(f\"Total API requests: {request_counter}\")\n",
    "        logger.info(f\"API errors: {error_counter}\")\n",
    "        if request_counter > 0:\n",
    "            success_rate = (request_counter - error_counter) / request_counter * 100\n",
    "            logger.info(f\"API success rate: {success_rate:.1f}%\")\n",
    "        logger.info(f\"Total time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "        logger.info(f\"Processing rate: {stats['total'] / elapsed_time:.2f} papers/sec\")\n",
    "        \n",
    "        # Show summary statistics if available\n",
    "        if summary_stats[\"papers_with_summary\"] > 0:\n",
    "            logger.info(\"-\" * 70)\n",
    "            logger.info(\"SUMMARY STATISTICS:\")\n",
    "            logger.info(f\"Papers with summary: {summary_stats['papers_with_summary']} \"\n",
    "                       f\"({summary_stats['papers_with_summary']/summary_stats['total_papers']*100:.1f}%)\")\n",
    "            \n",
    "            for field, completeness in summary_stats[\"field_completeness\"].items():\n",
    "                logger.info(f\"  {field:20s}: {completeness['count']:4d} papers \"\n",
    "                           f\"({completeness['percentage']:.1f}%)\")\n",
    "            \n",
    "            if summary_stats[\"summary_quality\"]:\n",
    "                quality = summary_stats[\"summary_quality\"]\n",
    "                logger.info(f\"  Average quality score: {quality['average_score']:.1f}/4.0\")\n",
    "        \n",
    "        logger.info(\"=\" * 70)\n",
    "        \n",
    "        # Clean up checkpoint\n",
    "        if checkpoint_file.exists():\n",
    "            try:\n",
    "                checkpoint_file.unlink()\n",
    "                logger.info(f\"Removed checkpoint file: {checkpoint_file}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to remove checkpoint: {e}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcessing interrupted by user.\")\n",
    "        logger.info(f\"Checkpoint saved. Resume by running the script again.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50551023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance_optional_summary.py\n",
    "\"\"\"\n",
    "Enhanced paper contribution summarization with optimized concurrent processing.\n",
    "\n",
    "This script processes academic papers from a JSONL file, calls the DeepSeek API\n",
    "to extract structured research contributions using concurrent processing with\n",
    "adaptive rate limiting and request batching for maximum throughput.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, Future\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Lock, Semaphore\n",
    "import re\n",
    "import queue\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('contribution_extraction_optimized.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = \"sk-79990d599cd74bc0a56f6ca2f200a621\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "REQUEST_TIMEOUT = 35\n",
    "MAX_WORKERS = 12  # Increased for higher concurrency\n",
    "MIN_DELAY = 0.15  # Minimum delay between requests (optimized)\n",
    "MAX_DELAY = 2.0   # Maximum delay for backoff\n",
    "MAX_RETRIES = 4\n",
    "BATCH_SIZE = 50\n",
    "MAX_TOKENS = 350  # Optimized for faster responses\n",
    "\n",
    "# Adaptive rate limiting\n",
    "@dataclass\n",
    "class RateLimiter:\n",
    "    \"\"\"Adaptive rate limiter that adjusts based on success/failure rates.\"\"\"\n",
    "    min_delay: float = MIN_DELAY\n",
    "    max_delay: float = MAX_DELAY\n",
    "    current_delay: float = MIN_DELAY\n",
    "    success_window: deque = field(default_factory=lambda: deque(maxlen=50))\n",
    "    lock: Lock = field(default_factory=Lock)\n",
    "    \n",
    "    def success(self):\n",
    "        \"\"\"Record a successful request.\"\"\"\n",
    "        with self.lock:\n",
    "            self.success_window.append(True)\n",
    "            # If last 10 requests had 90% success, decrease delay slightly\n",
    "            if len(self.success_window) >= 10:\n",
    "                success_rate = sum(1 for s in list(self.success_window)[-10:] if s) / 10\n",
    "                if success_rate > 0.9 and self.current_delay > self.min_delay:\n",
    "                    self.current_delay = max(self.min_delay, self.current_delay * 0.9)\n",
    "    \n",
    "    def failure(self):\n",
    "        \"\"\"Record a failed request.\"\"\"\n",
    "        with self.lock:\n",
    "            self.success_window.append(False)\n",
    "            # Increase delay on failure\n",
    "            self.current_delay = min(self.max_delay, self.current_delay * 1.5)\n",
    "    \n",
    "    def get_delay(self) -> float:\n",
    "        \"\"\"Get current delay with jitter.\"\"\"\n",
    "        with self.lock:\n",
    "            jitter = random.uniform(-0.05, 0.05) * self.current_delay\n",
    "            return max(self.min_delay, self.current_delay + jitter)\n",
    "\n",
    "# Global rate limiter and statistics\n",
    "rate_limiter = RateLimiter()\n",
    "request_counter = 0\n",
    "error_counter = 0\n",
    "last_request_time = 0\n",
    "rate_lock = Lock()\n",
    "\n",
    "# Expected contribution summary structure\n",
    "CONTRIBUTION_STRUCTURE = {\n",
    "    \"problem\": str,\n",
    "    \"method\": str,\n",
    "    \"key_contributions\": list,\n",
    "    \"application_scenarios\": list\n",
    "}\n",
    "\n",
    "def adaptive_rate_limit():\n",
    "    \"\"\"Adaptive rate limiting with dynamic adjustment.\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    with rate_lock:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        delay_needed = rate_limiter.get_delay()\n",
    "        \n",
    "        if elapsed < delay_needed:\n",
    "            sleep_time = delay_needed - elapsed\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        last_request_time = time.time()\n",
    "        return delay_needed\n",
    "\n",
    "def call_deepseek_fast(prompt: str, paper_id: str = \"unknown\", timeout: int = REQUEST_TIMEOUT) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Optimized API call with adaptive rate limiting and minimal overhead.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the API\n",
    "        paper_id: Paper identifier for logging\n",
    "        timeout: Request timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "        API response content as string, or None if failed\n",
    "    \"\"\"\n",
    "    global request_counter, error_counter\n",
    "    \n",
    "    # Apply adaptive rate limiting\n",
    "    adaptive_rate_limit()\n",
    "    request_counter += 1\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.25,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                API_URL,\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                # Rate limited - use exponential backoff\n",
    "                wait_time = min(30, 2 ** attempt + random.uniform(0, 1))\n",
    "                logger.warning(f\"Rate limited for paper {paper_id}, waiting {wait_time:.1f}s\")\n",
    "                time.sleep(wait_time)\n",
    "                rate_limiter.failure()\n",
    "                continue\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Fast JSON parsing\n",
    "            data = response.json()\n",
    "            content = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "            \n",
    "            if not content:\n",
    "                raise ValueError(\"Empty response content\")\n",
    "            \n",
    "            # Record success\n",
    "            rate_limiter.success()\n",
    "            \n",
    "            # Return cleaned response\n",
    "            return content.strip()\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.debug(f\"Timeout for paper {paper_id}, attempt {attempt + 1}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(1 << attempt)  # Exponential backoff\n",
    "            else:\n",
    "                error_counter += 1\n",
    "                rate_limiter.failure()\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code >= 500:\n",
    "                logger.debug(f\"Server error for paper {paper_id}: {e.response.status_code}\")\n",
    "                time.sleep(3 * (attempt + 1))\n",
    "            else:\n",
    "                logger.debug(f\"HTTP error for paper {paper_id}: {e.response.status_code}\")\n",
    "                break  # Client error, don't retry immediately\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                error_counter += 1\n",
    "                rate_limiter.failure()\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Request error for paper {paper_id}: {str(e)[:100]}\")\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                error_counter += 1\n",
    "                rate_limiter.failure()\n",
    "                return None\n",
    "            time.sleep(0.5 * (attempt + 1))\n",
    "    \n",
    "    error_counter += 1\n",
    "    rate_limiter.failure()\n",
    "    return None\n",
    "\n",
    "def validate_contribution_summary(summary: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Fast validation and cleaning of contribution summary.\"\"\"\n",
    "    if not isinstance(summary, dict):\n",
    "        return {}\n",
    "    \n",
    "    validated = {}\n",
    "    \n",
    "    for field, expected_type in CONTRIBUTION_STRUCTURE.items():\n",
    "        value = summary.get(field)\n",
    "        \n",
    "        if expected_type == str:\n",
    "            if isinstance(value, str):\n",
    "                validated[field] = value.strip()[:300]  # Limit length\n",
    "            elif value is not None:\n",
    "                validated[field] = str(value).strip()[:300]\n",
    "            else:\n",
    "                validated[field] = \"\"\n",
    "        \n",
    "        elif expected_type == list:\n",
    "            if isinstance(value, list):\n",
    "                # Fast cleaning of list items\n",
    "                cleaned = []\n",
    "                for item in value:\n",
    "                    if isinstance(item, str) and item.strip():\n",
    "                        cleaned.append(item.strip()[:200])\n",
    "                    elif item is not None:\n",
    "                        cleaned.append(str(item).strip()[:200])\n",
    "                    if len(cleaned) >= 8:  # Limit to 8 items\n",
    "                        break\n",
    "                validated[field] = cleaned\n",
    "            else:\n",
    "                validated[field] = []\n",
    "    \n",
    "    return validated\n",
    "\n",
    "def extract_contribution_summary_fast(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fast extraction of contribution summary with optimized prompt.\n",
    "    \n",
    "    Args:\n",
    "        paper: Dictionary containing paper metadata\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with original paper data plus contribution summary\n",
    "    \"\"\"\n",
    "    paper_id = paper.get(\"paper_id\", paper.get(\"id\", \"unknown\"))\n",
    "    \n",
    "    # Prepare content with length limits\n",
    "    title = paper.get(\"title\", \"\").strip()[:200]\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()[:1500]\n",
    "    \n",
    "    # Optimized prompt for speed\n",
    "    prompt = f\"\"\"Summarize this paper into JSON:\n",
    "\n",
    "Title: {title}\n",
    "Abstract: {abstract}\n",
    "\n",
    "Return JSON with:\n",
    "- \"problem\": research problem (1 sentence)\n",
    "- \"method\": approach used (1 sentence)\n",
    "- \"key_contributions\": list of 2-4 contributions\n",
    "- \"application_scenarios\": list of 1-3 applications\n",
    "\n",
    "JSON ONLY:\"\"\"\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    try:\n",
    "        api_response = call_deepseek_fast(prompt, paper_id, timeout=25)\n",
    "        \n",
    "        if api_response:\n",
    "            # Fast JSON extraction with fallback\n",
    "            try:\n",
    "                parsed = json.loads(api_response)\n",
    "                summary = validate_contribution_summary(parsed)\n",
    "            except json.JSONDecodeError:\n",
    "                # Quick fallback: extract JSON-like structure\n",
    "                json_match = re.search(r'\\{[^{}]*\\}', api_response, re.DOTALL)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        parsed = json.loads(json_match.group(0))\n",
    "                        summary = validate_contribution_summary(parsed)\n",
    "                    except:\n",
    "                        # Minimal text extraction\n",
    "                        lines = [l.strip() for l in api_response.split('\\n') if l.strip()]\n",
    "                        if lines:\n",
    "                            summary = {\n",
    "                                \"problem\": lines[0][:200] if len(lines) > 0 else \"\",\n",
    "                                \"method\": lines[1][:200] if len(lines) > 1 else \"\",\n",
    "                                \"key_contributions\": lines[2:4] if len(lines) > 2 else [],\n",
    "                                \"application_scenarios\": lines[4:6] if len(lines) > 4 else []\n",
    "                            }\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error for paper {paper_id}: {str(e)[:80]}\")\n",
    "    \n",
    "    # Create enhanced paper\n",
    "    enhanced_paper = paper.copy()\n",
    "    enhanced_paper[\"contribution_summary\"] = summary\n",
    "    \n",
    "    return enhanced_paper\n",
    "\n",
    "class BatchProcessor:\n",
    "    \"\"\"Batch processor for high-throughput concurrent processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_workers: int = MAX_WORKERS, batch_size: int = BATCH_SIZE):\n",
    "        self.max_workers = max_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.semaphore = Semaphore(max_workers * 2)  # Limit concurrent futures\n",
    "        self.results_lock = Lock()\n",
    "        \n",
    "    def process_batch(self, papers: List[Dict[str, Any]], \n",
    "                     desc: str = \"Processing\") -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a batch of papers with optimized concurrency.\n",
    "        \n",
    "        Args:\n",
    "            papers: List of papers to process\n",
    "            desc: Description for progress bar\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (processed papers, statistics)\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            \"total\": len(papers),\n",
    "            \"successful\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"with_summary\": 0\n",
    "        }\n",
    "        \n",
    "        # Prepare results array\n",
    "        results = [None] * len(papers)\n",
    "        \n",
    "        # Use ThreadPoolExecutor with optimized settings\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers, \n",
    "                              thread_name_prefix=\"api_worker\") as executor:\n",
    "            \n",
    "            # Submit all tasks\n",
    "            futures = {}\n",
    "            for idx, paper in enumerate(papers):\n",
    "                with self.semaphore:\n",
    "                    future = executor.submit(extract_contribution_summary_fast, paper)\n",
    "                    futures[future] = idx\n",
    "            \n",
    "            # Process completed futures with progress bar\n",
    "            completed = 0\n",
    "            with tqdm(total=len(papers), desc=desc, unit=\"paper\", \n",
    "                     mininterval=0.5, maxinterval=1.0) as pbar:\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    idx = futures[future]\n",
    "                    \n",
    "                    try:\n",
    "                        result = future.result(timeout=30)\n",
    "                        results[idx] = result\n",
    "                        \n",
    "                        # Update statistics\n",
    "                        summary = result.get(\"contribution_summary\", {})\n",
    "                        if summary:\n",
    "                            stats[\"with_summary\"] += 1\n",
    "                            stats[\"successful\"] += 1\n",
    "                        else:\n",
    "                            stats[\"failed\"] += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Batch processing error at index {idx}: {str(e)[:80]}\")\n",
    "                        stats[\"failed\"] += 1\n",
    "                        # Add fallback\n",
    "                        fallback = papers[idx].copy()\n",
    "                        fallback[\"contribution_summary\"] = {}\n",
    "                        results[idx] = fallback\n",
    "                    \n",
    "                    completed += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    # Update progress periodically\n",
    "                    if completed % 20 == 0:\n",
    "                        pbar.set_postfix({\n",
    "                            \"rate\": f\"{completed/(time.time()-pbar.start_t):.1f}/s\",\n",
    "                            \"delay\": f\"{rate_limiter.current_delay:.2f}s\"\n",
    "                        })\n",
    "        \n",
    "        # Filter out None results\n",
    "        processed_papers = [r for r in results if r is not None]\n",
    "        \n",
    "        return processed_papers, stats\n",
    "\n",
    "def load_papers_fast(input_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fast loading of papers from JSONL file.\"\"\"\n",
    "    papers = []\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    papers.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "        logger.info(f\"Loaded {len(papers)} papers from {input_path}\")\n",
    "        return papers\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input file not found: {input_path}\")\n",
    "        raise\n",
    "\n",
    "def save_checkpoint_fast(papers: List[Dict[str, Any]], checkpoint_path: Path):\n",
    "    \"\"\"Fast checkpoint saving.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "            for paper in papers:\n",
    "                f.write(json.dumps(paper, ensure_ascii=False) + '\\n')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Checkpoint save error: {e}\")\n",
    "\n",
    "def load_checkpoint_fast(checkpoint_path: Path) -> Tuple[List[Dict[str, Any]], set]:\n",
    "    \"\"\"Fast checkpoint loading.\"\"\"\n",
    "    if not checkpoint_path.exists():\n",
    "        return [], set()\n",
    "    \n",
    "    papers = []\n",
    "    processed_ids = set()\n",
    "    \n",
    "    try:\n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers.append(paper)\n",
    "                    paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                    if paper_id:\n",
    "                        processed_ids.add(paper_id)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "        logger.info(f\"Loaded {len(papers)} papers from checkpoint\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Checkpoint load error: {e}\")\n",
    "    \n",
    "    return papers, processed_ids\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function with optimized processing.\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_file = Path(\"papers_final_aligned.jsonl\")\n",
    "    output_file = Path(\"papers_enhanced_contributions_fast.jsonl\")\n",
    "    checkpoint_file = Path(\"fast_contribution_checkpoint.jsonl\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    processor = BatchProcessor(max_workers=MAX_WORKERS, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    try:\n",
    "        # Load papers\n",
    "        logger.info(\"Loading papers...\")\n",
    "        papers = load_papers_fast(input_file)\n",
    "        \n",
    "        if not papers:\n",
    "            logger.error(\"No papers loaded. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint_papers, processed_ids = load_checkpoint_fast(checkpoint_file)\n",
    "        \n",
    "        # Determine remaining papers\n",
    "        if processed_ids:\n",
    "            papers_to_process = []\n",
    "            for paper in papers:\n",
    "                paper_id = paper.get(\"paper_id\", paper.get(\"id\"))\n",
    "                if paper_id not in processed_ids:\n",
    "                    papers_to_process.append(paper)\n",
    "            \n",
    "            logger.info(f\"Resuming: {len(checkpoint_papers)} processed, {len(papers_to_process)} remaining\")\n",
    "            remaining_papers = papers_to_process\n",
    "        else:\n",
    "            logger.info(f\"Starting fresh: {len(papers)} papers to process\")\n",
    "            remaining_papers = papers\n",
    "        \n",
    "        # Process in chunks for better memory management\n",
    "        chunk_size = 500\n",
    "        all_processed = []\n",
    "        total_stats = {\n",
    "            \"total\": 0,\n",
    "            \"successful\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"with_summary\": 0\n",
    "        }\n",
    "        \n",
    "        for chunk_start in range(0, len(remaining_papers), chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, len(remaining_papers))\n",
    "            chunk = remaining_papers[chunk_start:chunk_end]\n",
    "            \n",
    "            logger.info(f\"Processing chunk {chunk_start//chunk_size + 1}/{(len(remaining_papers)-1)//chunk_size + 1}\")\n",
    "            \n",
    "            # Process chunk\n",
    "            processed_chunk, stats = processor.process_batch(\n",
    "                chunk, \n",
    "                desc=f\"Chunk {chunk_start//chunk_size + 1}\"\n",
    "            )\n",
    "            \n",
    "            # Update statistics\n",
    "            total_stats[\"total\"] += stats[\"total\"]\n",
    "            total_stats[\"successful\"] += stats[\"successful\"]\n",
    "            total_stats[\"failed\"] += stats[\"failed\"]\n",
    "            total_stats[\"with_summary\"] += stats[\"with_summary\"]\n",
    "            \n",
    "            # Save checkpoint\n",
    "            current_results = checkpoint_papers + all_processed + processed_chunk\n",
    "            save_checkpoint_fast(current_results, checkpoint_file)\n",
    "            \n",
    "            # Add to results\n",
    "            all_processed.extend(processed_chunk)\n",
    "            \n",
    "            # Show intermediate stats\n",
    "            logger.info(f\"Chunk complete: {stats['successful']}/{stats['total']} successful \"\n",
    "                       f\"({stats['successful']/stats['total']*100:.1f}%)\")\n",
    "        \n",
    "        # Combine all results\n",
    "        enhanced_papers = checkpoint_papers + all_processed\n",
    "        \n",
    "        # Save final results\n",
    "        logger.info(\"Saving final results...\")\n",
    "        save_checkpoint_fast(enhanced_papers, output_file)\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        papers_per_second = total_stats[\"total\"] / elapsed_time\n",
    "        \n",
    "        logger.info(\"=\" * 70)\n",
    "        logger.info(\"OPTIMIZED PROCESSING COMPLETE\")\n",
    "        logger.info(\"=\" * 70)\n",
    "        logger.info(f\"Total papers processed: {total_stats['total']}\")\n",
    "        logger.info(f\"Papers with summaries: {total_stats['with_summary']} \"\n",
    "                   f\"({total_stats['with_summary']/total_stats['total']*100:.1f}%)\")\n",
    "        logger.info(f\"Successful: {total_stats['successful']}\")\n",
    "        logger.info(f\"Failed: {total_stats['failed']}\")\n",
    "        logger.info(f\"Total API requests: {request_counter}\")\n",
    "        logger.info(f\"API errors: {error_counter}\")\n",
    "        if request_counter > 0:\n",
    "            success_rate = (request_counter - error_counter) / request_counter * 100\n",
    "            logger.info(f\"API success rate: {success_rate:.1f}%\")\n",
    "        logger.info(f\"Final rate limit delay: {rate_limiter.current_delay:.2f}s\")\n",
    "        logger.info(f\"Total time: {elapsed_time:.0f}s ({elapsed_time/60:.1f} min)\")\n",
    "        logger.info(f\"Processing rate: {papers_per_second:.2f} papers/sec\")\n",
    "        logger.info(f\"Estimated completion time for 10k papers: {10000/papers_per_second/60:.1f} min\")\n",
    "        logger.info(f\"Output saved to: {output_file}\")\n",
    "        logger.info(\"=\" * 70)\n",
    "        \n",
    "        # Remove checkpoint file\n",
    "        if checkpoint_file.exists():\n",
    "            try:\n",
    "                checkpoint_file.unlink()\n",
    "                logger.info(f\"Checkpoint removed: {checkpoint_file}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not remove checkpoint: {e}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcessing interrupted. Checkpoint saved for resume.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Performance optimization: disable debug logging for production\n",
    "    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb53737",
   "metadata": {},
   "source": [
    "#### Simple merge all the generated jsonl without filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_simple_dataset.py\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any, List\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def safe_string(value: Any) -> str:\n",
    "    \"\"\"Safely convert value to string and clean.\"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.strip()\n",
    "    return str(value).strip()\n",
    "\n",
    "def load_jsonl(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSONL file into a dictionary with paper_id as key.\"\"\"\n",
    "    data = {}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    # Try multiple possible ID fields\n",
    "                    pid = obj.get(\"paper_id\") or obj.get(\"id\") or f\"paper_{line_num}\"\n",
    "                    data[pid] = obj\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Line {line_num} in {path}: JSON decode error - {e}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Line {line_num} in {path}: Error - {e}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {path}: {e}\")\n",
    "    \n",
    "    logger.info(f\"Loaded {len(data)} items from {path}\")\n",
    "    return data\n",
    "\n",
    "def clean_list(lst: Any, max_len: int = None) -> List[str]:\n",
    "    \"\"\"Clean and deduplicate a list.\"\"\"\n",
    "    if not lst or not isinstance(lst, list):\n",
    "        return []\n",
    "    \n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    \n",
    "    for item in lst:\n",
    "        if item is None:\n",
    "            continue\n",
    "        \n",
    "        # Convert to string and clean\n",
    "        if isinstance(item, str):\n",
    "            clean_item = item.strip()\n",
    "        else:\n",
    "            clean_item = str(item).strip()\n",
    "        \n",
    "        if clean_item and clean_item not in seen:\n",
    "            cleaned.append(clean_item)\n",
    "            seen.add(clean_item)\n",
    "            \n",
    "        if max_len and len(cleaned) >= max_len:\n",
    "            break\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def safe_int(value: Any) -> int:\n",
    "    \"\"\"Safely convert to integer.\"\"\"\n",
    "    if value is None:\n",
    "        return 0\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def safe_float(value: Any) -> float:\n",
    "    \"\"\"Safely convert to float.\"\"\"\n",
    "    if value is None:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "def normalize_scores(scores: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Normalize quality scores to ensure consistent structure.\"\"\"\n",
    "    if not scores or not isinstance(scores, dict):\n",
    "        return {\n",
    "            \"novelty\": 0,\n",
    "            \"technical_depth\": 0,\n",
    "            \"clarity\": 0,\n",
    "            \"impact_potential\": 0,\n",
    "            \"overall_score\": 0.0,\n",
    "            \"confidence\": 0.0\n",
    "        }\n",
    "    \n",
    "    normalized = {}\n",
    "    \n",
    "    # Integer scores (0-10)\n",
    "    for key in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\"]:\n",
    "        value = scores.get(key)\n",
    "        if value is not None:\n",
    "            int_val = safe_int(value)\n",
    "            normalized[key] = max(0, min(10, int_val))  # Clamp to 0-10\n",
    "        else:\n",
    "            normalized[key] = 0\n",
    "    \n",
    "    # Float scores\n",
    "    overall = safe_float(scores.get(\"overall_score\"))\n",
    "    normalized[\"overall_score\"] = max(0.0, min(10.0, overall))  # Clamp to 0.0-10.0\n",
    "    \n",
    "    confidence = safe_float(scores.get(\"confidence\"))\n",
    "    normalized[\"confidence\"] = max(0.0, min(1.0, confidence))  # Clamp to 0.0-1.0\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def normalize_contributions(contrib: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Normalize contribution summary to ensure consistent structure.\"\"\"\n",
    "    if not contrib or not isinstance(contrib, dict):\n",
    "        return {\n",
    "            \"problem\": \"\",\n",
    "            \"method\": \"\",\n",
    "            \"key_contributions\": [],\n",
    "            \"application_scenarios\": []\n",
    "        }\n",
    "    \n",
    "    normalized = {}\n",
    "    \n",
    "    # String fields\n",
    "    normalized[\"problem\"] = safe_string(contrib.get(\"problem\"))\n",
    "    normalized[\"method\"] = safe_string(contrib.get(\"method\"))\n",
    "    \n",
    "    # List fields\n",
    "    normalized[\"key_contributions\"] = clean_list(contrib.get(\"key_contributions\"), max_len=10)\n",
    "    normalized[\"application_scenarios\"] = clean_list(contrib.get(\"application_scenarios\"), max_len=10)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def main():\n",
    "    \"\"\"Simple merge without filtering.\"\"\"\n",
    "    logger.info(\"Starting simple merge of all papers...\")\n",
    "    \n",
    "    # Load all data files\n",
    "    base = load_jsonl(\"papers_final_aligned.jsonl\")\n",
    "    fields = load_jsonl(\"papers_enhanced_fields.jsonl\")\n",
    "    keywords = load_jsonl(\"papers_enhanced_keywords.jsonl\")\n",
    "    scores = load_jsonl(\"papers_enhanced_scores.jsonl\")\n",
    "    contribs = load_jsonl(\"papers_enhanced_contributions.jsonl\")\n",
    "    \n",
    "    if not base:\n",
    "        logger.error(\"No base papers found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Base papers: {len(base)}\")\n",
    "    logger.info(f\"Fields data: {len(fields)}\")\n",
    "    logger.info(f\"Keywords data: {len(keywords)}\")\n",
    "    logger.info(f\"Scores data: {len(scores)}\")\n",
    "    logger.info(f\"Contributions data: {len(contribs)}\")\n",
    "    \n",
    "    final_papers = []\n",
    "    stats = defaultdict(int)\n",
    "    \n",
    "    for pid, paper in base.items():\n",
    "        merged = {}\n",
    "        \n",
    "        # Basic paper info (always include)\n",
    "        merged[\"paper_id\"] = pid\n",
    "        merged[\"title\"] = safe_string(paper.get(\"title\"))\n",
    "        merged[\"abstract\"] = safe_string(paper.get(\"abstract\"))\n",
    "        merged[\"source\"] = safe_string(paper.get(\"source\"))\n",
    "        merged[\"abstract_source\"] = safe_string(paper.get(\"abstract_source\"))\n",
    "        \n",
    "        # Optional fields (include if available)\n",
    "        authors = paper.get(\"authors\")\n",
    "        if authors is not None:\n",
    "            merged[\"authors\"] = clean_list(authors, max_len=20)\n",
    "            stats[\"has_authors\"] += 1\n",
    "        \n",
    "        publish_year = paper.get(\"publish_year\")\n",
    "        if publish_year is not None:\n",
    "            merged[\"publish_year\"] = safe_int(publish_year)\n",
    "            stats[\"has_publish_year\"] += 1\n",
    "        \n",
    "        venue = paper.get(\"venue\")\n",
    "        if venue is not None:\n",
    "            merged[\"venue\"] = safe_string(venue)\n",
    "            stats[\"has_venue\"] += 1\n",
    "        \n",
    "        citation_count = paper.get(\"citation_count\")\n",
    "        if citation_count is not None:\n",
    "            merged[\"citation_count\"] = safe_int(citation_count)\n",
    "            stats[\"has_citation_count\"] += 1\n",
    "        \n",
    "        url = paper.get(\"url\")\n",
    "        if url is not None:\n",
    "            merged[\"url\"] = safe_string(url)\n",
    "            stats[\"has_url\"] += 1\n",
    "        \n",
    "        # Enhanced fields\n",
    "        if pid in fields:\n",
    "            merged[\"fields_of_study\"] = clean_list(\n",
    "                fields[pid].get(\"fields_of_study\"), \n",
    "                max_len=8\n",
    "            )\n",
    "            stats[\"has_fields\"] += 1\n",
    "        else:\n",
    "            merged[\"fields_of_study\"] = []\n",
    "            stats[\"missing_fields\"] += 1\n",
    "        \n",
    "        # Keywords\n",
    "        if pid in keywords:\n",
    "            merged[\"keywords\"] = clean_list(\n",
    "                keywords[pid].get(\"keywords\"), \n",
    "                max_len=8\n",
    "            )\n",
    "            stats[\"has_keywords\"] += 1\n",
    "        else:\n",
    "            merged[\"keywords\"] = []\n",
    "            stats[\"missing_keywords\"] += 1\n",
    "        \n",
    "        # Quality scores\n",
    "        if pid in scores:\n",
    "            scores_data = scores[pid].get(\"quality_scores\", {})\n",
    "            merged[\"quality_scores\"] = normalize_scores(scores_data)\n",
    "            stats[\"has_scores\"] += 1\n",
    "        else:\n",
    "            merged[\"quality_scores\"] = normalize_scores({})\n",
    "            stats[\"missing_scores\"] += 1\n",
    "        \n",
    "        # Contribution summary\n",
    "        if pid in contribs:\n",
    "            contrib_data = contribs[pid].get(\"contribution_summary\", {})\n",
    "            merged[\"contribution_summary\"] = normalize_contributions(contrib_data)\n",
    "            stats[\"has_contributions\"] += 1\n",
    "        else:\n",
    "            merged[\"contribution_summary\"] = normalize_contributions({})\n",
    "            stats[\"missing_contributions\"] += 1\n",
    "        \n",
    "        # Add to final papers (NO FILTERING)\n",
    "        final_papers.append(merged)\n",
    "    \n",
    "    # Save all papers\n",
    "    output_file = \"papers_master_simple.jsonl\"\n",
    "    logger.info(f\"Saving {len(final_papers)} papers to {output_file}\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for paper in final_papers:\n",
    "                f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        logger.info(\"Successfully saved all papers\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save output file: {e}\")\n",
    "        # Try backup\n",
    "        backup_file = f\"{output_file}.backup\"\n",
    "        try:\n",
    "            with open(backup_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                for paper in final_papers:\n",
    "                    f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "            logger.info(f\"Saved backup to {backup_file}\")\n",
    "        except Exception as e2:\n",
    "            logger.error(f\"Failed to save backup: {e2}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"SIMPLE MERGE COMPLETE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(f\"Total papers processed: {len(final_papers)}\")\n",
    "    logger.info(f\"Total papers in base: {len(base)}\")\n",
    "    \n",
    "    logger.info(\"-\" * 60)\n",
    "    logger.info(\"DATA AVAILABILITY STATISTICS:\")\n",
    "    logger.info(f\"  Papers with authors: {stats['has_authors']} ({stats['has_authors']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with publish_year: {stats['has_publish_year']} ({stats['has_publish_year']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with venue: {stats['has_venue']} ({stats['has_venue']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with citation_count: {stats['has_citation_count']} ({stats['has_citation_count']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with url: {stats['has_url']} ({stats['has_url']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with fields_of_study: {stats['has_fields']} ({stats['has_fields']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with keywords: {stats['has_keywords']} ({stats['has_keywords']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with quality_scores: {stats['has_scores']} ({stats['has_scores']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"  Papers with contribution_summary: {stats['has_contributions']} ({stats['has_contributions']/len(base)*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate average scores for all papers\n",
    "    if final_papers:\n",
    "        avg_scores = defaultdict(float)\n",
    "        for paper in final_papers:\n",
    "            qs = paper.get(\"quality_scores\", {})\n",
    "            for key in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\", \"overall_score\", \"confidence\"]:\n",
    "                if key in qs:\n",
    "                    avg_scores[key] += qs[key]\n",
    "        \n",
    "        logger.info(\"-\" * 60)\n",
    "        logger.info(\"AVERAGE QUALITY SCORES:\")\n",
    "        for key in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\"]:\n",
    "            if key in avg_scores:\n",
    "                avg = avg_scores[key] / len(final_papers)\n",
    "                logger.info(f\"  {key:20s}: {avg:.2f}/10\")\n",
    "        \n",
    "        if \"overall_score\" in avg_scores:\n",
    "            avg = avg_scores[\"overall_score\"] / len(final_papers)\n",
    "            logger.info(f\"  overall_score:        {avg:.2f}/10\")\n",
    "        \n",
    "        if \"confidence\" in avg_scores:\n",
    "            avg = avg_scores[\"confidence\"] / len(final_papers)\n",
    "            logger.info(f\"  confidence:           {avg:.3f}\")\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78921775",
   "metadata": {},
   "source": [
    "#### Merge all the generated jsonl with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c210654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 12:28:16,998 - INFO - Loading data files...\n",
      "2025-12-23 12:28:17,036 - INFO - Loaded 6242 papers from papers_final_aligned.jsonl, skipped 0 lines\n",
      "2025-12-23 12:28:17,077 - INFO - Loaded 6242 papers from papers_enhanced_fields.jsonl, skipped 0 lines\n",
      "2025-12-23 12:28:17,266 - INFO - Loaded 6242 papers from papers_enhanced_keywords.jsonl, skipped 0 lines\n",
      "2025-12-23 12:28:17,316 - INFO - Loaded 6242 papers from papers_enhanced_scores.jsonl, skipped 0 lines\n",
      "2025-12-23 12:28:17,376 - INFO - Loaded 6242 papers from papers_enhanced_contributions.jsonl, skipped 0 lines\n",
      "2025-12-23 12:28:17,376 - INFO - Base papers: 6242\n",
      "2025-12-23 12:28:17,377 - INFO - Fields data: 6242\n",
      "2025-12-23 12:28:17,378 - INFO - Keywords data: 6242\n",
      "2025-12-23 12:28:17,378 - INFO - Scores data: 6242\n",
      "2025-12-23 12:28:17,378 - INFO - Contributions data: 6242\n",
      "2025-12-23 12:28:17,379 - INFO - Merging and filtering papers...\n",
      "2025-12-23 12:28:17,473 - INFO - Saving 3236 papers to papers_master_final.jsonl\n",
      "2025-12-23 12:28:17,557 - INFO - Successfully saved final papers\n",
      "2025-12-23 12:28:17,728 - INFO - ============================================================\n",
      "2025-12-23 12:28:17,728 - INFO - MASTER MERGE COMPLETE\n",
      "2025-12-23 12:28:17,729 - INFO - ============================================================\n",
      "2025-12-23 12:28:17,730 - INFO - Base papers: 6242\n",
      "2025-12-23 12:28:17,730 - INFO - Final papers: 3236\n",
      "2025-12-23 12:28:17,730 - INFO - Dropped papers: 3006\n",
      "2025-12-23 12:28:17,731 - INFO - Retention rate: 51.8%\n",
      "2025-12-23 12:28:17,731 - INFO - ------------------------------------------------------------\n",
      "2025-12-23 12:28:17,732 - INFO - DATA AVAILABILITY:\n",
      "2025-12-23 12:28:17,732 - INFO - Papers with fields: 6242 (100.0%)\n",
      "2025-12-23 12:28:17,733 - INFO - Papers with keywords: 6242 (100.0%)\n",
      "2025-12-23 12:28:17,733 - INFO - Papers with scores: 6242 (100.0%)\n",
      "2025-12-23 12:28:17,733 - INFO - Papers with contributions: 6242 (100.0%)\n",
      "2025-12-23 12:28:17,734 - INFO - ------------------------------------------------------------\n",
      "2025-12-23 12:28:17,734 - INFO - DROP REASONS:\n",
      "2025-12-23 12:28:17,734 - INFO -   Score too low: 295\n",
      "2025-12-23 12:28:17,735 - INFO -   Technical depth too low: 530\n",
      "2025-12-23 12:28:17,735 - INFO -   Confidence too low: 3\n",
      "2025-12-23 12:28:17,735 - INFO -   Empty title/abstract: 2472\n",
      "2025-12-23 12:28:17,737 - INFO - ------------------------------------------------------------\n",
      "2025-12-23 12:28:17,738 - INFO - AVERAGE SCORES IN FINAL DATASET:\n",
      "2025-12-23 12:28:17,738 - INFO -   novelty             : 5.93/10\n",
      "2025-12-23 12:28:17,739 - INFO -   technical_depth     : 6.33/10\n",
      "2025-12-23 12:28:17,739 - INFO -   clarity             : 8.02/10\n",
      "2025-12-23 12:28:17,740 - INFO -   impact_potential    : 6.96/10\n",
      "2025-12-23 12:28:17,740 - INFO -   overall_score:        6.81/10\n",
      "2025-12-23 12:28:17,741 - INFO -   confidence:           0.851\n",
      "2025-12-23 12:28:17,741 - INFO - ============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_jsonl(path):\n",
    "    \"\"\"Load JSONL file and return dictionary with paper_id as key.\"\"\"\n",
    "    data = {}\n",
    "    skipped = 0\n",
    "    \n",
    "    if not Path(path).exists():\n",
    "        logger.error(f\"File not found: {path}\")\n",
    "        return data\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                pid = obj.get(\"paper_id\") or obj.get(\"id\")\n",
    "                \n",
    "                if pid:\n",
    "                    data[pid] = obj\n",
    "                else:\n",
    "                    logger.warning(f\"Line {line_num}: No paper_id found\")\n",
    "                    skipped += 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.warning(f\"Line {line_num}: JSON decode error - {e}\")\n",
    "                skipped += 1\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Line {line_num}: Unexpected error - {e}\")\n",
    "                skipped += 1\n",
    "    \n",
    "    logger.info(f\"Loaded {len(data)} papers from {path}, skipped {skipped} lines\")\n",
    "    return data\n",
    "\n",
    "def clean_string(value):\n",
    "    \"\"\"Safely clean string, handling None values.\"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.strip()\n",
    "    return str(value).strip()\n",
    "\n",
    "def clean_list(lst, max_len=None):\n",
    "    \"\"\"Clean a list, handling None values and duplicates.\"\"\"\n",
    "    if not lst or not isinstance(lst, list):\n",
    "        return []\n",
    "    \n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    \n",
    "    for x in lst:\n",
    "        if x is None:\n",
    "            continue\n",
    "            \n",
    "        if isinstance(x, str):\n",
    "            item = x.strip()\n",
    "        else:\n",
    "            item = str(x).strip()\n",
    "            \n",
    "        if item and item not in seen:\n",
    "            cleaned.append(item)\n",
    "            seen.add(item)\n",
    "            \n",
    "        if max_len and len(cleaned) >= max_len:\n",
    "            break\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def safe_int(value):\n",
    "    \"\"\"Safely convert to integer.\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle string, int, float\n",
    "        if isinstance(value, str):\n",
    "            # Remove non-numeric characters from end\n",
    "            cleaned = value.strip()\n",
    "            # Try to extract first number\n",
    "            import re\n",
    "            match = re.search(r'[-+]?\\d+', cleaned)\n",
    "            if match:\n",
    "                return int(match.group())\n",
    "            return None\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return int(value)\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def safe_float(value):\n",
    "    \"\"\"Safely convert to float.\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            cleaned = value.strip()\n",
    "            # Try to parse as float\n",
    "            try:\n",
    "                return float(cleaned)\n",
    "            except ValueError:\n",
    "                # Try to extract number\n",
    "                import re\n",
    "                match = re.search(r'[-+]?\\d*\\.?\\d+', cleaned)\n",
    "                if match:\n",
    "                    return float(match.group())\n",
    "                return None\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def get_nested_value(obj, keys, default=None):\n",
    "    \"\"\"Safely get nested value from dictionary.\"\"\"\n",
    "    if not obj or not isinstance(obj, dict):\n",
    "        return default\n",
    "    \n",
    "    current = obj\n",
    "    for key in keys:\n",
    "        if not isinstance(current, dict) or key not in current:\n",
    "            return default\n",
    "        current = current[key]\n",
    "    \n",
    "    return current\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    base_path = \"papers_final_aligned.jsonl\"\n",
    "    fields_path = \"papers_enhanced_fields.jsonl\"\n",
    "    keywords_path = \"papers_enhanced_keywords.jsonl\"\n",
    "    scores_path = \"papers_enhanced_scores.jsonl\"\n",
    "    contribs_path = \"papers_enhanced_contributions.jsonl\"\n",
    "    output_path = \"papers_master_final.jsonl\"\n",
    "    \n",
    "    logger.info(\"Loading data files...\")\n",
    "    \n",
    "    # Load all data files\n",
    "    base = load_jsonl(base_path)\n",
    "    fields = load_jsonl(fields_path)\n",
    "    keywords = load_jsonl(keywords_path)\n",
    "    scores = load_jsonl(scores_path)\n",
    "    contribs = load_jsonl(contribs_path)\n",
    "    \n",
    "    if not base:\n",
    "        logger.error(\"No base papers loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Base papers: {len(base)}\")\n",
    "    logger.info(f\"Fields data: {len(fields)}\")\n",
    "    logger.info(f\"Keywords data: {len(keywords)}\")\n",
    "    logger.info(f\"Scores data: {len(scores)}\")\n",
    "    logger.info(f\"Contributions data: {len(contribs)}\")\n",
    "    \n",
    "    final_papers = []\n",
    "    dropped = 0\n",
    "    merge_stats = defaultdict(int)\n",
    "    \n",
    "    logger.info(\"Merging and filtering papers...\")\n",
    "    \n",
    "    for pid, paper in base.items():\n",
    "        merged = {}\n",
    "        \n",
    "        # Basic paper info\n",
    "        merged[\"source\"] = paper.get(\"source\")\n",
    "        merged[\"paper_id\"] = pid\n",
    "        merged[\"title\"] = clean_string(paper.get(\"title\"))\n",
    "        merged[\"abstract\"] = clean_string(paper.get(\"abstract\"))\n",
    "        merged[\"abstract_source\"] = paper.get(\"abstract_source\")\n",
    "        merged[\"authors\"] = clean_list(paper.get(\"authors\"), max_len=20)\n",
    "        merged[\"publish_year\"] = safe_int(paper.get(\"publish_year\"))\n",
    "        merged[\"venue\"] = clean_string(paper.get(\"venue\"))\n",
    "        merged[\"citation_count\"] = safe_int(paper.get(\"citation_count\")) or 0\n",
    "        merged[\"url\"] = paper.get(\"url\")\n",
    "        \n",
    "        # Fields of study\n",
    "        if pid in fields:\n",
    "            merged[\"fields_of_study\"] = clean_list(\n",
    "                get_nested_value(fields[pid], [\"fields_of_study\"]), \n",
    "                max_len=8\n",
    "            )\n",
    "            merge_stats[\"has_fields\"] += 1\n",
    "        else:\n",
    "            merged[\"fields_of_study\"] = []\n",
    "            merge_stats[\"missing_fields\"] += 1\n",
    "        \n",
    "        # Keywords\n",
    "        if pid in keywords:\n",
    "            merged[\"keywords\"] = clean_list(\n",
    "                get_nested_value(keywords[pid], [\"keywords\"]), \n",
    "                max_len=8\n",
    "            )\n",
    "            merge_stats[\"has_keywords\"] += 1\n",
    "        else:\n",
    "            merged[\"keywords\"] = []\n",
    "            merge_stats[\"missing_keywords\"] += 1\n",
    "        \n",
    "        # Quality scores\n",
    "        if pid in scores:\n",
    "            scores_data = get_nested_value(scores[pid], [\"quality_scores\"], {})\n",
    "            # Ensure all score fields exist and are valid\n",
    "            quality_scores = {}\n",
    "            \n",
    "            # Integer scores (0-10)\n",
    "            for int_key in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\"]:\n",
    "                value = scores_data.get(int_key)\n",
    "                int_val = safe_int(value)\n",
    "                if int_val is not None and 0 <= int_val <= 10:\n",
    "                    quality_scores[int_key] = int_val\n",
    "                else:\n",
    "                    quality_scores[int_key] = 0\n",
    "            \n",
    "            # Float scores\n",
    "            overall = safe_float(scores_data.get(\"overall_score\"))\n",
    "            if overall is not None and 0 <= overall <= 10:\n",
    "                quality_scores[\"overall_score\"] = round(overall, 1)\n",
    "            else:\n",
    "                quality_scores[\"overall_score\"] = 0.0\n",
    "            \n",
    "            confidence = safe_float(scores_data.get(\"confidence\"))\n",
    "            if confidence is not None and 0 <= confidence <= 1:\n",
    "                quality_scores[\"confidence\"] = round(confidence, 2)\n",
    "            else:\n",
    "                quality_scores[\"confidence\"] = 0.0\n",
    "            \n",
    "            merged[\"quality_scores\"] = quality_scores\n",
    "            merge_stats[\"has_scores\"] += 1\n",
    "        else:\n",
    "            merged[\"quality_scores\"] = {\n",
    "                \"novelty\": 0,\n",
    "                \"technical_depth\": 0,\n",
    "                \"clarity\": 0,\n",
    "                \"impact_potential\": 0,\n",
    "                \"overall_score\": 0.0,\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "            merge_stats[\"missing_scores\"] += 1\n",
    "        \n",
    "        # Contribution summary\n",
    "        if pid in contribs:\n",
    "            contrib_data = get_nested_value(contribs[pid], [\"contribution_summary\"], {})\n",
    "            # Clean and validate contribution summary\n",
    "            cleaned_contrib = {}\n",
    "            \n",
    "            # Problem and method (strings)\n",
    "            for str_key in [\"problem\", \"method\"]:\n",
    "                value = contrib_data.get(str_key)\n",
    "                if isinstance(value, str) and value.strip():\n",
    "                    cleaned_contrib[str_key] = value.strip()[:500]  # Limit length\n",
    "                else:\n",
    "                    cleaned_contrib[str_key] = \"\"\n",
    "            \n",
    "            # Lists\n",
    "            for list_key in [\"key_contributions\", \"application_scenarios\"]:\n",
    "                value = contrib_data.get(list_key)\n",
    "                if isinstance(value, list):\n",
    "                    cleaned_contrib[list_key] = clean_list(value, max_len=10)\n",
    "                else:\n",
    "                    cleaned_contrib[list_key] = []\n",
    "            \n",
    "            merged[\"contribution_summary\"] = cleaned_contrib\n",
    "            merge_stats[\"has_contribs\"] += 1\n",
    "        else:\n",
    "            merged[\"contribution_summary\"] = {\n",
    "                \"problem\": \"\",\n",
    "                \"method\": \"\",\n",
    "                \"key_contributions\": [],\n",
    "                \"application_scenarios\": []\n",
    "            }\n",
    "            merge_stats[\"missing_contribs\"] += 1\n",
    "        \n",
    "        # Apply quality filters - 移除对标题和摘要长度的检查\n",
    "        title = merged[\"title\"]\n",
    "        abstract = merged[\"abstract\"]\n",
    "        qs = merged[\"quality_scores\"]\n",
    "        \n",
    "        try:\n",
    "            # Check if paper meets minimum quality criteria\n",
    "            # 不再检查标题和摘要长度\n",
    "            score_too_low = qs.get(\"overall_score\", 0) < 2\n",
    "            depth_too_low = qs.get(\"technical_depth\", 0) < 2\n",
    "            confidence_too_low = qs.get(\"confidence\", 0) < 0.6\n",
    "            \n",
    "            # 只根据分数阈值过滤\n",
    "            if (score_too_low or depth_too_low or confidence_too_low):\n",
    "                dropped += 1\n",
    "                \n",
    "                # Log reason for dropping (for debugging)\n",
    "                if score_too_low:\n",
    "                    merge_stats[\"dropped_score\"] += 1\n",
    "                if depth_too_low:\n",
    "                    merge_stats[\"dropped_depth\"] += 1\n",
    "                if confidence_too_low:\n",
    "                    merge_stats[\"dropped_confidence\"] += 1\n",
    "                    \n",
    "                continue\n",
    "            \n",
    "            # Additional validation - 仍然检查标题和摘要是否为空\n",
    "            if not title or not abstract:\n",
    "                dropped += 1\n",
    "                merge_stats[\"dropped_empty\"] += 1\n",
    "                continue\n",
    "            \n",
    "            final_papers.append(merged)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error filtering paper {pid}: {e}\")\n",
    "            dropped += 1\n",
    "            merge_stats[\"dropped_error\"] += 1\n",
    "            continue\n",
    "    \n",
    "    # Save final papers\n",
    "    logger.info(f\"Saving {len(final_papers)} papers to {output_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for p in final_papers:\n",
    "                f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        logger.info(\"Successfully saved final papers\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save output file: {e}\")\n",
    "        # Try to save backup\n",
    "        backup_path = f\"{output_path}.backup\"\n",
    "        try:\n",
    "            with open(backup_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for p in final_papers:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            logger.info(f\"Saved backup to {backup_path}\")\n",
    "        except:\n",
    "            logger.error(\"Failed to save backup\")\n",
    "    \n",
    "    # Print summary\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"MASTER MERGE COMPLETE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(f\"Base papers: {len(base)}\")\n",
    "    logger.info(f\"Final papers: {len(final_papers)}\")\n",
    "    logger.info(f\"Dropped papers: {dropped}\")\n",
    "    logger.info(f\"Retention rate: {len(final_papers)/len(base)*100:.1f}%\")\n",
    "    \n",
    "    logger.info(\"-\" * 60)\n",
    "    logger.info(\"DATA AVAILABILITY:\")\n",
    "    logger.info(f\"Papers with fields: {merge_stats['has_fields']} ({merge_stats['has_fields']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"Papers with keywords: {merge_stats['has_keywords']} ({merge_stats['has_keywords']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"Papers with scores: {merge_stats['has_scores']} ({merge_stats['has_scores']/len(base)*100:.1f}%)\")\n",
    "    logger.info(f\"Papers with contributions: {merge_stats['has_contribs']} ({merge_stats['has_contribs']/len(base)*100:.1f}%)\")\n",
    "    \n",
    "    if dropped > 0:\n",
    "        logger.info(\"-\" * 60)\n",
    "        logger.info(\"DROP REASONS:\")\n",
    "        # 移除标题和摘要长度相关的丢弃原因\n",
    "        if merge_stats[\"dropped_score\"]:\n",
    "            logger.info(f\"  Score too low: {merge_stats['dropped_score']}\")\n",
    "        if merge_stats[\"dropped_depth\"]:\n",
    "            logger.info(f\"  Technical depth too low: {merge_stats['dropped_depth']}\")\n",
    "        if merge_stats[\"dropped_confidence\"]:\n",
    "            logger.info(f\"  Confidence too low: {merge_stats['dropped_confidence']}\")\n",
    "        if merge_stats[\"dropped_empty\"]:\n",
    "            logger.info(f\"  Empty title/abstract: {merge_stats['dropped_empty']}\")\n",
    "        if merge_stats[\"dropped_error\"]:\n",
    "            logger.info(f\"  Processing error: {merge_stats['dropped_error']}\")\n",
    "    \n",
    "    # Calculate average scores for final papers\n",
    "    if final_papers:\n",
    "        avg_scores = defaultdict(float)\n",
    "        for paper in final_papers:\n",
    "            qs = paper.get(\"quality_scores\", {})\n",
    "            for key in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\", \"overall_score\", \"confidence\"]:\n",
    "                if key in qs:\n",
    "                    avg_scores[key] += qs[key]\n",
    "        \n",
    "        logger.info(\"-\" * 60)\n",
    "        logger.info(\"AVERAGE SCORES IN FINAL DATASET:\")\n",
    "        for key in [\"novelty\", \"technical_depth\", \"clarity\", \"impact_potential\"]:\n",
    "            if key in avg_scores:\n",
    "                avg = avg_scores[key] / len(final_papers)\n",
    "                logger.info(f\"  {key:20s}: {avg:.2f}/10\")\n",
    "        \n",
    "        if \"overall_score\" in avg_scores:\n",
    "            avg = avg_scores[\"overall_score\"] / len(final_papers)\n",
    "            logger.info(f\"  overall_score:        {avg:.2f}/10\")\n",
    "        \n",
    "        if \"confidence\" in avg_scores:\n",
    "            avg = avg_scores[\"confidence\"] / len(final_papers)\n",
    "            logger.info(f\"  confidence:           {avg:.3f}\")\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
