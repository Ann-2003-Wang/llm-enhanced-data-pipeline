{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9022365",
   "metadata": {},
   "source": [
    "### Paper Collection from arXiv, Semantic Scholar and OpenAlex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a9287",
   "metadata": {},
   "source": [
    "### Import some required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9602d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import arxiv\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8583d0d3",
   "metadata": {},
   "source": [
    "#### Collect Paper from arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6851876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goush\\AppData\\Local\\Temp\\ipykernel_32844\\2220306550.py:25: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for r in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 300 from arXiv for 'artificial intelligence'\n",
      "Fetched 300 from arXiv for 'machine learning'\n",
      "Fetched 300 from arXiv for 'computer vision'\n",
      "Fetched 300 from arXiv for 'natural language processing'\n",
      "Fetched 300 from arXiv for 'robotics and human-computer interaction'\n",
      "Saved total 1500 papers from arXiv.\n"
     ]
    }
   ],
   "source": [
    "# arxiv_scraper.py\n",
    "import arxiv\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_arxiv_papers(query, max_results=300, time_range=\"month\"):\n",
    "    end_date = datetime.now()\n",
    "    if time_range == \"week\":\n",
    "        start_date = end_date - timedelta(days=7)\n",
    "    elif time_range == \"months\":\n",
    "        start_date = end_date - timedelta(days=30)\n",
    "    else:\n",
    "        start_date = end_date - timedelta(days=30)\n",
    "\n",
    "    time_filter = f\"submittedDate:[{start_date.strftime('%Y%m%d')}+TO+{end_date.strftime('%Y%m%d')}]\"\n",
    "    full_query = f\"{query}+AND+{time_filter}\"\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=full_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for r in search.results():\n",
    "        paper = {\n",
    "            \"source\": \"arxiv\",\n",
    "            \"paper_id\": r.get_short_id(),\n",
    "            \"title\": r.title,\n",
    "            \"authors\": [str(a) for a in r.authors],\n",
    "            \"abstract\": r.summary,\n",
    "            \"publish_date\": str(r.published.date()),\n",
    "            \"url\": f\"http://arxiv.org/abs/{r.get_short_id()}\",\n",
    "            \"categories\": r.categories\n",
    "        }\n",
    "        results.append(paper)\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topics = [\n",
    "    \"artificial intelligence\",\n",
    "    \"machine learning\",\n",
    "    \"computer vision\",\n",
    "    \"natural language processing\",\n",
    "    \"robotics and human-computer interaction\"\n",
    "    ]\n",
    "\n",
    "    all_papers = []\n",
    "\n",
    "    for t in topics:\n",
    "        papers = fetch_arxiv_papers(t, max_results=300, time_range=\"month\")\n",
    "        all_papers.extend(papers)\n",
    "        print(f\"Fetched {len(papers)} from arXiv for '{t}'\")\n",
    "\n",
    "    with open(\"arxiv_papers.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in all_papers:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved total {len(all_papers)} papers from arXiv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750fa07",
   "metadata": {},
   "source": [
    "#### Collect Paper from Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df2f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting semanticscholar\n",
      "  Downloading semanticscholar-0.11.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting tenacity (from semanticscholar)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: httpx in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from semanticscholar) (0.28.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from semanticscholar) (1.6.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from httpx->semanticscholar) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from httpx->semanticscholar) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from httpx->semanticscholar) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from httpx->semanticscholar) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from httpcore==1.*->httpx->semanticscholar) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from anyio->httpx->semanticscholar) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from anyio->httpx->semanticscholar) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\goush\\.conda\\envs\\llm-env\\lib\\site-packages (from anyio->httpx->semanticscholar) (4.15.0)\n",
      "Downloading semanticscholar-0.11.0-py3-none-any.whl (26 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, semanticscholar\n",
      "\n",
      "   -------------------- ------------------- 1/2 [semanticscholar]\n",
      "   -------------------- ------------------- 1/2 [semanticscholar]\n",
      "   ---------------------------------------- 2/2 [semanticscholar]\n",
      "\n",
      "Successfully installed semanticscholar-0.11.0 tenacity-9.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install semanticscholar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16ec93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Semantic Scholar for 'artificial intelligence' ...\n",
      "Got 1000 papers from Semantic Scholar for 'artificial intelligence'\n",
      "Searching Semantic Scholar for 'machine learning' ...\n",
      "Got 1000 papers from Semantic Scholar for 'machine learning'\n",
      "Searching Semantic Scholar for 'computer vision' ...\n",
      "Got 1000 papers from Semantic Scholar for 'computer vision'\n",
      "Searching Semantic Scholar for 'natural language processing' ...\n",
      "Got 1000 papers from Semantic Scholar for 'natural language processing'\n",
      "Searching Semantic Scholar for 'robotics and human-computer interaction' ...\n",
      "Got 1000 papers from Semantic Scholar for 'robotics and human-computer interaction'\n",
      "Saved total 5000 papers to semantic_scholar_papers.jsonl\n"
     ]
    }
   ],
   "source": [
    "# semantic_scholar_scraper.py\n",
    "import json\n",
    "import time\n",
    "from semanticscholar import SemanticScholar\n",
    "\n",
    "sch = SemanticScholar()\n",
    "\n",
    "\n",
    "def fetch_s2_papers(query, limit=200):\n",
    "    print(f\"Searching Semantic Scholar for '{query}' ...\")\n",
    "\n",
    "    results = sch.search_paper(\n",
    "        query=query,\n",
    "        limit=limit,\n",
    "        fields=[\n",
    "            \"paperId\",\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"authors\",\n",
    "            \"year\",\n",
    "            \"url\",\n",
    "            \"venue\",\n",
    "            \"fieldsOfStudy\",\n",
    "            \"citationCount\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    papers = []\n",
    "\n",
    "    for p in results:\n",
    "        paper = {\n",
    "            \"source\": \"semantic_scholar\",\n",
    "            \"paper_id\": p.paperId,\n",
    "            \"title\": p.title,\n",
    "            \"authors\": [a.name for a in p.authors] if p.authors else [],\n",
    "            \"abstract\": p.abstract or \"\",\n",
    "            \"publish_date\": str(p.year) if p.year else \"\",\n",
    "            \"venue\": p.venue or \"\",\n",
    "            \"fields_of_study\": p.fieldsOfStudy or [],\n",
    "            \"citation_count\": p.citationCount if p.citationCount is not None else 0,\n",
    "            \"url\": p.url or \"\"\n",
    "        }\n",
    "        papers.append(paper)\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    topics = [\n",
    "        \"artificial intelligence\",\n",
    "        \"machine learning\",\n",
    "        \"computer vision\",\n",
    "        \"natural language processing\",\n",
    "        \"robotics and human-computer interaction\"\n",
    "    ]\n",
    "\n",
    "    all_papers = []\n",
    "\n",
    "    for t in topics:\n",
    "        s2papers = fetch_s2_papers(t, limit=100)\n",
    "        all_papers.extend(s2papers)\n",
    "        print(f\"Got {len(s2papers)} papers from Semantic Scholar for '{t}'\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    output_file = \"semantic_scholar_papers.jsonl\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in all_papers:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved total {len(all_papers)} papers to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1efd15d",
   "metadata": {},
   "source": [
    "#### Collect Paper from OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching OpenAlex for 'artificial intelligence' ...\n",
      "Got 300 papers from OpenAlex for 'artificial intelligence'\n",
      "Searching OpenAlex for 'machine learning' ...\n",
      "Got 300 papers from OpenAlex for 'machine learning'\n",
      "Searching OpenAlex for 'computer vision' ...\n",
      "Got 300 papers from OpenAlex for 'computer vision'\n",
      "Searching OpenAlex for 'natural language processing' ...\n",
      "Got 300 papers from OpenAlex for 'natural language processing'\n",
      "Searching OpenAlex for 'robotics human computer interaction' ...\n",
      "Got 300 papers from OpenAlex for 'robotics human computer interaction'\n",
      "Saved total 1500 papers to openalex_papers.jsonl\n"
     ]
    }
   ],
   "source": [
    "# openalex_scraper.py (robust version)\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "\n",
    "def build_session():\n",
    "    session = requests.Session()\n",
    "\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False\n",
    "    )\n",
    "\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "\n",
    "    return session\n",
    "\n",
    "\n",
    "def fetch_openalex_papers(query, limit=200, per_page=50):\n",
    "    print(f\"Searching OpenAlex for '{query}' ...\")\n",
    "\n",
    "    session = build_session()\n",
    "    papers = []\n",
    "\n",
    "    page = 1\n",
    "    collected = 0\n",
    "\n",
    "    while collected < limit:\n",
    "        params = {\n",
    "            \"search\": query,\n",
    "            \"per-page\": per_page,\n",
    "            \"page\": page,\n",
    "            \"mailto\": \"research@example.com\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            resp = session.get(BASE_URL, params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[WARN] OpenAlex request failed (page={page}): {e}\")\n",
    "            print(\"       Sleeping 5s and skipping this page...\")\n",
    "            time.sleep(5)\n",
    "            page += 1\n",
    "            continue\n",
    "\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        for r in results:\n",
    "            if collected >= limit:\n",
    "                break\n",
    "\n",
    "            paper = {\n",
    "                \"source\": \"openalex\",\n",
    "                \"paper_id\": r.get(\"id\", \"\"),\n",
    "                \"title\": r.get(\"title\", \"\"),\n",
    "                \"abstract\": r.get(\"abstract\") or \"\",\n",
    "                \"authors\": [\n",
    "                    a[\"author\"][\"display_name\"]\n",
    "                    for a in r.get(\"authorships\", [])\n",
    "                    if \"author\" in a\n",
    "                ],\n",
    "                \"publish_date\": r.get(\"publication_year\", \"\"),\n",
    "                \"venue\": r.get(\"host_venue\", {}).get(\"display_name\", \"\"),\n",
    "                \"citation_count\": r.get(\"cited_by_count\", 0),\n",
    "                \"fields_of_study\": [\n",
    "                    c[\"display_name\"]\n",
    "                    for c in r.get(\"concepts\", [])\n",
    "                    if c.get(\"level\", 10) <= 1\n",
    "                ],\n",
    "                \"url\": r.get(\"id\", \"\")\n",
    "            }\n",
    "\n",
    "            papers.append(paper)\n",
    "            collected += 1\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    topics = [\n",
    "        \"artificial intelligence\",\n",
    "        \"machine learning\",\n",
    "        \"computer vision\",\n",
    "        \"natural language processing\",\n",
    "        \"robotics human computer interaction\"\n",
    "    ]\n",
    "\n",
    "    all_papers = []\n",
    "\n",
    "    for t in topics:\n",
    "        papers = fetch_openalex_papers(t, limit=300)\n",
    "        all_papers.extend(papers)\n",
    "        print(f\"Got {len(papers)} papers from OpenAlex for '{t}'\")\n",
    "        time.sleep(3)\n",
    "\n",
    "    output_file = \"openalex_papers.jsonl\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in all_papers:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved total {len(all_papers)} papers to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f34d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching OpenAlex for 'artificial intelligence' ...\n",
      "Got 300 papers from OpenAlex for 'artificial intelligence'\n",
      "Searching OpenAlex for 'machine learning' ...\n",
      "Got 300 papers from OpenAlex for 'machine learning'\n",
      "Searching OpenAlex for 'computer vision' ...\n",
      "Got 300 papers from OpenAlex for 'computer vision'\n",
      "Searching OpenAlex for 'natural language processing' ...\n",
      "Got 300 papers from OpenAlex for 'natural language processing'\n",
      "Searching OpenAlex for 'robotics human computer interaction' ...\n",
      "Got 300 papers from OpenAlex for 'robotics human computer interaction'\n",
      "\n",
      "Saved 1500 papers to openalex_papers.jsonl\n"
     ]
    }
   ],
   "source": [
    "# openalex_scraper.py\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "\n",
    "def build_session():\n",
    "    session = requests.Session()\n",
    "\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False\n",
    "    )\n",
    "\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "\n",
    "    return session\n",
    "\n",
    "\n",
    "def reconstruct_abstract(abstract_index):\n",
    "    if not abstract_index:\n",
    "        return \"\"\n",
    "\n",
    "    position_to_word = {}\n",
    "    for word, positions in abstract_index.items():\n",
    "        for pos in positions:\n",
    "            position_to_word[pos] = word\n",
    "\n",
    "    abstract_words = [\n",
    "        position_to_word[pos]\n",
    "        for pos in sorted(position_to_word.keys())\n",
    "    ]\n",
    "\n",
    "    return \" \".join(abstract_words)\n",
    "\n",
    "def fetch_openalex_papers(query, limit=200, per_page=50):\n",
    "    print(f\"Searching OpenAlex for '{query}' ...\")\n",
    "\n",
    "    session = build_session()\n",
    "    papers = []\n",
    "\n",
    "    page = 1\n",
    "    collected = 0\n",
    "\n",
    "    while collected < limit:\n",
    "        params = {\n",
    "            \"search\": query,\n",
    "            \"per-page\": per_page,\n",
    "            \"page\": page,\n",
    "            \"mailto\": \"research@example.com\"  # OpenAlex 官方推荐\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            resp = session.get(BASE_URL, params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[WARN] OpenAlex request failed (page={page}): {e}\")\n",
    "            print(\"       Sleep 5s and skip this page.\")\n",
    "            time.sleep(5)\n",
    "            page += 1\n",
    "            continue\n",
    "\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        for r in results:\n",
    "            if collected >= limit:\n",
    "                break\n",
    "\n",
    "            abstract = reconstruct_abstract(\n",
    "                r.get(\"abstract_inverted_index\", {})\n",
    "            )\n",
    "\n",
    "            paper = {\n",
    "                \"source\": \"openalex\",\n",
    "                \"paper_id\": r.get(\"id\", \"\"),\n",
    "                \"title\": r.get(\"title\", \"\"),\n",
    "                \"abstract\": abstract,\n",
    "                \"abstract_source\": \"openalex\" if abstract else \"\",\n",
    "                \"authors\": [\n",
    "                    a[\"author\"][\"display_name\"]\n",
    "                    for a in r.get(\"authorships\", [])\n",
    "                    if \"author\" in a\n",
    "                ],\n",
    "                \"publish_year\": r.get(\"publication_year\", \"\"),\n",
    "                \"venue\": r.get(\"host_venue\", {}).get(\"display_name\", \"\"),\n",
    "                \"citation_count\": r.get(\"cited_by_count\", 0),\n",
    "                \"fields_of_study\": [\n",
    "                    c[\"display_name\"]\n",
    "                    for c in r.get(\"concepts\", [])\n",
    "                    if c.get(\"level\", 10) <= 1\n",
    "                ],\n",
    "                \"url\": r.get(\"id\", \"\"),\n",
    "            }\n",
    "\n",
    "            papers.append(paper)\n",
    "            collected += 1\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1.5) \n",
    "    return papers\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    topics = [\n",
    "        \"artificial intelligence\",\n",
    "        \"machine learning\",\n",
    "        \"computer vision\",\n",
    "        \"natural language processing\",\n",
    "        \"robotics human computer interaction\"\n",
    "    ]\n",
    "\n",
    "    all_papers = []\n",
    "\n",
    "    for topic in topics:\n",
    "        papers = fetch_openalex_papers(topic, limit=300)\n",
    "        all_papers.extend(papers)\n",
    "        print(f\"Got {len(papers)} papers from OpenAlex for '{topic}'\")\n",
    "        time.sleep(3) \n",
    "\n",
    "    output_file = \"openalex_papers.jsonl\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in all_papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nSaved {len(all_papers)} papers to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f388e0a",
   "metadata": {},
   "source": [
    "#### Merge all collected paper into one .jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300e1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged total 7397 unique records into merged_papers.jsonl.\n"
     ]
    }
   ],
   "source": [
    "# merge_jsonl.py\n",
    "import json\n",
    "\n",
    "files = [\n",
    "    \"arxiv_papers.jsonl\",\n",
    "    \"semantic_scholar_papers.jsonl\",\n",
    "    \"openalex_papers.jsonl\"\n",
    "]\n",
    "\n",
    "merged_file = \"merged_papers.jsonl\"\n",
    "seen_ids = set()\n",
    "count = 0\n",
    "\n",
    "with open(merged_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as fin:\n",
    "            for line in fin:\n",
    "                data = json.loads(line.strip())\n",
    "                pid = data.get(\"paper_id\") or data.get(\"title\")\n",
    "                if pid not in seen_ids:\n",
    "                    seen_ids.add(pid)\n",
    "                    fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                    count += 1\n",
    "\n",
    "print(f\"Merged total {count} unique records into {merged_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
