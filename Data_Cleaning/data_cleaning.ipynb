{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566c36a4",
   "metadata": {},
   "source": [
    "### Deep Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83562a0c",
   "metadata": {},
   "source": [
    "#### Deduplication Steps\n",
    "\n",
    "##### Step 1 - ID-based Deduplication\n",
    "- **Process**: Remove duplicate papers based on exact `paper_id` matching\n",
    "- **Rules**:\n",
    "  - Papers with identical `paper_id` are considered duplicates\n",
    "  - Only one copy of each unique `paper_id` is retained\n",
    "  - Papers without `paper_id` are temporarily preserved for further processing\n",
    "\n",
    "##### Step 2 - Title Hash Deduplication\n",
    "- **Process**: Eliminate papers with identical titles using MD5 hashing\n",
    "- **Rules**:\n",
    "  - Paper titles are normalized (lowercased, trimmed) before hashing\n",
    "  - MD5 hash is computed for each normalized title\n",
    "  - Papers with identical title hashes are considered duplicates\n",
    "  - Only one paper per unique title hash is retained\n",
    "\n",
    "##### Step 3 - Title Similarity Deduplication\n",
    "- **Process**: Remove papers with highly similar titles using Jaccard similarity\n",
    "- **Rules**:\n",
    "  - Titles are tokenized into words after normalization\n",
    "  - Jaccard similarity is calculated between title word sets\n",
    "  - Papers with similarity score ≥ 0.9 are considered potential duplicates\n",
    "  - Priority rules for retaining duplicates:\n",
    "    1. Keep the paper with more recent `publish_year`\n",
    "    2. If `publish_year` is identical or unavailable, retain the first occurrence\n",
    "- **Jaccard Similarity Formula**:  \n",
    "  `similarity = |Title_A ∩ Title_B| / |Title_A ∪ Title_B|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6530a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original paper count: 7397\n",
      "Papers with None title: 1\n",
      "Step 1 - After ID deduplication: 7397 papers\n",
      "Step 2 - After title hash deduplication: 6259 papers\n",
      "Step 3 - After title similarity deduplication: 6242 papers\n",
      "Final deduplicated paper count: 6242\n"
     ]
    }
   ],
   "source": [
    "# strict_deduplication.py\n",
    "import json\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    papers = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            papers.append(json.loads(line))\n",
    "    return papers\n",
    "\n",
    "def save_jsonl(papers, file_path):\n",
    "    \"\"\"Save JSONL file\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def safe_get_title(paper):\n",
    "    \"\"\"Safely get and clean title from paper dictionary\"\"\"\n",
    "    title = paper.get(\"title\")\n",
    "    if title is None:\n",
    "        return \"\"\n",
    "    # Ensure it's a string\n",
    "    if not isinstance(title, str):\n",
    "        title = str(title)\n",
    "    return title.strip().lower()\n",
    "\n",
    "# 1. Exact ID Deduplication\n",
    "def dedup_by_id(papers):\n",
    "    seen_ids = set()\n",
    "    unique_papers = []\n",
    "    for paper in papers:\n",
    "        pid = paper.get(\"paper_id\")\n",
    "        if pid and pid not in seen_ids:\n",
    "            seen_ids.add(pid)\n",
    "            unique_papers.append(paper)\n",
    "        elif not pid:\n",
    "            unique_papers.append(paper)\n",
    "    print(f\"Step 1 - After ID deduplication: {len(unique_papers)} papers\")\n",
    "    return unique_papers\n",
    "\n",
    "# 2. Title Hash Deduplication (exact match)\n",
    "def dedup_by_title_hash(papers):\n",
    "    seen_hashes = set()\n",
    "    unique_papers = []\n",
    "    for paper in papers:\n",
    "        title = safe_get_title(paper)\n",
    "        if not title:  # Skip empty titles or keep them? For now, we keep them\n",
    "            unique_papers.append(paper)\n",
    "            continue\n",
    "        h = hashlib.md5(title.encode(\"utf-8\")).hexdigest()\n",
    "        if h not in seen_hashes:\n",
    "            seen_hashes.add(h)\n",
    "            unique_papers.append(paper)\n",
    "    print(f\"Step 2 - After title hash deduplication: {len(unique_papers)} papers\")\n",
    "    return unique_papers\n",
    "\n",
    "# 3. Title Similarity Deduplication (intersection ratio threshold)\n",
    "def dedup_by_title_similarity(papers, threshold=0.9):\n",
    "    to_remove = set()\n",
    "    n = len(papers)\n",
    "    for i in range(n):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "        title_i = safe_get_title(papers[i])\n",
    "        if not title_i:\n",
    "            continue  # Skip papers without titles for similarity comparison\n",
    "        title_i_words = title_i.split()\n",
    "        set_i = set(title_i_words)\n",
    "        if not set_i:\n",
    "            continue\n",
    "        for j in range(i+1, n):\n",
    "            if j in to_remove:\n",
    "                continue\n",
    "            title_j = safe_get_title(papers[j])\n",
    "            if not title_j:\n",
    "                continue\n",
    "            title_j_words = title_j.split()\n",
    "            set_j = set(title_j_words)\n",
    "            if not set_j:\n",
    "                continue\n",
    "            similarity = len(set_i & set_j) / len(set_i | set_j)\n",
    "            if similarity >= threshold:\n",
    "                # Keep the one with more recent publish_date\n",
    "                # Try different possible date fields\n",
    "                date_i = papers[i].get(\"publish_year\") or papers[i].get(\"publish_date\") or 0\n",
    "                date_j = papers[j].get(\"publish_year\") or papers[j].get(\"publish_date\") or 0\n",
    "                \n",
    "                # Convert to integer year if possible\n",
    "                if isinstance(date_i, str):\n",
    "                    try:\n",
    "                        # Try to extract year from string\n",
    "                        import re\n",
    "                        year_match = re.search(r'\\d{4}', date_i)\n",
    "                        date_i = int(year_match.group()) if year_match else 0\n",
    "                    except:\n",
    "                        date_i = 0\n",
    "                \n",
    "                if isinstance(date_j, str):\n",
    "                    try:\n",
    "                        import re\n",
    "                        year_match = re.search(r'\\d{4}', date_j)\n",
    "                        date_j = int(year_match.group()) if year_match else 0\n",
    "                    except:\n",
    "                        date_j = 0\n",
    "                \n",
    "                if date_i >= date_j:\n",
    "                    to_remove.add(j)\n",
    "                else:\n",
    "                    to_remove.add(i)\n",
    "                    break  # Break inner loop since i is being removed\n",
    "    unique_papers = [papers[k] for k in range(n) if k not in to_remove]\n",
    "    print(f\"Step 3 - After title similarity deduplication: {len(unique_papers)} papers\")\n",
    "    return unique_papers\n",
    "\n",
    "# Main Workflow\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"../Data_Collection/merged_papers.jsonl\"\n",
    "    output_file = \"merged_papers_dedup.jsonl\"\n",
    "\n",
    "    papers = load_jsonl(input_file)\n",
    "    print(f\"Original paper count: {len(papers)}\")\n",
    "    \n",
    "    # Check for papers with None titles\n",
    "    none_titles = sum(1 for p in papers if p.get(\"title\") is None)\n",
    "    print(f\"Papers with None title: {none_titles}\")\n",
    "    \n",
    "    # Print sample of problematic papers\n",
    "    for i, p in enumerate(papers[:10]):\n",
    "        if p.get(\"title\") is None:\n",
    "            print(f\"Paper {i} has None title: {p.get('paper_id', 'No ID')}\")\n",
    "\n",
    "    # Perform deduplication in sequence\n",
    "    papers = dedup_by_id(papers)\n",
    "    papers = dedup_by_title_hash(papers)\n",
    "    papers = dedup_by_title_similarity(papers, threshold=0.9)\n",
    "\n",
    "    save_jsonl(papers, output_file)\n",
    "    print(f\"Final deduplicated paper count: {len(papers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98fa32",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82129307",
   "metadata": {},
   "source": [
    "Perform deep text cleaning on the title and abstract (remove LaTeX, HTML, special characters, and standardize Spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed successfully.\n",
      "Original count: 6242 papers\n",
      "Output saved to: papers_cleaned_text.jsonl\n"
     ]
    }
   ],
   "source": [
    "# text_cleaning.py\n",
    "import json\n",
    "import re\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    papers = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            papers.append(json.loads(line))\n",
    "    return papers\n",
    "\n",
    "def save_jsonl(papers, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def clean_text(papers):\n",
    "    for paper in papers:\n",
    "        # Clean title\n",
    "        if 'title' in paper and paper['title'] is not None:\n",
    "            # Normalize whitespace in title\n",
    "            paper['title'] = re.sub(r'\\s+', ' ', str(paper['title'])).strip()\n",
    "        \n",
    "        # Clean abstract\n",
    "        if 'abstract' in paper and paper['abstract'] is not None:\n",
    "            abstract = str(paper['abstract'])\n",
    "            \n",
    "            # Remove LaTeX inline math expressions: $...$\n",
    "            abstract = re.sub(r'\\$.*?\\$', '', abstract)\n",
    "            \n",
    "            # Remove LaTeX display math expressions: $$...$$\n",
    "            abstract = re.sub(r'\\$\\$.*?\\$\\$', '', abstract)\n",
    "            \n",
    "            # Remove LaTeX commands with braces: \\command{...}\n",
    "            abstract = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', abstract)\n",
    "            \n",
    "            # Remove standalone LaTeX commands\n",
    "            abstract = re.sub(r'\\\\[a-zA-Z]+\\s*', '', abstract)\n",
    "            \n",
    "            # Remove HTML entities\n",
    "            abstract = re.sub(r'&[a-zA-Z]+;', '', abstract)\n",
    "            \n",
    "            # Remove non-ASCII characters (optional, can be adjusted)\n",
    "            abstract = re.sub(r'[^\\x00-\\x7F]+', ' ', abstract)\n",
    "            \n",
    "            # Normalize whitespace\n",
    "            abstract = re.sub(r'\\s+', ' ', abstract).strip()\n",
    "            \n",
    "            paper['abstract'] = abstract\n",
    "            paper['abstract_source'] = \"original_cleaned\"\n",
    "        \n",
    "        # Clean authors list\n",
    "        if 'authors' in paper and isinstance(paper['authors'], list):\n",
    "            cleaned_authors = []\n",
    "            for author in paper['authors']:\n",
    "                if author is not None:\n",
    "                    # Convert to string and strip whitespace\n",
    "                    cleaned_author = str(author).strip()\n",
    "                    if cleaned_author:  # Only add non-empty strings\n",
    "                        cleaned_authors.append(cleaned_author)\n",
    "            paper['authors'] = cleaned_authors\n",
    "    \n",
    "    return papers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input and output file paths\n",
    "    input_file = \"merged_papers_dedup.jsonl\"\n",
    "    output_file = \"papers_cleaned_text.jsonl\"\n",
    "    \n",
    "    papers = load_jsonl(input_file)\n",
    "    \n",
    "    papers = clean_text(papers)\n",
    "    \n",
    "    save_jsonl(papers, output_file)\n",
    "    \n",
    "    print(f\"Text cleaning completed successfully.\")\n",
    "    print(f\"Original count: {len(papers)} papers\")\n",
    "    print(f\"Output saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca732c",
   "metadata": {},
   "source": [
    "#### Filter out articles with exceptionally low citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1b443",
   "metadata": {},
   "source": [
    "Remove papers with abnormally low citation numbers (optional threshold) to ensure data quality.    \n",
    "Since our project focuses on newly published papers, especially on timeliness, some of the relatively recent papers collected were not cited.    \n",
    "Therefore, we have kept all of them. If there are specific requirements regarding the number of citations, we can make the necessary adjustments accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation filter: 6242 -> 6242\n"
     ]
    }
   ],
   "source": [
    "# citation_filter.py\n",
    "import json\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    papers = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            papers.append(json.loads(line))\n",
    "    return papers\n",
    "\n",
    "def save_jsonl(papers, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def filter_by_citations(papers, min_citations=0):\n",
    "    filtered = [p for p in papers if p.get(\"citation_count\", 0) >= min_citations]\n",
    "    print(f\"Citation filter: {len(papers)} -> {len(filtered)}\")\n",
    "    return filtered\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"papers_cleaned_text.jsonl\"\n",
    "    output_file = \"papers_filtered_citations.jsonl\"\n",
    "\n",
    "    papers = load_jsonl(input_file)\n",
    "    papers = filter_by_citations(papers, min_citations=0)\n",
    "    save_jsonl(papers, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfce4f1",
   "metadata": {},
   "source": [
    "#### Clean other content like field_of_study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0876d",
   "metadata": {},
   "source": [
    "Unify the capitalization of fields_of_study and remove duplicate null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f5dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields_of_study_clean.py\n",
    "import json\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    papers = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            papers.append(json.loads(line))\n",
    "    return papers\n",
    "\n",
    "def save_jsonl(papers, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def clean_fields_of_study(papers):\n",
    "    for paper in papers:\n",
    "        if 'fields_of_study' in paper and isinstance(paper['fields_of_study'], list):\n",
    "            cleaned = list({f.strip().title() for f in paper['fields_of_study'] if f.strip()})\n",
    "            paper['fields_of_study'] = cleaned\n",
    "    return papers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"papers_filtered_citations.jsonl\"\n",
    "    output_file = \"papers_cleaned_fields.jsonl\"\n",
    "\n",
    "    papers = load_jsonl(input_file)\n",
    "    papers = clean_fields_of_study(papers)\n",
    "    save_jsonl(papers, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445e8e3",
   "metadata": {},
   "source": [
    "#### Do the format alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32d5ad",
   "metadata": {},
   "source": [
    "Align all the fields into a uniform output format to facilitate subsequent data processing and training, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b0a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format alignment done. Papers count: 6242\n"
     ]
    }
   ],
   "source": [
    "# format_alignment.py\n",
    "import json\n",
    "\n",
    "REQUIRED_FIELDS = [\n",
    "    \"source\", \"paper_id\", \"title\", \"abstract\", \"abstract_source\",\n",
    "    \"authors\", \"publish_year\", \"venue\", \"citation_count\",\n",
    "    \"fields_of_study\", \"url\"\n",
    "]\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    papers = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            papers.append(json.loads(line))\n",
    "    return papers\n",
    "\n",
    "def save_jsonl(papers, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paper in papers:\n",
    "            f.write(json.dumps(paper, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def align_format(papers):\n",
    "    aligned = []\n",
    "    for paper in papers:\n",
    "        new_paper = {}\n",
    "        for field in REQUIRED_FIELDS:\n",
    "            new_paper[field] = paper.get(field, \"\" if field != \"authors\" and field != \"fields_of_study\" else [])\n",
    "        aligned.append(new_paper)\n",
    "    return aligned\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"papers_cleaned_fields.jsonl\"\n",
    "    output_file = \"papers_final_aligned.jsonl\"\n",
    "\n",
    "    papers = load_jsonl(input_file)\n",
    "    papers = align_format(papers)\n",
    "    save_jsonl(papers, output_file)\n",
    "    print(f\"Format alignment done. Papers count: {len(papers)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
