{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dec3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAG系统评估框架\n",
      "============================================================\n",
      "\n",
      "1. 加载论文数据...\n",
      "✓ 从 cleaned_papers.jsonl 成功加载 22 篇论文\n",
      "\n",
      "数据集统计:\n",
      "----------------------------------------\n",
      "论文总数: 22\n",
      "主题数量: 3\n",
      "\n",
      "质量等级分布:\n",
      "  high: 21 篇 (95.5%)\n",
      "  medium: 1 篇 (4.5%)\n",
      "作者总数: 128\n",
      "最新论文日期: 2025-11-25\n",
      "\n",
      "2. 初始化嵌入模型...\n",
      "✓ 使用本地嵌入模型\n",
      "  模型: all-MiniLM-L6-v2\n",
      "  维度: 384\n",
      "\n",
      "3. 构建向量数据库...\n",
      "开始处理 22 篇论文...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "添加论文: 100%|██████████| 22/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成嵌入向量...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 向量库构建完成\n",
      "  文档数量: 22\n",
      "  嵌入维度: 384\n",
      "  文档总数: 22\n",
      "  主题数量: 3\n",
      "\n",
      "4. 初始化RAG系统...\n",
      "\n",
      "5. 运行评估...\n",
      "开始评估 3 个问题...\n",
      "\n",
      "评估问题: 什么是医学影像中的开放词汇目标检测？它解决了什么问题？\n",
      "警告：使用模拟API响应（请设置正确的API密钥）\n",
      "警告：使用模拟API响应（请设置正确的API密钥）\n",
      "  自动评分: RAG=0.80, 基线=0.80\n",
      "  API评分: RAG=3.50, 基线=3.50\n",
      "  检索文档: 5篇\n",
      "\n",
      "评估问题: 最近在视频运动编辑方面有哪些新的技术突破？\n",
      "警告：使用模拟API响应（请设置正确的API密钥）\n",
      "警告：使用模拟API响应（请设置正确的API密钥）\n",
      "  自动评分: RAG=0.60, 基线=0.60\n",
      "  API评分: RAG=3.50, 基线=3.50\n",
      "  检索文档: 5篇\n",
      "\n",
      "评估问题: 对比学习在计算机视觉中有哪些应用？\n",
      "警告：使用模拟API响应（请设置正确的API密钥）\n",
      "警告：使用模拟API响应（请设置正确的API密钥）\n",
      "  自动评分: RAG=0.10, 基线=0.10\n",
      "  API评分: RAG=3.50, 基线=3.50\n",
      "  检索文档: 5篇\n",
      "\n",
      "✓ 完成所有评估\n",
      "\n",
      "6. 生成报告...\n",
      "\n",
      "7. 保存结果...\n",
      "✓ 评估报告已保存到 rag_evaluation_report.json\n",
      "✓ 详细结果已保存到 evaluation_results.csv\n",
      "\n",
      "8. 创建可视化图表...\n",
      "✓ 可视化图表已保存到 ./evaluation_visualization.png\n",
      "\n",
      "9. 评估报告:\n",
      "\n",
      "============================================================\n",
      "RAG系统评估报告\n",
      "============================================================\n",
      "\n",
      "总体统计:\n",
      "  评估问题总数: 3\n",
      "  平均自动评估分数 (RAG): 0.502\n",
      "  平均自动评估分数 (基线): 0.500\n",
      "  平均API评估分数 (RAG): 3.500\n",
      "  平均API评估分数 (基线): 3.500\n",
      "  平均自动评估改进: 0.002\n",
      "  平均API评估改进: 0.000\n",
      "  自动评估改进率: 100.00%\n",
      "  API评估改进率: 0.00%\n",
      "  自动评估与API评估相关性: nan\n",
      "  总token使用量: 0\n",
      "\n",
      "详细结果:\n",
      "\n",
      "问题 1: 什么是医学影像中的开放词汇目标检测？它解决了什么问题？...\n",
      "  自动评分: RAG=0.801, 基线=0.800\n",
      "  API评分: RAG=3.500, 基线=3.500\n",
      "  改进: 自动=0.001, API=0.000\n",
      "  检索文档: 5篇\n",
      "  相关文档:\n",
      "    - Dimension-counting bounds for equi-isoclinic subsp... (相似度: 0.018)\n",
      "    - Fighting AI with AI: Leveraging Foundation Models ... (相似度: 0.017)\n",
      "\n",
      "问题 2: 最近在视频运动编辑方面有哪些新的技术突破？...\n",
      "  自动评分: RAG=0.602, 基线=0.600\n",
      "  API评分: RAG=3.500, 基线=3.500\n",
      "  改进: 自动=0.002, API=0.000\n",
      "  检索文档: 5篇\n",
      "  相关文档:\n",
      "    - Image2Gcode: Image-to-G-code Generation for Additi... (相似度: 0.037)\n",
      "    - Evaluating the Performance of Deep Learning Models... (相似度: 0.023)\n",
      "\n",
      "问题 3: 对比学习在计算机视觉中有哪些应用？...\n",
      "  自动评分: RAG=0.102, 基线=0.100\n",
      "  API评分: RAG=3.500, 基线=3.500\n",
      "  改进: 自动=0.002, API=0.000\n",
      "  检索文档: 5篇\n",
      "  相关文档:\n",
      "    - Evaluating the Performance of Deep Learning Models... (相似度: 0.045)\n",
      "    - Unleashing the Power of Vision-Language Models for... (相似度: 0.034)\n",
      "\n",
      "10. 结果DataFrame:\n",
      "   question_id  auto_score_rag  auto_score_baseline  api_score_rag  api_score_baseline  auto_improvement  api_improvement\n",
      "0            1        0.801326                  0.8            3.5                 3.5          0.001326              0.0\n",
      "1            2        0.601863                  0.6            3.5                 3.5          0.001863              0.0\n",
      "2            3        0.102384                  0.1            3.5                 3.5          0.002384              0.0\n",
      "\n",
      "============================================================\n",
      "评估完成！\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # RAG系统性能评估框架\n",
    "# \n",
    "# 基于提供的论文数据结构，实现完整的RAG系统评估\n",
    "\n",
    "# %%\n",
    "# 首先安装必要的库（如果还没安装）\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.26.4 sentence-transformers scikit-learn pandas matplotlib seaborn tqdm --quiet\n",
    "\n",
    "# 设置Matplotlib后端，避免matplotlib_inline冲突\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 使用非交互式后端\n",
    "\n",
    "# 导入其他库\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hashlib\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import threading\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 尝试导入可视化库，如果失败则跳过\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    VISUALIZATION_AVAILABLE = True\n",
    "    # 设置中文显示\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "except ImportError:\n",
    "    VISUALIZATION_AVAILABLE = False\n",
    "    print(\"警告：无法导入matplotlib/seaborn，可视化功能将禁用\")\n",
    "\n",
    "# %%\n",
    "# ==================== 配置区域 ====================\n",
    "class Config:\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    # API配置\n",
    "    DEEPSEEK_API_KEY = \"sk-79990d599cd74bc0a56f6ca2f200a621\"  # 替换为你的API密钥\n",
    "    API_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "    \n",
    "    # 向量数据库配置\n",
    "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # 更小的模型，减少内存使用\n",
    "    TOP_K_RETRIEVAL = 5  # 检索的文档数量\n",
    "    SIMILARITY_THRESHOLD = 0.7  # 相似度阈值\n",
    "    \n",
    "    # 评估配置\n",
    "    MAX_ANSWER_LENGTH = 1500\n",
    "    TEMPERATURE = 0.1\n",
    "    \n",
    "    # 文件路径\n",
    "    DATA_FILE = \"cleaned_papers.jsonl\"\n",
    "    REPORT_FILE = \"rag_evaluation_report.json\"\n",
    "    EVALUATION_CSV = \"evaluation_results.csv\"\n",
    "    \n",
    "    # 测试问题\n",
    "    TEST_QUESTIONS = [\n",
    "        {\n",
    "            'question': '什么是医学影像中的开放词汇目标检测？它解决了什么问题？',\n",
    "            'topic': 'computer vision, medical imaging',\n",
    "            'difficulty': 'medium'\n",
    "        },\n",
    "        {\n",
    "            'question': '最近在视频运动编辑方面有哪些新的技术突破？',\n",
    "            'topic': 'computer vision, video editing',\n",
    "            'difficulty': 'medium'\n",
    "        },\n",
    "        {\n",
    "            'question': '对比学习在计算机视觉中有哪些应用？',\n",
    "            'topic': 'machine learning, computer vision',\n",
    "            'difficulty': 'medium'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# %%\n",
    "# ==================== 1. 数据加载和预处理 ====================\n",
    "\n",
    "@dataclass\n",
    "class Paper:\n",
    "    \"\"\"论文数据结构（适配您的JSONL格式）\"\"\"\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    authors: List[str]\n",
    "    first_author: str\n",
    "    topic: str\n",
    "    categories: List[str]\n",
    "    publish_date: str\n",
    "    url: str\n",
    "    embedding_text: str\n",
    "    quality_scores: Dict[str, float]\n",
    "    quality_tier: str\n",
    "    basic_keywords: List[str]\n",
    "    domain_keywords: List[str]\n",
    "    update_date: str = \"\"  # 添加update_date字段，设置默认值\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"初始化后处理\"\"\"\n",
    "        # 确保所有字段都是正确的类型\n",
    "        if isinstance(self.authors, str):\n",
    "            self.authors = [a.strip() for a in self.authors.split(',')]\n",
    "        elif not isinstance(self.authors, list):\n",
    "            self.authors = []\n",
    "        \n",
    "        if isinstance(self.categories, str):\n",
    "            self.categories = [c.strip() for c in self.categories.split(',')]\n",
    "        elif not isinstance(self.categories, list):\n",
    "            self.categories = []\n",
    "            \n",
    "        if isinstance(self.basic_keywords, str):\n",
    "            self.basic_keywords = [k.strip() for k in self.basic_keywords.split(',')]\n",
    "        elif not isinstance(self.basic_keywords, list):\n",
    "            self.basic_keywords = []\n",
    "            \n",
    "        if isinstance(self.domain_keywords, str):\n",
    "            self.domain_keywords = [k.strip() for k in self.domain_keywords.split(',')]\n",
    "        elif not isinstance(self.domain_keywords, list):\n",
    "            self.domain_keywords = []\n",
    "    \n",
    "    def to_text(self) -> str:\n",
    "        \"\"\"将论文转换为文本用于嵌入\"\"\"\n",
    "        # 使用embedding_text字段（您的数据中已有）\n",
    "        if self.embedding_text:\n",
    "            return self.embedding_text\n",
    "        \n",
    "        # 如果embedding_text不存在，构建文本\n",
    "        text_parts = [\n",
    "            f\"Paper Title: {self.title}\",\n",
    "            f\"Research Topic: {self.topic}\",\n",
    "            f\"Authors: {', '.join(self.authors[:3])}\",\n",
    "            f\"Abstract: {self.abstract[:500]}\",\n",
    "        ]\n",
    "        \n",
    "        if self.categories:\n",
    "            text_parts.append(f\"Categories: {', '.join(self.categories)}\")\n",
    "        \n",
    "        if self.basic_keywords:\n",
    "            text_parts.append(f\"Keywords: {', '.join(self.basic_keywords[:5])}\")\n",
    "        \n",
    "        return \"\\n\".join(text_parts)\n",
    "    \n",
    "    def get_quality_score(self) -> float:\n",
    "        \"\"\"获取论文质量分数\"\"\"\n",
    "        if self.quality_scores and 'overall_quality_score' in self.quality_scores:\n",
    "            return self.quality_scores['overall_quality_score']\n",
    "        return 0.5\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'Paper':\n",
    "        \"\"\"从字典创建Paper对象（适配您的JSONL格式）\"\"\"\n",
    "        # 提供默认值以避免KeyError\n",
    "        defaults = {\n",
    "            'paper_id': '',\n",
    "            'title': '',\n",
    "            'abstract': '',\n",
    "            'authors': [],\n",
    "            'first_author': '',\n",
    "            'topic': '',\n",
    "            'categories': [],\n",
    "            'publish_date': '',\n",
    "            'url': '',\n",
    "            'embedding_text': '',\n",
    "            'quality_scores': {},\n",
    "            'quality_tier': 'medium',\n",
    "            'basic_keywords': [],\n",
    "            'domain_keywords': [],\n",
    "            'update_date': ''  # 添加update_date默认值\n",
    "        }\n",
    "        \n",
    "        # 合并数据与默认值，只保留Paper类定义的字段\n",
    "        merged_data = {**defaults, **{k: v for k, v in data.items() if k in defaults}}\n",
    "        \n",
    "        return cls(**merged_data)\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"数据加载器\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_from_jsonl(file_path: str) -> List[Paper]:\n",
    "        \"\"\"从JSONL文件加载论文数据（适配您的格式）\"\"\"\n",
    "        papers = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            data = json.loads(line.strip())\n",
    "                            paper = Paper.from_dict(data)\n",
    "                            papers.append(paper)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"第{line_num}行JSON解析错误: {e}\")\n",
    "                            continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"第{line_num}行数据转换错误: {e}\")\n",
    "                            continue\n",
    "            \n",
    "            print(f\"✓ 从 {file_path} 成功加载 {len(papers)} 篇论文\")\n",
    "            \n",
    "            # 打印统计信息\n",
    "            if papers:\n",
    "                DataLoader._print_statistics(papers)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ 文件 {file_path} 不存在\")\n",
    "            # 创建一些示例数据用于测试\n",
    "            papers = DataLoader.create_sample_data()\n",
    "        \n",
    "        return papers\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sample_data() -> List[Paper]:\n",
    "        \"\"\"创建示例数据\"\"\"\n",
    "        print(\"创建示例数据用于测试...\")\n",
    "        \n",
    "        sample_papers = [\n",
    "            Paper(\n",
    "                paper_id=\"2511.20650\",\n",
    "                title=\"MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities\",\n",
    "                abstract=\"Traditional object detection models in medical imaging operate within a closed-set paradigm...\",\n",
    "                authors=[\"Tooba Tehreem Sheikh\", \"Jean Lahoud\", \"Rao Muhammad Anwer\"],\n",
    "                first_author=\"Tooba Tehreem Sheikh\",\n",
    "                topic=\"artificial intelligence\",\n",
    "                categories=[\"cs.CV\", \"cs.AI\"],\n",
    "                publish_date=\"2025-11-25\",\n",
    "                url=\"http://arxiv.org/abs/2511.20650\",\n",
    "                embedding_text=\"Paper Title: MedROV: Towards Real-Time Open-Vocabulary Detection...\",\n",
    "                quality_scores={\"overall_quality_score\": 0.975},\n",
    "                quality_tier=\"high\",\n",
    "                basic_keywords=[\"detection\", \"medical\", \"imaging\"],\n",
    "                domain_keywords=[\"object detection\", \"medical imaging\"]\n",
    "            ),\n",
    "            Paper(\n",
    "                paper_id=\"2511.20640\",\n",
    "                title=\"MotionV2V: Editing Motion in a Video\",\n",
    "                abstract=\"While generative video models have achieved remarkable fidelity and consistency...\",\n",
    "                authors=[\"Ryan Burgert\", \"Charles Herrmann\", \"Forrester Cole\"],\n",
    "                first_author=\"Ryan Burgert\",\n",
    "                topic=\"artificial intelligence\",\n",
    "                categories=[\"cs.CV\", \"cs.AI\", \"cs.GR\"],\n",
    "                publish_date=\"2025-11-25\",\n",
    "                url=\"http://arxiv.org/abs/2511.20640\",\n",
    "                embedding_text=\"Paper Title: MotionV2V: Editing Motion in a Video...\",\n",
    "                quality_scores={\"overall_quality_score\": 0.9},\n",
    "                quality_tier=\"high\",\n",
    "                basic_keywords=[\"video\", \"motion\", \"editing\"],\n",
    "                domain_keywords=[\"video editing\", \"motion control\"]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"✓ 创建了 {len(sample_papers)} 篇示例论文\")\n",
    "        return sample_papers\n",
    "    \n",
    "    @staticmethod\n",
    "    def _print_statistics(papers: List[Paper]):\n",
    "        \"\"\"打印数据集统计信息\"\"\"\n",
    "        print(\"\\n数据集统计:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"论文总数: {len(papers)}\")\n",
    "        \n",
    "        # 主题分布\n",
    "        topics = [p.topic for p in papers]\n",
    "        unique_topics = set(topics)\n",
    "        print(f\"主题数量: {len(unique_topics)}\")\n",
    "        \n",
    "        # 质量分布\n",
    "        quality_tiers = [p.quality_tier for p in papers]\n",
    "        tier_counts = {tier: quality_tiers.count(tier) for tier in set(quality_tiers)}\n",
    "        print(\"\\n质量等级分布:\")\n",
    "        for tier, count in tier_counts.items():\n",
    "            print(f\"  {tier}: {count} 篇 ({count/len(papers)*100:.1f}%)\")\n",
    "        \n",
    "        # 作者统计\n",
    "        all_authors = [author for p in papers for author in p.authors]\n",
    "        unique_authors = set(all_authors)\n",
    "        print(f\"作者总数: {len(unique_authors)}\")\n",
    "        \n",
    "        # 最新论文日期\n",
    "        dates = [p.publish_date for p in papers if p.publish_date]\n",
    "        if dates:\n",
    "            print(f\"最新论文日期: {max(dates)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_topic_distribution(papers: List[Paper]) -> pd.DataFrame:\n",
    "        \"\"\"分析主题分布\"\"\"\n",
    "        topic_counts = {}\n",
    "        for paper in papers:\n",
    "            topic = paper.topic\n",
    "            topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
    "        \n",
    "        # 转换为DataFrame\n",
    "        df = pd.DataFrame(list(topic_counts.items()), columns=['topic', 'count'])\n",
    "        df = df.sort_values('count', ascending=False)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# %%\n",
    "# ==================== 2. 嵌入和向量数据库 ====================\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"嵌入模型封装\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = None, use_api: bool = False, api_key: str = None):\n",
    "        \"\"\"\n",
    "        初始化嵌入模型\n",
    "        \n",
    "        Args:\n",
    "            model_name: 本地模型名称\n",
    "            use_api: 是否使用API\n",
    "            api_key: API密钥\n",
    "        \"\"\"\n",
    "        self.use_api = use_api\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        if use_api and api_key:\n",
    "            self.model = None\n",
    "            print(\"✓ 使用DeepSeek API进行嵌入\")\n",
    "        else:\n",
    "            print(f\"✓ 使用本地嵌入模型\")\n",
    "            try:\n",
    "                from sentence_transformers import SentenceTransformer\n",
    "                model_name = model_name or config.EMBEDDING_MODEL\n",
    "                self.model = SentenceTransformer(model_name)\n",
    "                print(f\"  模型: {model_name}\")\n",
    "                print(f\"  维度: {self.model.get_sentence_embedding_dimension()}\")\n",
    "            except ImportError:\n",
    "                print(\"警告：无法导入sentence_transformers，使用简单嵌入\")\n",
    "                self.model = None\n",
    "    \n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"生成嵌入向量\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        if not texts:\n",
    "            return np.array([])\n",
    "        \n",
    "        if self.use_api and self.api_key:\n",
    "            return self._embed_api(texts)\n",
    "        elif self.model:\n",
    "            return self._embed_local(texts)\n",
    "        else:\n",
    "            # 回退到简单词向量\n",
    "            return self._embed_simple(texts)\n",
    "    \n",
    "    def _embed_local(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"使用本地模型嵌入\"\"\"\n",
    "        try:\n",
    "            # 批量处理以避免内存问题\n",
    "            batch_size = 32\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                batch_embeddings = self.model.encode(batch, show_progress_bar=False)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            \n",
    "            return np.vstack(embeddings) if embeddings else np.array([])\n",
    "        except Exception as e:\n",
    "            print(f\"本地嵌入失败: {e}\")\n",
    "            return self._embed_simple(texts)\n",
    "    \n",
    "    def _embed_api(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"使用DeepSeek API嵌入\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            # 只处理前几个文本以避免API限制\n",
    "            texts = texts[:10] if len(texts) > 10 else texts\n",
    "            \n",
    "            data = {\n",
    "                \"model\": \"text-embedding-3-small\",\n",
    "                \"input\": texts,\n",
    "                \"encoding_format\": \"float\"\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{config.API_BASE_URL}/embeddings\",\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            embeddings = [item[\"embedding\"] for item in result[\"data\"]]\n",
    "            return np.array(embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"API嵌入失败: {e}\")\n",
    "            # 回退到本地模型\n",
    "            if self.model:\n",
    "                return self._embed_local(texts)\n",
    "            else:\n",
    "                return self._embed_simple(texts)\n",
    "    \n",
    "    def _embed_simple(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"简单的词向量嵌入（回退方案）\"\"\"\n",
    "        print(\"使用简单嵌入方法...\")\n",
    "        # 创建简单的词频向量\n",
    "        vocab = {}\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            vector = np.zeros(len(vocab))\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word in vocab:\n",
    "                    vector[vocab[word]] += 1\n",
    "            # 归一化\n",
    "            if np.linalg.norm(vector) > 0:\n",
    "                vector = vector / np.linalg.norm(vector)\n",
    "            embeddings.append(vector)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def get_dimension(self) -> int:\n",
    "        \"\"\"获取嵌入维度\"\"\"\n",
    "        if self.use_api:\n",
    "            return 1536  # DeepSeek text-embedding-3-small的维度\n",
    "        elif self.model:\n",
    "            return self.model.get_sentence_embedding_dimension()\n",
    "        return 100  # 简单嵌入的维度\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"向量数据库\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder: EmbeddingModel):\n",
    "        self.embedder = embedder\n",
    "        self.documents: List[str] = []\n",
    "        self.metadata: List[Dict] = []\n",
    "        self.embeddings: np.ndarray = None\n",
    "    \n",
    "    def add_papers(self, papers: List[Paper], use_embedding_text: bool = True):\n",
    "        \"\"\"添加论文到向量库\"\"\"\n",
    "        print(f\"开始处理 {len(papers)} 篇论文...\")\n",
    "        \n",
    "        for paper in tqdm(papers, desc=\"添加论文\"):\n",
    "            # 转换为文本\n",
    "            if use_embedding_text and paper.embedding_text:\n",
    "                doc_text = paper.embedding_text\n",
    "            else:\n",
    "                doc_text = paper.to_text()\n",
    "            \n",
    "            # 存储文档和元数据\n",
    "            self.documents.append(doc_text)\n",
    "            self.metadata.append({\n",
    "                'paper_id': paper.paper_id,\n",
    "                'title': paper.title,\n",
    "                'authors': paper.authors,\n",
    "                'first_author': paper.first_author,\n",
    "                'topic': paper.topic,\n",
    "                'categories': paper.categories,\n",
    "                'publish_date': paper.publish_date,\n",
    "                'quality_score': paper.get_quality_score(),\n",
    "                'quality_tier': paper.quality_tier\n",
    "            })\n",
    "        \n",
    "        # 生成嵌入向量\n",
    "        print(\"生成嵌入向量...\")\n",
    "        self.embeddings = self.embedder.embed(self.documents)\n",
    "        \n",
    "        print(f\"✓ 向量库构建完成\")\n",
    "        print(f\"  文档数量: {len(self.documents)}\")\n",
    "        print(f\"  嵌入维度: {self.embeddings.shape[1]}\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = None, threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"语义搜索\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = config.TOP_K_RETRIEVAL\n",
    "        if threshold is None:\n",
    "            threshold = config.SIMILARITY_THRESHOLD\n",
    "        \n",
    "        if len(self.documents) == 0 or self.embeddings is None:\n",
    "            print(\"警告：向量库为空\")\n",
    "            return []\n",
    "        \n",
    "        # 查询嵌入\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # 计算相似度\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # 获取最相似的文档\n",
    "        indices = np.argsort(similarities)[::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in indices:\n",
    "            similarity = float(similarities[idx])\n",
    "            \n",
    "            # 应用阈值过滤\n",
    "            if similarity < threshold and len(results) >= top_k:\n",
    "                continue\n",
    "            \n",
    "            result = {\n",
    "                'paper_id': self.metadata[idx]['paper_id'],\n",
    "                'document': self.documents[idx],\n",
    "                'metadata': self.metadata[idx],\n",
    "                'similarity': similarity,\n",
    "                'rank': len(results) + 1\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"获取统计信息\"\"\"\n",
    "        if len(self.documents) == 0:\n",
    "            return {'total_documents': 0}\n",
    "        \n",
    "        stats = {\n",
    "            'total_documents': len(self.documents),\n",
    "            'embedding_dimension': self.embeddings.shape[1] if self.embeddings is not None else 0,\n",
    "            'unique_topics': len(set(m['topic'] for m in self.metadata))\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# %%\n",
    "# ==================== 3. RAG系统 ====================\n",
    "\n",
    "class DeepSeekClient:\n",
    "    \"\"\"DeepSeek API客户端\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = config.API_BASE_URL\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    def generate_response(self, prompt: str, temperature: float = None, \n",
    "                         max_tokens: int = None, model: str = \"deepseek-chat\") -> str:\n",
    "        \"\"\"生成回答\"\"\"\n",
    "        if temperature is None:\n",
    "            temperature = config.TEMPERATURE\n",
    "        if max_tokens is None:\n",
    "            max_tokens = config.MAX_ANSWER_LENGTH\n",
    "        \n",
    "        # 如果没有API密钥，返回模拟回答\n",
    "        if not self.api_key or self.api_key == \"your-deepseek-api-key\":\n",
    "            print(\"警告：使用模拟API响应（请设置正确的API密钥）\")\n",
    "            return self._mock_response(prompt)\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            # 记录token使用情况\n",
    "            if 'usage' in result:\n",
    "                self.total_tokens += result['usage']['total_tokens']\n",
    "            \n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API请求失败: {e}\")\n",
    "            return f\"错误：无法生成回答 ({str(e)})\"\n",
    "        except KeyError as e:\n",
    "            print(f\"API响应解析失败: {e}\")\n",
    "            return \"错误：响应格式不正确\"\n",
    "    \n",
    "    def _mock_response(self, prompt: str) -> str:\n",
    "        \"\"\"模拟API响应\"\"\"\n",
    "        time.sleep(0.5)  # 模拟延迟\n",
    "        \n",
    "        # 根据提示内容生成模拟回答\n",
    "        if \"医学影像\" in prompt or \"medical\" in prompt.lower():\n",
    "            return \"\"\"医学影像中的开放词汇目标检测（Open-Vocabulary Object Detection, OVOD）是一种能够检测训练时未见过的目标类别的技术。\n",
    "\n",
    "它解决了传统目标检测的以下问题：\n",
    "1. 封闭词汇限制：传统方法只能检测训练集中出现的类别\n",
    "2. 数据稀缺问题：医学影像标注数据难以获取\n",
    "3. 泛化能力：能够识别新的病变或解剖结构\n",
    "\n",
    "MedROV是这方面的最新研究，实现了实时开放词汇检测[论文1]。\"\"\"\n",
    "        elif \"视频运动编辑\" in prompt or \"video\" in prompt.lower():\n",
    "            return \"\"\"最近在视频运动编辑方面的技术突破包括：\n",
    "\n",
    "1. MotionV2V模型：通过编辑稀疏轨迹来修改视频运动[论文2]\n",
    "2. 运动反事实生成：创建内容相同但运动不同的视频对\n",
    "3. 时间戳控制：可以从任何时间点开始编辑并自然传播\n",
    "4. 用户研究表明，MotionV2V在对比测试中获得超过65%的偏好率\"\"\"\n",
    "        else:\n",
    "            return \"这是一个模拟回答。实际使用需要设置正确的DeepSeek API密钥。\"\n",
    "    \n",
    "    def evaluate_answer(self, question: str, answer: str) -> Dict:\n",
    "        \"\"\"使用DeepSeek评估回答质量\"\"\"\n",
    "        # 如果没有API密钥，返回模拟评估\n",
    "        if not self.api_key or self.api_key == \"your-deepseek-api-key\":\n",
    "            return self._mock_evaluation(question, answer)\n",
    "        \n",
    "        prompt = f\"\"\"请评估以下回答的质量：\n",
    "\n",
    "问题：{question}\n",
    "\n",
    "回答：{answer}\n",
    "\n",
    "请从以下维度给出1-5分的评分（5分为最佳）：\n",
    "1. 准确性：回答内容是否准确无误\n",
    "2. 完整性：是否全面回答了问题\n",
    "3. 相关性：是否与问题紧密相关\n",
    "4. 具体性：是否包含具体细节和例子\n",
    "5. 整体质量：综合评分\n",
    "\n",
    "请以JSON格式返回结果，包含每个维度的分数和简要理由，以及一个总分（各项得分的平均值）。\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.generate_response(prompt, temperature=0.1, max_tokens=800)\n",
    "            \n",
    "            # 提取JSON部分\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                evaluation = json.loads(json_match.group())\n",
    "                return evaluation\n",
    "            else:\n",
    "                return {\n",
    "                    \"accuracy\": {\"score\": 3, \"reason\": \"无法解析评估结果\"},\n",
    "                    \"completeness\": {\"score\": 3, \"reason\": \"无法解析评估结果\"},\n",
    "                    \"relevance\": {\"score\": 3, \"reason\": \"无法解析评估结果\"},\n",
    "                    \"specificity\": {\"score\": 3, \"reason\": \"无法解析评估结果\"},\n",
    "                    \"overall_quality\": {\"score\": 3, \"reason\": \"无法解析评估结果\"},\n",
    "                    \"total_score\": 3.0\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"评估失败: {e}\")\n",
    "            return self._mock_evaluation(question, answer)\n",
    "    \n",
    "    def _mock_evaluation(self, question: str, answer: str) -> Dict:\n",
    "        \"\"\"模拟评估结果\"\"\"\n",
    "        return {\n",
    "            \"accuracy\": {\"score\": 4, \"reason\": \"回答内容基本准确\"},\n",
    "            \"completeness\": {\"score\": 3, \"reason\": \"回答了主要问题但不够全面\"},\n",
    "            \"relevance\": {\"score\": 4, \"reason\": \"与问题高度相关\"},\n",
    "            \"specificity\": {\"score\": 3, \"reason\": \"包含一些具体信息\"},\n",
    "            \"overall_quality\": {\"score\": 3.5, \"reason\": \"整体质量良好\"},\n",
    "            \"total_score\": 3.5\n",
    "        }\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"RAG系统\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, llm_client: DeepSeekClient):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def query(self, question: str, top_k: int = None, include_context: bool = True) -> Dict:\n",
    "        \"\"\"执行RAG查询\"\"\"\n",
    "        \n",
    "        # 1. 检索相关文档\n",
    "        retrieved_docs = self.vector_store.search(question, top_k=top_k)\n",
    "        \n",
    "        # 2. 构建提示词\n",
    "        if include_context and retrieved_docs:\n",
    "            context = self._build_context(retrieved_docs)\n",
    "            prompt = self._build_rag_prompt(question, context)\n",
    "            method = \"RAG\"\n",
    "        else:\n",
    "            prompt = self._build_baseline_prompt(question)\n",
    "            method = \"Baseline\"\n",
    "        \n",
    "        # 3. 生成回答\n",
    "        answer = self.llm.generate_response(prompt)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'method': method,\n",
    "            'prompt_preview': prompt[:200] + \"...\" if len(prompt) > 200 else prompt,\n",
    "        }\n",
    "    \n",
    "    def _build_context(self, docs: List[Dict]) -> str:\n",
    "        \"\"\"构建上下文\"\"\"\n",
    "        context_parts = [\"基于以下研究论文信息：\"]\n",
    "        \n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            meta = doc['metadata']\n",
    "            context_parts.append(\n",
    "                f\"[论文{i}] {meta['title']}\\n\"\n",
    "                f\"作者: {meta['first_author']}等\\n\"\n",
    "                f\"摘要: {self._truncate_text(meta.get('abstract', doc['document']), 200)}\\n\"\n",
    "                f\"相关度: {doc['similarity']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def _build_rag_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"构建RAG提示词\"\"\"\n",
    "        return f\"\"\"你是一个AI研究助手，请基于提供的学术文献回答用户的问题。\n",
    "\n",
    "可用文献：\n",
    "{context}\n",
    "\n",
    "用户问题：{question}\n",
    "\n",
    "请按照以下要求回答：\n",
    "1. 主要基于提供的文献信息进行回答\n",
    "2. 在回答中引用相关文献，格式为[论文1]、[论文2]等\n",
    "3. 如果文献信息不足，可以适当补充相关知识\n",
    "4. 保持学术严谨性\n",
    "\n",
    "请提供详细、准确的回答：\"\"\"\n",
    "    \n",
    "    def _build_baseline_prompt(self, question: str) -> str:\n",
    "        \"\"\"构建基线提示词（无RAG）\"\"\"\n",
    "        return f\"\"\"你是一个AI研究助手，请回答以下学术问题。\n",
    "\n",
    "问题：{question}\n",
    "\n",
    "请提供详细、准确的回答：\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _truncate_text(text: str, max_length: int) -> str:\n",
    "        \"\"\"截断文本\"\"\"\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length] + \"...\"\n",
    "\n",
    "# %%\n",
    "# ==================== 4. 双评估系统 ====================\n",
    "\n",
    "class AutoEvaluationMetrics:\n",
    "    \"\"\"自动评估指标计算\"\"\"\n",
    "    \n",
    "    def evaluate_response(self, \n",
    "                         question: str,\n",
    "                         answer: str,\n",
    "                         retrieved_docs: List[Dict] = None,\n",
    "                         baseline_answer: str = None) -> Dict:\n",
    "        \"\"\"评估单个响应\"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            'answer_length': len(answer),\n",
    "            'word_count': len(re.findall(r'\\w+', answer)),\n",
    "            'has_error': 1 if \"错误：\" in answer or \"error\" in answer.lower() else 0,\n",
    "        }\n",
    "        \n",
    "        # 检索相关指标\n",
    "        if retrieved_docs:\n",
    "            metrics.update(self._calculate_retrieval_metrics(retrieved_docs))\n",
    "        \n",
    "        # 内容质量指标\n",
    "        metrics.update(self._calculate_content_metrics(answer, question))\n",
    "        \n",
    "        # 引用质量指标\n",
    "        metrics.update(self._calculate_citation_metrics(answer))\n",
    "        \n",
    "        # 与基线对比\n",
    "        if baseline_answer:\n",
    "            metrics.update(self._calculate_comparison_metrics(answer, baseline_answer))\n",
    "        \n",
    "        # 计算自动评估总分\n",
    "        metrics['auto_score'] = self._calculate_overall_score(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_retrieval_metrics(self, retrieved_docs: List[Dict]) -> Dict:\n",
    "        \"\"\"计算检索相关指标\"\"\"\n",
    "        if not retrieved_docs:\n",
    "            return {}\n",
    "        \n",
    "        similarities = [doc['similarity'] for doc in retrieved_docs]\n",
    "        return {\n",
    "            'retrieved_docs_count': len(retrieved_docs),\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'max_similarity': max(similarities),\n",
    "        }\n",
    "    \n",
    "    def _calculate_content_metrics(self, answer: str, question: str) -> Dict:\n",
    "        \"\"\"计算内容质量指标\"\"\"\n",
    "        # 技术术语检测\n",
    "        technical_terms = ['模型', '算法', '检测', '学习', '训练', '精度', '准确率']\n",
    "        tech_term_count = sum(1 for term in technical_terms if term in answer)\n",
    "        \n",
    "        # 问题关键词匹配\n",
    "        question_words = set(re.findall(r'\\w+', question.lower()))\n",
    "        answer_words = set(re.findall(r'\\w+', answer.lower()))\n",
    "        keyword_matches = len(question_words.intersection(answer_words))\n",
    "        \n",
    "        return {\n",
    "            'technical_terms': tech_term_count,\n",
    "            'keyword_matches': keyword_matches,\n",
    "            'has_citation': 1 if '[' in answer and ']' in answer else 0,\n",
    "        }\n",
    "    \n",
    "    def _calculate_citation_metrics(self, answer: str) -> Dict:\n",
    "        \"\"\"计算引用质量指标\"\"\"\n",
    "        # 检测引用\n",
    "        citations = re.findall(r'\\[.*?\\d+.*?\\]', answer)\n",
    "        return {\n",
    "            'citation_count': len(citations),\n",
    "            'unique_citations': len(set(citations)),\n",
    "        }\n",
    "    \n",
    "    def _calculate_comparison_metrics(self, rag_answer: str, baseline_answer: str) -> Dict:\n",
    "        \"\"\"计算对比指标\"\"\"\n",
    "        rag_length = len(rag_answer)\n",
    "        baseline_length = len(baseline_answer)\n",
    "        \n",
    "        return {\n",
    "            'length_ratio': rag_length / max(baseline_length, 1),\n",
    "            'length_difference': rag_length - baseline_length,\n",
    "        }\n",
    "    \n",
    "    def _calculate_overall_score(self, metrics: Dict) -> float:\n",
    "        \"\"\"计算自动评估总分\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # 长度分数（适中为佳）\n",
    "        length = metrics.get('answer_length', 0)\n",
    "        if 200 <= length <= 800:\n",
    "            score += 0.3\n",
    "        elif length > 50:\n",
    "            score += 0.2\n",
    "        \n",
    "        # 技术术语分数\n",
    "        tech_terms = metrics.get('technical_terms', 0)\n",
    "        score += min(tech_terms * 0.1, 0.3)\n",
    "        \n",
    "        # 引用分数\n",
    "        citations = metrics.get('citation_count', 0)\n",
    "        score += min(citations * 0.2, 0.3)\n",
    "        \n",
    "        # 相似度分数\n",
    "        avg_sim = metrics.get('avg_similarity', 0)\n",
    "        score += avg_sim * 0.1\n",
    "        \n",
    "        # 无错误加分\n",
    "        if metrics.get('has_error', 1) == 0:\n",
    "            score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "\n",
    "class CombinedEvaluator:\n",
    "    \"\"\"组合评估器（自动评估 + API评估）\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem, llm_client: DeepSeekClient):\n",
    "        self.rag_system = rag_system\n",
    "        self.llm_client = llm_client\n",
    "        self.auto_evaluator = AutoEvaluationMetrics()\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_question(self, question_data: Dict) -> Dict:\n",
    "        \"\"\"评估单个问题\"\"\"\n",
    "        question = question_data['question']\n",
    "        topic = question_data.get('topic', '')\n",
    "        \n",
    "        print(f\"\\n评估问题: {question}\")\n",
    "        \n",
    "        # RAG查询\n",
    "        rag_response = self.rag_system.query(question, include_context=True)\n",
    "        \n",
    "        # 基线查询\n",
    "        baseline_response = self.rag_system.query(question, include_context=False)\n",
    "        \n",
    "        # 自动评估（RAG）\n",
    "        auto_metrics_rag = self.auto_evaluator.evaluate_response(\n",
    "            question=question,\n",
    "            answer=rag_response['answer'],\n",
    "            retrieved_docs=rag_response['retrieved_docs'],\n",
    "            baseline_answer=baseline_response['answer']\n",
    "        )\n",
    "        \n",
    "        # 自动评估（基线）\n",
    "        auto_metrics_baseline = self.auto_evaluator.evaluate_response(\n",
    "            question=question,\n",
    "            answer=baseline_response['answer'],\n",
    "            retrieved_docs=None,  # 基线没有检索文档\n",
    "            baseline_answer=None  # 基线没有对比对象\n",
    "        )\n",
    "        \n",
    "        # API评估（RAG回答）\n",
    "        api_evaluation_rag = self.llm_client.evaluate_answer(question, rag_response['answer'])\n",
    "        \n",
    "        # API评估（基线回答）\n",
    "        api_evaluation_baseline = self.llm_client.evaluate_answer(question, baseline_response['answer'])\n",
    "        \n",
    "        # 构建结果\n",
    "        result = {\n",
    "            'question_id': len(self.results) + 1,\n",
    "            'question': question,\n",
    "            'topic': topic,\n",
    "            \n",
    "            # RAG结果\n",
    "            'rag_answer': rag_response['answer'],\n",
    "            'rag_method': rag_response['method'],\n",
    "            'rag_retrieved_docs': [\n",
    "                {\n",
    "                    'title': doc['metadata']['title'],\n",
    "                    'similarity': float(doc['similarity']),\n",
    "                    'first_author': doc['metadata']['first_author']\n",
    "                }\n",
    "                for doc in rag_response['retrieved_docs']\n",
    "            ],\n",
    "            \n",
    "            # 基线结果\n",
    "            'baseline_answer': baseline_response['answer'],\n",
    "            'baseline_method': baseline_response['method'],\n",
    "            \n",
    "            # 自动评估结果（RAG）\n",
    "            'auto_evaluation_rag': auto_metrics_rag,\n",
    "            'auto_score_rag': auto_metrics_rag.get('auto_score', 0),\n",
    "            \n",
    "            # 自动评估结果（基线）\n",
    "            'auto_evaluation_baseline': auto_metrics_baseline,\n",
    "            'auto_score_baseline': auto_metrics_baseline.get('auto_score', 0),\n",
    "            \n",
    "            # API评估结果\n",
    "            'api_evaluation_rag': api_evaluation_rag,\n",
    "            'api_evaluation_baseline': api_evaluation_baseline,\n",
    "            'api_score_rag': api_evaluation_rag.get('total_score', 0),\n",
    "            'api_score_baseline': api_evaluation_baseline.get('total_score', 0),\n",
    "            \n",
    "            # 综合比较\n",
    "            'auto_improvement': auto_metrics_rag.get('auto_score', 0) - auto_metrics_baseline.get('auto_score', 0),\n",
    "            'api_improvement': api_evaluation_rag.get('total_score', 0) - api_evaluation_baseline.get('total_score', 0),\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        \n",
    "        # 打印简要结果\n",
    "        print(f\"  自动评分: RAG={auto_metrics_rag.get('auto_score', 0):.2f}, 基线={auto_metrics_baseline.get('auto_score', 0):.2f}\")\n",
    "        print(f\"  API评分: RAG={api_evaluation_rag.get('total_score', 0):.2f}, 基线={api_evaluation_baseline.get('total_score', 0):.2f}\")\n",
    "        print(f\"  检索文档: {len(rag_response['retrieved_docs'])}篇\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def evaluate_all(self, questions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"评估所有问题\"\"\"\n",
    "        print(f\"开始评估 {len(questions)} 个问题...\")\n",
    "        \n",
    "        for question_data in questions:\n",
    "            self.evaluate_question(question_data)\n",
    "        \n",
    "        print(f\"\\n✓ 完成所有评估\")\n",
    "        return self.results\n",
    "    \n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"生成评估报告\"\"\"\n",
    "        if not self.results:\n",
    "            return {}\n",
    "        \n",
    "        # 收集统计信息\n",
    "        auto_scores_rag = [r['auto_score_rag'] for r in self.results]\n",
    "        auto_scores_baseline = [r['auto_score_baseline'] for r in self.results]\n",
    "        api_scores_rag = [r['api_score_rag'] for r in self.results]\n",
    "        api_scores_baseline = [r['api_score_baseline'] for r in self.results]\n",
    "        auto_improvements = [r['auto_improvement'] for r in self.results]\n",
    "        api_improvements = [r['api_improvement'] for r in self.results]\n",
    "        \n",
    "        # 计算相关性（自动评估 vs API评估）\n",
    "        if len(auto_scores_rag) > 1:\n",
    "            correlation = np.corrcoef(auto_scores_rag, api_scores_rag)[0, 1]\n",
    "        else:\n",
    "            correlation = 0\n",
    "        \n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_questions': len(self.results),\n",
    "                'avg_auto_score_rag': float(np.mean(auto_scores_rag)),\n",
    "                'avg_auto_score_baseline': float(np.mean(auto_scores_baseline)),\n",
    "                'avg_api_score_rag': float(np.mean(api_scores_rag)),\n",
    "                'avg_api_score_baseline': float(np.mean(api_scores_baseline)),\n",
    "                'avg_auto_improvement': float(np.mean(auto_improvements)),\n",
    "                'avg_api_improvement': float(np.mean(api_improvements)),\n",
    "                'auto_improvement_rate': sum(1 for imp in auto_improvements if imp > 0) / len(auto_improvements),\n",
    "                'api_improvement_rate': sum(1 for imp in api_improvements if imp > 0) / len(api_improvements),\n",
    "                'correlation_auto_vs_api': float(correlation),\n",
    "                'total_tokens_used': self.llm_client.total_tokens\n",
    "            },\n",
    "            'detailed_results': self.results\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"保存结果到文件\"\"\"\n",
    "        # 保存JSON报告\n",
    "        report = self.generate_report()\n",
    "        with open(config.REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✓ 评估报告已保存到 {config.REPORT_FILE}\")\n",
    "        \n",
    "        # 保存CSV结果\n",
    "        df_data = []\n",
    "        for result in self.results:\n",
    "            row = {\n",
    "                'question_id': result['question_id'],\n",
    "                'question': result['question'],\n",
    "                'topic': result['topic'],\n",
    "                'rag_answer_length': len(result['rag_answer']),\n",
    "                'baseline_answer_length': len(result['baseline_answer']),\n",
    "                'auto_score_rag': result['auto_score_rag'],\n",
    "                'auto_score_baseline': result['auto_score_baseline'],\n",
    "                'api_score_rag': result['api_score_rag'],\n",
    "                'api_score_baseline': result['api_score_baseline'],\n",
    "                'auto_improvement': result['auto_improvement'],\n",
    "                'api_improvement': result['api_improvement'],\n",
    "                'retrieved_docs_count': len(result['rag_retrieved_docs']),\n",
    "            }\n",
    "            \n",
    "            # 添加RAG自动评估指标\n",
    "            for key, value in result['auto_evaluation_rag'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    row[f'auto_rag_{key}'] = value\n",
    "            \n",
    "            # 添加基线自动评估指标\n",
    "            for key, value in result['auto_evaluation_baseline'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    row[f'auto_baseline_{key}'] = value\n",
    "            \n",
    "            df_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        df.to_csv(config.EVALUATION_CSV, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✓ 详细结果已保存到 {config.EVALUATION_CSV}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# %%\n",
    "# ==================== 5. 可视化和主程序 ====================\n",
    "\n",
    "def create_visualizations(results: List[Dict], save_dir: str = \".\"):\n",
    "    \"\"\"创建可视化图表\"\"\"\n",
    "    if not VISUALIZATION_AVAILABLE:\n",
    "        print(\"警告：可视化库不可用，跳过图表生成\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 准备数据\n",
    "        df = pd.DataFrame([{\n",
    "            'question_id': r['question_id'],\n",
    "            'auto_score_rag': r['auto_score_rag'],\n",
    "            'auto_score_baseline': r['auto_score_baseline'],\n",
    "            'api_score_rag': r['api_score_rag'],\n",
    "            'api_score_baseline': r['api_score_baseline'],\n",
    "            'auto_improvement': r['auto_improvement'],\n",
    "            'api_improvement': r['api_improvement'],\n",
    "            'retrieved_docs': len(r['rag_retrieved_docs'])\n",
    "        } for r in results])\n",
    "        \n",
    "        # 创建图表\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # 1. API评分对比\n",
    "        x = range(len(df))\n",
    "        width = 0.35\n",
    "        axes[0, 0].bar([i - width/2 for i in x], df['api_score_baseline'], width, label='基线', alpha=0.7)\n",
    "        axes[0, 0].bar([i + width/2 for i in x], df['api_score_rag'], width, label='RAG', alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('问题编号')\n",
    "        axes[0, 0].set_ylabel('API评分')\n",
    "        axes[0, 0].set_title('RAG vs 基线 API评分对比')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        # 2. 自动评分对比\n",
    "        axes[0, 1].bar([i - width/2 for i in x], df['auto_score_baseline'], width, label='基线', alpha=0.7)\n",
    "        axes[0, 1].bar([i + width/2 for i in x], df['auto_score_rag'], width, label='RAG', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('问题编号')\n",
    "        axes[0, 1].set_ylabel('自动评分')\n",
    "        axes[0, 1].set_title('RAG vs 基线 自动评分对比')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        # 3. 改进分布（API）\n",
    "        axes[1, 0].bar(x, df['api_improvement'], color='green' if df['api_improvement'].mean() > 0 else 'red')\n",
    "        axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        axes[1, 0].set_xlabel('问题编号')\n",
    "        axes[1, 0].set_ylabel('API改进分数')\n",
    "        axes[1, 0].set_title('RAG API改进分数分布')\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        # 4. 改进分布（自动）\n",
    "        axes[1, 1].bar(x, df['auto_improvement'], color='blue' if df['auto_improvement'].mean() > 0 else 'red')\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        axes[1, 1].set_xlabel('问题编号')\n",
    "        axes[1, 1].set_ylabel('自动评估改进分数')\n",
    "        axes[1, 1].set_title('RAG 自动评估改进分数分布')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/evaluation_visualization.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()  # 关闭图表，避免内存泄漏\n",
    "        print(f\"✓ 可视化图表已保存到 {save_dir}/evaluation_visualization.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"可视化生成失败: {e}\")\n",
    "        print(\"跳过可视化生成\")\n",
    "\n",
    "def print_detailed_report(report: Dict):\n",
    "    \"\"\"打印详细报告\"\"\"\n",
    "    summary = report['summary']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG系统评估报告\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n总体统计:\")\n",
    "    print(f\"  评估问题总数: {summary['total_questions']}\")\n",
    "    print(f\"  平均自动评估分数 (RAG): {summary['avg_auto_score_rag']:.3f}\")\n",
    "    print(f\"  平均自动评估分数 (基线): {summary['avg_auto_score_baseline']:.3f}\")\n",
    "    print(f\"  平均API评估分数 (RAG): {summary['avg_api_score_rag']:.3f}\")\n",
    "    print(f\"  平均API评估分数 (基线): {summary['avg_api_score_baseline']:.3f}\")\n",
    "    print(f\"  平均自动评估改进: {summary['avg_auto_improvement']:.3f}\")\n",
    "    print(f\"  平均API评估改进: {summary['avg_api_improvement']:.3f}\")\n",
    "    print(f\"  自动评估改进率: {summary['auto_improvement_rate']:.2%}\")\n",
    "    print(f\"  API评估改进率: {summary['api_improvement_rate']:.2%}\")\n",
    "    print(f\"  自动评估与API评估相关性: {summary['correlation_auto_vs_api']:.3f}\")\n",
    "    print(f\"  总token使用量: {summary['total_tokens_used']}\")\n",
    "    \n",
    "    # 打印每个问题的详细结果\n",
    "    print(f\"\\n详细结果:\")\n",
    "    for result in report['detailed_results']:\n",
    "        print(f\"\\n问题 {result['question_id']}: {result['question'][:50]}...\")\n",
    "        print(f\"  自动评分: RAG={result['auto_score_rag']:.3f}, 基线={result['auto_score_baseline']:.3f}\")\n",
    "        print(f\"  API评分: RAG={result['api_score_rag']:.3f}, 基线={result['api_score_baseline']:.3f}\")\n",
    "        print(f\"  改进: 自动={result['auto_improvement']:.3f}, API={result['api_improvement']:.3f}\")\n",
    "        print(f\"  检索文档: {len(result['rag_retrieved_docs'])}篇\")\n",
    "        \n",
    "        # 显示检索到的文档\n",
    "        if result['rag_retrieved_docs']:\n",
    "            print(f\"  相关文档:\")\n",
    "            for doc in result['rag_retrieved_docs'][:2]:  # 只显示前2个\n",
    "                print(f\"    - {doc['title'][:50]}... (相似度: {doc['similarity']:.3f})\")\n",
    "\n",
    "# %%\n",
    "# ==================== 主程序 ====================\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"RAG系统评估框架\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. 加载数据\n",
    "    print(\"\\n1. 加载论文数据...\")\n",
    "    papers = DataLoader.load_from_jsonl(config.DATA_FILE)\n",
    "    \n",
    "    if not papers:\n",
    "        print(\"错误：没有加载到论文数据\")\n",
    "        return\n",
    "    \n",
    "    # 2. 初始化嵌入模型\n",
    "    print(\"\\n2. 初始化嵌入模型...\")\n",
    "    embedder = EmbeddingModel(\n",
    "        model_name=config.EMBEDDING_MODEL,\n",
    "        use_api=False,  # 使用本地模型以避免API限制\n",
    "        api_key=config.DEEPSEEK_API_KEY\n",
    "    )\n",
    "    \n",
    "    # 3. 构建向量数据库\n",
    "    print(\"\\n3. 构建向量数据库...\")\n",
    "    vector_store = VectorStore(embedder)\n",
    "    vector_store.add_papers(papers)\n",
    "    \n",
    "    # 打印向量库统计\n",
    "    stats = vector_store.get_stats()\n",
    "    print(f\"  文档总数: {stats['total_documents']}\")\n",
    "    print(f\"  主题数量: {stats['unique_topics']}\")\n",
    "    \n",
    "    # 4. 初始化RAG系统\n",
    "    print(\"\\n4. 初始化RAG系统...\")\n",
    "    llm_client = DeepSeekClient(config.DEEPSEEK_API_KEY)\n",
    "    rag_system = RAGSystem(vector_store, llm_client)\n",
    "    \n",
    "    # 5. 运行评估\n",
    "    print(\"\\n5. 运行评估...\")\n",
    "    evaluator = CombinedEvaluator(rag_system, llm_client)\n",
    "    results = evaluator.evaluate_all(config.TEST_QUESTIONS)\n",
    "    \n",
    "    # 6. 生成报告\n",
    "    print(\"\\n6. 生成报告...\")\n",
    "    report = evaluator.generate_report()\n",
    "    \n",
    "    # 7. 保存结果\n",
    "    print(\"\\n7. 保存结果...\")\n",
    "    df = evaluator.save_results()\n",
    "    \n",
    "    # 8. 创建可视化\n",
    "    if VISUALIZATION_AVAILABLE and len(results) > 0:\n",
    "        print(\"\\n8. 创建可视化图表...\")\n",
    "        create_visualizations(results)\n",
    "    \n",
    "    # 9. 打印报告\n",
    "    print(\"\\n9. 评估报告:\")\n",
    "    print_detailed_report(report)\n",
    "    \n",
    "    # 10. 显示DataFrame\n",
    "    print(\"\\n10. 结果DataFrame:\")\n",
    "    print(df[['question_id', 'auto_score_rag', 'auto_score_baseline', 'api_score_rag', 'api_score_baseline', 'auto_improvement', 'api_improvement']].to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"评估完成！\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# %%\n",
    "# 运行主程序\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# %%\n",
    "# 快速测试函数（可选）\n",
    "def quick_test():\n",
    "    \"\"\"快速测试函数\"\"\"\n",
    "    print(\"运行快速测试...\")\n",
    "    \n",
    "    # 创建示例数据\n",
    "    papers = DataLoader.create_sample_data()\n",
    "    \n",
    "    # 初始化组件\n",
    "    embedder = EmbeddingModel(use_api=False)\n",
    "    vector_store = VectorStore(embedder)\n",
    "    vector_store.add_papers(papers)\n",
    "    \n",
    "    llm_client = DeepSeekClient(config.DEEPSEEK_API_KEY)\n",
    "    rag_system = RAGSystem(vector_store, llm_client)\n",
    "    \n",
    "    # 测试单个问题\n",
    "    test_question = \"什么是医学影像中的开放词汇目标检测？\"\n",
    "    print(f\"\\n测试问题: {test_question}\")\n",
    "    \n",
    "    response = rag_system.query(test_question)\n",
    "    print(f\"\\nRAG回答预览: {response['answer'][:200]}...\")\n",
    "    \n",
    "    # 评估\n",
    "    evaluator = AutoEvaluationMetrics()\n",
    "    metrics = evaluator.evaluate_response(\n",
    "        question=test_question,\n",
    "        answer=response['answer'],\n",
    "        retrieved_docs=response['retrieved_docs']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n自动评估结果:\")\n",
    "    print(f\"  回答长度: {metrics['answer_length']}\")\n",
    "    print(f\"  技术术语: {metrics.get('technical_terms', 0)}\")\n",
    "    print(f\"  引用数量: {metrics.get('citation_count', 0)}\")\n",
    "    print(f\"  自动评分: {metrics.get('auto_score', 0):.2f}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 运行快速测试（取消注释以下行）\n",
    "# quick_test_response = quick_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLHW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
