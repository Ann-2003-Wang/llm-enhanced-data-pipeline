{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93dec3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAG System Evaluation Framework\n",
      "============================================================\n",
      "\n",
      "1. Loading paper data...\n",
      "✓ Successfully loaded 695 papers from cleaned_papers.jsonl\n",
      "\n",
      "Dataset Statistics:\n",
      "----------------------------------------\n",
      "Total Papers: 695\n",
      "Number of Topics: 3\n",
      "\n",
      "Quality Tier Distribution:\n",
      "  high: 661 papers (95.1%)\n",
      "  medium: 34 papers (4.9%)\n",
      "Total Authors: 3636\n",
      "Latest Paper Date: 2025-12-04\n",
      "\n",
      "2. Initializing embedding model...\n",
      "✓ Using local embedding model\n",
      "  Model: all-MiniLM-L6-v2\n",
      "  Dimension: 384\n",
      "\n",
      "3. Building vector database...\n",
      "Processing 695 papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding papers: 100%|██████████| 695/695 [00:00<00:00, 670741.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector store built successfully\n",
      "  Number of documents: 695\n",
      "  Embedding dimension: 384\n",
      "  Total Documents: 695\n",
      "  Number of Topics: 3\n",
      "\n",
      "4. Initializing RAG system...\n",
      "\n",
      "5. Running evaluation...\n",
      "Evaluating 3 questions...\n",
      "\n",
      "Evaluating question: Based on the latest papers from this month, could you please introduce to me the technological breakthroughs that have emerged in the field of video dynamic editing recently? And could you briefly explain them to me based on the abstracts of the papers?\n",
      "  Automatic scores: RAG=0.95, Baseline=0.50\n",
      "  API scores: RAG=4.83, Baseline=3.83\n",
      "  Retrieved documents: 5\n",
      "\n",
      "Evaluating question: How many core bottlenecks exist in current autoregressive video diffusion models?\n",
      "  Automatic scores: RAG=0.74, Baseline=0.40\n",
      "  API scores: RAG=4.67, Baseline=3.83\n",
      "  Retrieved documents: 5\n",
      "\n",
      "Evaluating question: What noteworthy papers in the machine learning and computer vision fields have been published in the past month? Please provide links or DOIs.\n",
      "  Automatic scores: RAG=0.94, Baseline=0.80\n",
      "  API scores: RAG=2.83, Baseline=1.42\n",
      "  Retrieved documents: 5\n",
      "\n",
      "✓ All evaluations completed\n",
      "\n",
      "6. Generating report...\n",
      "\n",
      "7. Saving results...\n",
      "✓ Evaluation report saved to rag_evaluation_report.json\n",
      "✓ Detailed results saved to evaluation_results.csv\n",
      "\n",
      "8. Creating visualization charts...\n",
      "✓ Visualization chart saved to ./evaluation_visualization.png\n",
      "\n",
      "9. Evaluation Report:\n",
      "\n",
      "============================================================\n",
      "RAG System Evaluation Report\n",
      "============================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Total Questions: 3\n",
      "  Average Automatic Score (RAG): 0.876\n",
      "  Average Automatic Score (Baseline): 0.567\n",
      "  Average API Score (RAG): 4.110\n",
      "  Average API Score (Baseline): 3.027\n",
      "  Average Automatic Improvement: 0.309\n",
      "  Average API Improvement: 1.083\n",
      "  Automatic Improvement Rate: 100.00%\n",
      "  API Improvement Rate: 100.00%\n",
      "  Correlation between Auto and API Evaluation: -0.411\n",
      "  Total Tokens Used: 18645\n",
      "\n",
      "Detailed Results:\n",
      "\n",
      "Question 1: Based on the latest papers from this month, could ...\n",
      "  Automatic Scores: RAG=0.945, Baseline=0.500\n",
      "  API Scores: RAG=4.830, Baseline=3.830\n",
      "  Improvement: Auto=0.445, API=1.000\n",
      "  Retrieved Documents: 5\n",
      "  Relevant Documents:\n",
      "    - MotionV2V: Editing Motion in a Video... (Similarity: 0.507)\n",
      "    - In-Context Sync-LoRA for Portrait Video Editing... (Similarity: 0.475)\n",
      "\n",
      "Question 2: How many core bottlenecks exist in current autoreg...\n",
      "  Automatic Scores: RAG=0.744, Baseline=0.400\n",
      "  API Scores: RAG=4.670, Baseline=3.830\n",
      "  Improvement: Auto=0.344, API=0.840\n",
      "  Retrieved Documents: 5\n",
      "  Relevant Documents:\n",
      "    - GalaxyDiT: Efficient Video Generation with Guidanc... (Similarity: 0.479)\n",
      "    - Infinity-RoPE: Action-Controllable Infinite Video ... (Similarity: 0.459)\n",
      "\n",
      "Question 3: What noteworthy papers in the machine learning and...\n",
      "  Automatic Scores: RAG=0.939, Baseline=0.800\n",
      "  API Scores: RAG=2.830, Baseline=1.420\n",
      "  Improvement: Auto=0.139, API=1.410\n",
      "  Retrieved Documents: 5\n",
      "  Relevant Documents:\n",
      "    - Nex-N1: Agentic Models Trained via a Unified Ecosy... (Similarity: 0.412)\n",
      "    - Challenging the Abilities of Large Language Models... (Similarity: 0.396)\n",
      "\n",
      "10. Results DataFrame:\n",
      "   question_id  auto_score_rag  auto_score_baseline  api_score_rag  api_score_baseline  auto_improvement  api_improvement\n",
      "0            1        0.945197                  0.5           4.83                3.83          0.445197             1.00\n",
      "1            2        0.744178                  0.4           4.67                3.83          0.344178             0.84\n",
      "2            3        0.938901                  0.8           2.83                1.42          0.138901             1.41\n",
      "\n",
      "============================================================\n",
      "Evaluation completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # RAG System Performance Evaluation Framework\n",
    "# \n",
    "# Implement a complete RAG system evaluation based on the provided paper data structure\n",
    "\n",
    "# %%\n",
    "# First install necessary libraries (if not already installed)\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.26.4 sentence-transformers scikit-learn pandas matplotlib seaborn tqdm --quiet\n",
    "\n",
    "# Set Matplotlib backend to avoid matplotlib_inline conflicts\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "\n",
    "# Import other libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hashlib\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import threading\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import visualization libraries, skip if failed\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    VISUALIZATION_AVAILABLE = True\n",
    "    # Set font for Chinese display (keeping this as it's a configuration)\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "except ImportError:\n",
    "    VISUALIZATION_AVAILABLE = False\n",
    "    print(\"Warning: Cannot import matplotlib/seaborn, visualization features will be disabled\")\n",
    "\n",
    "# %%\n",
    "# ==================== Configuration Area ====================\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters\"\"\"\n",
    "    # API Configuration\n",
    "    DEEPSEEK_API_KEY = \"sk-79990d599cd74bc0a56f6ca2f200a621\"  # Replace with your API key\n",
    "    API_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "    \n",
    "    # Vector Database Configuration\n",
    "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Smaller model to reduce memory usage\n",
    "    TOP_K_RETRIEVAL = 5  # Number of documents to retrieve\n",
    "    SIMILARITY_THRESHOLD = 0.7  # Similarity threshold\n",
    "    \n",
    "    # Evaluation Configuration\n",
    "    MAX_ANSWER_LENGTH = 1500\n",
    "    TEMPERATURE = 0.1\n",
    "    \n",
    "    # File Paths\n",
    "    DATA_FILE = \"cleaned_papers.jsonl\"\n",
    "    REPORT_FILE = \"rag_evaluation_report.json\"\n",
    "    EVALUATION_CSV = \"evaluation_results.csv\"\n",
    "    \n",
    "    # Test Questions\n",
    "    TEST_QUESTIONS = [\n",
    "         \n",
    "    {\n",
    "        \"question\": \"Based on the latest papers from this month, could you please introduce to me the technological breakthroughs that have emerged in the field of video dynamic editing recently? And could you briefly explain them to me based on the abstracts of the papers?\",\n",
    "        \"topic\": \"computer vision, video editing\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many core bottlenecks exist in current autoregressive video diffusion models?\",\n",
    "        \"topic\": \"machine learning, computer vision\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What noteworthy papers in the machine learning and computer vision fields have been published in the past month? Please provide links or DOIs.\",\n",
    "        \"topic\": \"machine learning, computer vision\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    }\n",
    "\n",
    "    ]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# %%\n",
    "# ==================== 1. Data Loading and Preprocessing ====================\n",
    "\n",
    "@dataclass\n",
    "class Paper:\n",
    "    \"\"\"Paper data structure (adapted to your JSONL format)\"\"\"\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    authors: List[str]\n",
    "    first_author: str\n",
    "    topic: str\n",
    "    categories: List[str]\n",
    "    publish_date: str\n",
    "    url: str\n",
    "    embedding_text: str\n",
    "    quality_scores: Dict[str, float]\n",
    "    quality_tier: str\n",
    "    basic_keywords: List[str]\n",
    "    domain_keywords: List[str]\n",
    "    update_date: str = \"\"  # Add update_date field with default value\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post-initialization processing\"\"\"\n",
    "        # Ensure all fields are of correct type\n",
    "        if isinstance(self.authors, str):\n",
    "            self.authors = [a.strip() for a in self.authors.split(',')]\n",
    "        elif not isinstance(self.authors, list):\n",
    "            self.authors = []\n",
    "        \n",
    "        if isinstance(self.categories, str):\n",
    "            self.categories = [c.strip() for c in self.categories.split(',')]\n",
    "        elif not isinstance(self.categories, list):\n",
    "            self.categories = []\n",
    "            \n",
    "        if isinstance(self.basic_keywords, str):\n",
    "            self.basic_keywords = [k.strip() for k in self.basic_keywords.split(',')]\n",
    "        elif not isinstance(self.basic_keywords, list):\n",
    "            self.basic_keywords = []\n",
    "            \n",
    "        if isinstance(self.domain_keywords, str):\n",
    "            self.domain_keywords = [k.strip() for k in self.domain_keywords.split(',')]\n",
    "        elif not isinstance(self.domain_keywords, list):\n",
    "            self.domain_keywords = []\n",
    "    \n",
    "    def to_text(self) -> str:\n",
    "        \"\"\"Convert paper to text for embedding\"\"\"\n",
    "        # Use embedding_text field (already available in your data)\n",
    "        if self.embedding_text:\n",
    "            return self.embedding_text\n",
    "        \n",
    "        # If embedding_text doesn't exist, construct text\n",
    "        text_parts = [\n",
    "            f\"Paper Title: {self.title}\",\n",
    "            f\"Research Topic: {self.topic}\",\n",
    "            f\"Authors: {', '.join(self.authors[:3])}\",\n",
    "            f\"Abstract: {self.abstract[:500]}\",\n",
    "        ]\n",
    "        \n",
    "        if self.categories:\n",
    "            text_parts.append(f\"Categories: {', '.join(self.categories)}\")\n",
    "        \n",
    "        if self.basic_keywords:\n",
    "            text_parts.append(f\"Keywords: {', '.join(self.basic_keywords[:5])}\")\n",
    "        \n",
    "        return \"\\n\".join(text_parts)\n",
    "    \n",
    "    def get_quality_score(self) -> float:\n",
    "        \"\"\"Get paper quality score\"\"\"\n",
    "        if self.quality_scores and 'overall_quality_score' in self.quality_scores:\n",
    "            return self.quality_scores['overall_quality_score']\n",
    "        return 0.5\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'Paper':\n",
    "        \"\"\"Create Paper object from dictionary (adapted to your JSONL format)\"\"\"\n",
    "        # Provide default values to avoid KeyError\n",
    "        defaults = {\n",
    "            'paper_id': '',\n",
    "            'title': '',\n",
    "            'abstract': '',\n",
    "            'authors': [],\n",
    "            'first_author': '',\n",
    "            'topic': '',\n",
    "            'categories': [],\n",
    "            'publish_date': '',\n",
    "            'url': '',\n",
    "            'embedding_text': '',\n",
    "            'quality_scores': {},\n",
    "            'quality_tier': 'medium',\n",
    "            'basic_keywords': [],\n",
    "            'domain_keywords': [],\n",
    "            'update_date': ''  # Add update_date default value\n",
    "        }\n",
    "        \n",
    "        # Merge data with defaults, keep only fields defined in Paper class\n",
    "        merged_data = {**defaults, **{k: v for k, v in data.items() if k in defaults}}\n",
    "        \n",
    "        return cls(**merged_data)\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Data loader\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_from_jsonl(file_path: str) -> List[Paper]:\n",
    "        \"\"\"Load paper data from JSONL file (adapted to your format)\"\"\"\n",
    "        papers = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            data = json.loads(line.strip())\n",
    "                            paper = Paper.from_dict(data)\n",
    "                            papers.append(paper)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"JSON parsing error on line {line_num}: {e}\")\n",
    "                            continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"Data conversion error on line {line_num}: {e}\")\n",
    "                            continue\n",
    "            \n",
    "            print(f\"✓ Successfully loaded {len(papers)} papers from {file_path}\")\n",
    "            \n",
    "            # Print statistics\n",
    "            if papers:\n",
    "                DataLoader._print_statistics(papers)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ File {file_path} not found\")\n",
    "            # Create some sample data for testing\n",
    "            papers = DataLoader.create_sample_data()\n",
    "        \n",
    "        return papers\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sample_data() -> List[Paper]:\n",
    "        \"\"\"Create sample data\"\"\"\n",
    "        print(\"Creating sample data for testing...\")\n",
    "        \n",
    "        sample_papers = [\n",
    "            Paper(\n",
    "                paper_id=\"2511.20650\",\n",
    "                title=\"MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities\",\n",
    "                abstract=\"Traditional object detection models in medical imaging operate within a closed-set paradigm...\",\n",
    "                authors=[\"Tooba Tehreem Sheikh\", \"Jean Lahoud\", \"Rao Muhammad Anwer\"],\n",
    "                first_author=\"Tooba Tehreem Sheikh\",\n",
    "                topic=\"artificial intelligence\",\n",
    "                categories=[\"cs.CV\", \"cs.AI\"],\n",
    "                publish_date=\"2025-11-25\",\n",
    "                url=\"http://arxiv.org/abs/2511.20650\",\n",
    "                embedding_text=\"Paper Title: MedROV: Towards Real-Time Open-Vocabulary Detection...\",\n",
    "                quality_scores={\"overall_quality_score\": 0.975},\n",
    "                quality_tier=\"high\",\n",
    "                basic_keywords=[\"detection\", \"medical\", \"imaging\"],\n",
    "                domain_keywords=[\"object detection\", \"medical imaging\"]\n",
    "            ),\n",
    "            Paper(\n",
    "                paper_id=\"2511.20640\",\n",
    "                title=\"MotionV2V: Editing Motion in a Video\",\n",
    "                abstract=\"While generative video models have achieved remarkable fidelity and consistency...\",\n",
    "                authors=[\"Ryan Burgert\", \"Charles Herrmann\", \"Forrester Cole\"],\n",
    "                first_author=\"Ryan Burgert\",\n",
    "                topic=\"artificial intelligence\",\n",
    "                categories=[\"cs.CV\", \"cs.AI\", \"cs.GR\"],\n",
    "                publish_date=\"2025-11-25\",\n",
    "                url=\"http://arxiv.org/abs/2511.20640\",\n",
    "                embedding_text=\"Paper Title: MotionV2V: Editing Motion in a Video...\",\n",
    "                quality_scores={\"overall_quality_score\": 0.9},\n",
    "                quality_tier=\"high\",\n",
    "                basic_keywords=[\"video\", \"motion\", \"editing\"],\n",
    "                domain_keywords=[\"video editing\", \"motion control\"]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"✓ Created {len(sample_papers)} sample papers\")\n",
    "        return sample_papers\n",
    "    \n",
    "    @staticmethod\n",
    "    def _print_statistics(papers: List[Paper]):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total Papers: {len(papers)}\")\n",
    "        \n",
    "        # Topic distribution\n",
    "        topics = [p.topic for p in papers]\n",
    "        unique_topics = set(topics)\n",
    "        print(f\"Number of Topics: {len(unique_topics)}\")\n",
    "        \n",
    "        # Quality distribution\n",
    "        quality_tiers = [p.quality_tier for p in papers]\n",
    "        tier_counts = {tier: quality_tiers.count(tier) for tier in set(quality_tiers)}\n",
    "        print(\"\\nQuality Tier Distribution:\")\n",
    "        for tier, count in tier_counts.items():\n",
    "            print(f\"  {tier}: {count} papers ({count/len(papers)*100:.1f}%)\")\n",
    "        \n",
    "        # Author statistics\n",
    "        all_authors = [author for p in papers for author in p.authors]\n",
    "        unique_authors = set(all_authors)\n",
    "        print(f\"Total Authors: {len(unique_authors)}\")\n",
    "        \n",
    "        # Latest paper date\n",
    "        dates = [p.publish_date for p in papers if p.publish_date]\n",
    "        if dates:\n",
    "            print(f\"Latest Paper Date: {max(dates)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_topic_distribution(papers: List[Paper]) -> pd.DataFrame:\n",
    "        \"\"\"Analyze topic distribution\"\"\"\n",
    "        topic_counts = {}\n",
    "        for paper in papers:\n",
    "            topic = paper.topic\n",
    "            topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(list(topic_counts.items()), columns=['topic', 'count'])\n",
    "        df = df.sort_values('count', ascending=False)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# %%\n",
    "# ==================== 2. Embedding and Vector Database ====================\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"Embedding model wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = None, use_api: bool = False, api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize embedding model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Local model name\n",
    "            use_api: Whether to use API\n",
    "            api_key: API key\n",
    "        \"\"\"\n",
    "        self.use_api = use_api\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        if use_api and api_key:\n",
    "            self.model = None\n",
    "            print(\"✓ Using DeepSeek API for embedding\")\n",
    "        else:\n",
    "            print(f\"✓ Using local embedding model\")\n",
    "            try:\n",
    "                from sentence_transformers import SentenceTransformer\n",
    "                model_name = model_name or config.EMBEDDING_MODEL\n",
    "                self.model = SentenceTransformer(model_name)\n",
    "                print(f\"  Model: {model_name}\")\n",
    "                print(f\"  Dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "            except ImportError:\n",
    "                print(\"Warning: Cannot import sentence_transformers, using simple embedding\")\n",
    "                self.model = None\n",
    "    \n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embedding vectors\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        if not texts:\n",
    "            return np.array([])\n",
    "        \n",
    "        if self.use_api and self.api_key:\n",
    "            return self._embed_api(texts)\n",
    "        elif self.model:\n",
    "            return self._embed_local(texts)\n",
    "        else:\n",
    "            # Fall back to simple word vectors\n",
    "            return self._embed_simple(texts)\n",
    "    \n",
    "    def _embed_local(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Embed using local model\"\"\"\n",
    "        try:\n",
    "            # Batch processing to avoid memory issues\n",
    "            batch_size = 32\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                batch_embeddings = self.model.encode(batch, show_progress_bar=False)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            \n",
    "            return np.vstack(embeddings) if embeddings else np.array([])\n",
    "        except Exception as e:\n",
    "            print(f\"Local embedding failed: {e}\")\n",
    "            return self._embed_simple(texts)\n",
    "    \n",
    "    def _embed_api(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Embed using DeepSeek API\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            # Only process first few texts to avoid API limits\n",
    "            texts = texts[:10] if len(texts) > 10 else texts\n",
    "            \n",
    "            data = {\n",
    "                \"model\": \"text-embedding-3-small\",\n",
    "                \"input\": texts,\n",
    "                \"encoding_format\": \"float\"\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{config.API_BASE_URL}/embeddings\",\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            embeddings = [item[\"embedding\"] for item in result[\"data\"]]\n",
    "            return np.array(embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"API embedding failed: {e}\")\n",
    "            # Fall back to local model\n",
    "            if self.model:\n",
    "                return self._embed_local(texts)\n",
    "            else:\n",
    "                return self._embed_simple(texts)\n",
    "    \n",
    "    def _embed_simple(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Simple word vector embedding (fallback solution)\"\"\"\n",
    "        print(\"Using simple embedding method...\")\n",
    "        # Create simple word frequency vectors\n",
    "        vocab = {}\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            vector = np.zeros(len(vocab))\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word in vocab:\n",
    "                    vector[vocab[word]] += 1\n",
    "            # Normalize\n",
    "            if np.linalg.norm(vector) > 0:\n",
    "                vector = vector / np.linalg.norm(vector)\n",
    "            embeddings.append(vector)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def get_dimension(self) -> int:\n",
    "        \"\"\"Get embedding dimension\"\"\"\n",
    "        if self.use_api:\n",
    "            return 1536  # Dimension of DeepSeek text-embedding-3-small\n",
    "        elif self.model:\n",
    "            return self.model.get_sentence_embedding_dimension()\n",
    "        return 100  # Dimension of simple embedding\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Vector database\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder: EmbeddingModel):\n",
    "        self.embedder = embedder\n",
    "        self.documents: List[str] = []\n",
    "        self.metadata: List[Dict] = []\n",
    "        self.embeddings: np.ndarray = None\n",
    "    \n",
    "    def add_papers(self, papers: List[Paper], use_embedding_text: bool = True):\n",
    "        \"\"\"Add papers to vector store\"\"\"\n",
    "        print(f\"Processing {len(papers)} papers...\")\n",
    "        \n",
    "        for paper in tqdm(papers, desc=\"Adding papers\"):\n",
    "            # Convert to text\n",
    "            if use_embedding_text and paper.embedding_text:\n",
    "                doc_text = paper.embedding_text\n",
    "            else:\n",
    "                doc_text = paper.to_text()\n",
    "            \n",
    "            # Store document and metadata\n",
    "            self.documents.append(doc_text)\n",
    "            self.metadata.append({\n",
    "                'paper_id': paper.paper_id,\n",
    "                'title': paper.title,\n",
    "                'authors': paper.authors,\n",
    "                'first_author': paper.first_author,\n",
    "                'topic': paper.topic,\n",
    "                'categories': paper.categories,\n",
    "                'publish_date': paper.publish_date,\n",
    "                'quality_score': paper.get_quality_score(),\n",
    "                'quality_tier': paper.quality_tier\n",
    "            })\n",
    "        \n",
    "        # Generate embedding vectors\n",
    "        print(\"Generating embedding vectors...\")\n",
    "        self.embeddings = self.embedder.embed(self.documents)\n",
    "        \n",
    "        print(f\"✓ Vector store built successfully\")\n",
    "        print(f\"  Number of documents: {len(self.documents)}\")\n",
    "        print(f\"  Embedding dimension: {self.embeddings.shape[1]}\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = None, threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Semantic search\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = config.TOP_K_RETRIEVAL\n",
    "        if threshold is None:\n",
    "            threshold = config.SIMILARITY_THRESHOLD\n",
    "        \n",
    "        if len(self.documents) == 0 or self.embeddings is None:\n",
    "            print(\"Warning: Vector store is empty\")\n",
    "            return []\n",
    "        \n",
    "        # Query embedding\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get most similar documents\n",
    "        indices = np.argsort(similarities)[::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in indices:\n",
    "            similarity = float(similarities[idx])\n",
    "            \n",
    "            # Apply threshold filtering\n",
    "            if similarity < threshold and len(results) >= top_k:\n",
    "                continue\n",
    "            \n",
    "            result = {\n",
    "                'paper_id': self.metadata[idx]['paper_id'],\n",
    "                'document': self.documents[idx],\n",
    "                'metadata': self.metadata[idx],\n",
    "                'similarity': similarity,\n",
    "                'rank': len(results) + 1\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics\"\"\"\n",
    "        if len(self.documents) == 0:\n",
    "            return {'total_documents': 0}\n",
    "        \n",
    "        stats = {\n",
    "            'total_documents': len(self.documents),\n",
    "            'embedding_dimension': self.embeddings.shape[1] if self.embeddings is not None else 0,\n",
    "            'unique_topics': len(set(m['topic'] for m in self.metadata))\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# %%\n",
    "# ==================== 3. RAG System ====================\n",
    "\n",
    "class DeepSeekClient:\n",
    "    \"\"\"DeepSeek API client\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = config.API_BASE_URL\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    def generate_response(self, prompt: str, temperature: float = None, \n",
    "                         max_tokens: int = None, model: str = \"deepseek-chat\") -> str:\n",
    "        \"\"\"Generate answer\"\"\"\n",
    "        if temperature is None:\n",
    "            temperature = config.TEMPERATURE\n",
    "        if max_tokens is None:\n",
    "            max_tokens = config.MAX_ANSWER_LENGTH\n",
    "        \n",
    "        # If no API key, return mock response\n",
    "        if not self.api_key or self.api_key == \"your-deepseek-api-key\":\n",
    "            print(\"Warning: Using mock API response (please set correct API key)\")\n",
    "            return self._mock_response(prompt)\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            # Record token usage\n",
    "            if 'usage' in result:\n",
    "                self.total_tokens += result['usage']['total_tokens']\n",
    "            \n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API request failed: {e}\")\n",
    "            return f\"Error: Failed to generate answer ({str(e)})\"\n",
    "        except KeyError as e:\n",
    "            print(f\"API response parsing failed: {e}\")\n",
    "            return \"Error: Incorrect response format\"\n",
    "    \n",
    "    def _mock_response(self, prompt: str) -> str:\n",
    "        \"\"\"Mock API response\"\"\"\n",
    "        time.sleep(0.5)  # Simulate delay\n",
    "        \n",
    "        # Generate mock answer based on prompt content\n",
    "        if \"医学影像\" in prompt or \"medical\" in prompt.lower():\n",
    "            return \"\"\"Open-Vocabulary Object Detection (OVOD) in medical imaging is a technology that can detect object categories not seen during training.\n",
    "\n",
    "It solves the following problems of traditional object detection:\n",
    "1. Closed vocabulary limitation: Traditional methods can only detect categories present in the training set\n",
    "2. Data scarcity issue: Medical image annotation data is difficult to obtain\n",
    "3. Generalization ability: Can identify new lesions or anatomical structures\n",
    "\n",
    "MedROV is the latest research in this area, achieving real-time open-vocabulary detection [Paper 1].\"\"\"\n",
    "        elif \"视频运动编辑\" in prompt or \"video\" in prompt.lower():\n",
    "            return \"\"\"Recent technical breakthroughs in video motion editing include:\n",
    "\n",
    "1. MotionV2V model: Modifies video motion by editing sparse trajectories [Paper 2]\n",
    "2. Motion counterfactual generation: Creates video pairs with the same content but different motions\n",
    "3. Timestamp control: Can start editing from any time point and propagate naturally\n",
    "4. User studies show that MotionV2V achieves over 65% preference rate in comparison tests\"\"\"\n",
    "        else:\n",
    "            return \"This is a mock response. Please set a correct DeepSeek API key for actual use.\"\n",
    "    \n",
    "    def evaluate_answer(self, question: str, answer: str) -> Dict:\n",
    "        \"\"\"Evaluate answer quality using DeepSeek\"\"\"\n",
    "        # If no API key, return mock evaluation\n",
    "        if not self.api_key or self.api_key == \"your-deepseek-api-key\":\n",
    "            return self._mock_evaluation(question, answer)\n",
    "        \n",
    "        prompt = f\"\"\"Please evaluate the quality of the following answer generated by a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Reference Paper Context Hint: The RAG system retrieves information from the latest academic papers (including but not limited to paper IDs: 2511.20650, 2511.20649). The answer should leverage the unique, up-to-date knowledge from these papers.\n",
    "\n",
    "Please provide a score from 1-5 (5 being best) for each of the following dimensions:\n",
    "1. Accuracy: Whether the answer content is factually correct and consistent with the retrieved paper information.\n",
    "2. Completeness: Whether the question is answered comprehensively, including both basic information and insights from the papers.\n",
    "3. Relevance: Whether the answer is closely related to the question, and whether the cited paper content is directly relevant to the question.\n",
    "4. Specificity: Whether it contains specific details (e.g., model names, datasets, metrics, technical components) and examples from the retrieved papers.\n",
    "5. Timeliness & Paper Utilization: Whether the answer uses up-to-date knowledge from the retrieved papers that cannot be obtained from general SPI calls; whether it correctly references the papers' core innovations.\n",
    "6. Overall quality: Comprehensive rating considering both text quality and RAG-specific advantages.\n",
    "\n",
    "Please return the results in JSON format, including scores and brief reasons for each dimension, as well as a total score (average of all scores, rounded to 2 decimal places).\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.generate_response(prompt, temperature=0.1, max_tokens=800)\n",
    "            \n",
    "            # Extract JSON part\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                evaluation = json.loads(json_match.group())\n",
    "                return evaluation\n",
    "            else:\n",
    "                return {\n",
    "                    \"accuracy\": {\"score\": 3, \"reason\": \"Cannot parse evaluation result\"},\n",
    "                    \"completeness\": {\"score\": 3, \"reason\": \"Cannot parse evaluation result\"},\n",
    "                    \"relevance\": {\"score\": 3, \"reason\": \"Cannot parse evaluation result\"},\n",
    "                    \"specificity\": {\"score\": 3, \"reason\": \"Cannot parse evaluation result\"},\n",
    "                    \"overall_quality\": {\"score\": 3, \"reason\": \"Cannot parse evaluation result\"},\n",
    "                    \"total_score\": 3.0\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            return self._mock_evaluation(question, answer)\n",
    "    \n",
    "    def _mock_evaluation(self, question: str, answer: str) -> Dict:\n",
    "        \"\"\"Mock evaluation result\"\"\"\n",
    "        return {\n",
    "            \"accuracy\": {\"score\": 4, \"reason\": \"Answer content is basically accurate\"},\n",
    "            \"completeness\": {\"score\": 3, \"reason\": \"Answers the main question but not comprehensively enough\"},\n",
    "            \"relevance\": {\"score\": 4, \"reason\": \"Highly relevant to the question\"},\n",
    "            \"specificity\": {\"score\": 3, \"reason\": \"Contains some specific information\"},\n",
    "            \"overall_quality\": {\"score\": 3.5, \"reason\": \"Good overall quality\"},\n",
    "            \"total_score\": 3.5\n",
    "        }\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, llm_client: DeepSeekClient):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def query(self, question: str, top_k: int = None, include_context: bool = True) -> Dict:\n",
    "        \"\"\"Execute RAG query\"\"\"\n",
    "        \n",
    "        # 1. Retrieve relevant documents\n",
    "        retrieved_docs = self.vector_store.search(question, top_k=top_k)\n",
    "        \n",
    "        # 2. Build prompt\n",
    "        if include_context and retrieved_docs:\n",
    "            context = self._build_context(retrieved_docs)\n",
    "            prompt = self._build_rag_prompt(question, context)\n",
    "            method = \"RAG\"\n",
    "        else:\n",
    "            prompt = self._build_baseline_prompt(question)\n",
    "            method = \"Baseline\"\n",
    "        \n",
    "        # 3. Generate answer\n",
    "        answer = self.llm.generate_response(prompt)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'method': method,\n",
    "            'prompt_preview': prompt[:200] + \"...\" if len(prompt) > 200 else prompt,\n",
    "        }\n",
    "    \n",
    "    def _build_context(self, docs: List[Dict]) -> str:\n",
    "        \"\"\"Build context\"\"\"\n",
    "        context_parts = [\"Based on the following research paper information:\"]\n",
    "        \n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            meta = doc['metadata']\n",
    "            context_parts.append(\n",
    "                f\"[Paper {i}] {meta['title']}\\n\"\n",
    "                f\"Authors: {meta['first_author']} et al.\\n\"\n",
    "                f\"Abstract: {self._truncate_text(meta.get('abstract', doc['document']), 200)}\\n\"\n",
    "                f\"Relevance: {doc['similarity']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def _build_rag_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"Build RAG prompt\"\"\"\n",
    "        return f\"\"\"You are an AI research assistant. Please answer the user's question based on the provided academic literature.\n",
    "\n",
    "Available Literature:\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Please answer according to the following requirements:\n",
    "1. Mainly based on the provided literature information\n",
    "2. Cite relevant literature in your answer, formatted as [Paper 1], [Paper 2], etc.\n",
    "3. If the literature information is insufficient, you can appropriately supplement relevant knowledge\n",
    "4. Maintain academic rigor\n",
    "\n",
    "Please provide a detailed and accurate answer:\"\"\"\n",
    "    \n",
    "    def _build_baseline_prompt(self, question: str) -> str:\n",
    "        \"\"\"Build baseline prompt (without RAG)\"\"\"\n",
    "        return f\"\"\"You are an AI research assistant. Please answer the following academic question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed and accurate answer:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _truncate_text(text: str, max_length: int) -> str:\n",
    "        \"\"\"Truncate text\"\"\"\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length] + \"...\"\n",
    "\n",
    "# %%\n",
    "# ==================== 4. Dual Evaluation System ====================\n",
    "\n",
    "class AutoEvaluationMetrics:\n",
    "    \"\"\"Automatic evaluation metrics calculation\"\"\"\n",
    "    \n",
    "    def evaluate_response(self, \n",
    "                         question: str,\n",
    "                         answer: str,\n",
    "                         retrieved_docs: List[Dict] = None,\n",
    "                         baseline_answer: str = None) -> Dict:\n",
    "        \"\"\"Evaluate a single response\"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            'answer_length': len(answer),\n",
    "            'word_count': len(re.findall(r'\\w+', answer)),\n",
    "            'has_error': 1 if \"Error:\" in answer or \"error\" in answer.lower() else 0,\n",
    "        }\n",
    "        \n",
    "        # Retrieval-related metrics\n",
    "        if retrieved_docs:\n",
    "            metrics.update(self._calculate_retrieval_metrics(retrieved_docs))\n",
    "        \n",
    "        # Content quality metrics\n",
    "        metrics.update(self._calculate_content_metrics(answer, question))\n",
    "        \n",
    "        # Citation quality metrics\n",
    "        metrics.update(self._calculate_citation_metrics(answer))\n",
    "        \n",
    "        # Comparison with baseline\n",
    "        if baseline_answer:\n",
    "            metrics.update(self._calculate_comparison_metrics(answer, baseline_answer))\n",
    "        \n",
    "        # Calculate automatic evaluation total score\n",
    "        metrics['auto_score'] = self._calculate_overall_score(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_retrieval_metrics(self, retrieved_docs: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate retrieval-related metrics\"\"\"\n",
    "        if not retrieved_docs:\n",
    "            return {}\n",
    "        \n",
    "        similarities = [doc['similarity'] for doc in retrieved_docs]\n",
    "        return {\n",
    "            'retrieved_docs_count': len(retrieved_docs),\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'max_similarity': max(similarities),\n",
    "        }\n",
    "    \n",
    "    def _calculate_content_metrics(self, answer: str, question: str) -> Dict:\n",
    "        \"\"\"Calculate content quality metrics\"\"\"\n",
    "        # Technical term detection\n",
    "        technical_terms = ['model', 'algorithm', 'detection', 'learning', 'training', 'accuracy', 'precision']\n",
    "        tech_term_count = sum(1 for term in technical_terms if term in answer)\n",
    "        \n",
    "        # Question keyword matching\n",
    "        question_words = set(re.findall(r'\\w+', question.lower()))\n",
    "        answer_words = set(re.findall(r'\\w+', answer.lower()))\n",
    "        keyword_matches = len(question_words.intersection(answer_words))\n",
    "        \n",
    "        return {\n",
    "            'technical_terms': tech_term_count,\n",
    "            'keyword_matches': keyword_matches,\n",
    "            'has_citation': 1 if '[' in answer and ']' in answer else 0,\n",
    "        }\n",
    "    \n",
    "    def _calculate_citation_metrics(self, answer: str) -> Dict:\n",
    "        \"\"\"Calculate citation quality metrics\"\"\"\n",
    "        # Detect citations\n",
    "        citations = re.findall(r'\\[.*?\\d+.*?\\]', answer)\n",
    "        return {\n",
    "            'citation_count': len(citations),\n",
    "            'unique_citations': len(set(citations)),\n",
    "        }\n",
    "    \n",
    "    def _calculate_comparison_metrics(self, rag_answer: str, baseline_answer: str) -> Dict:\n",
    "        \"\"\"Calculate comparison metrics\"\"\"\n",
    "        rag_length = len(rag_answer)\n",
    "        baseline_length = len(baseline_answer)\n",
    "        \n",
    "        return {\n",
    "            'length_ratio': rag_length / max(baseline_length, 1),\n",
    "            'length_difference': rag_length - baseline_length,\n",
    "        }\n",
    "    \n",
    "    def _calculate_overall_score(self, metrics: Dict) -> float:\n",
    "        \"\"\"Calculate automatic evaluation total score\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Length score (moderate is better)\n",
    "        length = metrics.get('answer_length', 0)\n",
    "        if 200 <= length <= 800:\n",
    "            score += 0.3\n",
    "        elif length > 50:\n",
    "            score += 0.2\n",
    "        \n",
    "        # Technical term score\n",
    "        tech_terms = metrics.get('technical_terms', 0)\n",
    "        score += min(tech_terms * 0.1, 0.3)\n",
    "        \n",
    "        # Citation score\n",
    "        citations = metrics.get('citation_count', 0)\n",
    "        score += min(citations * 0.2, 0.3)\n",
    "        \n",
    "        # Similarity score\n",
    "        avg_sim = metrics.get('avg_similarity', 0)\n",
    "        score += avg_sim * 0.1\n",
    "        \n",
    "        # No error bonus\n",
    "        if metrics.get('has_error', 1) == 0:\n",
    "            score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "\n",
    "class CombinedEvaluator:\n",
    "    \"\"\"Combined evaluator (automatic evaluation + API evaluation)\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem, llm_client: DeepSeekClient):\n",
    "        self.rag_system = rag_system\n",
    "        self.llm_client = llm_client\n",
    "        self.auto_evaluator = AutoEvaluationMetrics()\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_question(self, question_data: Dict) -> Dict:\n",
    "        \"\"\"Evaluate a single question\"\"\"\n",
    "        question = question_data['question']\n",
    "        topic = question_data.get('topic', '')\n",
    "        \n",
    "        print(f\"\\nEvaluating question: {question}\")\n",
    "        \n",
    "        # RAG query\n",
    "        rag_response = self.rag_system.query(question, include_context=True)\n",
    "        \n",
    "        # Baseline query\n",
    "        baseline_response = self.rag_system.query(question, include_context=False)\n",
    "        \n",
    "        # Automatic evaluation (RAG)\n",
    "        auto_metrics_rag = self.auto_evaluator.evaluate_response(\n",
    "            question=question,\n",
    "            answer=rag_response['answer'],\n",
    "            retrieved_docs=rag_response['retrieved_docs'],\n",
    "            baseline_answer=baseline_response['answer']\n",
    "        )\n",
    "        \n",
    "        # Automatic evaluation (baseline)\n",
    "        auto_metrics_baseline = self.auto_evaluator.evaluate_response(\n",
    "            question=question,\n",
    "            answer=baseline_response['answer'],\n",
    "            retrieved_docs=None,  # Baseline has no retrieved documents\n",
    "            baseline_answer=None  # Baseline has no comparison object\n",
    "        )\n",
    "        \n",
    "        # API evaluation (RAG answer)\n",
    "        api_evaluation_rag = self.llm_client.evaluate_answer(question, rag_response['answer'])\n",
    "        \n",
    "        # API evaluation (baseline answer)\n",
    "        api_evaluation_baseline = self.llm_client.evaluate_answer(question, baseline_response['answer'])\n",
    "        \n",
    "        # Build result\n",
    "        result = {\n",
    "            'question_id': len(self.results) + 1,\n",
    "            'question': question,\n",
    "            'topic': topic,\n",
    "            \n",
    "            # RAG results\n",
    "            'rag_answer': rag_response['answer'],\n",
    "            'rag_method': rag_response['method'],\n",
    "            'rag_retrieved_docs': [\n",
    "                {\n",
    "                    'title': doc['metadata']['title'],\n",
    "                    'similarity': float(doc['similarity']),\n",
    "                    'first_author': doc['metadata']['first_author']\n",
    "                }\n",
    "                for doc in rag_response['retrieved_docs']\n",
    "            ],\n",
    "            \n",
    "            # Baseline results\n",
    "            'baseline_answer': baseline_response['answer'],\n",
    "            'baseline_method': baseline_response['method'],\n",
    "            \n",
    "            # Automatic evaluation results (RAG)\n",
    "            'auto_evaluation_rag': auto_metrics_rag,\n",
    "            'auto_score_rag': auto_metrics_rag.get('auto_score', 0),\n",
    "            \n",
    "            # Automatic evaluation results (baseline)\n",
    "            'auto_evaluation_baseline': auto_metrics_baseline,\n",
    "            'auto_score_baseline': auto_metrics_baseline.get('auto_score', 0),\n",
    "            \n",
    "            # API evaluation results\n",
    "            'api_evaluation_rag': api_evaluation_rag,\n",
    "            'api_evaluation_baseline': api_evaluation_baseline,\n",
    "            'api_score_rag': api_evaluation_rag.get('total_score', 0),\n",
    "            'api_score_baseline': api_evaluation_baseline.get('total_score', 0),\n",
    "            \n",
    "            # Comprehensive comparison\n",
    "            'auto_improvement': auto_metrics_rag.get('auto_score', 0) - auto_metrics_baseline.get('auto_score', 0),\n",
    "            'api_improvement': api_evaluation_rag.get('total_score', 0) - api_evaluation_baseline.get('total_score', 0),\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        \n",
    "        # Print brief results\n",
    "        print(f\"  Automatic scores: RAG={auto_metrics_rag.get('auto_score', 0):.2f}, Baseline={auto_metrics_baseline.get('auto_score', 0):.2f}\")\n",
    "        print(f\"  API scores: RAG={api_evaluation_rag.get('total_score', 0):.2f}, Baseline={api_evaluation_baseline.get('total_score', 0):.2f}\")\n",
    "        print(f\"  Retrieved documents: {len(rag_response['retrieved_docs'])}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def evaluate_all(self, questions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Evaluate all questions\"\"\"\n",
    "        print(f\"Evaluating {len(questions)} questions...\")\n",
    "        \n",
    "        for question_data in questions:\n",
    "            self.evaluate_question(question_data)\n",
    "        \n",
    "        print(f\"\\n✓ All evaluations completed\")\n",
    "        return self.results\n",
    "    \n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"Generate evaluation report\"\"\"\n",
    "        if not self.results:\n",
    "            return {}\n",
    "        \n",
    "        # Collect statistics\n",
    "        auto_scores_rag = [r['auto_score_rag'] for r in self.results]\n",
    "        auto_scores_baseline = [r['auto_score_baseline'] for r in self.results]\n",
    "        api_scores_rag = [r['api_score_rag'] for r in self.results]\n",
    "        api_scores_baseline = [r['api_score_baseline'] for r in self.results]\n",
    "        auto_improvements = [r['auto_improvement'] for r in self.results]\n",
    "        api_improvements = [r['api_improvement'] for r in self.results]\n",
    "        \n",
    "        # Calculate correlation (automatic evaluation vs API evaluation)\n",
    "        if len(auto_scores_rag) > 1:\n",
    "            correlation = np.corrcoef(auto_scores_rag, api_scores_rag)[0, 1]\n",
    "        else:\n",
    "            correlation = 0\n",
    "        \n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_questions': len(self.results),\n",
    "                'avg_auto_score_rag': float(np.mean(auto_scores_rag)),\n",
    "                'avg_auto_score_baseline': float(np.mean(auto_scores_baseline)),\n",
    "                'avg_api_score_rag': float(np.mean(api_scores_rag)),\n",
    "                'avg_api_score_baseline': float(np.mean(api_scores_baseline)),\n",
    "                'avg_auto_improvement': float(np.mean(auto_improvements)),\n",
    "                'avg_api_improvement': float(np.mean(api_improvements)),\n",
    "                'auto_improvement_rate': sum(1 for imp in auto_improvements if imp > 0) / len(auto_improvements),\n",
    "                'api_improvement_rate': sum(1 for imp in api_improvements if imp > 0) / len(api_improvements),\n",
    "                'correlation_auto_vs_api': float(correlation),\n",
    "                'total_tokens_used': self.llm_client.total_tokens\n",
    "            },\n",
    "            'detailed_results': self.results\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save results to files\"\"\"\n",
    "        # Save JSON report\n",
    "        report = self.generate_report()\n",
    "        with open(config.REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✓ Evaluation report saved to {config.REPORT_FILE}\")\n",
    "        \n",
    "        # Save CSV results\n",
    "        df_data = []\n",
    "        for result in self.results:\n",
    "            row = {\n",
    "                'question_id': result['question_id'],\n",
    "                'question': result['question'],\n",
    "                'topic': result['topic'],\n",
    "                'rag_answer_length': len(result['rag_answer']),\n",
    "                'baseline_answer_length': len(result['baseline_answer']),\n",
    "                'auto_score_rag': result['auto_score_rag'],\n",
    "                'auto_score_baseline': result['auto_score_baseline'],\n",
    "                'api_score_rag': result['api_score_rag'],\n",
    "                'api_score_baseline': result['api_score_baseline'],\n",
    "                'auto_improvement': result['auto_improvement'],\n",
    "                'api_improvement': result['api_improvement'],\n",
    "                'retrieved_docs_count': len(result['rag_retrieved_docs']),\n",
    "            }\n",
    "            \n",
    "            # Add RAG automatic evaluation metrics\n",
    "            for key, value in result['auto_evaluation_rag'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    row[f'auto_rag_{key}'] = value\n",
    "            \n",
    "            # Add baseline automatic evaluation metrics\n",
    "            for key, value in result['auto_evaluation_baseline'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    row[f'auto_baseline_{key}'] = value\n",
    "            \n",
    "            df_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        df.to_csv(config.EVALUATION_CSV, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✓ Detailed results saved to {config.EVALUATION_CSV}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# %%\n",
    "# ==================== 5. Visualization and Main Program ====================\n",
    "\n",
    "def create_visualizations(results: List[Dict], save_dir: str = \".\"):\n",
    "    \"\"\"Create visualization charts\"\"\"\n",
    "    if not VISUALIZATION_AVAILABLE:\n",
    "        print(\"Warning: Visualization libraries not available, skipping chart generation\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        df = pd.DataFrame([{\n",
    "            'question_id': r['question_id'],\n",
    "            'auto_score_rag': r['auto_score_rag'],\n",
    "            'auto_score_baseline': r['auto_score_baseline'],\n",
    "            'api_score_rag': r['api_score_rag'],\n",
    "            'api_score_baseline': r['api_score_baseline'],\n",
    "            'auto_improvement': r['auto_improvement'],\n",
    "            'api_improvement': r['api_improvement'],\n",
    "            'retrieved_docs': len(r['rag_retrieved_docs'])\n",
    "        } for r in results])\n",
    "        \n",
    "        # Create charts\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # 1. API Score Comparison\n",
    "        x = range(len(df))\n",
    "        width = 0.35\n",
    "        axes[0, 0].bar([i - width/2 for i in x], df['api_score_baseline'], width, label='Baseline', alpha=0.7)\n",
    "        axes[0, 0].bar([i + width/2 for i in x], df['api_score_rag'], width, label='RAG', alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Question ID')\n",
    "        axes[0, 0].set_ylabel('API Score')\n",
    "        axes[0, 0].set_title('RAG vs Baseline API Score Comparison')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        # 2. Automatic Score Comparison\n",
    "        axes[0, 1].bar([i - width/2 for i in x], df['auto_score_baseline'], width, label='Baseline', alpha=0.7)\n",
    "        axes[0, 1].bar([i + width/2 for i in x], df['auto_score_rag'], width, label='RAG', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Question ID')\n",
    "        axes[0, 1].set_ylabel('Automatic Score')\n",
    "        axes[0, 1].set_title('RAG vs Baseline Automatic Score Comparison')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        # 3. API Improvement Distribution\n",
    "        axes[1, 0].bar(x, df['api_improvement'], color='green' if df['api_improvement'].mean() > 0 else 'red')\n",
    "        axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        axes[1, 0].set_xlabel('Question ID')\n",
    "        axes[1, 0].set_ylabel('API Improvement Score')\n",
    "        axes[1, 0].set_title('RAG API Improvement Score Distribution')\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        # 4. Automatic Improvement Distribution\n",
    "        axes[1, 1].bar(x, df['auto_improvement'], color='blue' if df['auto_improvement'].mean() > 0 else 'red')\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        axes[1, 1].set_xlabel('Question ID')\n",
    "        axes[1, 1].set_ylabel('Automatic Evaluation Improvement Score')\n",
    "        axes[1, 1].set_title('RAG Automatic Evaluation Improvement Score Distribution')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(df['question_id'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/evaluation_visualization.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()  # Close figure to avoid memory leak\n",
    "        print(f\"✓ Visualization chart saved to {save_dir}/evaluation_visualization.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Visualization generation failed: {e}\")\n",
    "        print(\"Skipping visualization generation\")\n",
    "\n",
    "def print_detailed_report(report: Dict):\n",
    "    \"\"\"Print detailed report\"\"\"\n",
    "    summary = report['summary']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG System Evaluation Report\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Total Questions: {summary['total_questions']}\")\n",
    "    print(f\"  Average Automatic Score (RAG): {summary['avg_auto_score_rag']:.3f}\")\n",
    "    print(f\"  Average Automatic Score (Baseline): {summary['avg_auto_score_baseline']:.3f}\")\n",
    "    print(f\"  Average API Score (RAG): {summary['avg_api_score_rag']:.3f}\")\n",
    "    print(f\"  Average API Score (Baseline): {summary['avg_api_score_baseline']:.3f}\")\n",
    "    print(f\"  Average Automatic Improvement: {summary['avg_auto_improvement']:.3f}\")\n",
    "    print(f\"  Average API Improvement: {summary['avg_api_improvement']:.3f}\")\n",
    "    print(f\"  Automatic Improvement Rate: {summary['auto_improvement_rate']:.2%}\")\n",
    "    print(f\"  API Improvement Rate: {summary['api_improvement_rate']:.2%}\")\n",
    "    print(f\"  Correlation between Auto and API Evaluation: {summary['correlation_auto_vs_api']:.3f}\")\n",
    "    print(f\"  Total Tokens Used: {summary['total_tokens_used']}\")\n",
    "    \n",
    "    # Print detailed results for each question\n",
    "    print(f\"\\nDetailed Results:\")\n",
    "    for result in report['detailed_results']:\n",
    "        print(f\"\\nQuestion {result['question_id']}: {result['question'][:50]}...\")\n",
    "        print(f\"  Automatic Scores: RAG={result['auto_score_rag']:.3f}, Baseline={result['auto_score_baseline']:.3f}\")\n",
    "        print(f\"  API Scores: RAG={result['api_score_rag']:.3f}, Baseline={result['api_score_baseline']:.3f}\")\n",
    "        print(f\"  Improvement: Auto={result['auto_improvement']:.3f}, API={result['api_improvement']:.3f}\")\n",
    "        print(f\"  Retrieved Documents: {len(result['rag_retrieved_docs'])}\")\n",
    "        \n",
    "        # Show retrieved documents\n",
    "        if result['rag_retrieved_docs']:\n",
    "            print(f\"  Relevant Documents:\")\n",
    "            for doc in result['rag_retrieved_docs'][:2]:  # Show only first 2\n",
    "                print(f\"    - {doc['title'][:50]}... (Similarity: {doc['similarity']:.3f})\")\n",
    "\n",
    "# %%\n",
    "# ==================== Main Program ====================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"RAG System Evaluation Framework\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\n1. Loading paper data...\")\n",
    "    papers = DataLoader.load_from_jsonl(config.DATA_FILE)\n",
    "    \n",
    "    if not papers:\n",
    "        print(\"Error: No paper data loaded\")\n",
    "        return\n",
    "    \n",
    "    # 2. Initialize embedding model\n",
    "    print(\"\\n2. Initializing embedding model...\")\n",
    "    embedder = EmbeddingModel(\n",
    "        model_name=config.EMBEDDING_MODEL,\n",
    "        use_api=False,  # Use local model to avoid API limits\n",
    "        api_key=config.DEEPSEEK_API_KEY\n",
    "    )\n",
    "    \n",
    "    # 3. Build vector database\n",
    "    print(\"\\n3. Building vector database...\")\n",
    "    vector_store = VectorStore(embedder)\n",
    "    vector_store.add_papers(papers)\n",
    "    \n",
    "    # Print vector store statistics\n",
    "    stats = vector_store.get_stats()\n",
    "    print(f\"  Total Documents: {stats['total_documents']}\")\n",
    "    print(f\"  Number of Topics: {stats['unique_topics']}\")\n",
    "    \n",
    "    # 4. Initialize RAG system\n",
    "    print(\"\\n4. Initializing RAG system...\")\n",
    "    llm_client = DeepSeekClient(config.DEEPSEEK_API_KEY)\n",
    "    rag_system = RAGSystem(vector_store, llm_client)\n",
    "    \n",
    "    # 5. Run evaluation\n",
    "    print(\"\\n5. Running evaluation...\")\n",
    "    evaluator = CombinedEvaluator(rag_system, llm_client)\n",
    "    results = evaluator.evaluate_all(config.TEST_QUESTIONS)\n",
    "    \n",
    "    # 6. Generate report\n",
    "    print(\"\\n6. Generating report...\")\n",
    "    report = evaluator.generate_report()\n",
    "    \n",
    "    # 7. Save results\n",
    "    print(\"\\n7. Saving results...\")\n",
    "    df = evaluator.save_results()\n",
    "    \n",
    "    # 8. Create visualizations\n",
    "    if VISUALIZATION_AVAILABLE and len(results) > 0:\n",
    "        print(\"\\n8. Creating visualization charts...\")\n",
    "        create_visualizations(results)\n",
    "    \n",
    "    # 9. Print report\n",
    "    print(\"\\n9. Evaluation Report:\")\n",
    "    print_detailed_report(report)\n",
    "    \n",
    "    # 10. Display DataFrame\n",
    "    print(\"\\n10. Results DataFrame:\")\n",
    "    print(df[['question_id', 'auto_score_rag', 'auto_score_baseline', 'api_score_rag', 'api_score_baseline', 'auto_improvement', 'api_improvement']].to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluation completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# %%\n",
    "# Run main program\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# %%\n",
    "# Quick test function (optional)\n",
    "def quick_test():\n",
    "    \"\"\"Quick test function\"\"\"\n",
    "    print(\"Running quick test...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    papers = DataLoader.create_sample_data()\n",
    "    \n",
    "    # Initialize components\n",
    "    embedder = EmbeddingModel(use_api=False)\n",
    "    vector_store = VectorStore(embedder)\n",
    "    vector_store.add_papers(papers)\n",
    "    \n",
    "    llm_client = DeepSeekClient(config.DEEPSEEK_API_KEY)\n",
    "    rag_system = RAGSystem(vector_store, llm_client)\n",
    "    \n",
    "    # Test single question\n",
    "    test_question = \"什么是医学影像中的开放词汇目标检测？\"\n",
    "    print(f\"\\nTest question: {test_question}\")\n",
    "    \n",
    "    response = rag_system.query(test_question)\n",
    "    print(f\"\\nRAG answer preview: {response['answer'][:200]}...\")\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = AutoEvaluationMetrics()\n",
    "    metrics = evaluator.evaluate_response(\n",
    "        question=test_question,\n",
    "        answer=response['answer'],\n",
    "        retrieved_docs=response['retrieved_docs']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAutomatic evaluation results:\")\n",
    "    print(f\"  Answer length: {metrics['answer_length']}\")\n",
    "    print(f\"  Technical terms: {metrics.get('technical_terms', 0)}\")\n",
    "    print(f\"  Citation count: {metrics.get('citation_count', 0)}\")\n",
    "    print(f\"  Automatic score: {metrics.get('auto_score', 0):.2f}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run quick test (uncomment the following line)\n",
    "# quick_test_response = quick_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLHW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
