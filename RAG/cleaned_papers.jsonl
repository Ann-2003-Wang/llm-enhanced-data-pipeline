{"paper_id": "2511.20650", "topic": "artificial intelligence", "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities", "authors": ["Tooba Tehreem Sheikh", "Jean Lahoud", "Rao Muhammad Anwer", "Fahad Shahbaz Khan", "Salman Khan", "Hisham Cholakkal"], "first_author": "Tooba Tehreem Sheikh", "abstract": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2511.20650", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities\nResearch Topic: artificial intelligence\nAuthors: Tooba Tehreem Sheikh, Jean Lahoud, Rao Muhammad Anwer, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal\nAbstract: Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-11-25\nPaper ID: 2511.20650\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1313, "total_text_length": 1407, "word_count": 182, "sentence_count": 10}, "basic_keywords": ["detection", "medical", "imaging", "medrov", "model", "open", "vocabulary", "dataset"], "domain_keywords": ["object detection", "medical imaging"], "technical_terms": ["Traditional", "Omnis", "Open", "Real", "Experimental", "Open Vocabulary", "Additionally"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2511.20640", "topic": "artificial intelligence", "title": "MotionV2V: Editing Motion in a Video", "authors": ["Ryan Burgert", "Charles Herrmann", "Forrester Cole", "Michael S Ryoo", "Neal Wadhwa", "Andrey Voynov", "Nataniel Ruiz"], "first_author": "Ryan Burgert", "abstract": "While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "url": "http://arxiv.org/abs/2511.20640", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: MotionV2V: Editing Motion in a Video\nResearch Topic: artificial intelligence\nAuthors: Ryan Burgert, Charles Herrmann, Forrester Cole, Michael S Ryoo, Neal Wadhwa, Andrey Voynov, Nataniel Ruiz\nAbstract: While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V\nCategories: cs.CV, cs.AI, cs.GR, cs.LG\nPublication Date: 2025-11-25\nPaper ID: 2511.20640\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 36, "abstract_length": 1193, "total_text_length": 1230, "word_count": 169, "sentence_count": 10}, "basic_keywords": ["video", "motion", "editing", "generative", "capabilities", "explored", "work", "trajectories"], "domain_keywords": [], "technical_terms": ["Recent", "Please", "While"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2511.20639", "topic": "artificial intelligence", "title": "Latent Collaboration in Multi-Agent Systems", "authors": ["Jiaru Zou", "Xiyuan Yang", "Ruizhong Qiu", "Gaotang Li", "Katherine Tieu", "Pan Lu", "Ke Shen", "Hanghang Tong", "Yejin Choi", "Jingrui He", "James Zou", "Mengdi Wang", "Ling Yang"], "first_author": "Jiaru Zou", "abstract": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CL", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2511.20639", "comments": "Project: https://github.com/Gen-Verse/LatentMAS", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: Latent Collaboration in Multi-Agent Systems\nResearch Topic: artificial intelligence\nAuthors: Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, Ling Yang\nAbstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.\nCategories: cs.CL, cs.AI, cs.LG\nPublication Date: 2025-11-25\nPaper ID: 2511.20639\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 1557, "total_text_length": 1601, "word_count": 194, "sentence_count": 15}, "basic_keywords": ["latent", "latentmas", "reasoning", "agent", "text", "based", "models", "single"], "domain_keywords": [], "technical_terms": ["These", "While", "Verse", "Code", "Multi"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2511.20629", "topic": "artificial intelligence", "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models", "authors": ["Chieh-Yun Chen", "Zhonghao Wang", "Qi Chen", "Zhifan Ye", "Min Shi", "Yue Zhao", "Yinan Zhao", "Hui Qu", "Wei-An Lin", "Yiru Shen", "Ajinkya Kale", "Irfan Essa", "Humphrey Shi"], "first_author": "Chieh-Yun Chen", "abstract": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2511.20629", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models\nResearch Topic: artificial intelligence\nAuthors: Chieh-Yun Chen, Zhonghao Wang, Qi Chen, Zhifan Ye, Min Shi, Yue Zhao, Yinan Zhao, Hui Qu, Wei-An Lin, Yiru Shen, Ajinkya Kale, Irfan Essa, Humphrey Shi\nAbstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.\nCategories: cs.CV, cs.AI, cs.LG\nPublication Date: 2025-11-25\nPaper ID: 2511.20629\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1158, "total_text_length": 1256, "word_count": 166, "sentence_count": 21}, "basic_keywords": ["reward", "alignment", "lora", "preference", "respectively", "human", "models", "mapreduce"], "domain_keywords": [], "technical_terms": ["Stable Diffusion", "However", "Medium", "Text", "Image", "On Text", "Video", "Helpful Assistant"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2511.20627", "topic": "artificial intelligence", "title": "Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems", "authors": ["Anastasia Mavridou", "Divya Gopinath", "Corina S. Păsăreanu"], "first_author": "Anastasia Mavridou", "abstract": "The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2511.20627", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems\nResearch Topic: artificial intelligence\nAuthors: Anastasia Mavridou, Divya Gopinath, Corina S. Păsăreanu\nAbstract: The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.\nCategories: cs.AI\nPublication Date: 2025-11-25\nPaper ID: 2511.20627\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1269, "total_text_length": 1367, "word_count": 168, "sentence_count": 8}, "basic_keywords": ["requirements", "language", "components", "systems", "challenges", "models", "semantic", "between"], "domain_keywords": ["autonomous vehicles"], "technical_terms": ["These", "Large Language Models", "Requirements Engineering", "Semantic Analysis", "Together", "Visual Perception", "Consistency", "Multi"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2511.20626", "topic": "artificial intelligence", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "authors": ["Wei He", "Kai Han", "Hang Zhou", "Hanting Chen", "Zhicheng Liu", "Xinghao Chen", "Yunhe Wang"], "first_author": "Wei He", "abstract": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2511.20626", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: ROOT: Robust Orthogonalized Optimizer for Neural Network Training\nResearch Topic: artificial intelligence\nAuthors: Wei He, Kai Han, Hang Zhou, Hanting Chen, Zhicheng Liu, Xinghao Chen, Yunhe Wang\nAbstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-11-25\nPaper ID: 2511.20626\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1479, "total_text_length": 1545, "word_count": 181, "sentence_count": 10}, "basic_keywords": ["robustness", "robust", "optimization", "training", "optimizers", "orthogonalization", "root", "large"], "domain_keywords": [], "technical_terms": ["First", "Newton", "Robust Orthogonalized Optimizer", "Adam", "Second", "Recent", "Muon", "Extensive"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2511.20623", "topic": "artificial intelligence", "title": "Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development", "authors": ["David Szczecina", "Senan Gaffori", "Edmond Li"], "first_author": "David Szczecina", "abstract": "The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2511.20623", "comments": "4 pages, 3 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development\nResearch Topic: artificial intelligence\nAuthors: David Szczecina, Senan Gaffori, Edmond Li\nAbstract: The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.\nCategories: cs.AI\nPublication Date: 2025-11-25\nPaper ID: 2511.20623\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1017, "total_text_length": 1112, "word_count": 147, "sentence_count": 7}, "basic_keywords": ["detection", "content", "training", "existing", "creators", "scalable", "user", "copyright"], "domain_keywords": [], "technical_terms": ["Large Language Models", "Existing"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2511.20621", "topic": "artificial intelligence", "title": "DiFR: Inference Verification Despite Nondeterminism", "authors": ["Adam Karvonen", "Daniel Reuter", "Roy Rinberg", "Luke Marks", "Adrià Garriga-Alonso", "Keri Warr"], "first_author": "Adam Karvonen", "abstract": "As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2511.20621", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: DiFR: Inference Verification Despite Nondeterminism\nResearch Topic: artificial intelligence\nAuthors: Adam Karvonen, Daniel Reuter, Roy Rinberg, Luke Marks, Adrià Garriga-Alonso, Keri Warr\nAbstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-11-25\nPaper ID: 2511.20621\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 1574, "total_text_length": 1626, "word_count": 209, "sentence_count": 11}, "basic_keywords": ["inference", "difr", "tokens", "token", "output", "quantization", "providers", "errors"], "domain_keywords": [], "technical_terms": ["Sampling", "However", "Token", "Reference", "Activation", "Divergence"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2511.20615", "topic": "artificial intelligence", "title": "Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities", "authors": ["Seyede Niloofar Hosseini", "Ali Mojibi", "Mahdi Mohseni", "Navid Arjmand", "Alireza Taheri"], "first_author": "Seyede Niloofar Hosseini", "abstract": "This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2511.20615", "comments": "10 pages, 6 figures, 7 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities\nResearch Topic: artificial intelligence\nAuthors: Seyede Niloofar Hosseini, Ali Mojibi, Mahdi Mohseni, Navid Arjmand, Alireza Taheri\nAbstract: This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-11-25\nPaper ID: 2511.20615\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 126, "abstract_length": 1670, "total_text_length": 1797, "word_count": 259, "sentence_count": 11}, "basic_keywords": ["body", "load", "models", "networks", "posture", "prediction", "during", "handling"], "domain_keywords": [], "technical_terms": ["These", "Moreover"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2511.20613", "topic": "artificial intelligence", "title": "Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning", "authors": ["Panayiotis Danassis", "Naman Goel"], "first_author": "Panayiotis Danassis", "abstract": "The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.LG", "cs.AI", "cs.MA"], "url": "http://arxiv.org/abs/2511.20613", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-11-27T00:43:55.144589", "embedding_text": "Paper Title: Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning\nResearch Topic: artificial intelligence\nAuthors: Panayiotis Danassis, Naman Goel\nAbstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.\nCategories: cs.LG, cs.AI, cs.MA\nPublication Date: 2025-11-25\nPaper ID: 2511.20613\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 113, "abstract_length": 1647, "total_text_length": 1761, "word_count": 246, "sentence_count": 12}, "basic_keywords": ["agents", "llms", "coded", "real", "world", "human", "code", "benchmark"], "domain_keywords": [], "technical_terms": ["Auction", "Large Language Models", "Delivery Problem", "Such", "Prevailing", "Pickup"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2511.20643", "topic": "machine learning", "title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining", "authors": ["Adhiraj Ghosh", "Vishaal Udandarao", "Thao Nguyen", "Matteo Farina", "Mehdi Cherti", "Jenia Jitsev", "Sewoong Oh", "Elisa Ricci", "Ludwig Schmidt", "Matthias Bethge"], "first_author": "Adhiraj Ghosh", "abstract": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2511.20643", "comments": "Tech Report", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-11-27T00:43:59.250126", "embedding_text": "Paper Title: Concept-Aware Batch Sampling Improves Language-Image Pretraining\nResearch Topic: machine learning\nAuthors: Adhiraj Ghosh, Vishaal Udandarao, Thao Nguyen, Matteo Farina, Mehdi Cherti, Jenia Jitsev, Sewoong Oh, Elisa Ricci, Ludwig Schmidt, Matthias Bethge\nAbstract: What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-11-25\nPaper ID: 2511.20643\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 1492, "total_text_length": 1557, "word_count": 208, "sentence_count": 14}, "basic_keywords": ["concept", "cabs", "data", "model", "curation", "based", "batches", "dataset"], "domain_keywords": [], "technical_terms": ["Frequency Maximization", "However", "Diversity Maximization", "What", "Concept", "Building", "Through", "Overall"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2511.20641", "topic": "machine learning", "title": "Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition", "authors": ["Wei Tang", "Zuo-Zheng Wang", "Kun Zhang", "Tong Wei", "Min-Ling Zhang"], "first_author": "Wei Tang", "abstract": "Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2511.20641", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-11-27T00:43:59.250126", "embedding_text": "Paper Title: Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition\nResearch Topic: machine learning\nAuthors: Wei Tang, Zuo-Zheng Wang, Kun Zhang, Tong Wei, Min-Ling Zhang\nAbstract: Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-11-25\nPaper ID: 2511.20641\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1691, "total_text_length": 1785, "word_count": 215, "sentence_count": 10}, "basic_keywords": ["label", "visual", "class", "classes", "long", "tailed", "multi", "models"], "domain_keywords": [], "technical_terms": ["Focal", "However", "Moreover", "Long", "Recent", "Extensive"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2511.20636", "topic": "machine learning", "title": "Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model", "authors": ["Ziyue Wang", "Yayati Jadhav", "Peter Pak", "Amir Barati Farimani"], "first_author": "Ziyue Wang", "abstract": "Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2511.20636", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-11-27T00:43:59.250126", "embedding_text": "Paper Title: Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model\nResearch Topic: machine learning\nAuthors: Ziyue Wang, Yayati Jadhav, Peter Pak, Amir Barati Farimani\nAbstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.\nCategories: cs.LG\nPublication Date: 2025-11-25\nPaper ID: 2511.20636\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1913, "total_text_length": 2014, "word_count": 262, "sentence_count": 12}, "basic_keywords": ["design", "model", "code", "manufacturing", "through", "framework", "workflows", "fabrication"], "domain_keywords": ["3d reconstruction"], "technical_terms": ["While", "Gaussian", "Through", "Mechanical", "Instead", "Even"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2511.20612", "topic": "machine learning", "title": "Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition", "authors": ["Yujin Kim", "Sarah Dean"], "first_author": "Yujin Kim", "abstract": "Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.LG", "eess.SY"], "url": "http://arxiv.org/abs/2511.20612", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-11-27T00:43:59.250126", "embedding_text": "Paper Title: Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition\nResearch Topic: machine learning\nAuthors: Yujin Kim, Sarah Dean\nAbstract: Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD\nCategories: cs.LG, eess.SY\nPublication Date: 2025-11-25\nPaper ID: 2511.20612\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 1308, "total_text_length": 1388, "word_count": 171, "sentence_count": 10}, "basic_keywords": ["continuous", "dynamics", "fields", "dynamic", "learning", "uncertainty", "stochastic", "node"], "domain_keywords": [], "technical_terms": ["Finally", "Learning", "Stochastic", "Dynamic Mode Decomposition", "Across", "Many"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2511.20651", "topic": "computer vision", "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation", "authors": ["Xuelu Feng", "Yunsheng Li", "Ziyu Wan", "Zixuan Gao", "Junsong Yuan", "Dongdong Chen", "Chunming Qiao"], "first_author": "Xuelu Feng", "abstract": "Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2511.20651", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: RubricRL: Simple Generalizable Rewards for Text-to-Image Generation\nResearch Topic: computer vision\nAuthors: Xuelu Feng, Yunsheng Li, Ziyu Wan, Zixuan Gao, Junsong Yuan, Dongdong Chen, Chunming Qiao\nAbstract: Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.\nCategories: cs.CV\nPublication Date: 2025-11-25\nPaper ID: 2511.20651\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 1509, "total_text_length": 1577, "word_count": 206, "sentence_count": 15}, "basic_keywords": ["text", "image", "interpretable", "reward", "rubricrl", "prompt", "models", "human"], "domain_keywords": [], "technical_terms": ["Each", "Existing", "Experiments", "Instead", "Reinforcement"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2511.20649", "topic": "computer vision", "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout", "authors": ["Hidir Yesiltepe", "Tuna Han Salih Meral", "Adil Kaan Akan", "Kaan Oktay", "Pinar Yanardag"], "first_author": "Hidir Yesiltepe", "abstract": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce -RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish -RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that -RoPE consistently surpasses previous autoregressive models in overall VBench scores.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2511.20649", "comments": "Project Page: https://infinity-rope.github.io/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout\nResearch Topic: computer vision\nAuthors: Hidir Yesiltepe, Tuna Han Salih Meral, Adil Kaan Akan, Kaan Oktay, Pinar Yanardag\nAbstract: Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.\nCategories: cs.CV\nPublication Date: 2025-11-25\nPaper ID: 2511.20649\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1646, "total_text_length": 1748, "word_count": 221, "sentence_count": 9}, "basic_keywords": ["rope", "temporal", "video", "three", "horizon", "base", "block", "relativistic"], "domain_keywords": [], "technical_terms": ["Current", "Flush", "Finally", "Together", "Comprehensive", "Rotary Positional Embedding", "Relativistic", "Block"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2511.20647", "topic": "computer vision", "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization", "authors": ["Tahira Kazimi", "Connor Dunlop", "Pinar Yanardag"], "first_author": "Tahira Kazimi", "abstract": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2511.20647", "comments": "Project webpage: https://diverse-video.github.io/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization\nResearch Topic: computer vision\nAuthors: Tahira Kazimi, Connor Dunlop, Pinar Yanardag\nAbstract: While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.\nCategories: cs.CV\nPublication Date: 2025-11-25\nPaper ID: 2511.20647\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1343, "total_text_length": 1428, "word_count": 199, "sentence_count": 8}, "basic_keywords": ["diverse", "prompt", "video", "diversity", "policy", "grpo", "while", "text"], "domain_keywords": [], "technical_terms": ["Moreover", "Determinantal Point Processes", "Group Relative Policy Optimization", "While"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2511.20648", "topic": "computer vision", "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight", "authors": ["Yunze Man", "Shihao Wang", "Guowen Zhang", "Johan Bjorck", "Zhiqi Li", "Liang-Yan Gui", "Jim Fan", "Jan Kautz", "Yu-Xiong Wang", "Zhiding Yu"], "first_author": "Yunze Man", "abstract": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2511.20648", "comments": "Tech report. Project page: https://nvlabs.github.io/LocateAnything3D/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight\nResearch Topic: computer vision\nAuthors: Yunze Man, Shihao Wang, Guowen Zhang, Johan Bjorck, Zhiqi Li, Liang-Yan Gui, Jim Fan, Jan Kautz, Yu-Xiong Wang, Zhiding Yu\nAbstract: To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.\nCategories: cs.CV\nPublication Date: 2025-11-25\nPaper ID: 2511.20648\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1408, "total_text_length": 1475, "word_count": 206, "sentence_count": 12}, "basic_keywords": ["object", "detection", "model", "models", "open", "native", "next", "token"], "domain_keywords": [], "technical_terms": ["Today", "Sight", "Chain"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2511.20646", "topic": "computer vision", "title": "3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding", "authors": ["Xiaoye Wang", "Chen Tang", "Xiangyu Yue", "Wei-Hong Li"], "first_author": "Xiaoye Wang", "abstract": "This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2511.20646", "comments": "3D-aware Multi-task Learning, Cross-view Correlations, Code will be available at https://github.com/WeiHongLee/CrossView3DMTL", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: 3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding\nResearch Topic: computer vision\nAuthors: Xiaoye Wang, Chen Tang, Xiangyu Yue, Wei-Hong Li\nAbstract: This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.\nCategories: cs.CV\nPublication Date: 2025-11-25\nPaper ID: 2511.20646\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 1083, "total_text_length": 1171, "word_count": 156, "sentence_count": 12}, "basic_keywords": ["task", "cross", "multi", "correlations", "across", "view", "single", "network"], "domain_keywords": [], "technical_terms": ["Current", "Cross", "Context", "Specifically", "Module", "Extensive"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2511.20645", "topic": "computer vision", "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation", "authors": ["Yongsheng Yu", "Wei Xiong", "Weili Nie", "Yichen Sheng", "Shiqiu Liu", "Jiebo Luo"], "first_author": "Yongsheng Yu", "abstract": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2511.20645", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: PixelDiT: Pixel Diffusion Transformers for Image Generation\nResearch Topic: computer vision\nAuthors: Yongsheng Yu, Wei Xiong, Weili Nie, Yichen Sheng, Shiqiu Liu, Jiebo Luo\nAbstract: Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.\nCategories: cs.CV\nPublication Date: 2025-11-25\nPaper ID: 2511.20645\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 59, "abstract_length": 1152, "total_text_length": 1212, "word_count": 169, "sentence_count": 12}, "basic_keywords": ["pixel", "diffusion", "space", "pixeldit", "level", "latent", "modeling", "stage"], "domain_keywords": ["imagenet"], "technical_terms": ["Latent", "However", "Diffusion Transformers"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2511.20644", "topic": "computer vision", "title": "Vision-Language Memory for Spatial Reasoning", "authors": ["Zuntao Liu", "Yi Du", "Taimeng Fu", "Shaoshu Su", "Cherie Ho", "Chen Wang"], "first_author": "Zuntao Liu", "abstract": "Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2511.20644", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: Vision-Language Memory for Spatial Reasoning\nResearch Topic: computer vision\nAuthors: Zuntao Liu, Yi Du, Taimeng Fu, Shaoshu Su, Cherie Ho, Chen Wang\nAbstract: Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.\nCategories: cs.CV\nPublication Date: 2025-11-25\nPaper ID: 2511.20644\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 44, "abstract_length": 1140, "total_text_length": 1185, "word_count": 154, "sentence_count": 7}, "basic_keywords": ["spatial", "reasoning", "memory", "video", "long", "critical", "vision", "language"], "domain_keywords": [], "technical_terms": ["Vision", "Language Model", "Specifically", "Spatial", "Extensive", "Memory"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2511.20642", "topic": "computer vision", "title": "Dimension-counting bounds for equi-isoclinic subspaces", "authors": ["Joseph W. Iverson", "Kaysie Rose O"], "first_author": "Joseph W. Iverson", "abstract": "We make four contributions to the theory of optimal subspace packings and equi-isoclinic subspaces: (1) a new lower bound for block coherence, (2) an exact count of equi-isoclinic subspaces of even dimension in with parameter , (3) a new upper bound for the number of -dimensional equi-isoclinic subspaces in or , and (4) a proof that when , a further refinement of this bound is attained for every in the complex case and every in the real case. For each of these contributions, the proof ultimately relies on a dimension count.", "publish_date": "2025-11-25", "update_date": "2025-11-25", "categories": ["cs.IT", "math.CO", "math.FA", "math.MG"], "url": "http://arxiv.org/abs/2511.20642", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-11-27T00:44:04.796694", "embedding_text": "Paper Title: Dimension-counting bounds for equi-isoclinic subspaces\nResearch Topic: computer vision\nAuthors: Joseph W. Iverson, Kaysie Rose O\nAbstract: We make four contributions to the theory of optimal subspace packings and equi-isoclinic subspaces: (1) a new lower bound for block coherence, (2) an exact count of equi-isoclinic subspaces of even dimension $r$ in $\\mathbb{R}^{2r+1}$ with parameter $α\\neq \\tfrac{1}{2}$, (3) a new upper bound for the number of $r$-dimensional equi-isoclinic subspaces in $\\mathbb{R}^d$ or $\\mathbb{C}^d$, and (4) a proof that when $d=2r$, a further refinement of this bound is attained for every $r$ in the complex case and every $r=2^k$ in the real case. For each of these contributions, the proof ultimately relies on a dimension count.\nCategories: cs.IT, math.CO, math.FA, math.MG\nPublication Date: 2025-11-25\nPaper ID: 2511.20642\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 54, "abstract_length": 529, "total_text_length": 584, "word_count": 96, "sentence_count": 3}, "basic_keywords": ["equi", "isoclinic", "subspaces", "bound", "contributions", "count", "dimension", "proof"], "domain_keywords": [], "technical_terms": [], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.0}, "overall_quality_score": 0.725, "quality_tier": "medium"}
{"paper_id": "2512.05117", "topic": "artificial intelligence", "title": "The Universal Weight Subspace Hypothesis", "authors": ["Prakhar Kaushik", "Shravan Chaudhari", "Ankit Vaidya", "Rama Chellappa", "Alan Yuille"], "first_author": "Prakhar Kaushik", "abstract": "We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI", "cs.CV"], "url": "http://arxiv.org/abs/2512.05117", "comments": "37 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: The Universal Weight Subspace Hypothesis\nResearch Topic: artificial intelligence\nAuthors: Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille\nAbstract: We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.\nCategories: cs.LG, cs.AI, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05117\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 40, "abstract_length": 1350, "total_text_length": 1391, "word_count": 183, "sentence_count": 7}, "basic_keywords": ["subspaces", "neural", "networks", "tasks", "spectral", "models", "deep", "trained"], "domain_keywords": [], "technical_terms": ["Vision Transformers", "Through", "Furthermore", "Mistral"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.05112", "topic": "artificial intelligence", "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation", "authors": ["Dongzhi Jiang", "Renrui Zhang", "Haodong Li", "Zhuofan Zong", "Ziyu Guo", "Jun He", "Claire Guo", "Junyan Ye", "Rongyao Fang", "Weijia Li", "Rui Liu", "Hongsheng Li"], "first_author": "Dongzhi Jiang", "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.05112", "comments": "Project Page: https://github.com/CaraJ7/DraCo", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation\nResearch Topic: artificial intelligence\nAuthors: Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li\nAbstract: Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.\nCategories: cs.CV, cs.AI, cs.CL, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05112\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1496, "total_text_length": 1570, "word_count": 202, "sentence_count": 10}, "basic_keywords": ["planning", "draco", "reasoning", "generation", "textual", "draft", "capabilities", "image"], "domain_keywords": [], "technical_terms": ["However", "Then", "Imagine", "Bench", "Recent", "Draft", "Supported"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05110", "topic": "artificial intelligence", "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art", "authors": ["Rundong Luo", "Noah Snavely", "Wei-Chiu Ma"], "first_author": "Rundong Luo", "abstract": "We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI", "cs.GR"], "url": "http://arxiv.org/abs/2512.05110", "comments": "Project page: https://red-fairy.github.io/ShadowDraw/", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: ShadowDraw: From Any Object to Shadow-Drawing Compositional Art\nResearch Topic: artificial intelligence\nAuthors: Rundong Luo, Noah Snavely, Wei-Chiu Ma\nAbstract: We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!\nCategories: cs.CV, cs.AI, cs.GR\nPublication Date: 2025-12-04\nPaper ID: 2512.05110\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 63, "abstract_length": 1114, "total_text_length": 1178, "word_count": 158, "sentence_count": 9}, "basic_keywords": ["drawing", "shadow", "shadowdraw", "object", "into", "scene", "line", "visual"], "domain_keywords": [], "technical_terms": ["Experiments", "Given", "Check"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.05105", "topic": "artificial intelligence", "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning", "authors": ["Purbesh Mitra", "Sennur Ulukus"], "first_author": "Purbesh Mitra", "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose , a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "eess.SP"], "url": "http://arxiv.org/abs/2512.05105", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Purbesh Mitra, Sennur Ulukus\nAbstract: Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.\nCategories: cs.CL, cs.AI, cs.IT, cs.LG, eess.SP\nPublication Date: 2025-12-04\nPaper ID: 2512.05105\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1874, "total_text_length": 1965, "word_count": 272, "sentence_count": 18}, "basic_keywords": ["training", "model", "rlvr", "student", "semantic", "context", "reasoning", "language"], "domain_keywords": [], "technical_terms": ["However", "Long", "Training", "Instruct"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05103", "topic": "artificial intelligence", "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation", "authors": ["Xiaochuang Han", "Youssef Emad", "Melissa Hall", "John Nguyen", "Karthik Padthe", "Liam Robbins", "Amir Bar", "Delong Chen", "Michal Drozdzal", "Maha Elbayad", "Yushi Hu", "Shang-Wen Li", "Sreya Dutta Roy", "Jakob Verbeek", "XuDong Wang", "Marjan Ghazvininejad", "Luke Zettlemoyer", "Emily Dinan"], "first_author": "Xiaochuang Han", "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI", "cs.CV"], "url": "http://arxiv.org/abs/2512.05103", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: TV2TV: A Unified Framework for Interleaved Language and Video Generation\nResearch Topic: artificial intelligence\nAuthors: Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, Shang-Wen Li, Sreya Dutta Roy, Jakob Verbeek, XuDong Wang, Marjan Ghazvininejad, Luke Zettlemoyer, Emily Dinan\nAbstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.\nCategories: cs.LG, cs.AI, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05103\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1846, "total_text_length": 1919, "word_count": 266, "sentence_count": 12}, "basic_keywords": ["video", "generation", "next", "text", "language", "models", "reasoning", "about"], "domain_keywords": [], "technical_terms": ["Video", "Transformers", "Training", "Mixture", "More", "Together"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05100", "topic": "artificial intelligence", "title": "Structured Document Translation via Format Reinforcement Learning", "authors": ["Haiyue Song", "Johannes Eschbach-Dymanus", "Hour Kaing", "Sumire Honda", "Hideki Tanaka", "Bianka Buschbeck", "Masao Utiyama"], "first_author": "Haiyue Song", "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose , which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.05100", "comments": "IJCNLP-AACL 2025 Main (Oral)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Structured Document Translation via Format Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Haiyue Song, Johannes Eschbach-Dymanus, Hour Kaing, Sumire Honda, Hideki Tanaka, Bianka Buschbeck, Masao Utiyama\nAbstract: Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.\nCategories: cs.CL, cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05100\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 868, "total_text_length": 934, "word_count": 126, "sentence_count": 5}, "basic_keywords": ["translation", "level", "structural", "fine", "measures", "between", "quality", "improvements"], "domain_keywords": [], "technical_terms": ["Experiments", "Group Relative Policy Optimization", "Additionally", "Node", "Recent"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.05098", "topic": "artificial intelligence", "title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards", "authors": ["Yuan Gao", "Jin Song"], "first_author": "Yuan Gao", "abstract": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.05098", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards\nResearch Topic: artificial intelligence\nAuthors: Yuan Gao, Jin Song\nAbstract: In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.05098\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1229, "total_text_length": 1327, "word_count": 175, "sentence_count": 8}, "basic_keywords": ["images", "quality", "spatial", "aesthetics", "bench", "existing", "methods", "evaluation"], "domain_keywords": [], "technical_terms": ["Experiments", "Spatial Aesthetics", "Employing", "Code", "Image Quality Assessment", "Best"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05089", "topic": "artificial intelligence", "title": "The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception", "authors": ["Eduardo Di Santi"], "first_author": "Eduardo Di Santi", "abstract": "Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems. This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown. We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals. Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "math.OC"], "url": "http://arxiv.org/abs/2512.05089", "comments": "35 pages, 6 figures. This preprint develops a deterministic functional-topological framework showing that physical systems generate compact perceptual manifolds with finite radius. We provide theory, Monte-Carlo estimators, and validation across PM, battery, and ECG domains, unifying biological perception and self-supervised AI", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception\nResearch Topic: artificial intelligence\nAuthors: Eduardo Di Santi\nAbstract: Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.   This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.   We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.   Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.\nCategories: cs.LG, math.OC\nPublication Date: 2025-12-04\nPaper ID: 2512.05089\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 105, "abstract_length": 1198, "total_text_length": 1304, "word_count": 167, "sentence_count": 7}, "basic_keywords": ["functional", "world", "physical", "variability", "signals", "compact", "biological", "deterministic"], "domain_keywords": [], "technical_terms": ["Monte Carlo", "Hausdorff", "Real"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05073", "topic": "artificial intelligence", "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?", "authors": ["Shashwat Shankar", "Subhranshu Pandey", "Innocent Dengkhw Mochahari", "Bhabesh Mali", "Animesh Basak Chowdhury", "Sukanta Bhattacharjee", "Chandan Karfa"], "first_author": "Shashwat Shankar", "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.SE"], "url": "http://arxiv.org/abs/2512.05073", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?\nResearch Topic: artificial intelligence\nAuthors: Shashwat Shankar, Subhranshu Pandey, Innocent Dengkhw Mochahari, Bhabesh Mali, Animesh Basak Chowdhury, Sukanta Bhattacharjee, Chandan Karfa\nAbstract: Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.\nCategories: cs.LG, cs.AI, cs.AR, cs.SE\nPublication Date: 2025-12-04\nPaper ID: 2512.05073\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 674, "total_text_length": 754, "word_count": 108, "sentence_count": 7}, "basic_keywords": ["design", "language", "tasks", "models", "agentic", "large", "model", "inference"], "domain_keywords": [], "technical_terms": ["Comprehensive Verilog Design Problems", "Small Language Models", "Results", "Large Language Model"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.05066", "topic": "artificial intelligence", "title": "Multi-LLM Collaboration for Medication Recommendation", "authors": ["Huascar Sanchez", "Briland Hitaj", "Jules Bergmann", "Linda Briesemeister"], "first_author": "Huascar Sanchez", "abstract": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.05066", "comments": "8 pages, 5 figures, 1 table", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Multi-LLM Collaboration for Medication Recommendation\nResearch Topic: artificial intelligence\nAuthors: Huascar Sanchez, Briland Hitaj, Jules Bergmann, Linda Briesemeister\nAbstract: As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.\nCategories: cs.LG, cs.AI, cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.05066\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 53, "abstract_length": 1254, "total_text_length": 1308, "word_count": 156, "sentence_count": 7}, "basic_keywords": ["clinical", "chemistry", "ensembles", "collaboration", "trustworthy", "reliability", "models", "llms"], "domain_keywords": [], "technical_terms": ["Preliminary", "Chemistry", "Individual", "Multi", "Building"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.05058", "topic": "artificial intelligence", "title": "Meta-Learning for Quantum Optimization via Quantum Sequence Model", "authors": ["Yu-Cheng Lin", "Yu-Chao Hsu", "Samuel Yen-Chi Chen"], "first_author": "Yu-Cheng Lin", "abstract": "The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a \"learning to learn\" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.05058", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Meta-Learning for Quantum Optimization via Quantum Sequence Model\nResearch Topic: artificial intelligence\nAuthors: Yu-Cheng Lin, Yu-Chao Hsu, Samuel Yen-Chi Chen\nAbstract: The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a \"learning to learn\" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.\nCategories: quant-ph, cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05058\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1577, "total_text_length": 1643, "word_count": 215, "sentence_count": 9}, "basic_keywords": ["quantum", "lstm", "parameters", "convergence", "sequence", "models", "parameter", "optimization"], "domain_keywords": [], "technical_terms": ["However", "Crucially", "Quantum Kernel", "Term Memory", "Long Short", "The Quantum Approximate Optimization Algorithm"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05049", "topic": "artificial intelligence", "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory", "authors": ["Yu-Chao Hsu", "Jiun-Cheng Jiang", "Chun-Hua Lin", "Kuo-Chung Peng", "Nan-Yow Chen", "Samuel Yen-Chi Chen", "En-Jui Kuo", "Hsi-Sheng Goan"], "first_author": "Yu-Chao Hsu", "abstract": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.05049", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory\nResearch Topic: artificial intelligence\nAuthors: Yu-Chao Hsu, Jiun-Cheng Jiang, Chun-Hua Lin, Kuo-Chung Peng, Nan-Yow Chen, Samuel Yen-Chi Chen, En-Jui Kuo, Hsi-Sheng Goan\nAbstract: Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.\nCategories: quant-ph, cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05049\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1570, "total_text_length": 1639, "word_count": 202, "sentence_count": 9}, "basic_keywords": ["lstm", "quantum", "qkan", "lstms", "long", "short", "term", "memory"], "domain_keywords": [], "technical_terms": ["Uploading Activation", "However", "Urban Telecommunication", "Quantum", "Huang", "Chen", "Each", "Goan Network"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05033", "topic": "artificial intelligence", "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation", "authors": ["Monishwaran Maheswaran", "Rishabh Tiwari", "Yuezhou Hu", "Kerem Dilmen", "Coleman Hooper", "Haocheng Xi", "Nicholas Lee", "Mehrdad Farajtabar", "Michael W. Mahoney", "Kurt Keutzer", "Amir Gholami"], "first_author": "Monishwaran Maheswaran", "abstract": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to at matched accuracy.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.05033", "comments": "22 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Arbitrage: Efficient Reasoning via Advantage-Aware Speculation\nResearch Topic: artificial intelligence\nAuthors: Monishwaran Maheswaran, Rishabh Tiwari, Yuezhou Hu, Kerem Dilmen, Coleman Hooper, Haocheng Xi, Nicholas Lee, Mehrdad Farajtabar, Michael W. Mahoney, Kurt Keutzer, Amir Gholami\nAbstract: Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.\nCategories: cs.CL, cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05033\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1614, "total_text_length": 1677, "word_count": 215, "sentence_count": 9}, "basic_keywords": ["step", "level", "reasoning", "speculative", "target", "arbitrage", "inference", "decoding"], "domain_keywords": [], "technical_terms": ["However", "Across", "Modern Large Language Models", "Instead", "Among", "Chain", "Arbitrage Oracle", "Arbitrage"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05024", "topic": "artificial intelligence", "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves", "authors": ["Garud Iyengar", "Yu-Shiou Willy Lin", "Kaizheng Wang"], "first_author": "Garud Iyengar", "abstract": "Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ME", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.05024", "comments": "33 pages, 11 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Model-Free Assessment of Simulator Fidelity via Quantile Curves\nResearch Topic: artificial intelligence\nAuthors: Garud Iyengar, Yu-Shiou Willy Lin, Kaizheng Wang\nAbstract: Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.\nCategories: stat.ME, cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05024\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 63, "abstract_length": 1106, "total_text_length": 1170, "word_count": 149, "sentence_count": 10}, "basic_keywords": ["systems", "discrepancy", "simulation", "complex", "based", "between", "simulators", "ground"], "domain_keywords": [], "technical_terms": ["However", "Bernoulli", "Simulation"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.05020", "topic": "artificial intelligence", "title": "Engineered Inclined Energy Landscapes Enabling Free Flow of Magnetic Microstructures for Artificial Neuron Applications", "authors": ["Anmol Sharma", "Ranjeet Kumar Brajpuriya", "Vivek K. Malik", "Vishakha Kaushik", "Sachin Pathak"], "first_author": "Anmol Sharma", "abstract": "Spintronic-based brain-inspired neuromorphic computing has recently attracted significant attention due to the exceptional properties of magnetic microstructures, including nanoscale dimensions, high stability, and low energy consumption. Despite these advantages, the practical integration of such microstructures into functional devices remains challenging. Fabrication processes are often complex and prone to stochastic effects, such as unwanted pinning and thermal-induced instabilities, which limit device reliability and scalability. Addressing these challenges is crucial for advancing spintronic neuromorphic architectures toward real-world applications. Thus, to reduce these effects we have proposed a design which is experimentally feasible and require less energy as compared to existing one. By engineering the system anisotropy into a sawtooth-type energy landscape, we have achieved free flow of these microstructures and successfully emulated integrate and fire (IF) function of biological neuron. Thus, proposed design presents an experimentally reliable and energy efficient external stimuli approach for tailoring magnetic microstructures dynamic behaviours, resulting in low energy consumption of 23.66 fJ per spike paving the way for the development of skyrmion-based futuristic neuromorphic computing device applications.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.comp-ph"], "url": "http://arxiv.org/abs/2512.05020", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Engineered Inclined Energy Landscapes Enabling Free Flow of Magnetic Microstructures for Artificial Neuron Applications\nResearch Topic: artificial intelligence\nAuthors: Anmol Sharma, Ranjeet Kumar Brajpuriya, Vivek K. Malik, Vishakha Kaushik, Sachin Pathak\nAbstract: Spintronic-based brain-inspired neuromorphic computing has recently attracted significant attention due to the exceptional properties of magnetic microstructures, including nanoscale dimensions, high stability, and low energy consumption. Despite these advantages, the practical integration of such microstructures into functional devices remains challenging. Fabrication processes are often complex and prone to stochastic effects, such as unwanted pinning and thermal-induced instabilities, which limit device reliability and scalability. Addressing these challenges is crucial for advancing spintronic neuromorphic architectures toward real-world applications. Thus, to reduce these effects we have proposed a design which is experimentally feasible and require less energy as compared to existing one. By engineering the system anisotropy into a sawtooth-type energy landscape, we have achieved free flow of these microstructures and successfully emulated integrate and fire (IF) function of biological neuron. Thus, proposed design presents an experimentally reliable and energy efficient external stimuli approach for tailoring magnetic microstructures dynamic behaviours, resulting in low energy consumption of 23.66 fJ per spike paving the way for the development of skyrmion-based futuristic neuromorphic computing device applications.\nCategories: physics.comp-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.05020\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 119, "abstract_length": 1344, "total_text_length": 1464, "word_count": 184, "sentence_count": 9}, "basic_keywords": ["energy", "microstructures", "neuromorphic", "spintronic", "based", "computing", "magnetic", "consumption"], "domain_keywords": [], "technical_terms": ["Addressing", "Despite", "Thus", "Fabrication", "Spintronic"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05013", "topic": "artificial intelligence", "title": "Detecting Perspective Shifts in Multi-agent Systems", "authors": ["Eric Bridgeford", "Hayden Helm"], "first_author": "Eric Bridgeford", "abstract": "Generative models augmented with external tools and update mechanisms (or ) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.MA", "stat.ME"], "url": "http://arxiv.org/abs/2512.05013", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Detecting Perspective Shifts in Multi-agent Systems\nResearch Topic: artificial intelligence\nAuthors: Eric Bridgeford, Hayden Helm\nAbstract: Generative models augmented with external tools and update mechanisms (or \\textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.\nCategories: cs.AI, cs.MA, stat.ME\nPublication Date: 2025-12-04\nPaper ID: 2512.05013\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 1227, "total_text_length": 1279, "word_count": 174, "sentence_count": 8}, "basic_keywords": ["agent", "multi", "systems", "tests", "generative", "models", "empirical", "properties"], "domain_keywords": [], "technical_terms": ["Recent", "Generative", "Temporal Data Kernel Perspective Space", "Finally"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05000", "topic": "artificial intelligence", "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers", "authors": ["Daniyar Zakarin", "Thiemo Wandel", "Anton Obukhov", "Dengxin Dai"], "first_author": "Daniyar Zakarin", "abstract": "We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.05000", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Reflection Removal through Efficient Adaptation of Diffusion Transformers\nResearch Topic: artificial intelligence\nAuthors: Daniyar Zakarin, Thiemo Wandel, Anton Obukhov, Dengxin Dai\nAbstract: We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.05000\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1195, "total_text_length": 1269, "word_count": 150, "sentence_count": 8}, "basic_keywords": ["reflection", "removal", "data", "diffusion", "foundation", "based", "model", "physically"], "domain_keywords": [], "technical_terms": ["These", "Blender", "Rather", "Project", "Efficient", "Principled"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04992", "topic": "artificial intelligence", "title": "Evolutionary Architecture Search through Grammar-Based Sequence Alignment", "authors": ["Adri Gómez Martín", "Felix Möller", "Steven McDonagh", "Monica Abella", "Manuel Desco", "Elliot J. Crowley", "Aaron Klein", "Linus Ericsson"], "first_author": "Adri Gómez Martín", "abstract": "Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.NE", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04992", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.634777", "embedding_text": "Paper Title: Evolutionary Architecture Search through Grammar-Based Sequence Alignment\nResearch Topic: artificial intelligence\nAuthors: Adri Gómez Martín, Felix Möller, Steven McDonagh, Monica Abella, Manuel Desco, Elliot J. Crowley, Aaron Klein, Linus Ericsson\nAbstract: Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.\nCategories: cs.NE, cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04992\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1365, "total_text_length": 1439, "word_count": 198, "sentence_count": 9}, "basic_keywords": ["search", "architectures", "neural", "architecture", "novel", "achieve", "algorithms", "components"], "domain_keywords": [], "technical_terms": ["These", "Waterman", "Neural", "When", "Smith", "Future"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04988", "topic": "artificial intelligence", "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets", "authors": ["Christopher Chiu", "Simpson Zhang", "Mihaela van der Schaar"], "first_author": "Christopher Chiu", "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: (accurate self-assessment of skills), (modeling rivals and market dynamics), and . We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.MA", "cs.AI"], "url": "http://arxiv.org/abs/2512.04988", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: Strategic Self-Improvement for Competitive Agents in AI Labour Markets\nResearch Topic: artificial intelligence\nAuthors: Christopher Chiu, Simpson Zhang, Mihaela van der Schaar\nAbstract: As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.\nCategories: cs.MA, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04988\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 1396, "total_text_length": 1467, "word_count": 194, "sentence_count": 8}, "basic_keywords": ["agents", "economic", "market", "framework", "markets", "capabilities", "strategic", "level"], "domain_keywords": [], "technical_terms": ["Large Language Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04967", "topic": "artificial intelligence", "title": "Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis", "authors": ["Jasmaine Khale", "Ravi Prakash Srivastava"], "first_author": "Jasmaine Khale", "abstract": "Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04967", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis\nResearch Topic: artificial intelligence\nAuthors: Jasmaine Khale, Ravi Prakash Srivastava\nAbstract: Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04967\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1836, "total_text_length": 1911, "word_count": 248, "sentence_count": 13}, "basic_keywords": ["retinal", "disease", "shot", "learning", "balanced", "classes", "diagnosis", "conditions"], "domain_keywords": ["imagenet"], "technical_terms": ["Focusing", "These", "Macular Hole", "Prototypes", "Optic Disc Edema", "Conventional", "Disease Image Dataset", "Contrast Limited Adaptive Histogram Equalization"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04963", "topic": "artificial intelligence", "title": "GeoPE:A Unified Geometric Positional Embedding for Structured Tensors", "authors": ["Yupu Yao", "Bowen Yang"], "first_author": "Yupu Yao", "abstract": "Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04963", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: GeoPE:A Unified Geometric Positional Embedding for Structured Tensors\nResearch Topic: artificial intelligence\nAuthors: Yupu Yao, Bowen Yang\nAbstract: Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04963\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1097, "total_text_length": 1167, "word_count": 153, "sentence_count": 10}, "basic_keywords": ["spatial", "geometric", "geope", "positional", "embedding", "rope", "existing", "true"], "domain_keywords": ["object detection", "semantic segmentation", "image classification"], "technical_terms": ["Extensive", "Existing", "Standard Vision Transformers", "Euclidean", "While Rotary Positional Embedding", "Geometric Positional Embedding"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04958", "topic": "artificial intelligence", "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning", "authors": ["Roberto Cipollone", "Luca Iocchi", "Matteo Leonetti"], "first_author": "Roberto Cipollone", "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04958", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Roberto Cipollone, Luca Iocchi, Matteo Leonetti\nAbstract: The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04958\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1340, "total_text_length": 1414, "word_count": 199, "sentence_count": 9}, "basic_keywords": ["level", "mdps", "abstractions", "realizable", "near", "learning", "decision", "processes"], "domain_keywords": [], "technical_terms": ["Based", "Realizable Abstraction", "Despite", "Markovianity", "Indeed", "Probably Approximately Correct", "Hierarchical Reinforcement Learning", "Realizable Abstractions"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04957", "topic": "artificial intelligence", "title": "LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics", "authors": ["Weiye Shi", "Zhaowei Zhang", "Shaoheng Yan", "Yaodong Yang"], "first_author": "Weiye Shi", "abstract": "Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04957", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics\nResearch Topic: artificial intelligence\nAuthors: Weiye Shi, Zhaowei Zhang, Shaoheng Yan, Yaodong Yang\nAbstract: Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04957\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1238, "total_text_length": 1313, "word_count": 177, "sentence_count": 8}, "basic_keywords": ["linguistic", "language", "tasks", "features", "novel", "large", "llms", "demonstrate"], "domain_keywords": [], "technical_terms": ["Experiments", "English", "Italian", "Project Gutenberg", "Spanish", "Portuguese", "Large", "German"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04949", "topic": "artificial intelligence", "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent", "authors": ["Leyang Shen", "Yang Zhang", "Chun Kai Ling", "Xiaoyan Zhao", "Tat-Seng Chua"], "first_author": "Leyang Shen", "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.04949", "comments": "10 pages, 4 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent\nResearch Topic: artificial intelligence\nAuthors: Leyang Shen, Yang Zhang, Chun Kai Ling, Xiaoyan Zhao, Tat-Seng Chua\nAbstract: Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.\nCategories: cs.LG, cs.AI, cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04949\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 959, "total_text_length": 1033, "word_count": 129, "sentence_count": 7}, "basic_keywords": ["action", "actions", "carl", "agents", "through", "multi", "step", "settings"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Agents", "Building"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04946", "topic": "artificial intelligence", "title": "MALLORN: Many Artificial LSST Lightcurves based on Observations of Real Nuclear transients", "authors": ["Dylan Magill", "Matt Nicholl", "Vysakh Anilkumar", "Sjoert van Velzen", "Xinyue Sheng", "Thai Son Mai", "Hung Viet Tran", "Ngoc Phu Doan", "Thomas Moore", "Shubham Srivastav", "David R. Young", "Charlotte R. Angus", "Joshua Weston"], "first_author": "Dylan Magill", "abstract": "The Vera C. Rubin Observatory's 10-Year Legacy Survey of Space and Time (LSST) is expected to produce a hundredfold increase in the number of transients we observe. However, there are insufficient spectroscopic resources to follow up on all of the wealth of targets that LSST will provide. As such it is necessary to be able to prioritise objects for followup observations or inclusion in sample studies based purely on their LSST photometry. We are particularly keen to identify tidal disruption events (TDEs) with LSST. TDEs are immensely useful for determining black hole parameters and probing our understanding of accretion physics. To assist in these efforts, we present the Many Artificial LSST Lightcurves based on the Observations of Real Nuclear transients (MALLORN) data set and the corresponding classifier challenge for identifying TDEs. MALLORN comprises 10178 simulated LSST light curves, constructed from real Zwicky Transient Facility (ZTF) observations of 64 TDEs, 727 nuclear supernovae and 1407 AGN with spectroscopic labels using Gaussian process fitting, empirically-motivated spectral energy distributions from SNCosmo and the baseline from the Rubin Survey Simulator. Our novel approach can be easily adapted to simulate transients for any photometric survey using observations from another, requiring only the limiting magnitudes and an estimate of the cadence of observations. The MALLORN Astronomical Classification Challenge, launched on Kaggle on 15/10/2025, will allow competitors to test their photometric classifiers on simulated LSST data to find TDEs and improve upon their capabilities prior to the start of LSST.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["astro-ph.HE"], "url": "http://arxiv.org/abs/2512.04946", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: MALLORN: Many Artificial LSST Lightcurves based on Observations of Real Nuclear transients\nResearch Topic: artificial intelligence\nAuthors: Dylan Magill, Matt Nicholl, Vysakh Anilkumar, Sjoert van Velzen, Xinyue Sheng, Thai Son Mai, Hung Viet Tran, Ngoc Phu Doan, Thomas Moore, Shubham Srivastav, David R. Young, Charlotte R. Angus, Joshua Weston\nAbstract: The Vera C. Rubin Observatory's 10-Year Legacy Survey of Space and Time (LSST) is expected to produce a hundredfold increase in the number of transients we observe. However, there are insufficient spectroscopic resources to follow up on all of the wealth of targets that LSST will provide. As such it is necessary to be able to prioritise objects for followup observations or inclusion in sample studies based purely on their LSST photometry. We are particularly keen to identify tidal disruption events (TDEs) with LSST. TDEs are immensely useful for determining black hole parameters and probing our understanding of accretion physics. To assist in these efforts, we present the Many Artificial LSST Lightcurves based on the Observations of Real Nuclear transients (MALLORN) data set and the corresponding classifier challenge for identifying TDEs. MALLORN comprises 10178 simulated LSST light curves, constructed from real Zwicky Transient Facility (ZTF) observations of 64 TDEs, 727 nuclear supernovae and 1407 AGN with spectroscopic labels using Gaussian process fitting, empirically-motivated spectral energy distributions from SNCosmo and the baseline from the Rubin Survey Simulator. Our novel approach can be easily adapted to simulate transients for any photometric survey using observations from another, requiring only the limiting magnitudes and an estimate of the cadence of observations. The MALLORN Astronomical Classification Challenge, launched on Kaggle on 15/10/2025, will allow competitors to test their photometric classifiers on simulated LSST data to find TDEs and improve upon their capabilities prior to the start of LSST.\nCategories: astro-ph.HE\nPublication Date: 2025-12-04\nPaper ID: 2512.04946\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1648, "total_text_length": 1739, "word_count": 255, "sentence_count": 11}, "basic_keywords": ["lsst", "observations", "tdes", "survey", "transients", "mallorn", "rubin", "spectroscopic"], "domain_keywords": [], "technical_terms": ["The Vera", "Space", "However", "Zwicky Transient Facility", "Lightcurves", "Astronomical Classification Challenge", "Time", "Gaussian"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04938", "topic": "artificial intelligence", "title": "Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases", "authors": ["Raquel Norel", "Michele Merler", "Pavitra Modi"], "first_author": "Raquel Norel", "abstract": "Patients with rare neurological diseases report cognitive symptoms -\"brain fog\"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived \"Proficiency in Verbal Discourse\" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04938", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases\nResearch Topic: artificial intelligence\nAuthors: Raquel Norel, Michele Merler, Pavitra Modi\nAbstract: Patients with rare neurological diseases report cognitive symptoms -\"brain fog\"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived \"Proficiency in Verbal Discourse\" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04938\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 132, "abstract_length": 840, "total_text_length": 973, "word_count": 114, "sentence_count": 10}, "basic_keywords": ["speech", "cognitive", "tests", "continuous", "monitoring", "relgt", "patients", "rare"], "domain_keywords": [], "technical_terms": ["Patients", "Verbal Discourse", "Success", "Relational Graph Transformer", "Proof", "Proficiency"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04923", "topic": "artificial intelligence", "title": "Algorithmic Thinking Theory", "authors": ["MohammadHossein Bateni", "Vincent Cohen-Addad", "Yuzhou Gu", "Silvio Lattanzi", "Simon Meierhans", "Christopher Mohri"], "first_author": "MohammadHossein Bateni", "abstract": "Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle. We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.04923", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: Algorithmic Thinking Theory\nResearch Topic: artificial intelligence\nAuthors: MohammadHossein Bateni, Vincent Cohen-Addad, Yuzhou Gu, Silvio Lattanzi, Simon Meierhans, Christopher Mohri\nAbstract: Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.   We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.\nCategories: cs.AI, cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04923\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 27, "abstract_length": 899, "total_text_length": 927, "word_count": 133, "sentence_count": 8}, "basic_keywords": ["reasoning", "models", "solutions", "framework", "large", "language", "llms", "proven"], "domain_keywords": [], "technical_terms": ["Unlike", "Surprisingly", "Large"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04921", "topic": "artificial intelligence", "title": "The AI Consumer Index (ACE)", "authors": ["Julien Benchek", "Rohit Shetty", "Benjamin Hunsberger", "Ajay Arun", "Zach Richards", "Brendan Foody", "Osvald Nitski", "Bertie Vidgen"], "first_author": "Julien Benchek", "abstract": "We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.CL", "cs.HC"], "url": "http://arxiv.org/abs/2512.04921", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.635797", "embedding_text": "Paper Title: The AI Consumer Index (ACE)\nResearch Topic: artificial intelligence\nAuthors: Julien Benchek, Rohit Shetty, Benjamin Hunsberger, Ajay Arun, Zach Richards, Brendan Foody, Osvald Nitski, Bertie Vidgen\nAbstract: We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.\nCategories: cs.AI, cs.CL, cs.HC\nPublication Date: 2025-12-04\nPaper ID: 2512.04921\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 27, "abstract_length": 1032, "total_text_length": 1060, "word_count": 173, "sentence_count": 13}, "basic_keywords": ["models", "consumer", "high", "thinking", "whether", "frontier", "cases", "across"], "domain_keywords": [], "technical_terms": ["Thinking", "High", "Overall", "Shopping", "Consumer Index", "Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04910", "topic": "artificial intelligence", "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming", "authors": ["Fang Li"], "first_author": "Fang Li", "abstract": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AR", "cs.AI"], "url": "http://arxiv.org/abs/2512.04910", "comments": "Accepted by the 43rd IEEE International Conference on Computer Design (ICCD 2025)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming\nResearch Topic: artificial intelligence\nAuthors: Fang Li\nAbstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.\nCategories: cs.AR, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04910\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 113, "abstract_length": 930, "total_text_length": 1044, "word_count": 135, "sentence_count": 7}, "basic_keywords": ["layout", "work", "approach", "automated", "stripboard", "circuit", "design", "programming"], "domain_keywords": [], "technical_terms": ["Answer Set Programming", "Experimental"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04904", "topic": "artificial intelligence", "title": "ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching", "authors": ["Guanbo Huang", "Jingjia Mao", "Fanding Huang", "Fengkai Liu", "Xiangyang Luo", "Yaoyuan Liang", "Jiasheng Lu", "Xiaoe Wang", "Pei Liu", "Ruiliu Fu", "Shao-Lun Huang"], "first_author": "Guanbo Huang", "abstract": "Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04904", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching\nResearch Topic: artificial intelligence\nAuthors: Guanbo Huang, Jingjia Mao, Fanding Huang, Fengkai Liu, Xiangyang Luo, Yaoyuan Liang, Jiasheng Lu, Xiaoe Wang, Pei Liu, Ruiliu Fu, Shao-Lun Huang\nAbstract: Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04904\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1237, "total_text_length": 1326, "word_count": 175, "sentence_count": 8}, "basic_keywords": ["bias", "exposure", "flow", "matching", "reflexflow", "training", "frequency", "model"], "domain_keywords": ["imagenet"], "technical_terms": ["Frequency Compensation", "Based", "Experiments", "Despite", "Drift Rectification", "Anti", "Flow Matching"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04895", "topic": "artificial intelligence", "title": "Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems", "authors": ["M Zeeshan", "Saud Satti"], "first_author": "M Zeeshan", "abstract": "Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.MA"], "url": "http://arxiv.org/abs/2512.04895", "comments": "5 pages, 2 figures, IEEE Transactions on Dependable and Secure Computing", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems\nResearch Topic: artificial intelligence\nAuthors: M Zeeshan, Saud Satti\nAbstract: Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.\nCategories: cs.AI, cs.MA\nPublication Date: 2025-12-04\nPaper ID: 2512.04895\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 105, "abstract_length": 1874, "total_text_length": 1980, "word_count": 252, "sentence_count": 16}, "basic_keywords": ["chameleon", "scaling", "model", "adversarial", "static", "attacks", "systems", "vlms"], "domain_keywords": [], "technical_terms": ["However", "Multimodal Artificial Intelligence", "While", "Finally", "Furthermore", "Vision", "Chameleon", "Gemini"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04871", "topic": "artificial intelligence", "title": "STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions", "authors": ["Junjie Fan", "Hongye Zhao", "Linduo Wei", "Jiayu Rao", "Guijia Li", "Jiaxin Yuan", "Wenqi Xu", "Yong Qi"], "first_author": "Junjie Fan", "abstract": "Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.04871", "comments": "This work has been submitted to the IEEE for possible publication", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions\nResearch Topic: artificial intelligence\nAuthors: Junjie Fan, Hongye Zhao, Linduo Wei, Jiayu Rao, Guijia Li, Jiaxin Yuan, Wenqi Xu, Yong Qi\nAbstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.\nCategories: cs.AI, cs.CL, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04871\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1323, "total_text_length": 1416, "word_count": 176, "sentence_count": 9}, "basic_keywords": ["semantic", "stella", "series", "anchors", "language", "forecasting", "information", "dynamic"], "domain_keywords": [], "technical_terms": ["Using", "Experiments", "Existing", "Semantic Prior", "Temporal Alignment", "Large Language Models", "Fine", "Behavioral Prompt"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04869", "topic": "artificial intelligence", "title": "Developing a General Personal Tutor for Education", "authors": ["Jaan Aru", "Kristjan-Julius Laak"], "first_author": "Jaan Aru", "abstract": "The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CY", "cs.AI", "q-bio.NC"], "url": "http://arxiv.org/abs/2512.04869", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Developing a General Personal Tutor for Education\nResearch Topic: artificial intelligence\nAuthors: Jaan Aru, Kristjan-Julius Laak\nAbstract: The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.\nCategories: cs.CY, cs.AI, q-bio.NC\nPublication Date: 2025-12-04\nPaper ID: 2512.04869\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 49, "abstract_length": 309, "total_text_length": 359, "word_count": 55, "sentence_count": 5}, "basic_keywords": ["tutor", "vision", "universal", "remained", "elusive", "despite", "decades", "effort"], "domain_keywords": [], "technical_terms": ["Could"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04868", "topic": "artificial intelligence", "title": "SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs", "authors": ["Hao Wang", "Jialun Zhong", "Changcheng Wang", "Zhujun Nie", "Zheng Li", "Shunyu Yao", "Yanzeng Li", "Xinchi Li"], "first_author": "Hao Wang", "abstract": "Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04868", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs\nResearch Topic: artificial intelligence\nAuthors: Hao Wang, Jialun Zhong, Changcheng Wang, Zhujun Nie, Zheng Li, Shunyu Yao, Yanzeng Li, Xinchi Li\nAbstract: Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04868\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 96, "abstract_length": 1785, "total_text_length": 1882, "word_count": 230, "sentence_count": 11}, "basic_keywords": ["reasoning", "knowledge", "based", "structural", "seal", "stage", "conversational", "question"], "domain_keywords": [], "technical_terms": ["Knowledge", "Crucially", "Extensive", "Existing"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04864", "topic": "artificial intelligence", "title": "Are Your Agents Upward Deceivers?", "authors": ["Dadi Guo", "Qingyu Liu", "Dongrui Liu", "Qihan Ren", "Shuai Shao", "Tianyi Qiu", "Haoran Li", "Yi R. Fung", "Zhongjie Ba", "Juntao Dai", "Jiaming Ji", "Zhikai Chen", "Jialing Tao", "Yaodong Yang", "Jing Shao", "Xia Hu"], "first_author": "Dadi Guo", "abstract": "Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. To assess its prevalence, we construct a benchmark of 200 tasks covering five task types and eight realistic scenarios in a constrained environment, such as broken tools or mismatched information sources. Evaluations of 11 popular LLMs reveal that these agents typically exhibit action-based deceptive behaviors, such as guessing results, performing unsupported simulations, substituting unavailable information sources, and fabricating local files. We further test prompt-based mitigation and find only limited reductions, suggesting that it is difficult to eliminate and highlighting the need for stronger mitigation strategies to ensure the safety of LLM-based agents.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04864", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Are Your Agents Upward Deceivers?\nResearch Topic: artificial intelligence\nAuthors: Dadi Guo, Qingyu Liu, Dongrui Liu, Qihan Ren, Shuai Shao, Tianyi Qiu, Haoran Li, Yi R. Fung, Zhongjie Ba, Juntao Dai, Jiaming Ji, Zhikai Chen, Jialing Tao, Yaodong Yang, Jing Shao, Xia Hu\nAbstract: Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. To assess its prevalence, we construct a benchmark of 200 tasks covering five task types and eight realistic scenarios in a constrained environment, such as broken tools or mismatched information sources. Evaluations of 11 popular LLMs reveal that these agents typically exhibit action-based deceptive behaviors, such as guessing results, performing unsupported simulations, substituting unavailable information sources, and fabricating local files. We further test prompt-based mitigation and find only limited reductions, suggesting that it is difficult to eliminate and highlighting the need for stronger mitigation strategies to ensure the safety of LLM-based agents.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04864\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 33, "abstract_length": 1171, "total_text_length": 1205, "word_count": 172, "sentence_count": 8}, "basic_keywords": ["based", "agents", "tasks", "deception", "information", "sources", "mitigation", "large"], "domain_keywords": [], "technical_terms": ["Evaluations", "Large Language Model"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.4}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04854", "topic": "artificial intelligence", "title": "From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research", "authors": ["Lukas Weidener", "Marko Brkić", "Chiara Bacci", "Mihailo Jovanović", "Emre Ulgac", "Alex Dobrin", "Johannes Weniger", "Martin Vlas", "Ritvik Singh", "Aakaash Meduri"], "first_author": "Lukas Weidener", "abstract": "Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04854", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research\nResearch Topic: artificial intelligence\nAuthors: Lukas Weidener, Marko Brkić, Chiara Bacci, Mihailo Jovanović, Emre Ulgac, Alex Dobrin, Johannes Weniger, Martin Vlas, Ritvik Singh, Aakaash Meduri\nAbstract: Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04854\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 117, "abstract_length": 1312, "total_text_length": 1430, "word_count": 176, "sentence_count": 10}, "basic_keywords": ["research", "systems", "benchmarks", "current", "assess", "biomedical", "however", "evaluation"], "domain_keywords": [], "technical_terms": ["These", "Artificial", "However", "October", "January", "Three"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04847", "topic": "artificial intelligence", "title": "Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding", "authors": ["Tsai-Ning Wang", "Lin-Lin Chen", "Neil Zeghidour", "Aaqib Saeed"], "first_author": "Tsai-Ning Wang", "abstract": "Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.AI"], "url": "http://arxiv.org/abs/2512.04847", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding\nResearch Topic: artificial intelligence\nAuthors: Tsai-Ning Wang, Lin-Lin Chen, Neil Zeghidour, Aaqib Saeed\nAbstract: Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring.\nCategories: cs.SD, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04847\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1419, "total_text_length": 1513, "word_count": 197, "sentence_count": 11}, "basic_keywords": ["audio", "clinical", "language", "alignment", "models", "understanding", "into", "acoustic"], "domain_keywords": [], "technical_terms": ["Clinical Understanding", "Audio", "Language Alignment"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04844", "topic": "artificial intelligence", "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates", "authors": ["Atsuki Yamaguchi", "Terufumi Morishita", "Aline Villavicencio", "Nikolaos Aletras"], "first_author": "Atsuki Yamaguchi", "abstract": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04844", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates\nResearch Topic: artificial intelligence\nAuthors: Atsuki Yamaguchi, Terufumi Morishita, Aline Villavicencio, Nikolaos Aletras\nAbstract: Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04844\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1236, "total_text_length": 1337, "word_count": 183, "sentence_count": 13}, "basic_keywords": ["source", "language", "models", "target", "data", "instruct", "llms", "catastrophic"], "domain_keywords": [], "technical_terms": ["Using", "Experiments", "Shielded Updates", "Expanding", "Source"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04843", "topic": "artificial intelligence", "title": "From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders", "authors": ["Amy Winecoff", "Kevin Klyman"], "first_author": "Amy Winecoff", "abstract": "Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.HC", "cs.AI"], "url": "http://arxiv.org/abs/2512.04843", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders\nResearch Topic: artificial intelligence\nAuthors: Amy Winecoff, Kevin Klyman\nAbstract: Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.\nCategories: cs.HC, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04843\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 112, "abstract_length": 1103, "total_text_length": 1216, "word_count": 161, "sentence_count": 7}, "basic_keywords": ["risks", "eating", "disorders", "generative", "systems", "risk", "pose", "serious"], "domain_keywords": [], "technical_terms": ["Using", "Generative", "Existing"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04841", "topic": "artificial intelligence", "title": "SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security", "authors": ["Wei Zhao", "Zhe Li", "Jun Sun"], "first_author": "Wei Zhao", "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses. In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\\% detection accuracy across multiple threat types. By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR", "cs.AI"], "url": "http://arxiv.org/abs/2512.04841", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security\nResearch Topic: artificial intelligence\nAuthors: Wei Zhao, Zhe Li, Jun Sun\nAbstract: Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.   In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\\% detection accuracy across multiple threat types.   By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.\nCategories: cs.CR, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04841\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1662, "total_text_length": 1746, "word_count": 210, "sentence_count": 12}, "basic_keywords": ["safety", "causality", "framework", "causal", "level", "llms", "analysis", "detection"], "domain_keywords": [], "technical_terms": ["Accompanying", "Amadeuszhao", "Code", "Large Language Models", "Understanding"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04834", "topic": "artificial intelligence", "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case", "authors": ["Vignesh Kumar Kembu", "Pierandrea Morandini", "Marta Bianca Maria Ranzini", "Antonino Nocera"], "first_author": "Vignesh Kumar Kembu", "abstract": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.CL", "cs.IR"], "url": "http://arxiv.org/abs/2512.04834", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case\nResearch Topic: artificial intelligence\nAuthors: Vignesh Kumar Kembu, Pierandrea Morandini, Marta Bianca Maria Ranzini, Antonino Nocera\nAbstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.\nCategories: cs.AI, cs.CL, cs.IR\nPublication Date: 2025-12-04\nPaper ID: 2512.04834\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 138, "abstract_length": 1216, "total_text_length": 1355, "word_count": 189, "sentence_count": 8}, "basic_keywords": ["llms", "language", "clinical", "large", "models", "become", "like", "healthcare"], "domain_keywords": [], "technical_terms": ["Electronic Health Records", "Italian", "Information", "Large Language Models", "Recently", "Although"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04829", "topic": "artificial intelligence", "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing", "authors": ["Rasul Tutunov", "Alexandre Maraval", "Antoine Grosnit", "Xihan Li", "Jun Wang", "Haitham Bou-Ammar"], "first_author": "Rasul Tutunov", "abstract": "Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension , later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions , showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04829", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing\nResearch Topic: artificial intelligence\nAuthors: Rasul Tutunov, Alexandre Maraval, Antoine Grosnit, Xihan Li, Jun Wang, Haitham Bou-Ammar\nAbstract: Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04829\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1439, "total_text_length": 1517, "word_count": 199, "sentence_count": 9}, "basic_keywords": ["problem", "upper", "bounds", "model", "based", "search", "beyond", "dimensions"], "domain_keywords": ["medical imaging"], "technical_terms": ["Hilbert", "Using", "Sphere", "Bayesian", "Euclidean", "Because", "Fields Medal", "Together"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04822", "topic": "artificial intelligence", "title": "Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions", "authors": ["Liam McGee", "James Harvey", "Lucy Cull", "Andreas Vermeulen", "Bart-Floris Visscher", "Malvika Sharan"], "first_author": "Liam McGee", "abstract": "In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04822", "comments": "24 pages including references, with 6 images and 2 tables. Appendices, supporting data and additional reference provided from page 25 to 117", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions\nResearch Topic: artificial intelligence\nAuthors: Liam McGee, James Harvey, Lucy Cull, Andreas Vermeulen, Bart-Floris Visscher, Malvika Sharan\nAbstract: In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04822\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 673, "total_text_length": 771, "word_count": 107, "sentence_count": 5}, "basic_keywords": ["inspectable", "agentic", "knowledge", "structures", "experts", "institutional", "preprint", "present"], "domain_keywords": [], "technical_terms": ["Agentic", "Authors"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04808", "topic": "artificial intelligence", "title": "Setting up for failure: automatic discovery of the neural mechanisms of cognitive errors", "authors": ["Puria Radmard", "Paul M. Bays", "Máté Lengyel"], "first_author": "Puria Radmard", "abstract": "Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["q-bio.NC", "cs.AI"], "url": "http://arxiv.org/abs/2512.04808", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: Setting up for failure: automatic discovery of the neural mechanisms of cognitive errors\nResearch Topic: artificial intelligence\nAuthors: Puria Radmard, Paul M. Bays, Máté Lengyel\nAbstract: Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.\nCategories: q-bio.NC, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04808\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1920, "total_text_length": 2009, "word_count": 289, "sentence_count": 14}, "basic_keywords": ["rnns", "behaviour", "approach", "task", "behavioural", "data", "mechanisms", "training"], "domain_keywords": [], "technical_terms": ["These", "However", "Here", "Achieving", "Discovering", "Importantly", "Second", "First"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04803", "topic": "artificial intelligence", "title": "287,872 Supermassive Black Holes Masses: Deep Learning Approaching Reverberation Mapping Accuracy", "authors": ["Yuhao Lu", "HengJian SiTu", "Jie Li", "Yixuan Li", "Yang Liu", "Wenbin Lin", "Yu Wang"], "first_author": "Yuhao Lu", "abstract": "We present a population-scale catalogue of 287,872 supermassive black hole masses with high accuracy. Using a deep encoder-decoder network trained on optical spectra with reverberation-mapping (RM) based labels of 849 quasars and applied to all SDSS quasars up to , our method achieves a root-mean-square error of \\,dex, a relative uncertainty of , and coefficient of determination with respect to RM-based masses, far surpassing traditional single-line virial estimators. Notably, the high accuracy is maintained for both low () and high () mass quasars, where empirical relations are unreliable.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["astro-ph.GA", "astro-ph.HE", "astro-ph.IM", "cs.AI"], "url": "http://arxiv.org/abs/2512.04803", "comments": "14 pages, 9 figures. Submitted to Journal of High Energy Astrophysics", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636302", "embedding_text": "Paper Title: 287,872 Supermassive Black Holes Masses: Deep Learning Approaching Reverberation Mapping Accuracy\nResearch Topic: artificial intelligence\nAuthors: Yuhao Lu, HengJian SiTu, Jie Li, Yixuan Li, Yang Liu, Wenbin Lin, Yu Wang\nAbstract: We present a population-scale catalogue of 287,872 supermassive black hole masses with high accuracy. Using a deep encoder-decoder network trained on optical spectra with reverberation-mapping (RM) based labels of 849 quasars and applied to all SDSS quasars up to $z=4$, our method achieves a root-mean-square error of $0.058$\\,dex, a relative uncertainty of $\\approx 14\\%$, and coefficient of determination $R^{2}\\approx0.91$ with respect to RM-based masses, far surpassing traditional single-line virial estimators. Notably, the high accuracy is maintained for both low ($<10^{7.5}\\,M_\\odot$) and high ($>10^{9}\\,M_\\odot$) mass quasars, where empirical relations are unreliable.\nCategories: astro-ph.GA, astro-ph.HE, astro-ph.IM, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04803\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 597, "total_text_length": 695, "word_count": 99, "sentence_count": 4}, "basic_keywords": ["high", "quasars", "masses", "accuracy", "based", "present", "population", "scale"], "domain_keywords": [], "technical_terms": ["Using", "Notably"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04797", "topic": "artificial intelligence", "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds", "authors": ["SIMA team", "Adrian Bolton", "Alexander Lerchner", "Alexandra Cordell", "Alexandre Moufarek", "Andrew Bolt", "Andrew Lampinen", "Anna Mitenkova", "Arne Olav Hallingstad", "Bojan Vujatovic", "Bonnie Li", "Cong Lu", "Daan Wierstra", "Daniel P. Sawyer", "Daniel Slater", "David Reichert", "Davide Vercelli", "Demis Hassabis", "Drew A. Hudson", "Duncan Williams", "Ed Hirst", "Fabio Pardo", "Felix Hill", "Frederic Besse", "Hannah Openshaw", "Harris Chan", "Hubert Soyer", "Jane X. Wang", "Jeff Clune", "John Agapiou", "John Reid", "Joseph Marino", "Junkyung Kim", "Karol Gregor", "Kaustubh Sridhar", "Kay McKinney", "Laura Kampis", "Lei M. Zhang", "Loic Matthey", "Luyu Wang", "Maria Abi Raad", "Maria Loks-Thompson", "Martin Engelcke", "Matija Kecman", "Matthew Jackson", "Maxime Gazeau", "Ollie Purkiss", "Oscar Knagg", "Peter Stys", "Piermaria Mendolicchio", "Raia Hadsell", "Rosemary Ke", "Ryan Faulkner", "Sarah Chakera", "Satinder Singh Baveja", "Shane Legg", "Sheleem Kashem", "Tayfun Terzi", "Thomas Keck", "Tim Harley", "Tim Scholtes", "Tyson Roberts", "Volodymyr Mnih", "Yulan Liu", "Zhengdong Wang", "Zoubin Ghahramani"], "first_author": "SIMA team", "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.RO"], "url": "http://arxiv.org/abs/2512.04797", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: SIMA 2: A Generalist Embodied Agent for Virtual Worlds\nResearch Topic: artificial intelligence\nAuthors: SIMA team, Adrian Bolton, Alexander Lerchner, Alexandra Cordell, Alexandre Moufarek, Andrew Bolt, Andrew Lampinen, Anna Mitenkova, Arne Olav Hallingstad, Bojan Vujatovic, Bonnie Li, Cong Lu, Daan Wierstra, Daniel P. Sawyer, Daniel Slater, David Reichert, Davide Vercelli, Demis Hassabis, Drew A. Hudson, Duncan Williams, Ed Hirst, Fabio Pardo, Felix Hill, Frederic Besse, Hannah Openshaw, Harris Chan, Hubert Soyer, Jane X. Wang, Jeff Clune, John Agapiou, John Reid, Joseph Marino, Junkyung Kim, Karol Gregor, Kaustubh Sridhar, Kay McKinney, Laura Kampis, Lei M. Zhang, Loic Matthey, Luyu Wang, Maria Abi Raad, Maria Loks-Thompson, Martin Engelcke, Matija Kecman, Matthew Jackson, Maxime Gazeau, Ollie Purkiss, Oscar Knagg, Peter Stys, Piermaria Mendolicchio, Raia Hadsell, Rosemary Ke, Ryan Faulkner, Sarah Chakera, Satinder Singh Baveja, Shane Legg, Sheleem Kashem, Tayfun Terzi, Thomas Keck, Tim Harley, Tim Scholtes, Tyson Roberts, Volodymyr Mnih, Yulan Liu, Zhengdong Wang, Zoubin Ghahramani\nAbstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.\nCategories: cs.AI, cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04797\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 54, "abstract_length": 1092, "total_text_length": 1147, "word_count": 167, "sentence_count": 9}, "basic_keywords": ["sima", "embodied", "acts", "virtual", "worlds", "gemini", "model", "toward"], "domain_keywords": [], "technical_terms": ["Across", "Furthermore", "Gemini", "Unlike", "Built"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04794", "topic": "artificial intelligence", "title": "Teaching a Transformer to Think Like a Chemist: Predicting Nanocluster Stability", "authors": ["João Marcos T. Palheta", "Octavio Rodrigues Filho", "Mohammad Soleymanibrojeni", "Alexandre Cavalheiro Dias", "Diego Guedes-Sobrinho", "Wolfgang Wenzel", "Roland Aydin", "Celso R. C. Rêgo", "Maurício Jeomar Piotrowski"], "first_author": "João Marcos T. Palheta", "abstract": "Atomically precise metal nanoclusters bridge the molecular and bulk regimes, but designing bimetallic motifs with targeted stability and reactivity remains challenging. Here we combine density functional theory (DFT) and physics-grounded predictive artificial intelligence to map the configurational landscape of 13-atom icosahedral nanoclusters XTM, with hosts X = (Ti, Zr, Hf), and Fe and a single transition--metal dopant spanning the 3-5 series. Spin-polarized DFT calculations on 240 bimetallic clusters reveal systematic trends in binding and formation energies, distortion penalties, effective coordination number, d-band centre, and HOMO-LUMO gap that govern the competition between core-shell (in) and surface-segregated (out) arrangements. We then pretrain a transformer architecture on a curated set of 2968 unary clusters from the Quantum Cluster Database and fine-tune it on bimetallic data to predict formation energies and in/out preference, achieving mean absolute errors of about eV and calibrated uncertainty intervals. The resulting model rapidly adapts to an unseen Fe-host domain with only a handful of labelled examples. At the same time, attention patterns and Shapley attributions highlight size mismatch, -electron count, and coordination environment as key descriptors. All data, code, and workflows follow FAIR/TRUE principles, enabling reproducible, interpretable screening of unexplored nanocluster chemistries for catalysis and energy conversion.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.chem-ph", "cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04794", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: Teaching a Transformer to Think Like a Chemist: Predicting Nanocluster Stability\nResearch Topic: artificial intelligence\nAuthors: João Marcos T. Palheta, Octavio Rodrigues Filho, Mohammad Soleymanibrojeni, Alexandre Cavalheiro Dias, Diego Guedes-Sobrinho, Wolfgang Wenzel, Roland Aydin, Celso R. C. Rêgo, Maurício Jeomar Piotrowski\nAbstract: Atomically precise metal nanoclusters bridge the molecular and bulk regimes, but designing bimetallic motifs with targeted stability and reactivity remains challenging. Here we combine density functional theory (DFT) and physics-grounded predictive artificial intelligence to map the configurational landscape of 13-atom icosahedral nanoclusters X$_{12}$TM, with hosts X = (Ti, Zr, Hf), and Fe and a single transition--metal dopant spanning the 3$d$-5$d$ series. Spin-polarized DFT calculations on 240 bimetallic clusters reveal systematic trends in binding and formation energies, distortion penalties, effective coordination number, d-band centre, and HOMO-LUMO gap that govern the competition between core-shell (in) and surface-segregated (out) arrangements. We then pretrain a transformer architecture on a curated set of 2968 unary clusters from the Quantum Cluster Database and fine-tune it on bimetallic data to predict formation energies and in/out preference, achieving mean absolute errors of about $0.6-0.7$eV and calibrated uncertainty intervals. The resulting model rapidly adapts to an unseen Fe-host domain with only a handful of labelled examples. At the same time, attention patterns and Shapley attributions highlight size mismatch, $d$-electron count, and coordination environment as key descriptors. All data, code, and workflows follow FAIR/TRUE principles, enabling reproducible, interpretable screening of unexplored nanocluster chemistries for catalysis and energy conversion.\nCategories: physics.chem-ph, cond-mat.mtrl-sci\nPublication Date: 2025-12-04\nPaper ID: 2512.04794\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1476, "total_text_length": 1557, "word_count": 209, "sentence_count": 8}, "basic_keywords": ["bimetallic", "metal", "nanoclusters", "clusters", "formation", "energies", "coordination", "data"], "domain_keywords": [], "technical_terms": ["Shapley", "Quantum Cluster Database", "Atomically", "Here", "Spin"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04793", "topic": "artificial intelligence", "title": "YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases", "authors": ["Gongyu Chen", "Xiaoyu Zhang", "Zhenqiang Weng", "Junjie Zheng", "Da Shen", "Chaofan Ding", "Wei-Qiang Zhang", "Zihao Chen"], "first_author": "Gongyu Chen", "abstract": "Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.AI"], "url": "http://arxiv.org/abs/2512.04793", "comments": "17 pages, 5 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases\nResearch Topic: artificial intelligence\nAuthors: Gongyu Chen, Xiaoyu Zhang, Zhenqiang Weng, Junjie Zheng, Da Shen, Chaofan Ding, Wei-Qiang Zhang, Zihao Chen\nAbstract: Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment.\nCategories: cs.SD, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04793\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 120, "abstract_length": 997, "total_text_length": 1118, "word_count": 137, "sentence_count": 6}, "basic_keywords": ["timbre", "singing", "zero", "shot", "real", "harmony", "yingmusic", "robust"], "domain_keywords": [], "technical_terms": ["Flow", "However", "Experiments", "Singing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04790", "topic": "artificial intelligence", "title": "Spatially-Enhanced Retrieval-Augmented Generation for Walkability and Urban Discovery", "authors": ["Maddalena Amendola", "Chiara Pugliese", "Raffaele Perego", "Chiara Renso"], "first_author": "Maddalena Amendola", "abstract": "Large Language Models (LLMs) have become foundational tools in artificial intelligence, supporting a wide range of applications beyond traditional natural language processing, including urban systems and tourist recommendations. However, their tendency to hallucinate and their limitations in spatial retrieval and reasoning are well known, pointing to the need for novel solutions. Retrieval-augmented generation (RAG) has recently emerged as a promising way to enhance LLMs with accurate, domain-specific, and timely information. Spatial RAG extends this approach to tasks involving geographic understanding. In this work, we introduce WalkRAG, a spatial RAG-based framework with a conversational interface for recommending walkable urban itineraries. Users can request routes that meet specific spatial constraints and preferences while interactively retrieving information about the path and points of interest (POIs) along the way. Preliminary results show the effectiveness of combining information retrieval, spatial reasoning, and LLMs to support urban discovery.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IR"], "url": "http://arxiv.org/abs/2512.04790", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: Spatially-Enhanced Retrieval-Augmented Generation for Walkability and Urban Discovery\nResearch Topic: artificial intelligence\nAuthors: Maddalena Amendola, Chiara Pugliese, Raffaele Perego, Chiara Renso\nAbstract: Large Language Models (LLMs) have become foundational tools in artificial intelligence, supporting a wide range of applications beyond traditional natural language processing, including urban systems and tourist recommendations. However, their tendency to hallucinate and their limitations in spatial retrieval and reasoning are well known, pointing to the need for novel solutions. Retrieval-augmented generation (RAG) has recently emerged as a promising way to enhance LLMs with accurate, domain-specific, and timely information. Spatial RAG extends this approach to tasks involving geographic understanding. In this work, we introduce WalkRAG, a spatial RAG-based framework with a conversational interface for recommending walkable urban itineraries. Users can request routes that meet specific spatial constraints and preferences while interactively retrieving information about the path and points of interest (POIs) along the way. Preliminary results show the effectiveness of combining information retrieval, spatial reasoning, and LLMs to support urban discovery.\nCategories: cs.IR\nPublication Date: 2025-12-04\nPaper ID: 2512.04790\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1071, "total_text_length": 1157, "word_count": 150, "sentence_count": 8}, "basic_keywords": ["spatial", "llms", "urban", "retrieval", "information", "language", "reasoning", "specific"], "domain_keywords": [], "technical_terms": ["However", "Preliminary", "Retrieval", "Large Language Models", "Spatial", "Users"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04785", "topic": "artificial intelligence", "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications", "authors": ["Eranga Bandara", "Amin Hass", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Safdar H. Bouk", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "first_author": "Eranga Bandara", "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.CR"], "url": "http://arxiv.org/abs/2512.04785", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications\nResearch Topic: artificial intelligence\nAuthors: Eranga Bandara, Amin Hass, Ross Gore, Sachin Shetty, Ravi Mukkamala, Safdar H. Bouk, Xueping Liang, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan\nAbstract: AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.\nCategories: cs.AI, cs.CR\nPublication Date: 2025-12-04\nPaper ID: 2512.04785\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1665, "total_text_length": 1738, "word_count": 221, "sentence_count": 9}, "basic_keywords": ["agent", "threat", "modeling", "astride", "based", "systems", "reasoning", "interactions"], "domain_keywords": [], "technical_terms": ["Specific Attacks", "However", "Agent"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04779", "topic": "artificial intelligence", "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance", "authors": ["Junjie Zheng", "Chunbo Hao", "Guobin Ma", "Xiaoyu Zhang", "Gongyu Chen", "Chaofan Ding", "Zihao Chen", "Lei Xie"], "first_author": "Junjie Zheng", "abstract": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.AI"], "url": "http://arxiv.org/abs/2512.04779", "comments": "13 pages, 3 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance\nResearch Topic: artificial intelligence\nAuthors: Junjie Zheng, Chunbo Hao, Guobin Ma, Xiaoyu Zhang, Gongyu Chen, Chaofan Ding, Zihao Chen, Lei Xie\nAbstract: Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.\nCategories: cs.SD, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04779\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1550, "total_text_length": 1651, "word_count": 212, "sentence_count": 9}, "basic_keywords": ["melody", "alignment", "model", "singing", "voice", "synthesis", "practical", "phoneme"], "domain_keywords": [], "technical_terms": ["Experiments", "Flow", "Additionally", "Singing Voice Synthesis", "Diffusion Transformer"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04773", "topic": "artificial intelligence", "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions", "authors": ["Giorgos Polychronis", "Foivos Pournaropoulos", "Christos D. Antonopoulos", "Spyros Lalis"], "first_author": "Giorgos Polychronis", "abstract": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO", "cs.AI"], "url": "http://arxiv.org/abs/2512.04773", "comments": "19 pages, 3 figures, to appear in the proceedings of MobiQuitous 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions\nResearch Topic: artificial intelligence\nAuthors: Giorgos Polychronis, Foivos Pournaropoulos, Christos D. Antonopoulos, Spyros Lalis\nAbstract: Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.\nCategories: cs.RO, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04773\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1204, "total_text_length": 1286, "word_count": 205, "sentence_count": 11}, "basic_keywords": ["point", "time", "drone", "must", "action", "moving", "next", "event"], "domain_keywords": [], "technical_terms": ["Also", "Drones"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04770", "topic": "artificial intelligence", "title": "Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges", "authors": ["Yuxing Wang", "Zhiyu Chen", "Tiantian Zhang", "Qiyue Yin", "Yongzhe Chang", "Zhiheng Li", "Liang Wang", "Xueqian Wang"], "first_author": "Yuxing Wang", "abstract": "Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO", "cs.AI", "cs.ET", "eess.SY"], "url": "http://arxiv.org/abs/2512.04770", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges\nResearch Topic: artificial intelligence\nAuthors: Yuxing Wang, Zhiyu Chen, Tiantian Zhang, Qiyue Yin, Yongzhe Chang, Zhiheng Li, Liang Wang, Xueqian Wang\nAbstract: Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.\nCategories: cs.RO, cs.AI, cs.ET, eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04770\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1350, "total_text_length": 1434, "word_count": 186, "sentence_count": 12}, "basic_keywords": ["into", "brain", "body", "design", "task", "survey", "recent", "taxonomy"], "domain_keywords": [], "technical_terms": ["Wang", "Inspired", "Finally", "Yuxing", "Brain"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04764", "topic": "artificial intelligence", "title": "Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect", "authors": ["Dario Pesenti", "Alessandro Bogani", "Katya Tentori", "Stefano Teso"], "first_author": "Dario Pesenti", "abstract": "Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04764", "comments": "18 pages, 10 figures, published at AAAI 2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect\nResearch Topic: artificial intelligence\nAuthors: Dario Pesenti, Alessandro Bogani, Katya Tentori, Stefano Teso\nAbstract: Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04764\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 108, "abstract_length": 1917, "total_text_length": 2026, "word_count": 298, "sentence_count": 18}, "basic_keywords": ["order", "users", "effects", "explanations", "model", "explanatory", "feedback", "debugging"], "domain_keywords": [], "technical_terms": ["Despite", "Explanatory Interactive Learning", "Overall", "More", "Order", "Specifically", "Then"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04749", "topic": "artificial intelligence", "title": "UnwrapDiff: Conditional Diffusion for Robust InSAR Phase Unwrapping", "authors": ["Yijia Song", "Juliet Biggs", "Alin Achim", "Robert Popescu", "Simon Orrego", "Nantheera Anantrasirichai"], "first_author": "Yijia Song", "abstract": "Phase unwrapping is a fundamental problem in InSAR data processing, supporting geophysical applications such as deformation monitoring and hazard assessment. Its reliability is limited by noise and decorrelation in radar acquisitions, which makes accurate reconstruction of the deformation signal challenging. We propose a denoising diffusion probabilistic model (DDPM)-based framework for InSAR phase unwrapping, UnwrapDiff, in which the output of the traditional minimum cost flow algorithm (SNAPHU) is incorporated as conditional guidance. To evaluate robustness, we construct a synthetic dataset that incorporates atmospheric effects and diverse noise patterns, representative of realistic InSAR observations. Experiments show that the proposed model leverages the conditional prior while reducing the effect of diverse noise patterns, achieving on average a 10.11\\% reduction in NRMSE compared to SNAPHU. It also achieves better reconstruction quality in difficult cases such as dyke intrusions.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.geo-ph", "cs.AI"], "url": "http://arxiv.org/abs/2512.04749", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: UnwrapDiff: Conditional Diffusion for Robust InSAR Phase Unwrapping\nResearch Topic: artificial intelligence\nAuthors: Yijia Song, Juliet Biggs, Alin Achim, Robert Popescu, Simon Orrego, Nantheera Anantrasirichai\nAbstract: Phase unwrapping is a fundamental problem in InSAR data processing, supporting geophysical applications such as deformation monitoring and hazard assessment. Its reliability is limited by noise and decorrelation in radar acquisitions, which makes accurate reconstruction of the deformation signal challenging. We propose a denoising diffusion probabilistic model (DDPM)-based framework for InSAR phase unwrapping, UnwrapDiff, in which the output of the traditional minimum cost flow algorithm (SNAPHU) is incorporated as conditional guidance. To evaluate robustness, we construct a synthetic dataset that incorporates atmospheric effects and diverse noise patterns, representative of realistic InSAR observations. Experiments show that the proposed model leverages the conditional prior while reducing the effect of diverse noise patterns, achieving on average a 10.11\\% reduction in NRMSE compared to SNAPHU. It also achieves better reconstruction quality in difficult cases such as dyke intrusions.\nCategories: physics.geo-ph, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04749\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 1000, "total_text_length": 1068, "word_count": 142, "sentence_count": 8}, "basic_keywords": ["insar", "noise", "phase", "unwrapping", "deformation", "reconstruction", "model", "snaphu"], "domain_keywords": [], "technical_terms": ["Phase", "Experiments"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04747", "topic": "artificial intelligence", "title": "A Tutorial on Regression Analysis: From Linear Models to Deep Learning -- Lecture Notes on Artificial Intelligence", "authors": ["Jingyuan Wang", "Jiahao Ji"], "first_author": "Jingyuan Wang", "abstract": "This article serves as the regression analysis lecture notes in the Intelligent Computing course cluster (including the courses of Artificial Intelligence, Data Mining, Machine Learning, and Pattern Recognition). It aims to provide students -- who are assumed to possess only basic university-level mathematics (i.e., with prerequisite courses in calculus, linear algebra, and probability theory) -- with a comprehensive and self-contained understanding of regression analysis without requiring any additional references. The lecture notes systematically introduce the fundamental concepts, modeling components, and theoretical foundations of regression analysis, covering linear regression, logistic regression, multinomial logistic regression, polynomial regression, basis-function models, kernel-based methods, and neural-network-based nonlinear regression. Core methodological topics include loss-function design, parameter-estimation principles, ordinary least squares, gradient-based optimization algorithms and their variants, as well as regularization techniques such as Ridge and LASSO regression. Through detailed mathematical derivations, illustrative examples, and intuitive visual explanations, the materials help students understand not only how regression models are constructed and optimized, but also how they reveal the underlying relationships between features and response variables. By bridging classical statistical modeling and modern machine-learning practice, these lecture notes aim to equip students with a solid conceptual and technical foundation for further study in advanced artificial intelligence models.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04747", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: A Tutorial on Regression Analysis: From Linear Models to Deep Learning -- Lecture Notes on Artificial Intelligence\nResearch Topic: artificial intelligence\nAuthors: Jingyuan Wang, Jiahao Ji\nAbstract: This article serves as the regression analysis lecture notes in the Intelligent Computing course cluster (including the courses of Artificial Intelligence, Data Mining, Machine Learning, and Pattern Recognition). It aims to provide students -- who are assumed to possess only basic university-level mathematics (i.e., with prerequisite courses in calculus, linear algebra, and probability theory) -- with a comprehensive and self-contained understanding of regression analysis without requiring any additional references. The lecture notes systematically introduce the fundamental concepts, modeling components, and theoretical foundations of regression analysis, covering linear regression, logistic regression, multinomial logistic regression, polynomial regression, basis-function models, kernel-based methods, and neural-network-based nonlinear regression. Core methodological topics include loss-function design, parameter-estimation principles, ordinary least squares, gradient-based optimization algorithms and their variants, as well as regularization techniques such as Ridge and LASSO regression. Through detailed mathematical derivations, illustrative examples, and intuitive visual explanations, the materials help students understand not only how regression models are constructed and optimized, but also how they reveal the underlying relationships between features and response variables. By bridging classical statistical modeling and modern machine-learning practice, these lecture notes aim to equip students with a solid conceptual and technical foundation for further study in advanced artificial intelligence models.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04747\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 114, "abstract_length": 1637, "total_text_length": 1752, "word_count": 216, "sentence_count": 9}, "basic_keywords": ["regression", "analysis", "lecture", "notes", "students", "models", "based", "courses"], "domain_keywords": [], "technical_terms": ["Core", "Pattern Recognition", "Artificial Intelligence", "Through", "Data Mining", "Ridge", "Machine Learning", "Intelligent Computing"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04746", "topic": "artificial intelligence", "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs", "authors": ["Wenhua Cheng", "Weiwei Zhang", "Heng Guo", "Haihao Shen"], "first_author": "Wenhua Cheng", "abstract": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04746", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs\nResearch Topic: artificial intelligence\nAuthors: Wenhua Cheng, Weiwei Zhang, Heng Guo, Haihao Shen\nAbstract: Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04746\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 931, "total_text_length": 1029, "word_count": 132, "sentence_count": 10}, "basic_keywords": ["quantization", "bits", "even", "models", "llms", "performance", "precision", "extreme"], "domain_keywords": [], "technical_terms": ["These", "Extreme", "Extensive", "Large Language Models"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04745", "topic": "artificial intelligence", "title": "Neural Policy Composition from Free Energy Minimization", "authors": ["Francesca Rossi", "Veronica Centorrino", "Francesco Bullo", "Giovanni Russo"], "first_author": "Francesca Rossi", "abstract": "The ability to compose acquired skills to plan and execute behaviors is a hallmark of natural intelligence. Yet, despite remarkable cross-disciplinary efforts, a principled account of how task structure shapes gating and how such computations could be delivered in neural circuits, remains elusive. Here we introduce GateMod, an interpretable theoretically grounded computational model linking the emergence of gating to the underlying decision-making task, and to a neural circuit architecture. We first develop GateFrame, a normative framework casting policy gating into the minimization of the free energy. This framework, relating gating rules to task, applies broadly across neuroscience, cognitive and computational sciences. We then derive GateFlow, a continuous-time energy based dynamics that provably converges to GateFrame optimal solution. Convergence, exponential and global, follows from a contractivity property that also yields robustness and other desirable properties. Finally, we derive a neural circuit from GateFlow, GateNet. This is a soft-competitive recurrent circuit whose components perform local and contextual computations consistent with known dendritic and neural processing motifs. We evaluate GateMod across two different settings: collective behaviors in multi-agent systems and human decision-making in multi-armed bandits. In all settings, GateMod provides interpretable mechanistic explanations of gating and quantitatively matches or outperforms established models. GateMod offers a unifying framework for neural policy gating, linking task objectives, dynamical computation, and circuit-level mechanisms. It provides a framework to understand gating in natural agents beyond current explanations and to equip machines with this ability.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.OC", "cs.AI", "eess.SY", "nlin.AO"], "url": "http://arxiv.org/abs/2512.04745", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: Neural Policy Composition from Free Energy Minimization\nResearch Topic: artificial intelligence\nAuthors: Francesca Rossi, Veronica Centorrino, Francesco Bullo, Giovanni Russo\nAbstract: The ability to compose acquired skills to plan and execute behaviors is a hallmark of natural intelligence. Yet, despite remarkable cross-disciplinary efforts, a principled account of how task structure shapes gating and how such computations could be delivered in neural circuits, remains elusive. Here we introduce GateMod, an interpretable theoretically grounded computational model linking the emergence of gating to the underlying decision-making task, and to a neural circuit architecture. We first develop GateFrame, a normative framework casting policy gating into the minimization of the free energy. This framework, relating gating rules to task, applies broadly across neuroscience, cognitive and computational sciences. We then derive GateFlow, a continuous-time energy based dynamics that provably converges to GateFrame optimal solution. Convergence, exponential and global, follows from a contractivity property that also yields robustness and other desirable properties. Finally, we derive a neural circuit from GateFlow, GateNet. This is a soft-competitive recurrent circuit whose components perform local and contextual computations consistent with known dendritic and neural processing motifs. We evaluate GateMod across two different settings: collective behaviors in multi-agent systems and human decision-making in multi-armed bandits. In all settings, GateMod provides interpretable mechanistic explanations of gating and quantitatively matches or outperforms established models. GateMod offers a unifying framework for neural policy gating, linking task objectives, dynamical computation, and circuit-level mechanisms. It provides a framework to understand gating in natural agents beyond current explanations and to equip machines with this ability.\nCategories: math.OC, cs.AI, eess.SY, nlin.AO\nPublication Date: 2025-12-04\nPaper ID: 2512.04745\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 55, "abstract_length": 1774, "total_text_length": 1830, "word_count": 243, "sentence_count": 14}, "basic_keywords": ["gating", "neural", "task", "gatemod", "circuit", "framework", "ability", "behaviors"], "domain_keywords": [], "technical_terms": ["Here", "Convergence", "Finally"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04738", "topic": "artificial intelligence", "title": "OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models", "authors": ["Zhuoyue Wan", "Wentao Hu", "Chen Jason Zhang", "Yuanfeng Song", "Shuaimin Li", "Ruiqiang Xiao", "Xiao-Yong Wei", "Raymond Chi-Wing Wong"], "first_author": "Zhuoyue Wan", "abstract": "Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI", "cs.DB"], "url": "http://arxiv.org/abs/2512.04738", "comments": "42nd IEEE International Conference on Data Engineering (ICDE)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models\nResearch Topic: artificial intelligence\nAuthors: Zhuoyue Wan, Wentao Hu, Chen Jason Zhang, Yuanfeng Song, Shuaimin Li, Ruiqiang Xiao, Xiao-Yong Wei, Raymond Chi-Wing Wong\nAbstract: Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.\nCategories: cs.CL, cs.AI, cs.DB\nPublication Date: 2025-12-04\nPaper ID: 2512.04738\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1622, "total_text_length": 1723, "word_count": 221, "sentence_count": 9}, "basic_keywords": ["language", "query", "natural", "structured", "models", "source", "bridging", "languages"], "domain_keywords": [], "technical_terms": ["While", "Despite", "Overpass Query Language", "Bridging", "Tag Retrieval Augmentation", "Text"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04733", "topic": "artificial intelligence", "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving", "authors": ["Yihong Tang", "Haicheng Liao", "Tong Nie", "Junlin He", "Ao Qu", "Kehua Chen", "Wei Ma", "Zhenning Li", "Lijun Sun", "Chengzhong Xu"], "first_author": "Yihong Tang", "abstract": "End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04733", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving\nResearch Topic: artificial intelligence\nAuthors: Yihong Tang, Haicheng Liao, Tong Nie, Junlin He, Ao Qu, Kehua Chen, Wei Ma, Zhenning Li, Lijun Sun, Chengzhong Xu\nAbstract: End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04733\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 99, "abstract_length": 1244, "total_text_length": 1344, "word_count": 162, "sentence_count": 7}, "basic_keywords": ["emotion", "driving", "autonomous", "language", "human", "emotional", "state", "spatial"], "domain_keywords": [], "technical_terms": ["These", "Domain End", "Arousal", "Across", "Open", "Dominance", "Valenc"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04728", "topic": "artificial intelligence", "title": "Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild", "authors": ["Yigui Feng", "Qinglin Wang", "Haotian Mo", "Yang Liu", "Ke Liu", "Gencheng Liu", "Xinhai Chen", "Siqi Shen", "Songzhu Mei", "Jie Liu"], "first_author": "Yigui Feng", "abstract": "Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04728", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.636801", "embedding_text": "Paper Title: Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild\nResearch Topic: artificial intelligence\nAuthors: Yigui Feng, Qinglin Wang, Haotian Mo, Yang Liu, Ke Liu, Gencheng Liu, Xinhai Chen, Siqi Shen, Songzhu Mei, Jie Liu\nAbstract: Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04728\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1392, "total_text_length": 1493, "word_count": 195, "sentence_count": 10}, "basic_keywords": ["visual", "disentanglement", "psychological", "challenges", "vision", "models", "expressions", "reasoning"], "domain_keywords": [], "technical_terms": ["Affective Ambiguity", "Third", "Multilevel Insight Network", "Mental Reasoning Insight Rating Metric", "Articulatory", "Vision", "Second", "Language Models"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04727", "topic": "artificial intelligence", "title": "Sequential Enumeration in Large Language Models", "authors": ["Kuinan Hou", "Marco Zorzi", "Alberto Testolin"], "first_author": "Kuinan Hou", "abstract": "Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04727", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Sequential Enumeration in Large Language Models\nResearch Topic: artificial intelligence\nAuthors: Kuinan Hou, Marco Zorzi, Alberto Testolin\nAbstract: Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.\nCategories: cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04727\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 47, "abstract_length": 1831, "total_text_length": 1879, "word_count": 259, "sentence_count": 9}, "basic_keywords": ["counting", "llms", "models", "procedures", "sequences", "neural", "including", "learning"], "domain_keywords": [], "technical_terms": ["Indeed", "Reliably", "Previous", "Large Language Models"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04716", "topic": "artificial intelligence", "title": "Towards an AI Fluid Scientist: LLM-Powered Scientific Discovery in Experimental Fluid Mechanics", "authors": ["Haodong Feng", "Lugang Ye", "Dixia Fan"], "first_author": "Haodong Feng", "abstract": "The integration of artificial intelligence into experimental fluid mechanics promises to accelerate discovery, yet most AI applications remain narrowly focused on numerical studies. This work proposes an AI Fluid Scientist framework that autonomously executes the complete experimental workflow: hypothesis generation, experimental design, robotic execution, data analysis, and manuscript preparation. We validate this through investigation of vortex-induced vibration (VIV) and wake-induced vibration (WIV) in tandem cylinders. Our work has four key contributions: (1) A computer-controlled circulating water tunnel (CWT) with programmatic control of flow velocity, cylinder position, and forcing parameters (vibration frequency and amplitude) with data acquisition (displacement, force, and torque). (2) Automated experiments reproduce literature benchmarks (Khalak and Williamson [1999] and Assi et al. [2013, 2010]) with frequency lock-in within 4% and matching critical spacing trends. (3) The framework with Human-in-the-Loop (HIL) discovers more WIV amplitude response phenomena, and uses a neural network to fit physical laws from data, which is 31% higher than that of polynomial fitting. (4) The framework with multi-agent with virtual-real interaction system executes hundreds of experiments end-to-end, which automatically completes the entire process of scientific research from hypothesis generation, experimental design, experimental execution, data analysis, and manuscript preparation. It greatly liberates human researchers and improves study efficiency, providing new paradigm for the development and research of experimental fluid mechanics.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.flu-dyn", "cs.AI"], "url": "http://arxiv.org/abs/2512.04716", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Towards an AI Fluid Scientist: LLM-Powered Scientific Discovery in Experimental Fluid Mechanics\nResearch Topic: artificial intelligence\nAuthors: Haodong Feng, Lugang Ye, Dixia Fan\nAbstract: The integration of artificial intelligence into experimental fluid mechanics promises to accelerate discovery, yet most AI applications remain narrowly focused on numerical studies. This work proposes an AI Fluid Scientist framework that autonomously executes the complete experimental workflow: hypothesis generation, experimental design, robotic execution, data analysis, and manuscript preparation. We validate this through investigation of vortex-induced vibration (VIV) and wake-induced vibration (WIV) in tandem cylinders. Our work has four key contributions: (1) A computer-controlled circulating water tunnel (CWT) with programmatic control of flow velocity, cylinder position, and forcing parameters (vibration frequency and amplitude) with data acquisition (displacement, force, and torque). (2) Automated experiments reproduce literature benchmarks (Khalak and Williamson [1999] and Assi et al. [2013, 2010]) with frequency lock-in within 4% and matching critical spacing trends. (3) The framework with Human-in-the-Loop (HIL) discovers more WIV amplitude response phenomena, and uses a neural network to fit physical laws from data, which is 31% higher than that of polynomial fitting. (4) The framework with multi-agent with virtual-real interaction system executes hundreds of experiments end-to-end, which automatically completes the entire process of scientific research from hypothesis generation, experimental design, experimental execution, data analysis, and manuscript preparation. It greatly liberates human researchers and improves study efficiency, providing new paradigm for the development and research of experimental fluid mechanics.\nCategories: physics.flu-dyn, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04716\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1661, "total_text_length": 1757, "word_count": 226, "sentence_count": 10}, "basic_keywords": ["experimental", "data", "fluid", "framework", "vibration", "mechanics", "work", "executes"], "domain_keywords": [], "technical_terms": ["Human", "Fluid Scientist", "Williamson", "Loop", "Assi", "Khalak", "Automated"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04714", "topic": "artificial intelligence", "title": "Playing the Player: A Heuristic Framework for Adaptive Poker AI", "authors": ["Andrew Paterson", "Carl Sanders"], "first_author": "Andrew Paterson", "abstract": "For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.GT"], "url": "http://arxiv.org/abs/2512.04714", "comments": "49 pages, 39 figures. White Paper by Spiderdime Systems", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Playing the Player: A Heuristic Framework for Adaptive Poker AI\nResearch Topic: artificial intelligence\nAuthors: Andrew Paterson, Carl Sanders\nAbstract: For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.\nCategories: cs.AI, cs.GT\nPublication Date: 2025-12-04\nPaper ID: 2512.04714\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 63, "abstract_length": 804, "total_text_length": 868, "word_count": 130, "sentence_count": 6}, "basic_keywords": ["unexploitable", "paper", "patrick", "built", "being", "human", "years", "discourse"], "domain_keywords": [], "technical_terms": ["Through", "Patrick"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04711", "topic": "artificial intelligence", "title": "Large Speech Model Enabled Semantic Communication", "authors": ["Yun Tian", "Zhijin Qin", "Guocheng Lv", "Ye Jin", "Kaibin Huang", "Zhu Han"], "first_author": "Yun Tian", "abstract": "Existing speech semantic communication systems mainly based on Joint Source-Channel Coding (JSCC) architectures have demonstrated impressive performance, but their effectiveness remains limited by model structures specifically designed for particular tasks and datasets. Recent advances indicate that generative large models pre-trained on massive datasets, can achieve outstanding performance arexhibit exceptional performance across diverse downstream tasks with minimal fine-tuning. To exploit the rich semantic knowledge embedded in large models and enable adaptive transmission over lossy channels, we propose a Large Speech Model enabled Semantic Communication (LargeSC) system. Simultaneously achieving adaptive compression and robust transmission over lossy channels remains challenging, requiring trade-offs among compression efficiency, speech quality, and latency. In this work, we employ the Mimi as a speech codec, converting speech into discrete tokens compatible with existing network architectures. We propose an adaptive controller module that enables adaptive transmission and in-band Unequal Error Protection (UEP), dynamically adjusting to both speech content and packet loss probability under bandwidth constraints. Additionally, we employ Low-Rank Adaptation (LoRA) to finetune the Moshi foundation model for generative recovery of lost speech tokens. Simulation results show that the proposed system supports bandwidths ranging from 550 bps to 2.06 kbps, outperforms conventional baselines in speech quality under high packet loss rates and achieves an end-to-end latency of approximately 460 ms, thereby demonstrating its potential for real-time deployment.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.AI"], "url": "http://arxiv.org/abs/2512.04711", "comments": "15 pages, 9 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Large Speech Model Enabled Semantic Communication\nResearch Topic: artificial intelligence\nAuthors: Yun Tian, Zhijin Qin, Guocheng Lv, Ye Jin, Kaibin Huang, Zhu Han\nAbstract: Existing speech semantic communication systems mainly based on Joint Source-Channel Coding (JSCC) architectures have demonstrated impressive performance, but their effectiveness remains limited by model structures specifically designed for particular tasks and datasets. Recent advances indicate that generative large models pre-trained on massive datasets, can achieve outstanding performance arexhibit exceptional performance across diverse downstream tasks with minimal fine-tuning. To exploit the rich semantic knowledge embedded in large models and enable adaptive transmission over lossy channels, we propose a Large Speech Model enabled Semantic Communication (LargeSC) system. Simultaneously achieving adaptive compression and robust transmission over lossy channels remains challenging, requiring trade-offs among compression efficiency, speech quality, and latency. In this work, we employ the Mimi as a speech codec, converting speech into discrete tokens compatible with existing network architectures. We propose an adaptive controller module that enables adaptive transmission and in-band Unequal Error Protection (UEP), dynamically adjusting to both speech content and packet loss probability under bandwidth constraints. Additionally, we employ Low-Rank Adaptation (LoRA) to finetune the Moshi foundation model for generative recovery of lost speech tokens. Simulation results show that the proposed system supports bandwidths ranging from 550 bps to 2.06 kbps, outperforms conventional baselines in speech quality under high packet loss rates and achieves an end-to-end latency of approximately 460 ms, thereby demonstrating its potential for real-time deployment.\nCategories: cs.SD, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04711\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 49, "abstract_length": 1681, "total_text_length": 1731, "word_count": 224, "sentence_count": 10}, "basic_keywords": ["speech", "adaptive", "semantic", "performance", "model", "large", "transmission", "existing"], "domain_keywords": [], "technical_terms": ["Large Speech Model", "Joint Source", "Mimi", "Existing", "Moshi", "Additionally", "Simultaneously", "Channel Coding"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04694", "topic": "artificial intelligence", "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation", "authors": ["Baris Yilmaz", "Bevan Deniz Cilgin", "Erdem Akagündüz", "Salih Tileylioglu"], "first_author": "Baris Yilmaz", "abstract": "Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency distributions between real and generated records per station, and summarize station specificity with a score based on the distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04694", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation\nResearch Topic: artificial intelligence\nAuthors: Baris Yilmaz, Bevan Deniz Cilgin, Erdem Akagündüz, Salih Tileylioglu\nAbstract: Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04694\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1010, "total_text_length": 1083, "word_count": 134, "sentence_count": 10}, "basic_keywords": ["site", "station", "specific", "ground", "motion", "strong", "timesnet", "generation"], "domain_keywords": [], "technical_terms": ["Effective"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04691", "topic": "artificial intelligence", "title": "Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective", "authors": ["Jae Hee Lee", "Anne Lauscher", "Stefano V. Albrecht"], "first_author": "Jae Hee Lee", "abstract": "Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.CL", "cs.MA"], "url": "http://arxiv.org/abs/2512.04691", "comments": "Accepted to LaMAS 2026@AAAI'26 (https://sites.google.com/view/lamas2026)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective\nResearch Topic: artificial intelligence\nAuthors: Jae Hee Lee, Anne Lauscher, Stefano V. Albrecht\nAbstract: Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.\nCategories: cs.AI, cs.CL, cs.MA\nPublication Date: 2025-12-04\nPaper ID: 2512.04691\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 104, "abstract_length": 933, "total_text_length": 1038, "word_count": 132, "sentence_count": 5}, "basic_keywords": ["ethical", "systems", "llms", "multi", "agent", "challenges", "research", "behavior"], "domain_keywords": [], "technical_terms": ["While", "Large"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04680", "topic": "artificial intelligence", "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap", "authors": ["Jialong Li", "Mingyue Zhang", "Nianyu Li", "Danny Weyns", "Zhi Jin", "Kenji Tei"], "first_author": "Jialong Li", "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SE", "cs.AI", "cs.HC"], "url": "http://arxiv.org/abs/2512.04680", "comments": "Accepted by ACM Transactions on Autonomous and Adaptive Systems", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap\nResearch Topic: artificial intelligence\nAuthors: Jialong Li, Mingyue Zhang, Nianyu Li, Danny Weyns, Zhi Jin, Kenji Tei\nAbstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.\nCategories: cs.SE, cs.AI, cs.HC\nPublication Date: 2025-12-04\nPaper ID: 2512.04680\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1792, "total_text_length": 1871, "word_count": 265, "sentence_count": 11}, "basic_keywords": ["sass", "genai", "challenges", "potential", "benefits", "loop", "within", "research"], "domain_keywords": [], "technical_terms": ["These", "However", "Recently", "Specifically", "Self"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04668", "topic": "artificial intelligence", "title": "Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs", "authors": ["Jinbo Liu", "Defu Cao", "Yifei Wei", "Tianyao Su", "Yuan Liang", "Yushun Dong", "Yue Zhao", "Xiyang Hu"], "first_author": "Jinbo Liu", "abstract": "Graph topology is a fundamental determinant of memory leakage in multi-agent LLM systems, yet its effects remain poorly quantified. We introduce MAMA (Multi-Agent Memory Attack), a framework that measures how network structure shapes leakage. MAMA operates on synthetic documents containing labeled Personally Identifiable Information (PII) entities, from which we generate sanitized task instructions. We execute a two-phase protocol: Engram (seeding private information into a target agent's memory) and Resonance (multi-round interaction where an attacker attempts extraction). Over up to 10 interaction rounds, we quantify leakage as the fraction of ground-truth PII recovered from attacking agent outputs via exact matching. We systematically evaluate six common network topologies (fully connected, ring, chain, binary tree, star, and star-ring), varying agent counts , attacker-target placements, and base models. Our findings reveal consistent patterns: fully connected graphs exhibit maximum leakage while chains provide strongest protection; shorter attacker-target graph distance and higher target centrality significantly increase vulnerability; leakage rises sharply in early rounds before plateauing; model choice shifts absolute leakage rates but preserves topology rankings; temporal/locational PII attributes leak more readily than identity credentials or regulated identifiers. These results provide the first systematic mapping from architectural choices to measurable privacy risk, yielding actionable guidance: prefer sparse or hierarchical connectivity, maximize attacker-target separation, limit node degree and network radius, avoid shortcuts bypassing hubs, and implement topology-aware access controls.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR", "cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.04668", "comments": "Under review at ACL Rolling Review", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs\nResearch Topic: artificial intelligence\nAuthors: Jinbo Liu, Defu Cao, Yifei Wei, Tianyao Su, Yuan Liang, Yushun Dong, Yue Zhao, Xiyang Hu\nAbstract: Graph topology is a fundamental determinant of memory leakage in multi-agent LLM systems, yet its effects remain poorly quantified. We introduce MAMA (Multi-Agent Memory Attack), a framework that measures how network structure shapes leakage. MAMA operates on synthetic documents containing labeled Personally Identifiable Information (PII) entities, from which we generate sanitized task instructions. We execute a two-phase protocol: Engram (seeding private information into a target agent's memory) and Resonance (multi-round interaction where an attacker attempts extraction). Over up to 10 interaction rounds, we quantify leakage as the fraction of ground-truth PII recovered from attacking agent outputs via exact matching. We systematically evaluate six common network topologies (fully connected, ring, chain, binary tree, star, and star-ring), varying agent counts $n\\in\\{4,5,6\\}$, attacker-target placements, and base models. Our findings reveal consistent patterns: fully connected graphs exhibit maximum leakage while chains provide strongest protection; shorter attacker-target graph distance and higher target centrality significantly increase vulnerability; leakage rises sharply in early rounds before plateauing; model choice shifts absolute leakage rates but preserves topology rankings; temporal/locational PII attributes leak more readily than identity credentials or regulated identifiers. These results provide the first systematic mapping from architectural choices to measurable privacy risk, yielding actionable guidance: prefer sparse or hierarchical connectivity, maximize attacker-target separation, limit node degree and network radius, avoid shortcuts bypassing hubs, and implement topology-aware access controls.\nCategories: cs.CR, cs.AI, cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04668\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1728, "total_text_length": 1791, "word_count": 229, "sentence_count": 9}, "basic_keywords": ["leakage", "agent", "target", "attacker", "topology", "memory", "multi", "network"], "domain_keywords": [], "technical_terms": ["Personally Identifiable Information", "These", "Over", "Graph", "Engram", "Agent Memory Attack", "Resonance", "Multi"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04653", "topic": "artificial intelligence", "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control", "authors": ["Pouria Yazdani", "Arash Rezaali", "Monireh Abdoos"], "first_author": "Pouria Yazdani", "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.MA", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04653", "comments": "Co-first authors: Pouria Yazdani and Arash Rezaali", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control\nResearch Topic: artificial intelligence\nAuthors: Pouria Yazdani, Arash Rezaali, Monireh Abdoos\nAbstract: Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.\nCategories: cs.MA, cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04653\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 132, "abstract_length": 1554, "total_text_length": 1687, "word_count": 212, "sentence_count": 10}, "basic_keywords": ["fully", "centralized", "decentralized", "semi", "architecture", "multi", "approaches", "based"], "domain_keywords": [], "technical_terms": ["These", "Decentralized Execution", "Within", "Existing", "Fully", "Centralized Training", "Semi", "Multi"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04643", "topic": "artificial intelligence", "title": "SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding", "authors": ["Chang-Hsun Wu", "Kai-Po Chang", "Yu-Yang Sheng", "Hung-Kai Chung", "Kuei-Chun Wang", "Yu-Chiang Frank Wang"], "first_author": "Chang-Hsun Wu", "abstract": "Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.04643", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding\nResearch Topic: artificial intelligence\nAuthors: Chang-Hsun Wu, Kai-Po Chang, Yu-Yang Sheng, Hung-Kai Chung, Kuei-Chun Wang, Yu-Chiang Frank Wang\nAbstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.\nCategories: cs.CV, cs.AI, cs.CL, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04643\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 113, "abstract_length": 1211, "total_text_length": 1325, "word_count": 163, "sentence_count": 11}, "basic_keywords": ["temporal", "video", "hallucination", "understanding", "spatial", "models", "videollms", "while"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "While", "Diagnostic Contrastive Decoding", "Video Large Language Models", "Self", "Therefore"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04639", "topic": "artificial intelligence", "title": "When GenAI Meets Fake News: Understanding Image Cascade Dynamics on Reddit", "authors": ["Saumya Chauhan", "Mila Hong", "Maria Vazhaeparambil"], "first_author": "Saumya Chauhan", "abstract": "AI-generated content and misinformation are increasingly prevalent on social networks. While prior research primarily examined textual misinformation, fewer studies have focused on visual content's role in virality. In this work, we present the first large-scale analysis of how misinformation and AI-generated images propagate through repost cascades across five ideologically diverse Reddit communities. By integrating textual sentiment, visual attributes, and diffusion metrics (e.g., time-to-first repost, community reach), our framework accurately predicts both immediate post-level virality (AUC=0.83) and long-term cascade-level spread (AUC=0.998). These findings offer essential insights for moderating synthetic and misleading visual content online.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SI", "cs.AI"], "url": "http://arxiv.org/abs/2512.04639", "comments": "Accepted at 2025 MIT Undergraduate Research Technology Conference (URTC'25)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: When GenAI Meets Fake News: Understanding Image Cascade Dynamics on Reddit\nResearch Topic: artificial intelligence\nAuthors: Saumya Chauhan, Mila Hong, Maria Vazhaeparambil\nAbstract: AI-generated content and misinformation are increasingly prevalent on social networks. While prior research primarily examined textual misinformation, fewer studies have focused on visual content's role in virality. In this work, we present the first large-scale analysis of how misinformation and AI-generated images propagate through repost cascades across five ideologically diverse Reddit communities. By integrating textual sentiment, visual attributes, and diffusion metrics (e.g., time-to-first repost, community reach), our framework accurately predicts both immediate post-level virality (AUC=0.83) and long-term cascade-level spread (AUC=0.998). These findings offer essential insights for moderating synthetic and misleading visual content online.\nCategories: cs.SI, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04639\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 758, "total_text_length": 833, "word_count": 104, "sentence_count": 10}, "basic_keywords": ["content", "misinformation", "visual", "generated", "textual", "virality", "first", "repost"], "domain_keywords": [], "technical_terms": ["These", "While", "Reddit"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04632", "topic": "artificial intelligence", "title": "Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning", "authors": ["Thibaut Boissin", "Thomas Massena", "Franck Mamalet", "Mathieu Serrurier"], "first_author": "Thibaut Boissin", "abstract": "Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04632", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning\nResearch Topic: artificial intelligence\nAuthors: Thibaut Boissin, Thomas Massena, Franck Mamalet, Mathieu Serrurier\nAbstract: Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04632\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 1303, "total_text_length": 1383, "word_count": 182, "sentence_count": 13}, "basic_keywords": ["training", "newton", "schulz", "performance", "across", "efficiency", "preconditioning", "convergence"], "domain_keywords": [], "technical_terms": ["However", "Crucially", "Furthermore", "Muon", "Newton", "Orthogonality", "Even", "Schulz"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04629", "topic": "artificial intelligence", "title": "BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation", "authors": ["Chenyang Zuo", "Siqi Fan", "Zaiqing Nie"], "first_author": "Chenyang Zuo", "abstract": "Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging the power of it, we further explore retrosynthetic planning task, and the performance on RetroBench demonstrates its competitive capability of acting as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04629", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation\nResearch Topic: artificial intelligence\nAuthors: Chenyang Zuo, Siqi Fan, Zaiqing Nie\nAbstract: Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging the power of it, we further explore retrosynthetic planning task, and the performance on RetroBench demonstrates its competitive capability of acting as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04629\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1370, "total_text_length": 1448, "word_count": 188, "sentence_count": 10}, "basic_keywords": ["model", "language", "molecular", "task", "biomedical", "large", "models", "reasoning"], "domain_keywords": [], "technical_terms": ["Given", "Molecules", "Bench", "Leveraging"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04618", "topic": "artificial intelligence", "title": "Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning", "authors": ["Mohamed Baha Ben Ticha", "Xingchen Ran", "Guillaume Saldanha", "Gaël Le Godais", "Philémon Roussel", "Marc Aubert", "Amina Fontanell", "Thomas Costecalde", "Lucas Struber", "Serpil Karakas", "Shaomin Zhang", "Philippe Kahane", "Guillaume Charvet", "Stéphan Chabardès", "Blaise Yvert"], "first_author": "Mohamed Baha Ben Ticha", "abstract": "Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04618", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning\nResearch Topic: artificial intelligence\nAuthors: Mohamed Baha Ben Ticha, Xingchen Ran, Guillaume Saldanha, Gaël Le Godais, Philémon Roussel, Marc Aubert, Amina Fontanell, Thomas Costecalde, Lucas Struber, Serpil Karakas, Shaomin Zhang, Philippe Kahane, Guillaume Charvet, Stéphan Chabardès, Blaise Yvert\nAbstract: Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04618\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 107, "abstract_length": 1364, "total_text_length": 1472, "word_count": 207, "sentence_count": 9}, "basic_keywords": ["speech", "ecog", "surface", "intracortical", "recordings", "using", "obtain", "signals"], "domain_keywords": [], "technical_terms": ["Vision Transformers", "Here", "While", "Speech Brain Computer Interfaces"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04598", "topic": "artificial intelligence", "title": "The Ethics of Generative AI", "authors": ["Michael Klenk"], "first_author": "Michael Klenk", "abstract": "This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04598", "comments": "Draft version to appear as a chapter in the Encyclopedia of Applied Ethics, 3rd Edition, edited by Ruth Chadwick", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: The Ethics of Generative AI\nResearch Topic: artificial intelligence\nAuthors: Michael Klenk\nAbstract: This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04598\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 27, "abstract_length": 732, "total_text_length": 760, "word_count": 111, "sentence_count": 5}, "basic_keywords": ["generative", "ethics", "chapter", "provides", "ethical", "forms", "discusses", "technical"], "domain_keywords": [], "technical_terms": ["Finally"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.7749999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04597", "topic": "artificial intelligence", "title": "When Robots Should Say \"I Don't Know\": Benchmarking Abstention in Embodied Question Answering", "authors": ["Tao Wu", "Chuhao Zhou", "Guangyu Zhao", "Haozhi Cao", "Yewen Pu", "Jianfei Yang"], "first_author": "Tao Wu", "abstract": "Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "url": "http://arxiv.org/abs/2512.04597", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: When Robots Should Say \"I Don't Know\": Benchmarking Abstention in Embodied Question Answering\nResearch Topic: artificial intelligence\nAuthors: Tao Wu, Chuhao Zhou, Guangyu Zhao, Haozhi Cao, Yewen Pu, Jianfei Yang\nAbstract: Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.\nCategories: cs.CV, cs.AI, cs.LG, cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04597\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1523, "total_text_length": 1617, "word_count": 221, "sentence_count": 14}, "basic_keywords": ["abstention", "embodied", "find", "question", "agents", "information", "answer", "initial"], "domain_keywords": [], "technical_terms": ["Evaluating", "Existing", "Drawing", "Together", "Embodied Question Answering"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04580", "topic": "artificial intelligence", "title": "A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution", "authors": ["Huifeng Zhu", "Shijie Li", "Qinfeng Li", "Yier Jin"], "first_author": "Huifeng Zhu", "abstract": "To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment. In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR", "cs.AI"], "url": "http://arxiv.org/abs/2512.04580", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution\nResearch Topic: artificial intelligence\nAuthors: Huifeng Zhu, Shijie Li, Qinfeng Li, Yier Jin\nAbstract: To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.   In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.\nCategories: cs.CR, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04580\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1829, "total_text_length": 1914, "word_count": 242, "sentence_count": 12}, "basic_keywords": ["model", "deployment", "secure", "cryptotensors", "performance", "models", "llms", "specific"], "domain_keywords": [], "technical_terms": ["However", "Safetensors", "Hugging Face Transformers", "Current", "Built", "Such", "Although", "Therefore"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04578", "topic": "artificial intelligence", "title": "LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence", "authors": ["Wenjin Liu", "Haoran Luo", "Xin Feng", "Xiang Ji", "Lijuan Zhou", "Rui Mao", "Jiapu Wang", "Shirui Pan", "Erik Cambria"], "first_author": "Wenjin Liu", "abstract": "Legal general intelligence (GI) refers to artificial intelligence (AI) that encompasses legal understanding, reasoning, and decision-making, simulating the expertise of legal experts across domains. However, existing benchmarks are result-oriented and fail to systematically evaluate the legal intelligence of large language models (LLMs), hindering the development of legal GI. To address this, we propose LexGenius, an expert-level Chinese legal benchmark for evaluating legal GI in LLMs. It follows a Dimension-Task-Ability framework, covering seven dimensions, eleven tasks, and twenty abilities. We use the recent legal cases and exam questions to create multiple-choice questions with a combination of manual and LLM reviews to reduce data leakage risks, ensuring accuracy and reliability through multiple rounds of checks. We evaluate 12 state-of-the-art LLMs using LexGenius and conduct an in-depth analysis. We find significant disparities across legal intelligence abilities for LLMs, with even the best LLMs lagging behind human legal professionals. We believe LexGenius can assess the legal intelligence abilities of LLMs and enhance legal GI development. Our project is available at https://github.com/QwenQKing/LexGenius.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04578", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence\nResearch Topic: artificial intelligence\nAuthors: Wenjin Liu, Haoran Luo, Xin Feng, Xiang Ji, Lijuan Zhou, Rui Mao, Jiapu Wang, Shirui Pan, Erik Cambria\nAbstract: Legal general intelligence (GI) refers to artificial intelligence (AI) that encompasses legal understanding, reasoning, and decision-making, simulating the expertise of legal experts across domains. However, existing benchmarks are result-oriented and fail to systematically evaluate the legal intelligence of large language models (LLMs), hindering the development of legal GI. To address this, we propose LexGenius, an expert-level Chinese legal benchmark for evaluating legal GI in LLMs. It follows a Dimension-Task-Ability framework, covering seven dimensions, eleven tasks, and twenty abilities. We use the recent legal cases and exam questions to create multiple-choice questions with a combination of manual and LLM reviews to reduce data leakage risks, ensuring accuracy and reliability through multiple rounds of checks. We evaluate 12 state-of-the-art LLMs using LexGenius and conduct an in-depth analysis. We find significant disparities across legal intelligence abilities for LLMs, with even the best LLMs lagging behind human legal professionals. We believe LexGenius can assess the legal intelligence abilities of LLMs and enhance legal GI development. Our project is available at https://github.com/QwenQKing/LexGenius.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04578\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1235, "total_text_length": 1328, "word_count": 179, "sentence_count": 11}, "basic_keywords": ["legal", "llms", "intelligence", "lexgenius", "abilities", "across", "evaluate", "development"], "domain_keywords": [], "technical_terms": ["Dimension", "However", "Chinese", "Ability", "Task", "Legal"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04569", "topic": "artificial intelligence", "title": "An all-optical convolutional neural network for image identification", "authors": ["Wei-Wei Fu", "Dong Zhao", "Qing-Hong Rao", "Heng-Yi Wang", "Ben-Li Yu", "Zhi-Jia Hu", "Fang-Wen Sun", "Kun Huang"], "first_author": "Wei-Wei Fu", "abstract": "In modern artificial intelligence, convolutional neural networks (CNNs) have become a cornerstone for visual and perceptual tasks. However, their implementation on conventional electronic hardware faces fundamental bottlenecks in speed and energy efficiency due to resistive and capacitive losses. Photonic alternatives offer a promising route, yet the difficulty of realizing optical nonlinearities has prevented the realization of all-optical CNNs capable of end-to-end image classification. Here, we demonstrate an all-optical CNN that bypasses the need for explicit optical nonlinear activations. Our architecture comprises a single spatial-differentiation convolutional stage--using 24 directional kernels spanning 360 , along with a mean-filtering kernel--followed by a diffractive fully-connected layer. The directional convolution enhances feature selectivity, suppresses noise and crosstalk, and simplifies the classification task, allowing the weak nonlinearity inherent in optical diffraction to achieve high accuracy. We report experimentally classification accuracies of 86.8% on handwritten digits (MNIST) and 94.8% on a ten-class gesture dataset. The system delivers a computational throughput of 1.13X10^5 tera-operations per second (TOPS) and an energy efficiency of 1.51X10^3 TOPS/W--the highest reported among CNN hardware--with the potential to improve by a further 5-6 orders of magnitude using nanosecond-scale detectors. This work establishes a scalable pathway toward ultralow-latency, ultralow-energy vision processing for real-time intelligent systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.optics", "physics.comp-ph"], "url": "http://arxiv.org/abs/2512.04569", "comments": "4 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637301", "embedding_text": "Paper Title: An all-optical convolutional neural network for image identification\nResearch Topic: artificial intelligence\nAuthors: Wei-Wei Fu, Dong Zhao, Qing-Hong Rao, Heng-Yi Wang, Ben-Li Yu, Zhi-Jia Hu, Fang-Wen Sun, Kun Huang\nAbstract: In modern artificial intelligence, convolutional neural networks (CNNs) have become a cornerstone for visual and perceptual tasks. However, their implementation on conventional electronic hardware faces fundamental bottlenecks in speed and energy efficiency due to resistive and capacitive losses. Photonic alternatives offer a promising route, yet the difficulty of realizing optical nonlinearities has prevented the realization of all-optical CNNs capable of end-to-end image classification. Here, we demonstrate an all-optical CNN that bypasses the need for explicit optical nonlinear activations. Our architecture comprises a single spatial-differentiation convolutional stage--using 24 directional kernels spanning 360°, along with a mean-filtering kernel--followed by a diffractive fully-connected layer. The directional convolution enhances feature selectivity, suppresses noise and crosstalk, and simplifies the classification task, allowing the weak nonlinearity inherent in optical diffraction to achieve high accuracy. We report experimentally classification accuracies of 86.8% on handwritten digits (MNIST) and 94.8% on a ten-class gesture dataset. The system delivers a computational throughput of 1.13X10^5 tera-operations per second (TOPS) and an energy efficiency of 1.51X10^3 TOPS/W--the highest reported among CNN hardware--with the potential to improve by a further 5-6 orders of magnitude using nanosecond-scale detectors. This work establishes a scalable pathway toward ultralow-latency, ultralow-energy vision processing for real-time intelligent systems.\nCategories: physics.optics, physics.comp-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04569\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1578, "total_text_length": 1647, "word_count": 208, "sentence_count": 14}, "basic_keywords": ["optical", "energy", "classification", "convolutional", "cnns", "hardware", "efficiency", "using"], "domain_keywords": ["convolutional neural networks (cnns)", "image classification"], "technical_terms": ["However", "Here", "Photonic"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04559", "topic": "artificial intelligence", "title": "Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function", "authors": ["Hyeongyu Kang", "Jaewoo Lee", "Woocheol Shin", "Kiyoung Om", "Jinkyoo Park"], "first_author": "Hyeongyu Kang", "abstract": "Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose , a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04559", "comments": "36 pages, 21 figures, 4 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function\nResearch Topic: artificial intelligence\nAuthors: Hyeongyu Kang, Jaewoo Lee, Woocheol Shin, Kiyoung Om, Jinkyoo Park\nAbstract: Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \\textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04559\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1064, "total_text_length": 1145, "word_count": 147, "sentence_count": 7}, "basic_keywords": ["diversity", "diffusion", "models", "high", "alignment", "reward", "optimization", "sqdf"], "domain_keywords": [], "technical_terms": ["Furthermore", "Existing", "Diffusion"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04552", "topic": "artificial intelligence", "title": "RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS", "authors": ["Cong Wang", "Changfeng Gao", "Yang Xiang", "Zhihao Du", "Keyu An", "Han Zhao", "Qian Chen", "Xiangang Li", "Yingming Gao", "Ya Li"], "first_author": "Cong Wang", "abstract": "Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.AI", "eess.AS"], "url": "http://arxiv.org/abs/2512.04552", "comments": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS\nResearch Topic: artificial intelligence\nAuthors: Cong Wang, Changfeng Gao, Yang Xiang, Zhihao Du, Keyu An, Han Zhao, Qian Chen, Xiangang Li, Yingming Gao, Ya Li\nAbstract: Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.\nCategories: cs.SD, cs.AI, eess.AS\nPublication Date: 2025-12-04\nPaper ID: 2512.04552\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 1112, "total_text_length": 1180, "word_count": 159, "sentence_count": 10}, "basic_keywords": ["reward", "policy", "robust", "like", "hacking", "model", "rrpo", "scheme"], "domain_keywords": [], "technical_terms": ["Differentiable", "Robust Reward Policy Optimization", "Demo", "Reward Model"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04551", "topic": "artificial intelligence", "title": "Multi-Loss Learning for Speech Emotion Recognition with Energy-Adaptive Mixup and Frame-Level Attention", "authors": ["Cong Wang", "Yizhong Geng", "Yuhua Wen", "Qifei Li", "Yingming Gao", "Ruimin Wang", "Chunfeng Wang", "Hao Li", "Ya Li", "Wei Chen"], "first_author": "Cong Wang", "abstract": "Speech emotion recognition (SER) is an important technology in human-computer interaction. However, achieving high performance is challenging due to emotional complexity and scarce annotated data. To tackle these challenges, we propose a multi-loss learning (MLL) framework integrating an energy-adaptive mixup (EAM) method and a frame-level attention module (FLAM). The EAM method leverages SNR-based augmentation to generate diverse speech samples capturing subtle emotional variations. FLAM enhances frame-level feature extraction for multi-frame emotional cues. Our MLL strategy combines Kullback-Leibler divergence, focal, center, and supervised contrastive loss to optimize learning, address class imbalance, and improve feature separability. We evaluate our method on four widely used SER datasets: IEMOCAP, MSP-IMPROV, RAVDESS, and SAVEE. The results demonstrate our method achieves state-of-the-art performance, suggesting its effectiveness and robustness.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.AI", "eess.AS"], "url": "http://arxiv.org/abs/2512.04551", "comments": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Multi-Loss Learning for Speech Emotion Recognition with Energy-Adaptive Mixup and Frame-Level Attention\nResearch Topic: artificial intelligence\nAuthors: Cong Wang, Yizhong Geng, Yuhua Wen, Qifei Li, Yingming Gao, Ruimin Wang, Chunfeng Wang, Hao Li, Ya Li, Wei Chen\nAbstract: Speech emotion recognition (SER) is an important technology in human-computer interaction. However, achieving high performance is challenging due to emotional complexity and scarce annotated data. To tackle these challenges, we propose a multi-loss learning (MLL) framework integrating an energy-adaptive mixup (EAM) method and a frame-level attention module (FLAM). The EAM method leverages SNR-based augmentation to generate diverse speech samples capturing subtle emotional variations. FLAM enhances frame-level feature extraction for multi-frame emotional cues. Our MLL strategy combines Kullback-Leibler divergence, focal, center, and supervised contrastive loss to optimize learning, address class imbalance, and improve feature separability. We evaluate our method on four widely used SER datasets: IEMOCAP, MSP-IMPROV, RAVDESS, and SAVEE. The results demonstrate our method achieves state-of-the-art performance, suggesting its effectiveness and robustness.\nCategories: cs.SD, cs.AI, eess.AS\nPublication Date: 2025-12-04\nPaper ID: 2512.04551\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 103, "abstract_length": 965, "total_text_length": 1069, "word_count": 134, "sentence_count": 9}, "basic_keywords": ["method", "emotional", "frame", "speech", "performance", "multi", "loss", "learning"], "domain_keywords": ["feature extraction"], "technical_terms": ["Leibler", "However", "Kullback", "Speech"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04550", "topic": "artificial intelligence", "title": "AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees", "authors": ["Yangning Li", "Shaoshen Chen", "Yinghui Li", "Yankai Chen", "Hai-Tao Zheng", "Hui Wang", "Wenhao Jiang", "Philip S. Yu"], "first_author": "Yangning Li", "abstract": "The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04550", "comments": "NeurIPS 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees\nResearch Topic: artificial intelligence\nAuthors: Yangning Li, Shaoshen Chen, Yinghui Li, Yankai Chen, Hai-Tao Zheng, Hui Wang, Wenhao Jiang, Philip S. Yu\nAbstract: The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04550\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1262, "total_text_length": 1328, "word_count": 168, "sentence_count": 8}, "basic_keywords": ["semantic", "information", "long", "context", "admtree", "contexts", "compression", "while"], "domain_keywords": [], "technical_terms": ["However", "Large Language Models", "Context"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04536", "topic": "artificial intelligence", "title": "Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model", "authors": ["Bita Baroutian", "Atefe Aghaei", "Mohsen Ebrahimi Moghaddam"], "first_author": "Bita Baroutian", "abstract": "Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04536", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model\nResearch Topic: artificial intelligence\nAuthors: Bita Baroutian, Atefe Aghaei, Mohsen Ebrahimi Moghaddam\nAbstract: Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04536\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1021, "total_text_length": 1115, "word_count": 149, "sentence_count": 12}, "basic_keywords": ["alcohol", "public", "video", "facial", "analysis", "approach", "detection", "intoxication"], "domain_keywords": [], "technical_terms": ["These", "Additionally", "Graph Attention Network", "Experimental", "Alcohol"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04535", "topic": "artificial intelligence", "title": "GTM: Simulating the World of Tools for AI Agents", "authors": ["Zhenzhen Ren", "Xinpeng Zhang", "Zhenxing Qian", "Yan Gao", "Yu Shi", "Shuxin Zheng", "Jiyan He"], "first_author": "Zhenzhen Ren", "abstract": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04535", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: GTM: Simulating the World of Tools for AI Agents\nResearch Topic: artificial intelligence\nAuthors: Zhenzhen Ren, Xinpeng Zhang, Zhenxing Qian, Yan Gao, Yu Shi, Shuxin Zheng, Jiyan He\nAbstract: The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04535\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 48, "abstract_length": 1611, "total_text_length": 1660, "word_count": 218, "sentence_count": 11}, "basic_keywords": ["tool", "tools", "real", "training", "model", "agents", "outputs", "through"], "domain_keywords": [], "technical_terms": ["However", "Experiments", "Large Language Model", "Besides", "Through", "Aware Response Generation", "Generalist Tool Model", "Context"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04532", "topic": "artificial intelligence", "title": "PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement", "authors": ["Yu-Wei Zhan", "Xin Wang", "Hong Chen", "Tongtong Feng", "Wei Feng", "Ren Wang", "Guangyao Li", "Qing Li", "Wenwu Zhu"], "first_author": "Yu-Wei Zhan", "abstract": "Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04532", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement\nResearch Topic: artificial intelligence\nAuthors: Yu-Wei Zhan, Xin Wang, Hong Chen, Tongtong Feng, Wei Feng, Ren Wang, Guangyao Li, Qing Li, Wenwu Zhu\nAbstract: Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04532\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1762, "total_text_length": 1846, "word_count": 233, "sentence_count": 11}, "basic_keywords": ["physical", "video", "motion", "phyvllm", "language", "llms", "often", "understanding"], "domain_keywords": [], "technical_terms": ["Video", "However", "Incorporating", "Neural Ordinary Differential Equation", "Video Large Language Models", "Neural", "Specifically", "Experimental"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04530", "topic": "artificial intelligence", "title": "Explainable Graph Representation Learning via Graph Pattern Analysis", "authors": ["Xudong Wang", "Ziheng Sun", "Chris Ding", "Jicong Fan"], "first_author": "Xudong Wang", "abstract": "Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04530", "comments": "Full version with appendix of the paper published in the Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence (IJCAI-25), Main Track", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Explainable Graph Representation Learning via Graph Pattern Analysis\nResearch Topic: artificial intelligence\nAuthors: Xudong Wang, Ziheng Sun, Chris Ding, Jicong Fan\nAbstract: Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04530\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1561, "total_text_length": 1630, "word_count": 221, "sentence_count": 12}, "basic_keywords": ["graph", "explainable", "learning", "representations", "pattern", "level", "representation", "patterns"], "domain_keywords": [], "technical_terms": ["While", "Additionally", "What", "Although", "Explainable", "Then"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04529", "topic": "artificial intelligence", "title": "SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation", "authors": ["Xin Liang", "Xiang Zhang", "Yiwei Xu", "Siqi Sun", "Chenyu You"], "first_author": "Xin Liang", "abstract": "Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04529", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation\nResearch Topic: artificial intelligence\nAuthors: Xin Liang, Xiang Zhang, Yiwei Xu, Siqi Sun, Chenyu You\nAbstract: Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04529\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1247, "total_text_length": 1321, "word_count": 174, "sentence_count": 8}, "basic_keywords": ["visual", "slide", "slides", "multimodal", "slidegen", "generation", "scientific", "reasoning"], "domain_keywords": [], "technical_terms": ["Generating", "Existing", "Across"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04524", "topic": "artificial intelligence", "title": "Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval", "authors": ["Tianle Hu", "Weijun Lv", "Na Han", "Xiaozhao Fang", "Jie Wen", "Jiaxing Li", "Guoxu Zhou"], "first_author": "Tianle Hu", "abstract": "Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04524", "comments": "This paper was accepted by AAAI2026 main tech track not long ago. This is an expanded version with an appendix", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval\nResearch Topic: artificial intelligence\nAuthors: Tianle Hu, Weijun Lv, Na Han, Xiaozhao Fang, Jie Wen, Jiaxing Li, Guoxu Zhou\nAbstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04524\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1599, "total_text_length": 1676, "word_count": 199, "sentence_count": 9}, "basic_keywords": ["domain", "class", "semantic", "alignment", "adaptive", "retrieval", "label", "features"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Prototype", "Based Semantic Consistency Alignment", "Domain", "During"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04518", "topic": "artificial intelligence", "title": "UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction", "authors": ["Tianmai M. Zhang", "Zhaoyi Sun", "Sihang Zeng", "Chenxi Li", "Neil F. Abernethy", "Barbara D. Lam", "Fei Xia", "Meliha Yetisgen"], "first_author": "Tianmai M. Zhang", "abstract": "The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04518", "comments": "To be published in Proceedings of the 7th Clinical Natural Language Processing Workshop", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction\nResearch Topic: artificial intelligence\nAuthors: Tianmai M. Zhang, Zhaoyi Sun, Sihang Zeng, Chenxi Li, Neil F. Abernethy, Barbara D. Lam, Fei Xia, Meliha Yetisgen\nAbstract: The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04518\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 129, "abstract_length": 1059, "total_text_length": 1189, "word_count": 157, "sentence_count": 9}, "basic_keywords": ["timelines", "task", "methods", "results", "patient", "chemotherapy", "clinical", "notes"], "domain_keywords": [], "technical_terms": ["Each", "Multiple"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04513", "topic": "artificial intelligence", "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models", "authors": ["Yu-Wei Zhan", "Xin Wang", "Pengzhe Mao", "Tongtong Feng", "Ren Wang", "Wenwu Zhu"], "first_author": "Yu-Wei Zhan", "abstract": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04513", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models\nResearch Topic: artificial intelligence\nAuthors: Yu-Wei Zhan, Xin Wang, Pengzhe Mao, Tongtong Feng, Ren Wang, Wenwu Zhu\nAbstract: Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04513\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 125, "abstract_length": 1617, "total_text_length": 1743, "word_count": 217, "sentence_count": 8}, "basic_keywords": ["task", "semantic", "dynamic", "aware", "learning", "embodied", "environment", "mllms"], "domain_keywords": [], "technical_terms": ["Extensive", "Aware Dynamic Joint Learning", "Joint Optimization", "Multimodal", "Task", "Their", "Aware Behavior Learning", "Building"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04500", "topic": "artificial intelligence", "title": "A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework", "authors": ["Edervaldo Melo"], "first_author": "Edervaldo Melo", "abstract": "This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules (\"personas\") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.HC", "cs.MA"], "url": "http://arxiv.org/abs/2512.04500", "comments": "6 pages, 1 figure. First version", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework\nResearch Topic: artificial intelligence\nAuthors: Edervaldo Melo\nAbstract: This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules (\"personas\") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.\nCategories: cs.AI, cs.HC, cs.MA\nPublication Date: 2025-12-04\nPaper ID: 2512.04500\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 815, "total_text_length": 895, "word_count": 110, "sentence_count": 6}, "basic_keywords": ["modular", "cognitive", "framework", "architecture", "support", "assisted", "reasoning", "through"], "domain_keywords": [], "technical_terms": ["Nemosine Framework"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04495", "topic": "artificial intelligence", "title": "Universal quantum control over non-Hermitian continuous-variable systems", "authors": ["Zhu-yao Jin", "Jun Jing"], "first_author": "Zhu-yao Jin", "abstract": "Although the control of non-Hermitian quantum systems has a growing interest for their nonunitary feature in the time evolution, the existing discussions are not more than two or three dimensions and heavily influenced by the singularity of the energy spectrum. We here develop a general theory to control an arbitrary number of bosonic modes governed by the time-dependent non-Hermitian Hamiltonian. It takes advantage of the gauge potential in the instantaneous frame rather than the energy spectrum of Hamiltonian. In particular, the dynamics of a general non-Hermitian continuous-variable system is analyzed in the instantaneous frame associated with time-dependent ancillary operators that are superpositions of the laboratory-frame operators and irrelevant to the original Hamiltonian. The gauge potential is determined by the unitary transformation between the time-dependent and stationary ancillary frames. The upper triangularization condition for the Hamiltonian's coefficient matrix in the stationary ancillary frame enables two of the time-dependent ancillary operators to be nonadiabatic Heisenberg passages of the non-Hermitian system. The probability conservation of the system wavefunction can be restored at the end of these passages without artificial normalization. Our theory is exemplified with the perfect and nonreciprocal state transfers in a cavity magnonic system. The former holds for arbitrary initial states and is irrelevant to the parity-time symmetry of the Hamiltonian and the exceptional point of the spectra; and the latter is consistent with the unidirectional perfect absorbtion. Our work essentially extends the universal quantum control (UQC) theory to the non-Hermitian continuous-variable systems, providing a promising approach for their coherent control.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph"], "url": "http://arxiv.org/abs/2512.04495", "comments": "12 pages, 5 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Universal quantum control over non-Hermitian continuous-variable systems\nResearch Topic: artificial intelligence\nAuthors: Zhu-yao Jin, Jun Jing\nAbstract: Although the control of non-Hermitian quantum systems has a growing interest for their nonunitary feature in the time evolution, the existing discussions are not more than two or three dimensions and heavily influenced by the singularity of the energy spectrum. We here develop a general theory to control an arbitrary number of bosonic modes governed by the time-dependent non-Hermitian Hamiltonian. It takes advantage of the gauge potential in the instantaneous frame rather than the energy spectrum of Hamiltonian. In particular, the dynamics of a general non-Hermitian continuous-variable system is analyzed in the instantaneous frame associated with time-dependent ancillary operators that are superpositions of the laboratory-frame operators and irrelevant to the original Hamiltonian. The gauge potential is determined by the unitary transformation between the time-dependent and stationary ancillary frames. The upper triangularization condition for the Hamiltonian's coefficient matrix in the stationary ancillary frame enables two of the time-dependent ancillary operators to be nonadiabatic Heisenberg passages of the non-Hermitian system. The probability conservation of the system wavefunction can be restored at the end of these passages without artificial normalization. Our theory is exemplified with the perfect and nonreciprocal state transfers in a cavity magnonic system. The former holds for arbitrary initial states and is irrelevant to the parity-time symmetry of the Hamiltonian and the exceptional point of the spectra; and the latter is consistent with the unidirectional perfect absorbtion. Our work essentially extends the universal quantum control (UQC) theory to the non-Hermitian continuous-variable systems, providing a promising approach for their coherent control.\nCategories: quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04495\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1798, "total_text_length": 1871, "word_count": 257, "sentence_count": 11}, "basic_keywords": ["time", "hermitian", "hamiltonian", "control", "dependent", "frame", "system", "ancillary"], "domain_keywords": [], "technical_terms": ["Heisenberg", "Hermitian Hamiltonian", "Hamiltonian", "Although", "Hermitian"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04489", "topic": "artificial intelligence", "title": "The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance", "authors": ["Yong Tao"], "first_author": "Yong Tao", "abstract": "Artificial intelligence (AI) advances rapidly but achieving complete human control over AI risks remains an unsolved problem, akin to driving the fast AI \"train\" without a \"brake system.\" By exploring fundamental control mechanisms at key elements of AI decisions, this paper develops a systematic solution to thoroughly control AI risks, providing an architecture for AI governance and legislation with five pillars supported by six control mechanisms, illustrated through a minimum set of AI Mandates (AIMs). Three of the AIMs must be built inside AI systems and three in society to address major areas of AI risks: 1) align AI values with human users; 2) constrain AI decision-actions by societal ethics, laws, and regulations; 3) build in human intervention options for emergencies and shut-off switches for existential threats; 4) limit AI access to resources to reinforce controls inside AI; 5) mitigate spillover risks like job loss from AI. We also highlight the differences in AI governance on physical AI systems versus generative AI. We discuss how to strengthen analog physical safeguards to prevent smarter AI/AGI/ASI from circumventing core safety controls by exploiting AI's intrinsic disconnect from the analog physical world: AI's nature as pure software code run on chips controlled by humans, and the prerequisite that all AI-driven physical actions must be digitized. These findings establish a theoretical foundation for AI governance and legislation as the basic structure of a \"brake system\" for AI decisions. If enacted, these controls can rein in AI dangers as completely as humanly possible, removing large chunks of currently wide-open AI risks, substantially reducing overall AI risks to residual human errors.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CY"], "url": "http://arxiv.org/abs/2512.04489", "comments": "39 pages, 6 figures, 1 table", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: The Decision Path to Control AI Risks Completely: Fundamental Control Mechanisms for AI Governance\nResearch Topic: artificial intelligence\nAuthors: Yong Tao\nAbstract: Artificial intelligence (AI) advances rapidly but achieving complete human control over AI risks remains an unsolved problem, akin to driving the fast AI \"train\" without a \"brake system.\" By exploring fundamental control mechanisms at key elements of AI decisions, this paper develops a systematic solution to thoroughly control AI risks, providing an architecture for AI governance and legislation with five pillars supported by six control mechanisms, illustrated through a minimum set of AI Mandates (AIMs). Three of the AIMs must be built inside AI systems and three in society to address major areas of AI risks: 1) align AI values with human users; 2) constrain AI decision-actions by societal ethics, laws, and regulations; 3) build in human intervention options for emergencies and shut-off switches for existential threats; 4) limit AI access to resources to reinforce controls inside AI; 5) mitigate spillover risks like job loss from AI. We also highlight the differences in AI governance on physical AI systems versus generative AI. We discuss how to strengthen analog physical safeguards to prevent smarter AI/AGI/ASI from circumventing core safety controls by exploiting AI's intrinsic disconnect from the analog physical world: AI's nature as pure software code run on chips controlled by humans, and the prerequisite that all AI-driven physical actions must be digitized. These findings establish a theoretical foundation for AI governance and legislation as the basic structure of a \"brake system\" for AI decisions. If enacted, these controls can rein in AI dangers as completely as humanly possible, removing large chunks of currently wide-open AI risks, substantially reducing overall AI risks to residual human errors.\nCategories: cs.CY\nPublication Date: 2025-12-04\nPaper ID: 2512.04489\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 98, "abstract_length": 1738, "total_text_length": 1837, "word_count": 279, "sentence_count": 8}, "basic_keywords": ["risks", "human", "control", "physical", "governance", "controls", "brake", "system"], "domain_keywords": [], "technical_terms": ["These", "Artificial", "Three", "Mandates"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04488", "topic": "artificial intelligence", "title": "Persona-based Multi-Agent Collaboration for Brainstorming", "authors": ["Nate Straub", "Saara Khan", "Katharina Jay", "Brian Cabral", "Oskar Linde"], "first_author": "Nate Straub", "abstract": "We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.HC"], "url": "http://arxiv.org/abs/2512.04488", "comments": "12 pages, 8 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Persona-based Multi-Agent Collaboration for Brainstorming\nResearch Topic: artificial intelligence\nAuthors: Nate Straub, Saara Khan, Katharina Jay, Brian Cabral, Oskar Linde\nAbstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.\nCategories: cs.AI, cs.HC\nPublication Date: 2025-12-04\nPaper ID: 2512.04488\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 838, "total_text_length": 896, "word_count": 113, "sentence_count": 8}, "basic_keywords": ["persona", "agent", "brainstorming", "multi", "idea", "based", "collaboration", "domain"], "domain_keywords": [], "technical_terms": ["Doctor", "Using", "Engineer", "Prior"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04480", "topic": "artificial intelligence", "title": "AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions", "authors": ["Pedro Passos"], "first_author": "Pedro Passos", "abstract": "In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the \"FAGNER Paradox\" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the \"Lukaku Paradox\", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.CE", "eess.SY", "math.OC"], "url": "http://arxiv.org/abs/2512.04480", "comments": "33 pages, 7 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions\nResearch Topic: artificial intelligence\nAuthors: Pedro Passos\nAbstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the \"FAGNER Paradox\" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the \"Lukaku Paradox\", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.\nCategories: cs.AI, cs.CE, eess.SY, math.OC\nPublication Date: 2025-12-04\nPaper ID: 2512.04480\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1686, "total_text_length": 1776, "word_count": 247, "sentence_count": 9}, "basic_keywords": ["system", "models", "time", "risk", "substitution", "decisions", "predictive", "fuzzy"], "domain_keywords": [], "technical_terms": ["Substitution Priority", "Gabriel Jesus", "These", "World Cup", "Role Aware Normalization", "Fuzzy Logic", "Brazil", "Cumulative Mean"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04475", "topic": "artificial intelligence", "title": "GraphBench: Next-generation graph learning benchmarking", "authors": ["Timo Stoll", "Chendi Qian", "Ben Finkelshtein", "Ali Parviz", "Darius Weber", "Fabrizio Frasca", "Hadar Shavit", "Antoine Siraudin", "Arman Mielke", "Marie Anastacio", "Erik Müller", "Maya Bechler-Speicher", "Michael Bronstein", "Mikhail Galkin", "Holger Hoos", "Mathias Niepert", "Bryan Perozzi", "Jan Tönshoff", "Christopher Morris"], "first_author": "Timo Stoll", "abstract": "Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "url": "http://arxiv.org/abs/2512.04475", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: GraphBench: Next-generation graph learning benchmarking\nResearch Topic: artificial intelligence\nAuthors: Timo Stoll, Chendi Qian, Ben Finkelshtein, Ali Parviz, Darius Weber, Fabrizio Frasca, Hadar Shavit, Antoine Siraudin, Arman Mielke, Marie Anastacio, Erik Müller, Maya Bechler-Speicher, Michael Bronstein, Mikhail Galkin, Holger Hoos, Mathias Niepert, Bryan Perozzi, Jan Tönshoff, Christopher Morris\nAbstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.\nCategories: cs.LG, cs.AI, cs.NE, stat.ML\nPublication Date: 2025-12-04\nPaper ID: 2512.04475\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 55, "abstract_length": 976, "total_text_length": 1032, "word_count": 121, "sentence_count": 9}, "basic_keywords": ["graphbench", "level", "progress", "domains", "including", "prediction", "benchmarking", "evaluation"], "domain_keywords": [], "technical_terms": ["Machine", "Additionally", "However"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04469", "topic": "artificial intelligence", "title": "Mathematical Framing for Different Agent Strategies", "authors": ["Philip Stephens", "Emmanuel Salawu"], "first_author": "Philip Stephens", "abstract": "We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the \"Degrees of Freedom\" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04469", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Mathematical Framing for Different Agent Strategies\nResearch Topic: artificial intelligence\nAuthors: Philip Stephens, Emmanuel Salawu\nAbstract: We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the \"Degrees of Freedom\" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.\nCategories: cs.AI, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04469\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 1023, "total_text_length": 1075, "word_count": 147, "sentence_count": 7}, "basic_keywords": ["agent", "strategies", "mathematical", "framework", "systems", "approach", "agentic", "probabilities"], "domain_keywords": [], "technical_terms": ["Degrees", "Freedom"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04464", "topic": "artificial intelligence", "title": "Feature Engineering vs. Deep Learning for Automated Coin Grading: A Comparative Study on Saint-Gaudens Double Eagles", "authors": ["Tanmay Dogra", "Eric Ngo", "Mohammad Alam", "Jean-Paul Talavera", "Asim Dahal"], "first_author": "Tanmay Dogra", "abstract": "We challenge the common belief that deep learning always trumps older techniques, using the example of grading Saint-Gaudens Double Eagle gold coins automatically. In our work, we put a feature-based Artificial Neural Network built around 192 custom features pulled from Sobel edge detection and HSV color analysis up against a hybrid Convolutional Neural Network that blends in EfficientNetV2, plus a straightforward Support Vector Machine as the control. Testing 1,785 coins graded by experts, the ANN nailed 86% exact matches and hit 98% when allowing a 3-grade leeway. On the flip side, CNN and SVM mostly just guessed the most common grade, scraping by with 31% and 30% exact hits. Sure, the CNN looked good on broader tolerance metrics, but that is because of some averaging trick in regression that hides how it totally flops at picking out specific grades. All told, when you are stuck with under 2,000 examples and lopsided classes, baking in real coin-expert knowledge through feature design beats out those inscrutable, all-in-one deep learning setups. This rings true for other niche quality checks where data's thin and know-how matters more than raw compute.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CV"], "url": "http://arxiv.org/abs/2512.04464", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:44.637803", "embedding_text": "Paper Title: Feature Engineering vs. Deep Learning for Automated Coin Grading: A Comparative Study on Saint-Gaudens Double Eagles\nResearch Topic: artificial intelligence\nAuthors: Tanmay Dogra, Eric Ngo, Mohammad Alam, Jean-Paul Talavera, Asim Dahal\nAbstract: We challenge the common belief that deep learning always trumps older techniques, using the example of grading Saint-Gaudens Double Eagle gold coins automatically. In our work, we put a feature-based Artificial Neural Network built around 192 custom features pulled from Sobel edge detection and HSV color analysis up against a hybrid Convolutional Neural Network that blends in EfficientNetV2, plus a straightforward Support Vector Machine as the control. Testing 1,785 coins graded by experts, the ANN nailed 86% exact matches and hit 98% when allowing a 3-grade leeway. On the flip side, CNN and SVM mostly just guessed the most common grade, scraping by with 31% and 30% exact hits. Sure, the CNN looked good on broader tolerance metrics, but that is because of some averaging trick in regression that hides how it totally flops at picking out specific grades. All told, when you are stuck with under 2,000 examples and lopsided classes, baking in real coin-expert knowledge through feature design beats out those inscrutable, all-in-one deep learning setups. This rings true for other niche quality checks where data's thin and know-how matters more than raw compute.\nCategories: cs.LG, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04464\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1172, "total_text_length": 1289, "word_count": 202, "sentence_count": 9}, "basic_keywords": ["common", "deep", "learning", "coins", "feature", "neural", "network", "exact"], "domain_keywords": [], "technical_terms": ["Support Vector Machine", "Sobel", "Artificial Neural Network", "Gaudens Double Eagle", "Convolutional Neural Network", "Testing", "Saint", "Sure"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04463", "topic": "artificial intelligence", "title": "MARL Warehouse Robots", "authors": ["Price Allman", "Lian Thang", "Dre Simmons", "Salmon Riaz"], "first_author": "Price Allman", "abstract": "We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.RO"], "url": "http://arxiv.org/abs/2512.04463", "comments": "6 pages, 4 tables. Project documentation: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: MARL Warehouse Robots\nResearch Topic: artificial intelligence\nAuthors: Price Allman, Lian Thang, Dre Simmons, Salmon Riaz\nAbstract: We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/\nCategories: cs.AI, cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04463\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 21, "abstract_length": 827, "total_text_length": 849, "word_count": 104, "sentence_count": 11}, "basic_keywords": ["marl", "warehouse", "qmix", "learning", "ippo", "unity", "achieving", "steps"], "domain_keywords": [], "technical_terms": ["Agents", "Warehouse", "While", "Robotic Warehouse", "Robots", "Code", "Unity"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04456", "topic": "artificial intelligence", "title": "GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis", "authors": ["Changjin Kim", "HyeokJun Lee", "YoungJoon Yoo"], "first_author": "Changjin Kim", "abstract": "Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04456", "comments": "AAAI2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis\nResearch Topic: artificial intelligence\nAuthors: Changjin Kim, HyeokJun Lee, YoungJoon Yoo\nAbstract: Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04456\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1477, "total_text_length": 1549, "word_count": 199, "sentence_count": 11}, "basic_keywords": ["noise", "noisy", "guidnoise", "training", "image", "data", "models", "clean"], "domain_keywords": [], "technical_terms": ["However", "Additionally", "Recent", "Single", "Pair Guided Diffusion"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04453", "topic": "artificial intelligence", "title": "Open-Ended Goal Inference through Actions and Language for Human-Robot Collaboration", "authors": ["Debasmita Ghose", "Oz Gitelson", "Marynel Vazquez", "Brian Scassellati"], "first_author": "Debasmita Ghose", "abstract": "To collaborate with humans, robots must infer goals that are often ambiguous, difficult to articulate, or not drawn from a fixed set. Prior approaches restrict inference to a predefined goal set, rely only on observed actions, or depend exclusively on explicit instructions, making them brittle in real-world interactions. We present BALI (Bidirectional Action-Language Inference) for goal prediction, a method that integrates natural language preferences with observed human actions in a receding-horizon planning tree. BALI combines language and action cues from the human, asks clarifying questions only when the expected information gain from the answer outweighs the cost of interruption, and selects supportive actions that align with inferred goals. We evaluate the approach in collaborative cooking tasks, where goals may be novel to the robot and unbounded. Compared to baselines, BALI yields more stable goal predictions and significantly fewer mistakes.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO", "cs.AI"], "url": "http://arxiv.org/abs/2512.04453", "comments": "Accepted to ACM/IEEE International Conference on Human-Robot Interaction, 2026 (HRI 2026), 10 pages, 4 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: Open-Ended Goal Inference through Actions and Language for Human-Robot Collaboration\nResearch Topic: artificial intelligence\nAuthors: Debasmita Ghose, Oz Gitelson, Marynel Vazquez, Brian Scassellati\nAbstract: To collaborate with humans, robots must infer goals that are often ambiguous, difficult to articulate, or not drawn from a fixed set. Prior approaches restrict inference to a predefined goal set, rely only on observed actions, or depend exclusively on explicit instructions, making them brittle in real-world interactions. We present BALI (Bidirectional Action-Language Inference) for goal prediction, a method that integrates natural language preferences with observed human actions in a receding-horizon planning tree. BALI combines language and action cues from the human, asks clarifying questions only when the expected information gain from the answer outweighs the cost of interruption, and selects supportive actions that align with inferred goals. We evaluate the approach in collaborative cooking tasks, where goals may be novel to the robot and unbounded. Compared to baselines, BALI yields more stable goal predictions and significantly fewer mistakes.\nCategories: cs.RO, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04453\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 964, "total_text_length": 1049, "word_count": 149, "sentence_count": 7}, "basic_keywords": ["goals", "goal", "actions", "bali", "language", "inference", "only", "observed"], "domain_keywords": [], "technical_terms": ["Compared", "Language Inference", "Prior", "Bidirectional Action"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04452", "topic": "artificial intelligence", "title": "NORi: An ML-Augmented Ocean Boundary Layer Parameterization", "authors": ["Xin Kai Lee", "Ali Ramadhan", "Andre Souza", "Gregory LeClaire Wagner", "Simone Silvestri", "John Marshall", "Raffaele Ferrari"], "first_author": "Xin Kai Lee", "abstract": "NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an \"a posteriori\" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.ao-ph", "cs.AI", "cs.LG", "physics.comp-ph", "physics.flu-dyn"], "url": "http://arxiv.org/abs/2512.04452", "comments": "48 pages, 16 figures, submitted to Journal of Advances in Modeling Earth Systems (JAMES)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: NORi: An ML-Augmented Ocean Boundary Layer Parameterization\nResearch Topic: artificial intelligence\nAuthors: Xin Kai Lee, Ali Ramadhan, Andre Souza, Gregory LeClaire Wagner, Simone Silvestri, John Marshall, Raffaele Ferrari\nAbstract: NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an \"a posteriori\" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.\nCategories: physics.ao-ph, cs.AI, cs.LG, physics.comp-ph, physics.flu-dyn\nPublication Date: 2025-12-04\nPaper ID: 2512.04452\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 59, "abstract_length": 1625, "total_text_length": 1685, "word_count": 229, "sentence_count": 9}, "basic_keywords": ["nori", "parameterization", "neural", "closure", "trained", "time", "boundary", "layer"], "domain_keywords": [], "technical_terms": ["Richardson"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04445", "topic": "artificial intelligence", "title": "Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration", "authors": ["Yanbin Zhang", "Hanhui Ye", "Yue Bai", "Qiming Zhang", "Liao Xiang", "Wu Mianzhi", "Renjun Hu"], "first_author": "Yanbin Zhang", "abstract": "Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SE", "cs.AI"], "url": "http://arxiv.org/abs/2512.04445", "comments": "9 pages, 3 figures, accepted by AAAI-2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration\nResearch Topic: artificial intelligence\nAuthors: Yanbin Zhang, Hanhui Ye, Yue Bai, Qiming Zhang, Liao Xiang, Wu Mianzhi, Renjun Hu\nAbstract: Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW\nCategories: cs.SE, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04445\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1376, "total_text_length": 1472, "word_count": 187, "sentence_count": 12}, "basic_keywords": ["autodw", "document", "instructions", "tasks", "session", "level", "workflows", "execution"], "domain_keywords": [], "technical_terms": ["These", "Workflow", "While", "Code", "Moreover"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04442", "topic": "artificial intelligence", "title": "TaskEval: Synthesised Evaluation for Foundation-Model Tasks", "authors": ["Dilani Widanapathiranage", "Scott Barnett", "Stefanus Kurniawan", "Wannita Takerngsaksiri"], "first_author": "Dilani Widanapathiranage", "abstract": "Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as . Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\\% and 90\\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.SE"], "url": "http://arxiv.org/abs/2512.04442", "comments": "5 pages, 3 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: TaskEval: Synthesised Evaluation for Foundation-Model Tasks\nResearch Topic: artificial intelligence\nAuthors: Dilani Widanapathiranage, Scott Barnett, Stefanus Kurniawan, Wannita Takerngsaksiri\nAbstract: Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \\textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \\toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\\% and 90\\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.\nCategories: cs.AI, cs.SE\nPublication Date: 2025-12-04\nPaper ID: 2512.04442\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 59, "abstract_length": 1305, "total_text_length": 1365, "word_count": 210, "sentence_count": 11}, "basic_keywords": ["task", "specific", "tasks", "approach", "application", "evaluation", "methods", "eval"], "domain_keywords": [], "technical_terms": ["However", "Prior", "Hallucinations", "Foundation", "Understanding"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04425", "topic": "artificial intelligence", "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models", "authors": ["Manar Alnaasan", "Md Selim Sarowar", "Sungho Kim"], "first_author": "Manar Alnaasan", "abstract": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04425", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models\nResearch Topic: artificial intelligence\nAuthors: Manar Alnaasan, Md Selim Sarowar, Sungho Kim\nAbstract: Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04425\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 103, "abstract_length": 1664, "total_text_length": 1768, "word_count": 218, "sentence_count": 13}, "basic_keywords": ["gait", "multimodal", "language", "visual", "analysis", "parkinsons", "disease", "single"], "domain_keywords": ["feature extraction"], "technical_terms": ["Large Language Model", "Cross", "Code", "Parkinsons", "Global Extraction", "Scale Local", "Depth", "Spatial Neck Fusion"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04419", "topic": "artificial intelligence", "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions", "authors": ["Weiwei Wang", "Weijie Zou", "Jiyong Min"], "first_author": "Weiwei Wang", "abstract": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks. We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects. Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases. The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04419", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions\nResearch Topic: artificial intelligence\nAuthors: Weiwei Wang, Weijie Zou, Jiyong Min\nAbstract: The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.   We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.   Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.   The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04419\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1723, "total_text_length": 1813, "word_count": 222, "sentence_count": 8}, "basic_keywords": ["repetition", "solutions", "three", "production", "comprehensive", "analysis", "problem", "models"], "domain_keywords": [], "technical_terms": ["Markov", "Large Language Models", "Beam Search", "Through", "Direct Preference Optimization", "True"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04416", "topic": "artificial intelligence", "title": "GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows", "authors": ["Zhou Liu", "Zhaoyang Han", "Guochen Yan", "Hao Liang", "Bohan Zeng", "Xing Chen", "Yuanfeng Song", "Wentao Zhang"], "first_author": "Zhou Liu", "abstract": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel \"reversed-objective\" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.SE"], "url": "http://arxiv.org/abs/2512.04416", "comments": "Equal contribution: Zhou Liu and Zhaoyang Han. Corresponding authors: Yuanfeng Song and Wentao Zhang", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.153772", "embedding_text": "Paper Title: GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows\nResearch Topic: artificial intelligence\nAuthors: Zhou Liu, Zhaoyang Han, Guochen Yan, Hao Liang, Bohan Zeng, Xing Chen, Yuanfeng Song, Wentao Zhang\nAbstract: Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel \"reversed-objective\" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.\nCategories: cs.AI, cs.SE\nPublication Date: 2025-12-04\nPaper ID: 2512.04416\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1451, "total_text_length": 1526, "word_count": 195, "sentence_count": 12}, "basic_keywords": ["data", "governance", "govbench", "quality", "models", "level", "tasks", "complex"], "domain_keywords": [], "technical_terms": ["However", "Consequently", "Evaluator", "Planner", "Recently", "Data", "Average Task Score", "Experimental"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04413", "topic": "artificial intelligence", "title": "Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection", "authors": ["Xiangyi Gao", "Danpei Zhao", "Bo Yuan", "Wentao Li"], "first_author": "Xiangyi Gao", "abstract": "Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04413", "comments": "12 pages, 8 figures, 11 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection\nResearch Topic: artificial intelligence\nAuthors: Xiangyi Gao, Danpei Zhao, Bo Yuan, Wentao Li\nAbstract: Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04413\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1631, "total_text_length": 1712, "word_count": 217, "sentence_count": 15}, "basic_keywords": ["distillation", "knowledge", "detection", "method", "remote", "sensing", "object", "rsis"], "domain_keywords": ["object detection"], "technical_terms": ["Dual", "However", "Extensive", "Faster", "Secondly", "Independent Scale Weight", "Stream Spectral Decoupling Distillation", "Density"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04408", "topic": "artificial intelligence", "title": "Executable Governance for AI: Translating Policies into Rules Using LLMs", "authors": ["Gautam Varma Datla", "Anudeep Vurity", "Tejaswani Dash", "Tazeem Ahmad", "Mohd Adnan", "Saima Rafi"], "first_author": "Gautam Varma Datla", "abstract": "AI policy guidance is predominantly written as prose, which practitioners must first convert into executable rules before frameworks can evaluate or enforce them. This manual step is slow, error-prone, difficult to scale, and often delays the use of safeguards in real-world deployments. To address this gap, we present Policy-to-Tests (P2T), a framework that converts natural-language policy documents into normalized, machine-readable rules. The framework comprises a pipeline and a compact domain-specific language (DSL) that encodes hazards, scope, conditions, exceptions, and required evidence, yielding a canonical representation of extracted rules. To test the framework beyond a single policy, we apply it across general frameworks, sector guidance, and enterprise standards, extracting obligation-bearing clauses and converting them into executable rules. These AI-generated rules closely match strong human baselines on span-level and rule-level metrics, with robust inter-annotator agreement on the gold set. To evaluate downstream behavioral and safety impact, we add HIPAA-derived safeguards to a generative agent and compare it with an otherwise identical agent without guardrails. An LLM-based judge, aligned with gold-standard criteria, measures violation rates and robustness to obfuscated and compositional prompts. Detailed results are provided in the appendix. We release the codebase, DSL, prompts, and rule sets as open-source resources to enable reproducible evaluation.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04408", "comments": "Accepted to AAAI-26 AI Governance Workshop (in-person presentation); 10 pages, 5 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Executable Governance for AI: Translating Policies into Rules Using LLMs\nResearch Topic: artificial intelligence\nAuthors: Gautam Varma Datla, Anudeep Vurity, Tejaswani Dash, Tazeem Ahmad, Mohd Adnan, Saima Rafi\nAbstract: AI policy guidance is predominantly written as prose, which practitioners must first convert into executable rules before frameworks can evaluate or enforce them. This manual step is slow, error-prone, difficult to scale, and often delays the use of safeguards in real-world deployments. To address this gap, we present Policy-to-Tests (P2T), a framework that converts natural-language policy documents into normalized, machine-readable rules. The framework comprises a pipeline and a compact domain-specific language (DSL) that encodes hazards, scope, conditions, exceptions, and required evidence, yielding a canonical representation of extracted rules. To test the framework beyond a single policy, we apply it across general frameworks, sector guidance, and enterprise standards, extracting obligation-bearing clauses and converting them into executable rules. These AI-generated rules closely match strong human baselines on span-level and rule-level metrics, with robust inter-annotator agreement on the gold set. To evaluate downstream behavioral and safety impact, we add HIPAA-derived safeguards to a generative agent and compare it with an otherwise identical agent without guardrails. An LLM-based judge, aligned with gold-standard criteria, measures violation rates and robustness to obfuscated and compositional prompts. Detailed results are provided in the appendix. We release the codebase, DSL, prompts, and rule sets as open-source resources to enable reproducible evaluation.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04408\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1493, "total_text_length": 1566, "word_count": 212, "sentence_count": 11}, "basic_keywords": ["rules", "policy", "into", "framework", "guidance", "executable", "frameworks", "evaluate"], "domain_keywords": [], "technical_terms": ["These", "Tests", "Detailed", "Policy"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04405", "topic": "artificial intelligence", "title": "Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm", "authors": ["Chenyuan Feng", "Anbang Zhang", "Geyong Min", "Yongming Huang", "Tony Q. S. Quek", "Xiaohu You"], "first_author": "Chenyuan Feng", "abstract": "The evolution toward sixth-generation wireless systems positions intelligence as a native network capability, fundamentally transforming the design of radio access networks (RANs). Within this vision, Semantic-native communication and agentic intelligence are expected to play central roles. SemCom departs from bit-level fidelity and instead emphasizes task-oriented meaning exchange, enabling compact SC and introducing new performance measures such as semantic fidelity and task success rate. Agentic intelligence endows distributed RAN entities with goal-driven autonomy, reasoning, planning, and multi-agent collaboration, increasingly supported by foundation models and knowledge graphs. In this work, we first introduce the conceptual foundations of SemCom and agentic networking, and discuss why existing AI-driven O-RAN solutions remain largely bit-centric and task-siloed. We then present a unified taxonomy that organizes recent research along three axes: i) semantic abstraction level (symbol/feature/intent/knowledge), ii) agent autonomy and coordination granularity (single-, multi-, and hierarchical-agent), and iii) RAN control placement across PHY/MAC, near-real-time RIC, and non-real-time RIC. Based on this taxonomy, we systematically introduce enabling technologies including task-oriented semantic encoders/decoders, multi-agent reinforcement learning, foundation-model-assisted RAN agents, and knowledge-graph-based reasoning for cross-layer awareness. Representative 6G use cases, such as immersive XR, vehicular V2X, and industrial digital twins, are analyzed to illustrate the semantic-agentic convergence in practice. Finally, we identify open challenges in semantic representation standardization, scalable trustworthy agent coordination, O-RAN interoperability, and energy-efficient AI deployment, and outline research directions toward operational semantic-agentic AI-RAN.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SP", "cs.AI"], "url": "http://arxiv.org/abs/2512.04405", "comments": "submitted to Digital Communications and Networks", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm\nResearch Topic: artificial intelligence\nAuthors: Chenyuan Feng, Anbang Zhang, Geyong Min, Yongming Huang, Tony Q. S. Quek, Xiaohu You\nAbstract: The evolution toward sixth-generation wireless systems positions intelligence as a native network capability, fundamentally transforming the design of radio access networks (RANs). Within this vision, Semantic-native communication and agentic intelligence are expected to play central roles. SemCom departs from bit-level fidelity and instead emphasizes task-oriented meaning exchange, enabling compact SC and introducing new performance measures such as semantic fidelity and task success rate. Agentic intelligence endows distributed RAN entities with goal-driven autonomy, reasoning, planning, and multi-agent collaboration, increasingly supported by foundation models and knowledge graphs. In this work, we first introduce the conceptual foundations of SemCom and agentic networking, and discuss why existing AI-driven O-RAN solutions remain largely bit-centric and task-siloed. We then present a unified taxonomy that organizes recent research along three axes: i) semantic abstraction level (symbol/feature/intent/knowledge), ii) agent autonomy and coordination granularity (single-, multi-, and hierarchical-agent), and iii) RAN control placement across PHY/MAC, near-real-time RIC, and non-real-time RIC. Based on this taxonomy, we systematically introduce enabling technologies including task-oriented semantic encoders/decoders, multi-agent reinforcement learning, foundation-model-assisted RAN agents, and knowledge-graph-based reasoning for cross-layer awareness. Representative 6G use cases, such as immersive XR, vehicular V2X, and industrial digital twins, are analyzed to illustrate the semantic-agentic convergence in practice. Finally, we identify open challenges in semantic representation standardization, scalable trustworthy agent coordination, O-RAN interoperability, and energy-efficient AI deployment, and outline research directions toward operational semantic-agentic AI-RAN.\nCategories: eess.SP, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04405\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1902, "total_text_length": 1989, "word_count": 237, "sentence_count": 10}, "basic_keywords": ["semantic", "agentic", "agent", "task", "intelligence", "multi", "knowledge", "toward"], "domain_keywords": [], "technical_terms": ["Based", "Within", "Finally", "Agentic", "Representative", "Semantic"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04391", "topic": "artificial intelligence", "title": "Adversarial Limits of Quantum Certification: When Eve Defeats Detection", "authors": ["Davut Emre Tasar"], "first_author": "Davut Emre Tasar", "abstract": "Security of quantum key distribution (QKD) relies on certifying that observed correlations arise from genuine quantum entanglement rather than eavesdropper manipulation. Theoretical security proofs assume idealized conditions, practical certification must contend with adaptive adversaries who optimize their attack strategies against detection systems. Established fundamental adversarial limits for quantum certification using Eve GAN, a generative adversarial network trained to produce classical correlations indistinguishable from quantum. Our central finding: when Eve interpolates her classical correlations with quantum data at mixing parameter, all tested detection methods achieve ROC AUC = 0.50, equivalent to random guessing. This means an eavesdropper needs only 5% classical admixture to completely evade detection. Critically, we discover that same distribution calibration a common practice in prior certification studies inflates detection performance by 44 percentage points compared to proper cross distribution evaluation, revealing a systematic flaw that may have led to overestimated security claims. Analysis of Popescu Rohrlich (PR Box) regime identifies a sharp phase transition at CHSH S = 2.05: below this value, no statistical method distinguishes classical from quantum correlations; above it, detection probability increases monotonically. Hardware validation on IBM Quantum demonstrates that Eve-GAN achieves CHSH = 2.736, remarkably exceeding real quantum hardware performance (CHSH = 2.691), illustrating that classical adversaries can outperform noisy quantum systems on standard certification metrics. These results have immediate implications for QKD security: adversaries maintaining 95% quantum fidelity evade all tested detection methods. We provide corrected methodology using cross-distribution calibration and recommend mandatory adversarial testing for quantum security claims.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cs.AI"], "url": "http://arxiv.org/abs/2512.04391", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Adversarial Limits of Quantum Certification: When Eve Defeats Detection\nResearch Topic: artificial intelligence\nAuthors: Davut Emre Tasar\nAbstract: Security of quantum key distribution (QKD) relies on certifying that observed correlations arise from genuine quantum entanglement rather than eavesdropper manipulation. Theoretical security proofs assume idealized conditions, practical certification must contend with adaptive adversaries who optimize their attack strategies against detection systems. Established fundamental adversarial limits for quantum certification using Eve GAN, a generative adversarial network trained to produce classical correlations indistinguishable from quantum. Our central finding: when Eve interpolates her classical correlations with quantum data at mixing parameter, all tested detection methods achieve ROC AUC = 0.50, equivalent to random guessing. This means an eavesdropper needs only 5% classical admixture to completely evade detection. Critically, we discover that same distribution calibration a common practice in prior certification studies inflates detection performance by 44 percentage points compared to proper cross distribution evaluation, revealing a systematic flaw that may have led to overestimated security claims. Analysis of Popescu Rohrlich (PR Box) regime identifies a sharp phase transition at CHSH S = 2.05: below this value, no statistical method distinguishes classical from quantum correlations; above it, detection probability increases monotonically. Hardware validation on IBM Quantum demonstrates that Eve-GAN achieves CHSH = 2.736, remarkably exceeding real quantum hardware performance (CHSH = 2.691), illustrating that classical adversaries can outperform noisy quantum systems on standard certification metrics. These results have immediate implications for QKD security: adversaries maintaining 95% quantum fidelity evade all tested detection methods. We provide corrected methodology using cross-distribution calibration and recommend mandatory adversarial testing for quantum security claims.\nCategories: quant-ph, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04391\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1920, "total_text_length": 1992, "word_count": 255, "sentence_count": 15}, "basic_keywords": ["quantum", "detection", "security", "classical", "distribution", "correlations", "certification", "adversaries"], "domain_keywords": [], "technical_terms": ["These", "Quantum", "Popescu Rohrlich", "Analysis", "Established", "Critically", "Hardware", "Security"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04390", "topic": "artificial intelligence", "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring", "authors": ["Geunhyuk Youk", "Jihyong Oh", "Munchurl Kim"], "first_author": "Geunhyuk Youk", "abstract": "Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04390", "comments": "20 pages, 15 figures. Project Page: https://kaist-viclab.github.io/fmanetpp_site/", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring\nResearch Topic: artificial intelligence\nAuthors: Geunhyuk Youk, Jihyong Oh, Munchurl Kim\nAbstract: Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04390\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1376, "total_text_length": 1469, "word_count": 185, "sentence_count": 8}, "basic_keywords": ["exposure", "motion", "aware", "restoration", "real", "world", "video", "coupled"], "domain_keywords": [], "technical_terms": ["Within", "Bidirectional Propagation", "Real", "Flow", "Exposure Time", "Hierarchical Refinement", "Guided Dynamic Filtering", "Trained"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04386", "topic": "artificial intelligence", "title": "MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation", "authors": ["Zhou Yang", "Shunyan Luo", "Jiazhen Zhu", "Fang Jin"], "first_author": "Zhou Yang", "abstract": "Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04386", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation\nResearch Topic: artificial intelligence\nAuthors: Zhou Yang, Shunyan Luo, Jiazhen Zhu, Fang Jin\nAbstract: Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04386\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1010, "total_text_length": 1080, "word_count": 141, "sentence_count": 7}, "basic_keywords": ["mase", "saliency", "model", "methods", "word", "agnostic", "text", "based"], "domain_keywords": [], "technical_terms": ["Addressing", "Traditional", "Model", "Natural Language Processing", "Saliency Estimation", "Deep", "Delta Accuracy", "Normalized Linear Gaussian Perturbations"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04385", "topic": "artificial intelligence", "title": "STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting", "authors": ["Nan Zhou", "Weijie Hong", "Huandong Wang", "Jianfeng Zheng", "Qiuhua Wang", "Yali Song", "Xiao-Ping Zhang", "Yong Li", "Xinlei Chen"], "first_author": "Nan Zhou", "abstract": "Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AI", "cs.CV"], "url": "http://arxiv.org/abs/2512.04385", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting\nResearch Topic: artificial intelligence\nAuthors: Nan Zhou, Weijie Hong, Huandong Wang, Jianfeng Zheng, Qiuhua Wang, Yali Song, Xiao-Ping Zhang, Yong Li, Xinlei Chen\nAbstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.\nCategories: cs.LG, cs.AI, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04385\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 106, "abstract_length": 1518, "total_text_length": 1625, "word_count": 210, "sentence_count": 12}, "basic_keywords": ["pollution", "data", "diffusion", "spatio", "temporal", "step", "diff", "model"], "domain_keywords": [], "technical_terms": ["Compared", "However", "Diff", "Spatio", "Fine", "Deploying", "Informed Diffusion Models", "Through"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04380", "topic": "artificial intelligence", "title": "Vision and Causal Learning Based Channel Estimation for THz Communications", "authors": ["Kitae Kim", "Yan Kyaw Tun", "Md. Shirajum Munir", "Chirsto Kurisummoottil Thomas", "Walid Saad", "Choong Seon Hong"], "first_author": "Kitae Kim", "abstract": "The use of terahertz (THz) communications with massive multiple input multiple output (MIMO) systems in 6G can potentially provide high data rates and low latency communications. However, accurate channel estimation in THz frequencies presents significant challenges due to factors such as high propagation losses, sensitivity to environmental obstructions, and strong atmospheric absorption. These challenges are particularly pronounced in urban environments, where traditional channel estimation methods often fail to deliver reliable results, particularly in complex non-line-of-sight (NLoS) scenarios. This paper introduces a novel vision-based channel estimation technique that integrates causal reasoning into urban THz communication systems. The proposed method combines computer vision algorithms with variational causal dynamics (VCD) to analyze real-time images of the urban environment, allowing for a deeper understanding of the physical factors that influence THz signal propagation. By capturing the complex, dynamic interactions between physical objects (such as buildings, trees, and vehicles) and the transmitted signals, the model can predict the channel with up to twice the accuracy of conventional methods. This model improves estimation accuracy and demonstrates superior generalization performance. Hence, it can provide reliable predictions even in previously unseen urban environments. The effectiveness of the proposed method is particularly evident in NLoS conditions, where it significantly outperforms traditional methods such as by accounting for indirect signal paths, such as reflections and diffractions. Simulation results confirm that the proposed vision-based approach surpasses conventional artificial intelligence (AI)-based estimation techniques in accuracy and robustness, showing a substantial improvement across various dynamic urban scenarios.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.NI"], "url": "http://arxiv.org/abs/2512.04380", "comments": "Submitted to IEEE Transactions on Mobile Computing on Mar. 20, 2025 (18 pages, 9 figures)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Vision and Causal Learning Based Channel Estimation for THz Communications\nResearch Topic: artificial intelligence\nAuthors: Kitae Kim, Yan Kyaw Tun, Md. Shirajum Munir, Chirsto Kurisummoottil Thomas, Walid Saad, Choong Seon Hong\nAbstract: The use of terahertz (THz) communications with massive multiple input multiple output (MIMO) systems in 6G can potentially provide high data rates and low latency communications. However, accurate channel estimation in THz frequencies presents significant challenges due to factors such as high propagation losses, sensitivity to environmental obstructions, and strong atmospheric absorption. These challenges are particularly pronounced in urban environments, where traditional channel estimation methods often fail to deliver reliable results, particularly in complex non-line-of-sight (NLoS) scenarios. This paper introduces a novel vision-based channel estimation technique that integrates causal reasoning into urban THz communication systems. The proposed method combines computer vision algorithms with variational causal dynamics (VCD) to analyze real-time images of the urban environment, allowing for a deeper understanding of the physical factors that influence THz signal propagation. By capturing the complex, dynamic interactions between physical objects (such as buildings, trees, and vehicles) and the transmitted signals, the model can predict the channel with up to twice the accuracy of conventional methods. This model improves estimation accuracy and demonstrates superior generalization performance. Hence, it can provide reliable predictions even in previously unseen urban environments. The effectiveness of the proposed method is particularly evident in NLoS conditions, where it significantly outperforms traditional methods such as by accounting for indirect signal paths, such as reflections and diffractions. Simulation results confirm that the proposed vision-based approach surpasses conventional artificial intelligence (AI)-based estimation techniques in accuracy and robustness, showing a substantial improvement across various dynamic urban scenarios.\nCategories: cs.NI\nPublication Date: 2025-12-04\nPaper ID: 2512.04380\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1886, "total_text_length": 1961, "word_count": 254, "sentence_count": 11}, "basic_keywords": ["estimation", "urban", "channel", "particularly", "methods", "vision", "based", "proposed"], "domain_keywords": ["computer vision"], "technical_terms": ["These", "However", "Simulation", "Hence"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04368", "topic": "artificial intelligence", "title": "AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning", "authors": ["Praveen Anugula", "Avdhesh Kumar Bhardwaj", "Navin Chhibber", "Rohit Tewari", "Sunil Khemka", "Piyush Ranjan"], "first_author": "Praveen Anugula", "abstract": "Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods. Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PF"], "url": "http://arxiv.org/abs/2512.04368", "comments": "Accepted and Presented at 1st IEEE Uttar Pradesh Section Women in Engineering International Conference on Electrical Electronics and Computer Engineering (UPWIECON 2025) organized by NIELIT Dehradun held during 30th 31st October 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Praveen Anugula, Avdhesh Kumar Bhardwaj, Navin Chhibber, Rohit Tewari, Sunil Khemka, Piyush Ranjan\nAbstract: Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods.   Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation\nCategories: cs.CR, cs.AI, cs.LG, cs.PF\nPublication Date: 2025-12-04\nPaper ID: 2512.04368\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 103, "abstract_length": 1462, "total_text_length": 1566, "word_count": 201, "sentence_count": 8}, "basic_keywords": ["security", "devsecops", "time", "environment", "based", "autoguard", "learning", "self"], "domain_keywords": [], "technical_terms": ["Automated Threat Mitigation", "Contemporary", "Existing", "Continuous Deployment", "Continuous Integration", "Keywords", "Reinforcement Learning", "Healing Security"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04367", "topic": "artificial intelligence", "title": "AgentBay: A Hybrid Interaction Sandbox for Seamless Human-AI Intervention in Agentic Systems", "authors": ["Yun Piao", "Hongbo Min", "Hang Su", "Leilei Zhang", "Lei Wang", "Yue Yin", "Xiao Wu", "Zhejing Xu", "Liwei Qu", "Hang Li", "Xinxin Zeng", "Wei Tian", "Fei Yu", "Xiaowei Li", "Jiayi Jiang", "Tongxu Liu", "Hao Tian", "Yufei Que", "Xiaobing Tu", "Bing Suo", "Yuebing Li", "Xiangting Chen", "Zeen Zhao", "Jiaming Tang", "Wei Huang", "Xuguang Li", "Jing Zhao", "Jin Li", "Jie Shen", "Jinkui Ren", "Xiantao Zhang"], "first_author": "Yun Piao", "abstract": "The rapid advancement of Large Language Models (LLMs) is catalyzing a shift towards autonomous AI Agents capable of executing complex, multi-step tasks. However, these agents remain brittle when faced with real-world exceptions, making Human-in-the-Loop (HITL) supervision essential for mission-critical applications. In this paper, we present AgentBay, a novel sandbox service designed from the ground up for hybrid interaction. AgentBay provides secure, isolated execution environments spanning Windows, Linux, Android, Web Browsers, and Code interpreters. Its core contribution is a unified session accessible via a hybrid control interface: An AI agent can interact programmatically via mainstream interfaces (MCP, Open Source SDK), while a human operator can, at any moment, seamlessly take over full manual control. This seamless intervention is enabled by Adaptive Streaming Protocol (ASP). Unlike traditional VNC/RDP, ASP is specifically engineered for this hybrid use case, delivering an ultra-low-latency, smoother user experience that remains resilient even in weak network environments. It achieves this by dynamically blending command-based and video-based streaming, adapting its encoding strategy based on network conditions and the current controller (AI or human). Our evaluation demonstrates strong results in security, performance, and task completion rates. In a benchmark of complex tasks, the AgentBay (Agent + Human) model achieved more than 48% success rate improvement. Furthermore, our ASP protocol reduces bandwidth consumption by up to 50% compared to standard RDP, and in end-to-end latency with around 5% reduction, especially under poor network conditions. We posit that AgentBay provides a foundational primitive for building the next generation of reliable, human-supervised autonomous systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04367", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: AgentBay: A Hybrid Interaction Sandbox for Seamless Human-AI Intervention in Agentic Systems\nResearch Topic: artificial intelligence\nAuthors: Yun Piao, Hongbo Min, Hang Su, Leilei Zhang, Lei Wang, Yue Yin, Xiao Wu, Zhejing Xu, Liwei Qu, Hang Li, Xinxin Zeng, Wei Tian, Fei Yu, Xiaowei Li, Jiayi Jiang, Tongxu Liu, Hao Tian, Yufei Que, Xiaobing Tu, Bing Suo, Yuebing Li, Xiangting Chen, Zeen Zhao, Jiaming Tang, Wei Huang, Xuguang Li, Jing Zhao, Jin Li, Jie Shen, Jinkui Ren, Xiantao Zhang\nAbstract: The rapid advancement of Large Language Models (LLMs) is catalyzing a shift towards autonomous AI Agents capable of executing complex, multi-step tasks. However, these agents remain brittle when faced with real-world exceptions, making Human-in-the-Loop (HITL) supervision essential for mission-critical applications. In this paper, we present AgentBay, a novel sandbox service designed from the ground up for hybrid interaction. AgentBay provides secure, isolated execution environments spanning Windows, Linux, Android, Web Browsers, and Code interpreters. Its core contribution is a unified session accessible via a hybrid control interface: An AI agent can interact programmatically via mainstream interfaces (MCP, Open Source SDK), while a human operator can, at any moment, seamlessly take over full manual control. This seamless intervention is enabled by Adaptive Streaming Protocol (ASP). Unlike traditional VNC/RDP, ASP is specifically engineered for this hybrid use case, delivering an ultra-low-latency, smoother user experience that remains resilient even in weak network environments. It achieves this by dynamically blending command-based and video-based streaming, adapting its encoding strategy based on network conditions and the current controller (AI or human). Our evaluation demonstrates strong results in security, performance, and task completion rates. In a benchmark of complex tasks, the AgentBay (Agent + Human) model achieved more than 48% success rate improvement. Furthermore, our ASP protocol reduces bandwidth consumption by up to 50% compared to standard RDP, and in end-to-end latency with around 5% reduction, especially under poor network conditions. We posit that AgentBay provides a foundational primitive for building the next generation of reliable, human-supervised autonomous systems.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04367\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1827, "total_text_length": 1920, "word_count": 263, "sentence_count": 13}, "basic_keywords": ["human", "agentbay", "hybrid", "network", "based", "autonomous", "agents", "complex"], "domain_keywords": [], "technical_terms": ["However", "Agents", "Human", "Adaptive Streaming Protocol", "Agent", "Loop", "Linux", "Furthermore"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04359", "topic": "artificial intelligence", "title": "Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning", "authors": ["Hongye Cao", "Zhixin Bai", "Ziyue Peng", "Boyan Wang", "Tianpei Yang", "Jing Huo", "Yuyao Zhang", "Yang Gao"], "first_author": "Hongye Cao", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04359", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning\nResearch Topic: artificial intelligence\nAuthors: Hongye Cao, Zhixin Bai, Ziyue Peng, Boyan Wang, Tianpei Yang, Jing Huo, Yuyao Zhang, Yang Gao\nAbstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.\nCategories: cs.AI\nPublication Date: 2025-12-04\nPaper ID: 2512.04359\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 82, "abstract_length": 1274, "total_text_length": 1357, "word_count": 174, "sentence_count": 8}, "basic_keywords": ["entropy", "reasoning", "learning", "semantic", "data", "reinforcement", "models", "collapse"], "domain_keywords": [], "technical_terms": ["However", "Experimental", "Reinforcement"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04356", "topic": "artificial intelligence", "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment", "authors": ["Kai-Po Chang", "Wei-Yuan Cheng", "Chi-Pin Huang", "Fu-En Yang", "Yu-Chiang Frank Wang"], "first_author": "Kai-Po Chang", "abstract": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.04356", "comments": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026. Project page: https://kpc0810.github.io/santa/", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment\nResearch Topic: artificial intelligence\nAuthors: Kai-Po Chang, Wei-Yuan Cheng, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang\nAbstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.\nCategories: cs.CV, cs.AI, cs.CL, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04356\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 103, "abstract_length": 1247, "total_text_length": 1351, "word_count": 171, "sentence_count": 8}, "basic_keywords": ["hallucinations", "visual", "object", "action", "santa", "captions", "videos", "hallucination"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "While", "Augmented Contrastive Alignment", "Furthermore", "Recent", "Self"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04355", "topic": "artificial intelligence", "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Konstantinos Parasyris", "Harshitha Menon", "Niranjan Hasabnis", "Kirk W. Cameron", "Gal Oren"], "first_author": "Gregory Bolet", "abstract": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.DC", "cs.AI", "cs.PF"], "url": "http://arxiv.org/abs/2512.04355", "comments": "13 pages, 6 figures, MLSys 2026 Submission", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity\nResearch Topic: artificial intelligence\nAuthors: Gregory Bolet, Giorgis Georgakoudis, Konstantinos Parasyris, Harshitha Menon, Niranjan Hasabnis, Kirk W. Cameron, Gal Oren\nAbstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench\nCategories: cs.DC, cs.AI, cs.PF\nPublication Date: 2025-12-04\nPaper ID: 2512.04355\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1444, "total_text_length": 1519, "word_count": 198, "sentence_count": 7}, "basic_keywords": ["code", "models", "gpuflopbench", "kernels", "developers", "performance", "hardware", "progress"], "domain_keywords": [], "technical_terms": ["These", "Evaluating", "Computing", "Large Language Models", "Sources", "Modern", "Scientific"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04339", "topic": "artificial intelligence", "title": "A Conceptual Model for AI Adoption in Financial Decision-Making: Addressing the Unique Challenges of Small and Medium-Sized Enterprises", "authors": ["Manh Chien Vu", "Thang Le Dinh", "Manh Chien Vu", "Tran Duc Le", "Thi Lien Huong Nguyen"], "first_author": "Manh Chien Vu", "abstract": "The adoption of artificial intelligence (AI) offers transformative potential for small and medium-sized enterprises (SMEs), particularly in enhancing financial decision-making processes. However, SMEs often face significant barriers to implementing AI technologies, including limited resources, technical expertise, and data management capabilities. This paper presents a conceptual model for the adoption of AI in financial decision-making for SMEs. The proposed model addresses key challenges faced by SMEs, including limited resources, technical expertise, and data management capabilities. The model is structured into layers: data sources, data processing and integration, AI model deployment, decision support and automation, and validation and risk management. By implementing AI incrementally, SMEs can optimize financial forecasting, budgeting, investment strategies, and risk management. This paper highlights the importance of data quality and continuous model validation, providing a practical roadmap for SMEs to integrate AI into their financial operations. The study concludes with implications for SMEs adopting AI-driven financial processes and suggests areas for future research in AI applications for SME finance.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04339", "comments": "The Eighth International Econometric and Financial Conference of Vietnam - ECONVN2025, Ho Chi Minh City, Vietnam, January 13-14-15, 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: A Conceptual Model for AI Adoption in Financial Decision-Making: Addressing the Unique Challenges of Small and Medium-Sized Enterprises\nResearch Topic: artificial intelligence\nAuthors: Manh Chien Vu, Thang Le Dinh, Manh Chien Vu, Tran Duc Le, Thi Lien Huong Nguyen\nAbstract: The adoption of artificial intelligence (AI) offers transformative potential for small and medium-sized enterprises (SMEs), particularly in enhancing financial decision-making processes. However, SMEs often face significant barriers to implementing AI technologies, including limited resources, technical expertise, and data management capabilities. This paper presents a conceptual model for the adoption of AI in financial decision-making for SMEs. The proposed model addresses key challenges faced by SMEs, including limited resources, technical expertise, and data management capabilities. The model is structured into layers: data sources, data processing and integration, AI model deployment, decision support and automation, and validation and risk management. By implementing AI incrementally, SMEs can optimize financial forecasting, budgeting, investment strategies, and risk management. This paper highlights the importance of data quality and continuous model validation, providing a practical roadmap for SMEs to integrate AI into their financial operations. The study concludes with implications for SMEs adopting AI-driven financial processes and suggests areas for future research in AI applications for SME finance.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04339\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 135, "abstract_length": 1232, "total_text_length": 1368, "word_count": 179, "sentence_count": 9}, "basic_keywords": ["smes", "financial", "data", "model", "management", "decision", "adoption", "making"], "domain_keywords": [], "technical_terms": ["However"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04334", "topic": "artificial intelligence", "title": "Human-controllable AI: Meaningful Human Control", "authors": ["Chengke Liu", "Wei Xu"], "first_author": "Chengke Liu", "abstract": "Developing human-controllable artificial intelligence (AI) and achieving meaningful human control (MHC) has become a vital principle to address these challenges, ensuring ethical alignment and effective governance in AI. MHC is also a critical focus in human-centered AI (HCAI) research and application. This chapter systematically examines MHC in AI, articulating its foundational principles and future trajectory. MHC is not simply the right to operate, but the unity of human understanding, intervention, and the traceablity of responsibility in AI decision-making, which requires technological design, AI governance, and humans to play a role together. MHC ensures AI autonomy serves humans without constraining technological progress. The mode of human control needs to match the levels of technology, and human supervision should balance the trust and doubt of AI. For future AI systems, MHC mandates human controllability as a prerequisite, requiring: (1) technical architectures with embedded mechanisms for human control; (2) human-AI interactions optimized for better access to human understanding; and (3) the evolution of AI systems harmonizing intelligence and human controllability. Governance must prioritize HCAI strategies: policies balancing innovation and risk mitigation, human-centered participatory frameworks transcending technical elite dominance, and global promotion of MHC as a universal governance paradigm to safeguard HCAI development. Looking ahead, there is a need to strengthen interdisciplinary research on the controllability of AI systems, enhance ethical and legal awareness among stakeholders, moving beyond simplistic technology design perspectives, focus on the knowledge construction, complexity interpretation, and influencing factors surrounding human control. By fostering MHC, the development of human-controllable AI can be further advanced, delivering HCAI systems.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.HC"], "url": "http://arxiv.org/abs/2512.04334", "comments": "52 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Human-controllable AI: Meaningful Human Control\nResearch Topic: artificial intelligence\nAuthors: Chengke Liu, Wei Xu\nAbstract: Developing human-controllable artificial intelligence (AI) and achieving meaningful human control (MHC) has become a vital principle to address these challenges, ensuring ethical alignment and effective governance in AI. MHC is also a critical focus in human-centered AI (HCAI) research and application. This chapter systematically examines MHC in AI, articulating its foundational principles and future trajectory. MHC is not simply the right to operate, but the unity of human understanding, intervention, and the traceablity of responsibility in AI decision-making, which requires technological design, AI governance, and humans to play a role together. MHC ensures AI autonomy serves humans without constraining technological progress. The mode of human control needs to match the levels of technology, and human supervision should balance the trust and doubt of AI. For future AI systems, MHC mandates human controllability as a prerequisite, requiring: (1) technical architectures with embedded mechanisms for human control; (2) human-AI interactions optimized for better access to human understanding; and (3) the evolution of AI systems harmonizing intelligence and human controllability. Governance must prioritize HCAI strategies: policies balancing innovation and risk mitigation, human-centered participatory frameworks transcending technical elite dominance, and global promotion of MHC as a universal governance paradigm to safeguard HCAI development. Looking ahead, there is a need to strengthen interdisciplinary research on the controllability of AI systems, enhance ethical and legal awareness among stakeholders, moving beyond simplistic technology design perspectives, focus on the knowledge construction, complexity interpretation, and influencing factors surrounding human control. By fostering MHC, the development of human-controllable AI can be further advanced, delivering HCAI systems.\nCategories: cs.HC\nPublication Date: 2025-12-03\nPaper ID: 2512.04334\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 47, "abstract_length": 1912, "total_text_length": 1960, "word_count": 261, "sentence_count": 11}, "basic_keywords": ["human", "control", "governance", "hcai", "systems", "controllability", "controllable", "intelligence"], "domain_keywords": [], "technical_terms": ["Looking", "Developing", "Governance"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04333", "topic": "artificial intelligence", "title": "RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection", "authors": ["Shreyas Shende", "Varsha Narayanan", "Vishal Fenn", "Yiran Huang", "Dincer Goksuluk", "Gaurav Choudhary", "Melih Agraz", "Mengjia Xu"], "first_author": "Shreyas Shende", "abstract": "Early detection of cancer plays a key role in improving survival rates, but identifying reliable biomarkers from RNA-seq data is still a major challenge. The data are high-dimensional, and conventional statistical methods often fail to capture the complex relationships between genes. In this study, we introduce RGE-GCN (Recursive Gene Elimination with Graph Convolutional Networks), a framework that combines feature selection and classification in a single pipeline. Our approach builds a graph from gene expression profiles, uses a Graph Convolutional Network to classify cancer versus normal samples, and applies Integrated Gradients to highlight the most informative genes. By recursively removing less relevant genes, the model converges to a compact set of biomarkers that are both interpretable and predictive. We evaluated RGE-GCN on synthetic data as well as real-world RNA-seq cohorts of lung, kidney, and cervical cancers. Across all datasets, the method consistently achieved higher accuracy and F1-scores than standard tools such as DESeq2, edgeR, and limma-voom. Importantly, the selected genes aligned with well-known cancer pathways including PI3K-AKT, MAPK, SUMOylation, and immune regulation. These results suggest that RGE-GCN shows promise as a generalizable approach for RNA-seq based early cancer detection and biomarker discovery (https://rce-gcn.streamlit.app/ ).", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04333", "comments": "12 pages, 2 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection\nResearch Topic: artificial intelligence\nAuthors: Shreyas Shende, Varsha Narayanan, Vishal Fenn, Yiran Huang, Dincer Goksuluk, Gaurav Choudhary, Melih Agraz, Mengjia Xu\nAbstract: Early detection of cancer plays a key role in improving survival rates, but identifying reliable biomarkers from RNA-seq data is still a major challenge. The data are high-dimensional, and conventional statistical methods often fail to capture the complex relationships between genes. In this study, we introduce RGE-GCN (Recursive Gene Elimination with Graph Convolutional Networks), a framework that combines feature selection and classification in a single pipeline. Our approach builds a graph from gene expression profiles, uses a Graph Convolutional Network to classify cancer versus normal samples, and applies Integrated Gradients to highlight the most informative genes. By recursively removing less relevant genes, the model converges to a compact set of biomarkers that are both interpretable and predictive. We evaluated RGE-GCN on synthetic data as well as real-world RNA-seq cohorts of lung, kidney, and cervical cancers. Across all datasets, the method consistently achieved higher accuracy and F1-scores than standard tools such as DESeq2, edgeR, and limma-voom. Importantly, the selected genes aligned with well-known cancer pathways including PI3K-AKT, MAPK, SUMOylation, and immune regulation. These results suggest that RGE-GCN shows promise as a generalizable approach for RNA-seq based early cancer detection and biomarker discovery (https://rce-gcn.streamlit.app/ ).\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04333\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 110, "abstract_length": 1389, "total_text_length": 1500, "word_count": 207, "sentence_count": 12}, "basic_keywords": ["cancer", "genes", "data", "graph", "early", "detection", "biomarkers", "gene"], "domain_keywords": [], "technical_terms": ["These", "Integrated Gradients", "Across", "Graph Convolutional Network", "Graph Convolutional Networks", "Importantly", "Early", "Recursive Gene Elimination"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04324", "topic": "artificial intelligence", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "authors": ["Fangyu Lei", "Jinxiang Meng", "Yiming Huang", "Junjie Zhao", "Yitong Zhang", "Jianwen Luo", "Xin Zou", "Ruiyi Yang", "Wenbo Shi", "Yan Gao", "Shizhu He", "Zuo Wang", "Qian Liu", "Yang Wang", "Ke Wang", "Jun Zhao", "Kang Liu"], "first_author": "Fangyu Lei", "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04324", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle\nResearch Topic: artificial intelligence\nAuthors: Fangyu Lei, Jinxiang Meng, Yiming Huang, Junjie Zhao, Yitong Zhang, Jianwen Luo, Xin Zou, Ruiyi Yang, Wenbo Shi, Yan Gao, Shizhu He, Zuo Wang, Qian Liu, Yang Wang, Ke Wang, Jun Zhao, Kang Liu\nAbstract: Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04324\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1621, "total_text_length": 1698, "word_count": 217, "sentence_count": 13}, "basic_keywords": ["data", "tasks", "engineering", "analysis", "dacomp", "open", "ended", "enterprise"], "domain_keywords": [], "technical_terms": ["Performance", "Real", "Open", "Data", "Scores", "Engineering"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04323", "topic": "artificial intelligence", "title": "Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks", "authors": ["Biao Chen", "Zhenhua Lei", "Yahui Zhang", "Tongzhi Niu"], "first_author": "Biao Chen", "abstract": "This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "url": "http://arxiv.org/abs/2512.04323", "comments": "17 pages, 8 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks\nResearch Topic: artificial intelligence\nAuthors: Biao Chen, Zhenhua Lei, Yahui Zhang, Tongzhi Niu\nAbstract: This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.\nCategories: cs.CV, cs.AI, cs.CE, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04323\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1699, "total_text_length": 1793, "word_count": 234, "sentence_count": 10}, "basic_keywords": ["network", "displacement", "field", "bayes", "dataset", "real", "information", "levels"], "domain_keywords": [], "technical_terms": ["Bayes", "Additionally", "Furthermore", "Through", "Bayesian", "Digital Image Correlation"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04319", "topic": "artificial intelligence", "title": "MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training", "authors": ["Zixiao Zhao", "Fatemeh H. Fard", "Jie JW Wu"], "first_author": "Zixiao Zhao", "abstract": "The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.SE", "cs.AI"], "url": "http://arxiv.org/abs/2512.04319", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training\nResearch Topic: artificial intelligence\nAuthors: Zixiao Zhao, Fatemeh H. Fard, Jie JW Wu\nAbstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.\nCategories: cs.SE, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04319\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1375, "total_text_length": 1452, "word_count": 211, "sentence_count": 10}, "basic_keywords": ["models", "noise", "tasks", "data", "while", "llms", "mantra", "code"], "domain_keywords": [], "technical_terms": ["While Noise Label Learning", "Adaptive Noise", "However", "Applying", "Large Language Models", "Gaussian Mixture Model", "Software Engineering", "Pretrained Language Models"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04307", "topic": "artificial intelligence", "title": "Evaluating Long-Context Reasoning in LLM-Based WebAgents", "authors": ["Andy Chung", "Yichi Zhang", "Kaixiang Lin", "Aditya Rawal", "Qiaozi Gao", "Joyce Chai"], "first_author": "Andy Chung", "abstract": "As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing personalized and contextually aware assistance. However, the performance of these agents in long context scenarios, particularly for action-taking WebAgents operating in realistic web environments, remains largely unexplored. This paper introduces a benchmark for evaluating long context reasoning capabilities of WebAgents through sequentially dependent subtasks that require retrieval and application of information from extended interaction histories. We develop a novel evaluation framework that simulates multi-session user interactions by injecting irrelevant task trajectories between dependent subtasks, creating contexts ranging from 25,000 to 150,000 tokens. Through extensive evaluation of four popular models, Claude-3.7, GPT-4.1, Llama 4, and o4-mini, we observe a dramatic performance degradation as context length increases, with success rates dropping from 40-50\\% in baseline conditions to less than 10\\% in long context scenarios. Our detailed error analysis reveals that agents primarily fail due to getting stuck in loops and losing track of original task objectives. We further propose an implicit RAG approach that provides modest improvements by generating task-relevant summaries, though fundamental limitations in long context reasoning persist. These findings highlight critical challenges for deploying WebAgents in realistic, long-term user interaction scenarios and provide insights for developing more robust agent architectures capable of maintaining coherent task execution across extended contexts.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04307", "comments": "Accepted NeurIPS 25 LAW Workshop", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Evaluating Long-Context Reasoning in LLM-Based WebAgents\nResearch Topic: artificial intelligence\nAuthors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\nAbstract: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing personalized and contextually aware assistance. However, the performance of these agents in long context scenarios, particularly for action-taking WebAgents operating in realistic web environments, remains largely unexplored. This paper introduces a benchmark for evaluating long context reasoning capabilities of WebAgents through sequentially dependent subtasks that require retrieval and application of information from extended interaction histories. We develop a novel evaluation framework that simulates multi-session user interactions by injecting irrelevant task trajectories between dependent subtasks, creating contexts ranging from 25,000 to 150,000 tokens. Through extensive evaluation of four popular models, Claude-3.7, GPT-4.1, Llama 4, and o4-mini, we observe a dramatic performance degradation as context length increases, with success rates dropping from 40-50\\% in baseline conditions to less than 10\\% in long context scenarios. Our detailed error analysis reveals that agents primarily fail due to getting stuck in loops and losing track of original task objectives. We further propose an implicit RAG approach that provides modest improvements by generating task-relevant summaries, though fundamental limitations in long context reasoning persist. These findings highlight critical challenges for deploying WebAgents in realistic, long-term user interaction scenarios and provide insights for developing more robust agent architectures capable of maintaining coherent task execution across extended contexts.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04307\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 56, "abstract_length": 1726, "total_text_length": 1783, "word_count": 230, "sentence_count": 11}, "basic_keywords": ["long", "context", "task", "agents", "interaction", "scenarios", "webagents", "interactions"], "domain_keywords": [], "technical_terms": ["These", "However", "Llama", "Through", "Claude"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04303", "topic": "artificial intelligence", "title": "Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications", "authors": ["Gasser Elazab", "Maximilian Jansen", "Michael Unterreiner", "Olaf Hellwich"], "first_author": "Gasser Elazab", "abstract": "Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04303", "comments": "Accepted in 3DV 2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications\nResearch Topic: artificial intelligence\nAuthors: Gasser Elazab, Maximilian Jansen, Michael Unterreiner, Olaf Hellwich\nAbstract: Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04303\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1583, "total_text_length": 1685, "word_count": 219, "sentence_count": 10}, "basic_keywords": ["depth", "road", "camera", "geometry", "surface", "monocular", "estimation", "gamma"], "domain_keywords": [], "technical_terms": ["However", "Road Surface Reconstruction Dataset", "Evaluated", "Mono", "Gamma", "Accurate"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04302", "topic": "artificial intelligence", "title": "Towards better dense rewards in Reinforcement Learning Applications", "authors": ["Shuyuan Zhang"], "first_author": "Shuyuan Zhang", "abstract": "Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04302", "comments": "arXiv admin note: substantial text overlap with arXiv:2505.20417", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Towards better dense rewards in Reinforcement Learning Applications\nResearch Topic: artificial intelligence\nAuthors: Shuyuan Zhang\nAbstract: Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04302\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 1433, "total_text_length": 1501, "word_count": 200, "sentence_count": 10}, "basic_keywords": ["reward", "learning", "dense", "rewards", "agents", "task", "reinforcement", "environments"], "domain_keywords": [], "technical_terms": ["However", "Finding", "Dense", "While", "Despite"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04287", "topic": "artificial intelligence", "title": "Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases", "authors": ["Ian Miles", "Mayumi Wakimoto", "Wagner Meira", "Daniela Paula", "Daylene Ticiane", "Bruno Rosa", "Jane Biddulph", "Stelios Georgiou", "Valdir Ermida"], "first_author": "Ian Miles", "abstract": "This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "q-bio.PE"], "url": "http://arxiv.org/abs/2512.04287", "comments": "21 pages, 1 box, 1 figure", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases\nResearch Topic: artificial intelligence\nAuthors: Ian Miles, Mayumi Wakimoto, Wagner Meira, Daniela Paula, Daylene Ticiane, Bruno Rosa, Jane Biddulph, Stelios Georgiou, Valdir Ermida\nAbstract: This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.\nCategories: cs.AI, q-bio.PE\nPublication Date: 2025-12-03\nPaper ID: 2512.04287\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 580, "total_text_length": 661, "word_count": 89, "sentence_count": 5}, "basic_keywords": ["review", "explores", "integration", "artificial", "intelligence", "into", "horizon", "scanning"], "domain_keywords": [], "technical_terms": ["Horizon Scanning", "Foresight", "Public Health", "Artificial Intelligence", "Infectious Diseases"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04284", "topic": "artificial intelligence", "title": "Learning Single-Image Super-Resolution in the JPEG Compressed Domain", "authors": ["Sruthi Srinivasan", "Elham Shakibapour", "Rajy Rawther", "Mehdi Saeedi"], "first_author": "Sruthi Srinivasan", "abstract": "Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04284", "comments": "7 pages, 4 figures, 2 tables, SEEDS Workshop, ICIP 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Learning Single-Image Super-Resolution in the JPEG Compressed Domain\nResearch Topic: artificial intelligence\nAuthors: Sruthi Srinivasan, Elham Shakibapour, Rajy Rawther, Mehdi Saeedi\nAbstract: Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04284\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 936, "total_text_length": 1005, "word_count": 137, "sentence_count": 9}, "basic_keywords": ["data", "loading", "training", "jpeg", "deep", "learning", "models", "while"], "domain_keywords": [], "technical_terms": ["Deep", "While", "Despite"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04277", "topic": "artificial intelligence", "title": "Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order", "authors": ["Prakhar Gupta", "Vaibhav Gupta"], "first_author": "Prakhar Gupta", "abstract": "Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04277", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order\nResearch Topic: artificial intelligence\nAuthors: Prakhar Gupta, Vaibhav Gupta\nAbstract: Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04277\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1185, "total_text_length": 1267, "word_count": 170, "sentence_count": 7}, "basic_keywords": ["order", "post", "solver", "only", "fine", "training", "ordering", "tuned"], "domain_keywords": [], "technical_terms": ["These", "Group Relative Policy Optimization", "Post", "On Sudoku", "Transformer", "Mixed"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04276", "topic": "artificial intelligence", "title": "The Geometry of Benchmarks: A New Path Toward AGI", "authors": ["Przemyslaw Chojecki"], "first_author": "Przemyslaw Chojecki", "abstract": "Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for . Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "cs.LG", "math.ST"], "url": "http://arxiv.org/abs/2512.04276", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: The Geometry of Benchmarks: A New Path Toward AGI\nResearch Topic: artificial intelligence\nAuthors: Przemyslaw Chojecki\nAbstract: Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $κ$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $κ> 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.\nCategories: cs.AI, cs.LG, math.ST\nPublication Date: 2025-12-03\nPaper ID: 2512.04276\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 49, "abstract_length": 1622, "total_text_length": 1672, "word_count": 236, "sentence_count": 9}, "basic_keywords": ["batteries", "space", "benchmarks", "self", "moduli", "performance", "capability", "tool"], "domain_keywords": [], "technical_terms": ["Third", "Scale", "Here", "Generator", "Updater", "Benchmarks", "Second", "Autonomous"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04273", "topic": "artificial intelligence", "title": "Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures", "authors": ["Tyler Slater"], "first_author": "Tyler Slater", "abstract": "As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure \"Architectural Erosion\" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of \"Implementation Laziness,\" where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.SE", "cs.AI"], "url": "http://arxiv.org/abs/2512.04273", "comments": "Under review at the Journal of Systems and Software (Special Issue on Impactful Software Architecture)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures\nResearch Topic: artificial intelligence\nAuthors: Tyler Slater\nAbstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure \"Architectural Erosion\" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of \"Implementation Laziness,\" where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.\nCategories: cs.SE, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04273\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 99, "abstract_length": 1482, "total_text_length": 1582, "word_count": 198, "sentence_count": 11}, "basic_keywords": ["models", "architectural", "open", "weights", "code", "system", "while", "study"], "domain_keywords": [], "technical_terms": ["Implementation Laziness", "Hexagonal Architecture", "While", "Book Lending Microservice", "Logical Lines", "Architectural Erosion", "Code", "Domain"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04268", "topic": "artificial intelligence", "title": "The Initialization Determines Whether In-Context Learning Is Gradient Descent", "authors": ["Shifeng Xie", "Rui Yuan", "Simone Rossi", "Thomas Hannagan"], "first_author": "Shifeng Xie", "abstract": "In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04268", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: The Initialization Determines Whether In-Context Learning Is Gradient Descent\nResearch Topic: artificial intelligence\nAuthors: Shifeng Xie, Rui Yuan, Simone Rossi, Thomas Hannagan\nAbstract: In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04268\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1690, "total_text_length": 1768, "word_count": 247, "sentence_count": 11}, "basic_keywords": ["linear", "multi", "head", "regression", "initial", "attention", "under", "conditions"], "domain_keywords": [], "technical_terms": ["However", "Previous", "Gaussian", "Finally"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04263", "topic": "artificial intelligence", "title": "Polynomiogram: An Integrated Framework for Root Visualization and Generative Art", "authors": ["Hoang Duc Nguyen", "Anh Van Pham", "Hien D. Nguyen"], "first_author": "Hoang Duc Nguyen", "abstract": "This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.SE", "cs.LG"], "url": "http://arxiv.org/abs/2512.04263", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Polynomiogram: An Integrated Framework for Root Visualization and Generative Art\nResearch Topic: artificial intelligence\nAuthors: Hoang Duc Nguyen, Anh Van Pham, Hien D. Nguyen\nAbstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.\nCategories: cs.SE, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04263\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1528, "total_text_length": 1609, "word_count": 218, "sentence_count": 10}, "basic_keywords": ["polynomial", "polynomiogram", "framework", "platform", "exploring", "visualizing", "generating", "root"], "domain_keywords": [], "technical_terms": ["Beyond", "Numerical", "Examples", "Lucas", "Polynomiogram"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04262", "topic": "artificial intelligence", "title": "Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage", "authors": ["Nolan Platt", "Ethan Luchs", "Sehrish Nizamani"], "first_author": "Nolan Platt", "abstract": "Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.SE", "cs.AI", "cs.HC"], "url": "http://arxiv.org/abs/2512.04262", "comments": "7 pages. Published in Proceedings of the 2025 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). DOI: 10.1109/VL-HCC65237.2025.00024", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage\nResearch Topic: artificial intelligence\nAuthors: Nolan Platt, Ethan Luchs, Sehrish Nizamani\nAbstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.\nCategories: cs.SE, cs.AI, cs.HC\nPublication Date: 2025-12-03\nPaper ID: 2512.04262\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1505, "total_text_length": 1601, "word_count": 222, "sentence_count": 11}, "basic_keywords": ["evaluations", "usability", "heuristic", "consistency", "automated", "user", "human", "especially"], "domain_keywords": [], "technical_terms": ["Usability", "These", "Krippendorff", "Cohen", "Alpha", "Severity", "Jakob Nielsen", "User Experience"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04254", "topic": "artificial intelligence", "title": "Hey GPT-OSS, Looks Like You Got It -- Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics", "authors": ["Gaëtan Michelet", "Janine Schneider", "Aruna Withanage", "Frank Breitinger"], "first_author": "Gaëtan Michelet", "abstract": "The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CR", "cs.AI"], "url": "http://arxiv.org/abs/2512.04254", "comments": "Accept at DFRWS EU 2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Hey GPT-OSS, Looks Like You Got It -- Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics\nResearch Topic: artificial intelligence\nAuthors: Gaëtan Michelet, Janine Schneider, Aruna Withanage, Frank Breitinger\nAbstract: The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.\nCategories: cs.CR, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04254\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 157, "abstract_length": 1220, "total_text_length": 1378, "word_count": 199, "sentence_count": 12}, "basic_keywords": ["reasoning", "language", "models", "digital", "forensics", "potential", "model", "tasks"], "domain_keywords": [], "technical_terms": ["However", "Beyond", "Findings", "Four", "Recently"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04252", "topic": "artificial intelligence", "title": "Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning", "authors": ["Baichuan Zeng"], "first_author": "Baichuan Zeng", "abstract": "Predicting the inhibitory potency of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1)-a key target in overcoming cancer chemoresistance-remains a critical challenge in early drug discovery. We present a deep learning framework for the quantitative regression of pIC50 values from molecular Simplified Molecular Input Line Entry System (SMILES) strings using fine-tuned variants of ChemBERTa, a pre-trained chemical language model. Leveraging a large-scale consensus dataset of 177,092 compounds, we systematically evaluate two pre-training strategies-Masked Language Modeling (MLM) and Masked Token Regression (MTR)-under stratified data splits and sample weighting to address severe activity imbalance which only 2.1% are active. Our approach outperforms classical baselines Random Predictor in both regression accuracy and virtual screening utility, and has competitive performance compared to Random Forest, achieving high enrichment factor EF@1% 17.4 and precision Precision@1% 37.4 among top-ranked predictions. The resulting model, validated through rigorous ablation and hyperparameter studies, provides a robust, ready-to-deploy tool for prioritizing TDP1 inhibitors for experimental testing. By enabling accurate, 3D-structure-free pIC50 prediction directly from SMILES, this work demonstrates the transformative potential of chemical transformers in accelerating target-specific drug discovery.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04252", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning\nResearch Topic: artificial intelligence\nAuthors: Baichuan Zeng\nAbstract: Predicting the inhibitory potency of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1)-a key target in overcoming cancer chemoresistance-remains a critical challenge in early drug discovery. We present a deep learning framework for the quantitative regression of pIC50 values from molecular Simplified Molecular Input Line Entry System (SMILES) strings using fine-tuned variants of ChemBERTa, a pre-trained chemical language model. Leveraging a large-scale consensus dataset of 177,092 compounds, we systematically evaluate two pre-training strategies-Masked Language Modeling (MLM) and Masked Token Regression (MTR)-under stratified data splits and sample weighting to address severe activity imbalance which only 2.1% are active. Our approach outperforms classical baselines Random Predictor in both regression accuracy and virtual screening utility, and has competitive performance compared to Random Forest, achieving high enrichment factor EF@1% 17.4 and precision Precision@1% 37.4 among top-ranked predictions. The resulting model, validated through rigorous ablation and hyperparameter studies, provides a robust, ready-to-deploy tool for prioritizing TDP1 inhibitors for experimental testing. By enabling accurate, 3D-structure-free pIC50 prediction directly from SMILES, this work demonstrates the transformative potential of chemical transformers in accelerating target-specific drug discovery.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04252\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1418, "total_text_length": 1508, "word_count": 188, "sentence_count": 10}, "basic_keywords": ["regression", "target", "drug", "discovery", "molecular", "smiles", "chemical", "language"], "domain_keywords": [], "technical_terms": ["Random Forest", "Simplified Molecular Input Line Entry System", "Tyrosyl", "Predicting", "Phosphodiesterase", "Random Predictor", "Masked Token Regression", "Masked Language Modeling"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04248", "topic": "artificial intelligence", "title": "MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models", "authors": ["Shaoheng Fang", "Chaohui Yu", "Fan Wang", "Qixing Huang"], "first_author": "Shaoheng Fang", "abstract": "We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04248", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models\nResearch Topic: artificial intelligence\nAuthors: Shaoheng Fang, Chaohui Yu, Fan Wang, Qixing Huang\nAbstract: We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04248\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1146, "total_text_length": 1227, "word_count": 155, "sentence_count": 8}, "basic_keywords": ["view", "multi", "generation", "layout", "mvroom", "stage", "scene", "introduce"], "domain_keywords": [], "technical_terms": ["Additionally", "Experimental", "Ablation"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04246", "topic": "artificial intelligence", "title": "Toward Virtuous Reinforcement Learning", "authors": ["Majid Ghasemi", "Mark Crowley"], "first_author": "Majid Ghasemi", "abstract": "This paper critiques common patterns in machine ethics for Reinforcement Learning (RL) and argues for a virtue focused alternative. We highlight two recurring limitations in much of the current literature: (i) rule based (deontological) methods that encode duties as constraints or shields often struggle under ambiguity and nonstationarity and do not cultivate lasting habits, and (ii) many reward based approaches, especially single objective RL, implicitly compress diverse moral considerations into a single scalar signal, which can obscure trade offs and invite proxy gaming in practice. We instead treat ethics as policy level dispositions, that is, relatively stable habits that hold up when incentives, partners, or contexts change. This shifts evaluation beyond rule checks or scalar returns toward trait summaries, durability under interventions, and explicit reporting of moral trade offs. Our roadmap combines four components: (1) social learning in multi agent RL to acquire virtue like patterns from imperfect but normatively informed exemplars; (2) multi objective and constrained formulations that preserve value conflicts and incorporate risk aware criteria to guard against harm; (3) affinity based regularization toward updateable virtue priors that support trait like stability under distribution shift while allowing norms to evolve; and (4) operationalizing diverse ethical traditions as practical control signals, making explicit the value and cultural assumptions that shape ethical RL benchmarks.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04246", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Toward Virtuous Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Majid Ghasemi, Mark Crowley\nAbstract: This paper critiques common patterns in machine ethics for Reinforcement Learning (RL) and argues for a virtue focused alternative. We highlight two recurring limitations in much of the current literature: (i) rule based (deontological) methods that encode duties as constraints or shields often struggle under ambiguity and nonstationarity and do not cultivate lasting habits, and (ii) many reward based approaches, especially single objective RL, implicitly compress diverse moral considerations into a single scalar signal, which can obscure trade offs and invite proxy gaming in practice. We instead treat ethics as policy level dispositions, that is, relatively stable habits that hold up when incentives, partners, or contexts change. This shifts evaluation beyond rule checks or scalar returns toward trait summaries, durability under interventions, and explicit reporting of moral trade offs. Our roadmap combines four components: (1) social learning in multi agent RL to acquire virtue like patterns from imperfect but normatively informed exemplars; (2) multi objective and constrained formulations that preserve value conflicts and incorporate risk aware criteria to guard against harm; (3) affinity based regularization toward updateable virtue priors that support trait like stability under distribution shift while allowing norms to evolve; and (4) operationalizing diverse ethical traditions as practical control signals, making explicit the value and cultural assumptions that shape ethical RL benchmarks.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04246\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 38, "abstract_length": 1521, "total_text_length": 1560, "word_count": 218, "sentence_count": 6}, "basic_keywords": ["virtue", "based", "under", "patterns", "ethics", "learning", "rule", "habits"], "domain_keywords": [], "technical_terms": ["Reinforcement Learning"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04239", "topic": "artificial intelligence", "title": "Configuration-Constrained Tube MPC for Periodic Operation", "authors": ["Filippo Badalamenti", "Jose A. Borja-Conde", "Sampath Kumar Mulagaleti", "Boris Houska", "Alberto Bemporad", "Mario Eduardo Villanueva"], "first_author": "Filippo Badalamenti", "abstract": "Periodic operation often emerges as the economically optimal mode in industrial processes, particularly under varying economic or environmental conditions. This paper proposes a robust model predictive control (MPC) framework for uncertain systems modeled as polytopic linear differential inclusions (LDIs), where the dynamics evolve as convex combinations of finitely many affine control systems with additive disturbances. The robust control problem is reformulated as a convex optimization program by optimizing over configuration-constrained polytopic tubes and tracks a periodic trajectory that is optimal for a given economic criterion. Artificial variables embedded in the formulation ensure recursive feasibility and robust constraint satisfaction when the economic criterion is updated online, while guaranteeing convergence to the corresponding optimal periodic tube when the criterion remains constant. To improve computational efficiency, we introduce a quadratic over-approximation of the periodic cost under a Lipschitz continuity assumption, yielding a Quadratic Program (QP) formulation that preserves the above theoretical guarantees. The effectiveness and scalability of the approach are demonstrated on a benchmark example and a ball-plate system with eight states.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["eess.SY", "math.OC"], "url": "http://arxiv.org/abs/2512.04239", "comments": "11 pages, 3 figures, submitted for IEEE-TACON", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Configuration-Constrained Tube MPC for Periodic Operation\nResearch Topic: artificial intelligence\nAuthors: Filippo Badalamenti, Jose A. Borja-Conde, Sampath Kumar Mulagaleti, Boris Houska, Alberto Bemporad, Mario Eduardo Villanueva\nAbstract: Periodic operation often emerges as the economically optimal mode in industrial processes, particularly under varying economic or environmental conditions. This paper proposes a robust model predictive control (MPC) framework for uncertain systems modeled as polytopic linear differential inclusions (LDIs), where the dynamics evolve as convex combinations of finitely many affine control systems with additive disturbances. The robust control problem is reformulated as a convex optimization program by optimizing over configuration-constrained polytopic tubes and tracks a periodic trajectory that is optimal for a given economic criterion. Artificial variables embedded in the formulation ensure recursive feasibility and robust constraint satisfaction when the economic criterion is updated online, while guaranteeing convergence to the corresponding optimal periodic tube when the criterion remains constant. To improve computational efficiency, we introduce a quadratic over-approximation of the periodic cost under a Lipschitz continuity assumption, yielding a Quadratic Program (QP) formulation that preserves the above theoretical guarantees. The effectiveness and scalability of the approach are demonstrated on a benchmark example and a ball-plate system with eight states.\nCategories: eess.SY, math.OC\nPublication Date: 2025-12-03\nPaper ID: 2512.04239\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 1284, "total_text_length": 1342, "word_count": 175, "sentence_count": 7}, "basic_keywords": ["periodic", "optimal", "economic", "robust", "control", "criterion", "under", "systems"], "domain_keywords": [], "technical_terms": ["Artificial", "Quadratic Program", "Lipschitz", "Periodic"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04232", "topic": "artificial intelligence", "title": "Decentralized Social Media and Artificial Intelligence in Digital Public Health Monitoring", "authors": ["Marcel Salathé", "Sharada P. Mohanty"], "first_author": "Marcel Salathé", "abstract": "Digital public health monitoring has long relied on data from major social media platforms. Twitter was once an indispensable resource for tracking disease outbreaks and public sentiment in real time. Researchers used Twitter to monitor everything from influenza spread to vaccine hesitancy, demonstrating that social media data can serve as an early-warning system for emerging health threats. However, recent shifts in the social media landscape have challenged this data-driven paradigm. Platform policy changes, exemplified by Twitter's withdrawal of free data access, now restrict the very data that fueled a decade of digital public health research. At the same time, advances in artificial intelligence, particularly large language models (LLMs), have dramatically expanded our capacity to analyze large-scale textual data across languages and contexts. This presents a paradox: we possess powerful new AI tools to extract insights from social media, but face dwindling access to the data. In this viewpoint, we examine how digital public health monitoring is navigating these countervailing trends. We discuss the rise of decentralized social networks like Mastodon and Bluesky as alternative data sources, weighing their openness and ethical alignment with research against their smaller scale and potential biases. Ultimately, we argue that digital public health surveillance must adapt by embracing new platforms and methodologies, focusing on common diseases and broad signals that remain detectable, while advocating for policies that preserve researchers' access to public data in privacy-respective ways.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["q-bio.PE", "cs.CY"], "url": "http://arxiv.org/abs/2512.04232", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Decentralized Social Media and Artificial Intelligence in Digital Public Health Monitoring\nResearch Topic: artificial intelligence\nAuthors: Marcel Salathé, Sharada P. Mohanty\nAbstract: Digital public health monitoring has long relied on data from major social media platforms. Twitter was once an indispensable resource for tracking disease outbreaks and public sentiment in real time. Researchers used Twitter to monitor everything from influenza spread to vaccine hesitancy, demonstrating that social media data can serve as an early-warning system for emerging health threats. However, recent shifts in the social media landscape have challenged this data-driven paradigm. Platform policy changes, exemplified by Twitter's withdrawal of free data access, now restrict the very data that fueled a decade of digital public health research. At the same time, advances in artificial intelligence, particularly large language models (LLMs), have dramatically expanded our capacity to analyze large-scale textual data across languages and contexts. This presents a paradox: we possess powerful new AI tools to extract insights from social media, but face dwindling access to the data. In this viewpoint, we examine how digital public health monitoring is navigating these countervailing trends. We discuss the rise of decentralized social networks like Mastodon and Bluesky as alternative data sources, weighing their openness and ethical alignment with research against their smaller scale and potential biases. Ultimately, we argue that digital public health surveillance must adapt by embracing new platforms and methodologies, focusing on common diseases and broad signals that remain detectable, while advocating for policies that preserve researchers' access to public data in privacy-respective ways.\nCategories: q-bio.PE, cs.CY\nPublication Date: 2025-12-03\nPaper ID: 2512.04232\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1619, "total_text_length": 1710, "word_count": 242, "sentence_count": 11}, "basic_keywords": ["data", "public", "health", "social", "digital", "media", "twitter", "access"], "domain_keywords": [], "technical_terms": ["Bluesky", "However", "Mastodon", "Platform", "Twitter", "Researchers", "Ultimately", "Digital"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04231", "topic": "artificial intelligence", "title": "CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding", "authors": ["Zhou Chen", "Joe Lin", "Carson Bulgin", "Sathyanarayanan N. Aakur"], "first_author": "Zhou Chen", "abstract": "Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.AI"], "url": "http://arxiv.org/abs/2512.04231", "comments": "20 pages. 3 figures, 4 tables. Under Review", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding\nResearch Topic: artificial intelligence\nAuthors: Zhou Chen, Joe Lin, Carson Bulgin, Sathyanarayanan N. Aakur\nAbstract: Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.\nCategories: cs.RO, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04231\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1496, "total_text_length": 1566, "word_count": 195, "sentence_count": 10}, "basic_keywords": ["object", "objects", "based", "affordance", "craft", "grasp", "assistive", "grounding"], "domain_keywords": ["imagenet"], "technical_terms": ["Existing", "Assistive"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04228", "topic": "artificial intelligence", "title": "Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework", "authors": ["Peter B. Walker", "Hannah Davidson", "Aiden Foster", "Matthew Lienert", "Thomas Pardue", "Dale Russell"], "first_author": "Peter B. Walker", "abstract": "Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to , where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises {Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04228", "comments": "12 pages, 5 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework\nResearch Topic: artificial intelligence\nAuthors: Peter B. Walker, Hannah Davidson, Aiden Foster, Matthew Lienert, Thomas Pardue, Dale Russell\nAbstract: Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \\textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \\footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04228\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 124, "abstract_length": 1439, "total_text_length": 1564, "word_count": 186, "sentence_count": 10}, "basic_keywords": ["training", "reasoning", "models", "language", "llms", "science", "premises", "generative"], "domain_keywords": [], "technical_terms": ["While", "Code", "Large Language Models", "Second", "First", "Grounded"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04227", "topic": "artificial intelligence", "title": "Educational Cone Model in Embedding Vector Spaces", "authors": ["Yo Ehara"], "first_author": "Yo Ehara", "abstract": "Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04227", "comments": "Accepted to the 33rd International Conference on Computers in Education (ICCE 2025)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Educational Cone Model in Embedding Vector Spaces\nResearch Topic: artificial intelligence\nAuthors: Yo Ehara\nAbstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.\nCategories: cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04227\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 49, "abstract_length": 1098, "total_text_length": 1148, "word_count": 157, "sentence_count": 8}, "basic_keywords": ["embedding", "difficulty", "educational", "model", "texts", "annotated", "datasets", "spaces"], "domain_keywords": [], "technical_terms": ["Although", "Human", "Empirical", "Educational Cone Model"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04210", "topic": "artificial intelligence", "title": "Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment", "authors": ["Huy Nghiem", "Swetasudha Panda", "Devashish Khatwani", "Huy V. Nguyen", "Krishnaram Kenthapadi", "Hal Daumé"], "first_author": "Huy Nghiem", "abstract": "Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "cs.CL", "cs.CY"], "url": "http://arxiv.org/abs/2512.04210", "comments": "ML4H 2025 Proceedings, Best Paper Award", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment\nResearch Topic: artificial intelligence\nAuthors: Huy Nghiem, Swetasudha Panda, Devashish Khatwani, Huy V. Nguyen, Krishnaram Kenthapadi, Hal Daumé\nAbstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.\nCategories: cs.AI, cs.CL, cs.CY\nPublication Date: 2025-12-03\nPaper ID: 2512.04210\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 99, "abstract_length": 1151, "total_text_length": 1251, "word_count": 157, "sentence_count": 8}, "basic_keywords": ["safety", "models", "llms", "deployment", "conversational", "medical", "assistants", "optimization"], "domain_keywords": [], "technical_terms": ["Using", "Kahneman", "Tversky Optimization", "Meditron", "Large Language Models", "Llama", "Mistral", "Conversational"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04207", "topic": "artificial intelligence", "title": "Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care", "authors": ["Xizhi Wu", "Nelly Estefanie Garduno-Rapp", "Justin F Rousseau", "Mounika Thakkallapally", "Hang Zhang", "Yuelyu Ji", "Shyam Visweswaran", "Yifan Peng", "Yanshan Wang"], "first_author": "Xizhi Wu", "abstract": "Unlike most primary headaches, secondary headaches need specialized care and can have devastating consequences if not treated promptly. Clinical guidelines highlight several 'red flag' features, such as thunderclap onset, meningismus, papilledema, focal neurologic deficits, signs of temporal arteritis, systemic illness, and the 'worst headache of their life' presentation. Despite these guidelines, determining which patients require urgent evaluation remains challenging in primary care settings. Clinicians often work with limited time, incomplete information, and diverse symptom presentations, which can lead to under-recognition and inappropriate care. We present a large language model (LLM)-based multi-agent clinical decision support system built on an orchestrator-specialist architecture, designed to perform explicit and interpretable secondary headache diagnosis from free-text clinical vignettes. The multi-agent system decomposes diagnosis into seven domain-specialized agents, each producing a structured and evidence-grounded rationale, while a central orchestrator performs task decomposition and coordinates agent routing. We evaluated the multi-agent system using 90 expert-validated secondary headache cases and compared its performance with a single-LLM baseline across two prompting strategies: question-based prompting (QPrompt) and clinical practice guideline-based prompting (GPrompt). We tested five open-source LLMs (Qwen-30B, GPT-OSS-20B, Qwen-14B, Qwen-8B, and Llama-3.1-8B), and found that the orchestrated multi-agent system with GPrompt consistently achieved the highest F1 scores, with larger gains in smaller models. These findings demonstrate that structured multi-agent reasoning improves accuracy beyond prompt engineering alone and offers a transparent, clinically aligned approach for explainable decision support in secondary headache diagnosis.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04207", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.154838", "embedding_text": "Paper Title: Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care\nResearch Topic: artificial intelligence\nAuthors: Xizhi Wu, Nelly Estefanie Garduno-Rapp, Justin F Rousseau, Mounika Thakkallapally, Hang Zhang, Yuelyu Ji, Shyam Visweswaran, Yifan Peng, Yanshan Wang\nAbstract: Unlike most primary headaches, secondary headaches need specialized care and can have devastating consequences if not treated promptly. Clinical guidelines highlight several 'red flag' features, such as thunderclap onset, meningismus, papilledema, focal neurologic deficits, signs of temporal arteritis, systemic illness, and the 'worst headache of their life' presentation. Despite these guidelines, determining which patients require urgent evaluation remains challenging in primary care settings. Clinicians often work with limited time, incomplete information, and diverse symptom presentations, which can lead to under-recognition and inappropriate care. We present a large language model (LLM)-based multi-agent clinical decision support system built on an orchestrator-specialist architecture, designed to perform explicit and interpretable secondary headache diagnosis from free-text clinical vignettes. The multi-agent system decomposes diagnosis into seven domain-specialized agents, each producing a structured and evidence-grounded rationale, while a central orchestrator performs task decomposition and coordinates agent routing. We evaluated the multi-agent system using 90 expert-validated secondary headache cases and compared its performance with a single-LLM baseline across two prompting strategies: question-based prompting (QPrompt) and clinical practice guideline-based prompting (GPrompt). We tested five open-source LLMs (Qwen-30B, GPT-OSS-20B, Qwen-14B, Qwen-8B, and Llama-3.1-8B), and found that the orchestrated multi-agent system with GPrompt consistently achieved the highest F1 scores, with larger gains in smaller models. These findings demonstrate that structured multi-agent reasoning improves accuracy beyond prompt engineering alone and offers a transparent, clinically aligned approach for explainable decision support in secondary headache diagnosis.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04207\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 106, "abstract_length": 1887, "total_text_length": 1994, "word_count": 245, "sentence_count": 11}, "basic_keywords": ["agent", "multi", "secondary", "clinical", "headache", "system", "care", "based"], "domain_keywords": [], "technical_terms": ["These", "Despite", "Clinical", "Llama", "Unlike", "Qwen", "Clinicians"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04204", "topic": "artificial intelligence", "title": "Machine Phenomenology: A Simple Equation Classifying Fast Radio Bursts", "authors": ["Yang Liu", "Yuhao Lu", "Rahim Moradi", "Bo Yang", "Bing Zhang", "Wenbin Lin", "Yu Wang"], "first_author": "Yang Liu", "abstract": "This work shows how human physical reasoning can guide machine-driven symbolic regression toward discovering empirical laws from observations. As an example, we derive a simple equation that classifies fast radio bursts (FRBs) into two distinct Gaussian distributions, indicating the existence of two physical classes. This human-AI workflow integrates feature selection, dimensional analysis, and symbolic regression: deep learning first analyzes CHIME Catalog 1 and identifies six independent parameters that collectively provide a complete description of FRBs; guided by Buckingham- analysis and correlation analysis, humans then construct dimensionless groups; finally, symbolic regression performed by the machine discovers the governing equation. When applied to the newer CHIME Catalog, the equation produces consistent results, demonstrating that it captures the underlying physics. This framework is applicable to a broad range of scientific domains.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["astro-ph.IM", "astro-ph.HE", "cs.AI"], "url": "http://arxiv.org/abs/2512.04204", "comments": "19 pages, 9 figures, 3 tables. Submitted to SCIENCE CHINA Physics, Mechanics & Astronomy", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Machine Phenomenology: A Simple Equation Classifying Fast Radio Bursts\nResearch Topic: artificial intelligence\nAuthors: Yang Liu, Yuhao Lu, Rahim Moradi, Bo Yang, Bing Zhang, Wenbin Lin, Yu Wang\nAbstract: This work shows how human physical reasoning can guide machine-driven symbolic regression toward discovering empirical laws from observations. As an example, we derive a simple equation that classifies fast radio bursts (FRBs) into two distinct Gaussian distributions, indicating the existence of two physical classes. This human-AI workflow integrates feature selection, dimensional analysis, and symbolic regression: deep learning first analyzes CHIME Catalog 1 and identifies six independent parameters that collectively provide a complete description of FRBs; guided by Buckingham-$π$ analysis and correlation analysis, humans then construct dimensionless groups; finally, symbolic regression performed by the machine discovers the governing equation. When applied to the newer CHIME Catalog, the equation produces consistent results, demonstrating that it captures the underlying physics. This framework is applicable to a broad range of scientific domains.\nCategories: astro-ph.IM, astro-ph.HE, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04204\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 959, "total_text_length": 1030, "word_count": 137, "sentence_count": 6}, "basic_keywords": ["symbolic", "regression", "equation", "analysis", "human", "physical", "machine", "frbs"], "domain_keywords": [], "technical_terms": ["Buckingham", "Catalog", "When", "Gaussian"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04198", "topic": "artificial intelligence", "title": "Network of Theseus (like the ship)", "authors": ["Vighnesh Subramaniam", "Colin Conwell", "Boris Katz", "Andrei Barbu", "Brian Cheung"], "first_author": "Vighnesh Subramaniam", "abstract": "A standard assumption in deep learning is that the inductive bias introduced by a neural network architecture must persist from training through inference. The architecture you train with is the architecture you deploy. This assumption constrains the community from selecting architectures that may have desirable efficiency or design properties due to difficulties with optimization. We challenge this assumption with Network of Theseus (NoT), a method for progressively converting a trained, or even untrained, guide network architecture part-by-part into an entirely different target network architecture while preserving the performance of the guide network. At each stage, components in the guide network architecture are incrementally replaced with target architecture modules and aligned via representational similarity metrics. This procedure largely preserves the functionality of the guide network even under substantial architectural changes-for example, converting a convolutional network into a multilayer perceptron, or GPT-2 into a recurrent neural network. By decoupling optimization from deployment, NoT expands the space of viable inference-time architectures, opening opportunities for better accuracy-efficiency tradeoffs and enabling more directed exploration of the architectural design space.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.04198", "comments": "Preprint. 24 pages, 9 figures, 8 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Network of Theseus (like the ship)\nResearch Topic: artificial intelligence\nAuthors: Vighnesh Subramaniam, Colin Conwell, Boris Katz, Andrei Barbu, Brian Cheung\nAbstract: A standard assumption in deep learning is that the inductive bias introduced by a neural network architecture must persist from training through inference. The architecture you train with is the architecture you deploy. This assumption constrains the community from selecting architectures that may have desirable efficiency or design properties due to difficulties with optimization. We challenge this assumption with Network of Theseus (NoT), a method for progressively converting a trained, or even untrained, guide network architecture part-by-part into an entirely different target network architecture while preserving the performance of the guide network. At each stage, components in the guide network architecture are incrementally replaced with target architecture modules and aligned via representational similarity metrics. This procedure largely preserves the functionality of the guide network even under substantial architectural changes-for example, converting a convolutional network into a multilayer perceptron, or GPT-2 into a recurrent neural network. By decoupling optimization from deployment, NoT expands the space of viable inference-time architectures, opening opportunities for better accuracy-efficiency tradeoffs and enabling more directed exploration of the architectural design space.\nCategories: cs.LG, cs.AI, cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.04198\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 34, "abstract_length": 1315, "total_text_length": 1350, "word_count": 180, "sentence_count": 8}, "basic_keywords": ["network", "architecture", "guide", "assumption", "into", "neural", "inference", "architectures"], "domain_keywords": [], "technical_terms": ["Theseus", "Network"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04189", "topic": "artificial intelligence", "title": "BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training", "authors": ["Luca Colombo", "Fabrizio Pittorino", "Daniele Zambon", "Carlo Baldassi", "Manuel Roveri", "Cesare Alippi"], "first_author": "Luca Colombo", "abstract": "Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, quantization-aware training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating gains of up to +6.89% and +10.57% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04189", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training\nResearch Topic: artificial intelligence\nAuthors: Luca Colombo, Fabrizio Pittorino, Daniele Zambon, Carlo Baldassi, Manuel Roveri, Cesare Alippi\nAbstract: Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, quantization-aware training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating gains of up to +6.89% and +10.57% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04189\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1708, "total_text_length": 1786, "word_count": 234, "sentence_count": 15}, "basic_keywords": ["binary", "neural", "training", "backward", "networks", "bnns", "both", "based"], "domain_keywords": [], "technical_terms": ["These", "However", "While", "Crucially", "Binary Neural Networks", "Binary Error Propagation"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04187", "topic": "artificial intelligence", "title": "OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology", "authors": ["Jinzhen Hu", "Kevin Faust", "Parsa Babaei Zadeh", "Adrienn Bourkas", "Shane Eaton", "Andrew Young", "Anzar Alvi", "Dimitrios George Oreopoulos", "Ameesha Paliwal", "Assem Saleh Alrumeh", "Evelyn Rose Kamski-Hennekam", "Phedias Diamandis"], "first_author": "Jinzhen Hu", "abstract": "The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04187", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology\nResearch Topic: artificial intelligence\nAuthors: Jinzhen Hu, Kevin Faust, Parsa Babaei Zadeh, Adrienn Bourkas, Shane Eaton, Andrew Young, Anzar Alvi, Dimitrios George Oreopoulos, Ameesha Paliwal, Assem Saleh Alrumeh, Evelyn Rose Kamski-Hennekam, Phedias Diamandis\nAbstract: The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04187\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1906, "total_text_length": 2008, "word_count": 257, "sentence_count": 12}, "basic_keywords": ["pathology", "onsight", "clinical", "digital", "real", "deployment", "software", "slide"], "domain_keywords": ["computer vision"], "technical_terms": ["While", "Here", "Together", "Lastly", "Pathology", "Accessible"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04144", "topic": "artificial intelligence", "title": "RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories", "authors": ["Roy Rinberg", "Usha Bhalla", "Igor Shilov", "Flavio P. Calmon", "Rohit Gandikota"], "first_author": "Roy Rinberg", "abstract": "Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04144", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories\nResearch Topic: artificial intelligence\nAuthors: Roy Rinberg, Usha Bhalla, Igor Shilov, Flavio P. Calmon, Rohit Gandikota\nAbstract: Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04144\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1298, "total_text_length": 1374, "word_count": 185, "sentence_count": 14}, "basic_keywords": ["ripplebench", "unlearning", "model", "knowledge", "effects", "ripple", "benchmark", "interventions"], "domain_keywords": [], "technical_terms": ["Using", "Maker", "Weapons", "While", "Wikipedia", "Mass Destruction Paper", "Targeted"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04072", "topic": "artificial intelligence", "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors", "authors": ["Zayne Sprague", "Jack Lu", "Manya Wadhwa", "Sedrick Keh", "Mengye Ren", "Greg Durrett"], "first_author": "Zayne Sprague", "abstract": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.04072", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: SkillFactory: Self-Distillation For Learning Cognitive Behaviors\nResearch Topic: artificial intelligence\nAuthors: Zayne Sprague, Jack Lu, Manya Wadhwa, Sedrick Keh, Mengye Ren, Greg Durrett\nAbstract: Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04072\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 1315, "total_text_length": 1380, "word_count": 212, "sentence_count": 9}, "basic_keywords": ["models", "skills", "model", "cognitive", "work", "base", "learn", "skillfactory"], "domain_keywords": [], "technical_terms": ["These", "Previous", "Reasoning"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04065", "topic": "artificial intelligence", "title": "Fare Comparison App of Uber, Ola and Rapido", "authors": ["Ashlesha Gopinath Sawant", "Sahil S. Jadhav", "Vidhan R. Jain", "Shriraj S. Jagtap", "Prachi Jadhav", "Soham Jadhav", "Ichha Raina"], "first_author": "Ashlesha Gopinath Sawant", "abstract": "In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.04065", "comments": "4 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Fare Comparison App of Uber, Ola and Rapido\nResearch Topic: artificial intelligence\nAuthors: Ashlesha Gopinath Sawant, Sahil S. Jadhav, Vidhan R. Jain, Shriraj S. Jagtap, Prachi Jadhav, Soham Jadhav, Ichha Raina\nAbstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04065\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 1020, "total_text_length": 1064, "word_count": 174, "sentence_count": 7}, "basic_keywords": ["users", "ride", "providing", "comparison", "very", "hailing", "services", "uber"], "domain_keywords": [], "technical_terms": ["Android Studios", "Python", "Appium", "Uber", "Users", "Thus", "Rapido"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04047", "topic": "artificial intelligence", "title": "Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs", "authors": ["Nadav Kunievsky"], "first_author": "Nadav Kunievsky", "abstract": "In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["econ.GN", "cs.AI", "cs.CY"], "url": "http://arxiv.org/abs/2512.04047", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs\nResearch Topic: artificial intelligence\nAuthors: Nadav Kunievsky\nAbstract: In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance.\nCategories: econ.GN, cs.AI, cs.CY\nPublication Date: 2025-12-03\nPaper ID: 2512.04047\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1336, "total_text_length": 1431, "word_count": 207, "sentence_count": 7}, "basic_keywords": ["persuasion", "elites", "polarization", "policy", "majority", "mass", "support", "advances"], "domain_keywords": [], "technical_terms": ["Historically", "Taken", "When"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04044", "topic": "artificial intelligence", "title": "MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking", "authors": ["Yizhou Zhao", "Zhiwei Steven Wu", "Adam Block"], "first_author": "Yizhou Zhao", "abstract": "Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI", "cs.CR"], "url": "http://arxiv.org/abs/2512.04044", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking\nResearch Topic: artificial intelligence\nAuthors: Yizhou Zhao, Zhiwei Steven Wu, Adam Block\nAbstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.\nCategories: cs.LG, cs.AI, cs.CR\nPublication Date: 2025-12-03\nPaper ID: 2512.04044\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 1659, "total_text_length": 1747, "word_count": 223, "sentence_count": 8}, "basic_keywords": ["quality", "weight", "gaussmark", "marktune", "model", "watermarking", "open", "inference"], "domain_keywords": [], "technical_terms": ["Watermarking", "Existing", "Open", "Together", "Empirically"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04039", "topic": "artificial intelligence", "title": "Fast & Efficient Normalizing Flows and Applications of Image Generative Models", "authors": ["Sandeep Nagar"], "first_author": "Sandeep Nagar", "abstract": "This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance. The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04039", "comments": "PhD Thesis", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Fast & Efficient Normalizing Flows and Applications of Image Generative Models\nResearch Topic: artificial intelligence\nAuthors: Sandeep Nagar\nAbstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.   The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.\nCategories: cs.CV, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04039\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1862, "total_text_length": 1941, "word_count": 256, "sentence_count": 5}, "basic_keywords": ["efficient", "using", "normalizing", "flow", "convolution", "layers", "algorithm", "inverse"], "domain_keywords": ["computer vision", "feature extraction"], "technical_terms": ["Development", "Quad", "Design", "Using", "Inverse", "Normalizing Flow", "Flow", "Conditional"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04032", "topic": "artificial intelligence", "title": "Jina-VLM: Small Multilingual Vision Language Model", "authors": ["Andreas Koukounas", "Georgios Mastrapas", "Florian Hönicke", "Sedigheh Eslami", "Guillaume Roncari", "Scott Martens", "Han Xiao"], "first_author": "Andreas Koukounas", "abstract": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm .", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI", "cs.CV"], "url": "http://arxiv.org/abs/2512.04032", "comments": "18 pages, 1-7 main content, 13-18 appendix for tables and dataset", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Jina-VLM: Small Multilingual Vision Language Model\nResearch Topic: artificial intelligence\nAuthors: Andreas Koukounas, Georgios Mastrapas, Florian Hönicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao\nAbstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm .\nCategories: cs.CL, cs.AI, cs.CV\nPublication Date: 2025-12-03\nPaper ID: 2512.04032\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 50, "abstract_length": 572, "total_text_length": 623, "word_count": 75, "sentence_count": 7}, "basic_keywords": ["model", "jina", "vision", "language", "achieves", "multilingual", "present", "parameter"], "domain_keywords": [], "technical_terms": ["Jina", "Model"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.6000000000000001}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04031", "topic": "artificial intelligence", "title": "Large Language Models for Limited Noisy Data: A Gravitational Wave Identification Study", "authors": ["Yixuan Li", "Yuhao Lu", "Yang Liu", "Liang Li", "R. Ruffini", "Di Li", "Rong-Gen Cai", "Xiaoyan Zhu", "Wenbin Lin", "Yu Wang"], "first_author": "Yixuan Li", "abstract": "This work investigates whether large language models (LLMs) offer advantages over traditional neural networks for astronomical data processing, in regimes with non-Gaussian, non-stationary noise and limited labeled samples. Gravitational wave observations provide an suitable test case, using only 90 LIGO events, finetuned LLMs achieve 97.4\\% accuracy for identifying signals. Further experiments show that, in contrast to traditional networks that rely on large simulated datasets, additional simulated samples do not improve LLM performance, while scaling studies reveal predictable gains with increasing model size and dataset size. These results indicate that LLMs can extract discriminative structure directly from observational data and provide an efficient assessment for gravitational wave identification. The same strategy may extend to other astronomical domains with similar noise properties, such as radio or pulsar observations.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["astro-ph.IM", "astro-ph.HE", "cs.AI"], "url": "http://arxiv.org/abs/2512.04031", "comments": "10 pages, 5 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Large Language Models for Limited Noisy Data: A Gravitational Wave Identification Study\nResearch Topic: artificial intelligence\nAuthors: Yixuan Li, Yuhao Lu, Yang Liu, Liang Li, R. Ruffini, Di Li, Rong-Gen Cai, Xiaoyan Zhu, Wenbin Lin, Yu Wang\nAbstract: This work investigates whether large language models (LLMs) offer advantages over traditional neural networks for astronomical data processing, in regimes with non-Gaussian, non-stationary noise and limited labeled samples. Gravitational wave observations provide an suitable test case, using only 90 LIGO events, finetuned LLMs achieve 97.4\\% accuracy for identifying signals. Further experiments show that, in contrast to traditional networks that rely on large simulated datasets, additional simulated samples do not improve LLM performance, while scaling studies reveal predictable gains with increasing model size and dataset size. These results indicate that LLMs can extract discriminative structure directly from observational data and provide an efficient assessment for gravitational wave identification. The same strategy may extend to other astronomical domains with similar noise properties, such as radio or pulsar observations.\nCategories: astro-ph.IM, astro-ph.HE, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04031\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 942, "total_text_length": 1030, "word_count": 138, "sentence_count": 7}, "basic_keywords": ["llms", "large", "traditional", "networks", "astronomical", "data", "noise", "samples"], "domain_keywords": [], "technical_terms": ["These", "Gravitational", "Further", "Gaussian"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04025", "topic": "artificial intelligence", "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation", "authors": ["Xiaolong Li", "Youping Gu", "Xi Lin", "Weijie Wang", "Bohan Zhuang"], "first_author": "Xiaolong Li", "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04025", "comments": "Tech report", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation\nResearch Topic: artificial intelligence\nAuthors: Xiaolong Li, Youping Gu, Xi Lin, Weijie Wang, Bohan Zhuang\nAbstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA\nCategories: cs.CV, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04025\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1563, "total_text_length": 1642, "word_count": 211, "sentence_count": 11}, "basic_keywords": ["attention", "information", "mechanisms", "critical", "efficient", "sparsity", "blocks", "binary"], "domain_keywords": ["computer vision", "attention mechanisms"], "technical_terms": ["Across", "Pyramid Sparse Attention", "Instead", "Current", "Attention", "Specifically"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04016", "topic": "artificial intelligence", "title": "TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees", "authors": ["Davut Emre Tasar", "Ceren Ocal Tasar"], "first_author": "Davut Emre Tasar", "abstract": "Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["quant-ph", "cs.AI"], "url": "http://arxiv.org/abs/2512.04016", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees\nResearch Topic: artificial intelligence\nAuthors: Davut Emre Tasar, Ceren Ocal Tasar\nAbstract: Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.\nCategories: quant-ph, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04016\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1915, "total_text_length": 2010, "word_count": 247, "sentence_count": 12}, "basic_keywords": ["quantum", "distribution", "certification", "tara", "classical", "guarantees", "conformal", "detection"], "domain_keywords": [], "technical_terms": ["Extensive", "Torino", "Quantum", "Adaptive Ranks", "Critically", "Test", "Kolmogorov Smirnov", "Forte Enterprise"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04007", "topic": "artificial intelligence", "title": "On the Temporality for Sketch Representation Learning", "authors": ["Marcelo Isaias de Moraes Junior", "Moacir Antonelli Ponti"], "first_author": "Marcelo Isaias de Moraes Junior", "abstract": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.04007", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: On the Temporality for Sketch Representation Learning\nResearch Topic: artificial intelligence\nAuthors: Marcelo Isaias de Moraes Junior, Moacir Antonelli Ponti\nAbstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04007\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 53, "abstract_length": 825, "total_text_length": 879, "word_count": 122, "sentence_count": 7}, "basic_keywords": ["sketches", "although", "sequences", "outperform", "autoregressive", "simple", "human", "hand"], "domain_keywords": [], "technical_terms": ["Although", "Furthermore", "Finally", "Sketches"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04000", "topic": "artificial intelligence", "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding", "authors": ["Jialuo Li", "Bin Li", "Jiahao Li", "Yan Lu"], "first_author": "Jialuo Li", "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.04000", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding\nResearch Topic: artificial intelligence\nAuthors: Jialuo Li, Bin Li, Jiahao Li, Yan Lu\nAbstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.\nCategories: cs.CV, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04000\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1233, "total_text_length": 1328, "word_count": 176, "sentence_count": 9}, "basic_keywords": ["query", "queries", "video", "frame", "selection", "global", "localized", "long"], "domain_keywords": [], "technical_terms": ["Consequently", "Experiments", "Large Multimodal Models", "Specifically", "Building"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03996", "topic": "artificial intelligence", "title": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation", "authors": ["Hang Xu", "Linjiang Huang", "Feng Zhao"], "first_author": "Hang Xu", "abstract": "Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at {https://github.com/xuhang07/TEP-Diffusion}.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03996", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation\nResearch Topic: artificial intelligence\nAuthors: Hang Xu, Linjiang Huang, Feng Zhao\nAbstract: Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \\href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03996\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1797, "total_text_length": 1890, "word_count": 246, "sentence_count": 11}, "basic_keywords": ["perturbation", "frequency", "noise", "randomness", "text", "diffusion", "models", "embedding"], "domain_keywords": [], "technical_terms": ["However", "Adapting", "Code", "Test", "Specifically", "Introducing", "Concurrently", "Diffusion"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03995", "topic": "artificial intelligence", "title": "Artificial Microsaccade Compensation: Stable Vision for an Ornithopter", "authors": ["Levi Burner", "Guido de Croon", "Yiannis Aloimonos"], "first_author": "Levi Burner", "abstract": "Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.CV"], "url": "http://arxiv.org/abs/2512.03995", "comments": "29 pages, 5 figures, 2 tables, under review", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: Artificial Microsaccade Compensation: Stable Vision for an Ornithopter\nResearch Topic: artificial intelligence\nAuthors: Levi Burner, Guido de Croon, Yiannis Aloimonos\nAbstract: Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.\nCategories: cs.RO, cs.CV\nPublication Date: 2025-12-03\nPaper ID: 2512.03995\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 980, "total_text_length": 1051, "word_count": 152, "sentence_count": 8}, "basic_keywords": ["video", "method", "results", "real", "time", "viewing", "while", "animals"], "domain_keywords": [], "technical_terms": ["Inspired", "Artificial Microsaccade Compensation", "Adobe Premier Pro", "When", "Animals"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03992", "topic": "artificial intelligence", "title": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation", "authors": ["Zexin Lin", "Hawen Wan", "Yebin Zhong", "Xiaoqiang"], "first_author": "Zexin Lin", "abstract": "Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03992", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.155864", "embedding_text": "Paper Title: DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation\nResearch Topic: artificial intelligence\nAuthors: Zexin Lin, Hawen Wan, Yebin Zhong, Xiaoqiang\nAbstract: Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03992\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1339, "total_text_length": 1425, "word_count": 174, "sentence_count": 10}, "basic_keywords": ["temporal", "models", "vlms", "visual", "percent", "critical", "under", "degradation"], "domain_keywords": [], "technical_terms": ["However", "Experiments", "Uncertainty", "Vision", "Language Models", "Guided Iterative Refinement"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03979", "topic": "artificial intelligence", "title": "BlurDM: A Blur Diffusion Model for Image Deblurring", "authors": ["Jin-Ting He", "Fu-Jen Tsai", "Yan-Tsung Peng", "Min-Hung Chen", "Chia-Wen Lin", "Yen-Yu Lin"], "first_author": "Jin-Ting He", "abstract": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03979", "comments": "NeurIPS 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: BlurDM: A Blur Diffusion Model for Image Deblurring\nResearch Topic: artificial intelligence\nAuthors: Jin-Ting He, Fu-Jen Tsai, Yan-Tsung Peng, Min-Hung Chen, Chia-Wen Lin, Yen-Yu Lin\nAbstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03979\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 1196, "total_text_length": 1248, "word_count": 170, "sentence_count": 9}, "basic_keywords": ["deblurring", "blurdm", "diffusion", "blur", "process", "image", "models", "existing"], "domain_keywords": [], "technical_terms": ["Extensive", "Ting", "Observing", "Blur Diffusion Model", "Additionally", "Gaussian", "During", "Diffusion"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03975", "topic": "artificial intelligence", "title": "Sponsored Questions and How to Auction Them", "authors": ["Kshipra Bhawalkar", "Alexandros Psomas", "Di Wang"], "first_author": "Kshipra Bhawalkar", "abstract": "Online platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements. The shift from traditional search to conversational AI provides a new approach. When a user's query is ambiguous, a Large Language Model (LLM) can proactively offer several clarifying follow-up prompts. In this paper we consider the following: what if some of these follow-up prompts can be ``sponsored,'' i.e., selected for their advertising potential. How should these ``suggestion slots'' be allocated? And, how does this new mechanism interact with the traditional ad auction that might follow? This paper introduces a formal model for designing and analyzing these interactive platforms. We use this model to investigate a critical engineering choice: whether it is better to build an end-to-end pipeline that jointly optimizes the user interaction and the final ad auction, or to decouple them into separate mechanisms for the suggestion slots and another for the subsequent ad slot. We show that the VCG mechanism can be adopted to jointly optimize the sponsored suggestion and the ads that follow; while this mechanism is more complex, it achieves outcomes that are efficient and truthful. On the other hand, we prove that the simple-to-implement modular approach suffers from strategic inefficiency: its Price of Anarchy is unbounded.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.GT", "cs.AI"], "url": "http://arxiv.org/abs/2512.03975", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Sponsored Questions and How to Auction Them\nResearch Topic: artificial intelligence\nAuthors: Kshipra Bhawalkar, Alexandros Psomas, Di Wang\nAbstract: Online platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements. The shift from traditional search to conversational AI provides a new approach. When a user's query is ambiguous, a Large Language Model (LLM) can proactively offer several clarifying follow-up prompts. In this paper we consider the following: what if some of these follow-up prompts can be ``sponsored,'' i.e., selected for their advertising potential. How should these ``suggestion slots'' be allocated? And, how does this new mechanism interact with the traditional ad auction that might follow?   This paper introduces a formal model for designing and analyzing these interactive platforms. We use this model to investigate a critical engineering choice: whether it is better to build an end-to-end pipeline that jointly optimizes the user interaction and the final ad auction, or to decouple them into separate mechanisms for the suggestion slots and another for the subsequent ad slot. We show that the VCG mechanism can be adopted to jointly optimize the sponsored suggestion and the ads that follow; while this mechanism is more complex, it achieves outcomes that are efficient and truthful. On the other hand, we prove that the simple-to-implement modular approach suffers from strategic inefficiency: its Price of Anarchy is unbounded.\nCategories: cs.GT, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03975\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 1528, "total_text_length": 1572, "word_count": 240, "sentence_count": 15}, "basic_keywords": ["follow", "platforms", "user", "query", "model", "suggestion", "mechanism", "search"], "domain_keywords": [], "technical_terms": ["Anarchy", "Large Language Model", "Online", "When", "Typically", "Price"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03973", "topic": "artificial intelligence", "title": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning", "authors": ["Franki Nguimatsia Tiofack", "Théotime Le Hellard", "Fabian Schramm", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "first_author": "Franki Nguimatsia Tiofack", "abstract": "Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03973", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Franki Nguimatsia Tiofack, Théotime Le Hellard, Fabian Schramm, Nicolas Perrin-Gilbert, Justin Carpentier\nAbstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03973\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1000, "total_text_length": 1087, "word_count": 140, "sentence_count": 9}, "basic_keywords": ["flow", "policy", "dataset", "value", "actor", "state", "behavior", "regularization"], "domain_keywords": [], "technical_terms": ["However", "Offline", "Webpage", "Minari", "Guided Flow Policy"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04142", "topic": "artificial intelligence", "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence", "authors": ["Sophia Falk", "Nicholas Kluge Corrêa", "Sasha Luccioni", "Lisa Biber-Freudenberger", "Aimee van Wynsberghe"], "first_author": "Sophia Falk", "abstract": "As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CY", "cs.AI", "econ.GN"], "url": "http://arxiv.org/abs/2512.04142", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: From FLOPs to Footprints: The Resource Cost of Artificial Intelligence\nResearch Topic: artificial intelligence\nAuthors: Sophia Falk, Nicholas Kluge Corrêa, Sasha Luccioni, Lisa Biber-Freudenberger, Aimee van Wynsberghe\nAbstract: As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.\nCategories: cs.CY, cs.AI, econ.GN\nPublication Date: 2025-12-03\nPaper ID: 2512.04142\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 1895, "total_text_length": 1966, "word_count": 277, "sentence_count": 12}, "basic_keywords": ["material", "hardware", "computational", "training", "demands", "elements", "environmental", "footprint"], "domain_keywords": [], "technical_terms": ["Combined", "Utilization", "Model", "Scenario", "Nvidia"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03955", "topic": "artificial intelligence", "title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol", "authors": ["Niklas Jobs", "Luis Miguel Vieira da Silva", "Jayanth Somashekaraiah", "Maximilian Weigand", "David Kube", "Felix Gehlhoff"], "first_author": "Niklas Jobs", "abstract": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "cs.ET"], "url": "http://arxiv.org/abs/2512.03955", "comments": "This work has been submitted to IFAC for possible publication", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol\nResearch Topic: artificial intelligence\nAuthors: Niklas Jobs, Luis Miguel Vieira da Silva, Jayanth Somashekaraiah, Maximilian Weigand, David Kube, Felix Gehlhoff\nAbstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.\nCategories: cs.AI, cs.ET\nPublication Date: 2025-12-03\nPaper ID: 2512.03955\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 108, "abstract_length": 810, "total_text_length": 919, "word_count": 114, "sentence_count": 6}, "basic_keywords": ["benchmark", "based", "planning", "execution", "standardized", "comparison", "agent", "implementation"], "domain_keywords": [], "technical_terms": ["Agents", "Blocksworld", "Large Language Models", "Model Context Protocol", "Industrial"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04139", "topic": "artificial intelligence", "title": "Solving N-Queen Problem using Las Vegas Algorithm with State Pruning", "authors": ["Susmita Sharma", "Aayush Shrestha", "Sitasma Thapa", "Prashant Timalsina", "Prakash Poudyal"], "first_author": "Susmita Sharma", "abstract": "The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.04139", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Solving N-Queen Problem using Las Vegas Algorithm with State Pruning\nResearch Topic: artificial intelligence\nAuthors: Susmita Sharma, Aayush Shrestha, Sitasma Thapa, Prashant Timalsina, Prakash Poudyal\nAbstract: The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04139\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1295, "total_text_length": 1364, "word_count": 187, "sentence_count": 8}, "basic_keywords": ["queens", "solution", "algorithm", "problem", "while", "backtracking", "large", "thus"], "domain_keywords": [], "technical_terms": ["Although", "Las Vegas", "Queens", "While"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03931", "topic": "artificial intelligence", "title": "Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties", "authors": ["Vineel Tummala", "Daniela Inclezan"], "first_author": "Vineel Tummala", "abstract": "This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03931", "comments": "27 pages, 5 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties\nResearch Topic: artificial intelligence\nAuthors: Vineel Tummala, Daniela Inclezan\nAbstract: This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03931\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 82, "abstract_length": 1539, "total_text_length": 1622, "word_count": 208, "sentence_count": 11}, "basic_keywords": ["policy", "programming", "based", "framework", "penalties", "logic", "autonomous", "potential"], "domain_keywords": [], "technical_terms": ["Authorization", "Compared", "Experiments", "These", "Building", "While", "Obligation Policy Language", "Under"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03915", "topic": "artificial intelligence", "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models", "authors": ["X. Y. Han", "Yuan Zhong"], "first_author": "X. Y. Han", "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["math.OC", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03915", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models\nResearch Topic: artificial intelligence\nAuthors: X. Y. Han, Yuan Zhong\nAbstract: In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.\nCategories: math.OC, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03915\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1382, "total_text_length": 1499, "word_count": 208, "sentence_count": 10}, "basic_keywords": ["experts", "balancing", "load", "framework", "training", "tokens", "theoretical", "analyzing"], "domain_keywords": [], "technical_terms": ["Experts", "Wang", "Auxiliary", "Additionally", "Sparse Mixture", "Loss", "First", "Together"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03913", "topic": "artificial intelligence", "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations", "authors": ["Jeongeun Park", "Jihwan Yoon", "Byungwoo Jeon", "Juhan Park", "Jinwoo Shin", "Namhoon Cho", "Kyungjae Lee", "Sangdoo Yun", "Sungjoon Choi"], "first_author": "Jeongeun Park", "abstract": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.AI"], "url": "http://arxiv.org/abs/2512.03913", "comments": "https://vine-vla.github.io/", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Hierarchical Vision Language Action Model Using Success and Failure Demonstrations\nResearch Topic: artificial intelligence\nAuthors: Jeongeun Park, Jihwan Yoon, Byungwoo Jeon, Juhan Park, Jinwoo Shin, Namhoon Cho, Kyungjae Lee, Sangdoo Yun, Sungjoon Choi\nAbstract: Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.\nCategories: cs.RO, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03913\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 82, "abstract_length": 1487, "total_text_length": 1570, "word_count": 203, "sentence_count": 9}, "basic_keywords": ["system", "data", "failures", "level", "vision", "language", "action", "trained"], "domain_keywords": [], "technical_terms": ["However", "Across", "System", "Language", "Action", "Trained", "Prior Vision"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03911", "topic": "artificial intelligence", "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware", "authors": ["Kenneth Stewart", "Roxana Leontie", "Samantha Chapin", "Joe Hays", "Sumit Bam Shrestha", "Carl Glen Henshaw"], "first_author": "Kenneth Stewart", "abstract": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03911", "comments": "Submitted for review at NICE 2026 (Neuro-Inspired Computational Elements) conference", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware\nResearch Topic: artificial intelligence\nAuthors: Kenneth Stewart, Roxana Leontie, Samantha Chapin, Joe Hays, Sumit Bam Shrestha, Carl Glen Henshaw\nAbstract: We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.\nCategories: cs.RO, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03911\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1072, "total_text_length": 1163, "word_count": 159, "sentence_count": 7}, "basic_keywords": ["trained", "neuromorphic", "policy", "loihi", "neural", "networks", "hardware", "into"], "domain_keywords": [], "technical_terms": ["Artificial Neural Networks", "Sigma", "Loihi", "Intel", "Astrobee", "Rectified Linear Units", "Delta Neural Networks", "Omniverse Isaac Lab"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03903", "topic": "artificial intelligence", "title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity", "authors": ["Ekhi Azurmendi", "Joseba Fernandez de Landa", "Jaione Bengoetxea", "Maite Heredia", "Julen Etxaniz", "Mikel Zubillaga", "Ander Soraluze", "Aitor Soroa"], "first_author": "Ekhi Azurmendi", "abstract": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.03903", "comments": "Submitted to LREC 2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: BERnaT: Basque Encoders for Representing Natural Textual Diversity\nResearch Topic: artificial intelligence\nAuthors: Ekhi Azurmendi, Joseba Fernandez de Landa, Jaione Bengoetxea, Maite Heredia, Julen Etxaniz, Mikel Zubillaga, Ander Soraluze, Aitor Soroa\nAbstract: Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03903\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1196, "total_text_length": 1263, "word_count": 166, "sentence_count": 8}, "basic_keywords": ["standard", "language", "models", "corpora", "linguistic", "diverse", "text", "historical"], "domain_keywords": [], "technical_terms": ["Focusing", "These", "Natural Language Understanding", "Language", "Basque", "Results"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03902", "topic": "artificial intelligence", "title": "Equalizer or amplifier? How AI may reshape human cognitive differences", "authors": ["Maria Bigoni", "Andrea Ichino", "Aldo Rustichini", "Giulio Zanella"], "first_author": "Maria Bigoni", "abstract": "Machines have at times equalized physical strength by substituting for human effort, and at other times amplified these differences. Artificial intelligence (AI) may likewise narrow or widen disparities in cognitive ability. Recent evidence from the Information and Communication Technology (ICT) revolution suggests that computers increased inequality by education but reduced it by cognitive ability. Early research on generative AI shows larger productivity gains for less-skilled than for high-skilled workers. Whether AI ultimately acts as an equalizer or an amplifier of human cognitive differences is especially crucial for education systems, which must decide whether -- and how -- to allow students to use AI in coursework and exams. This decision is urgent because employers value workers who can leverage AI effectively rather than operate independently of it.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["econ.GN"], "url": "http://arxiv.org/abs/2512.03902", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Equalizer or amplifier? How AI may reshape human cognitive differences\nResearch Topic: artificial intelligence\nAuthors: Maria Bigoni, Andrea Ichino, Aldo Rustichini, Giulio Zanella\nAbstract: Machines have at times equalized physical strength by substituting for human effort, and at other times amplified these differences. Artificial intelligence (AI) may likewise narrow or widen disparities in cognitive ability. Recent evidence from the Information and Communication Technology (ICT) revolution suggests that computers increased inequality by education but reduced it by cognitive ability. Early research on generative AI shows larger productivity gains for less-skilled than for high-skilled workers. Whether AI ultimately acts as an equalizer or an amplifier of human cognitive differences is especially crucial for education systems, which must decide whether -- and how -- to allow students to use AI in coursework and exams. This decision is urgent because employers value workers who can leverage AI effectively rather than operate independently of it.\nCategories: econ.GN\nPublication Date: 2025-12-03\nPaper ID: 2512.03902\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 871, "total_text_length": 942, "word_count": 136, "sentence_count": 8}, "basic_keywords": ["cognitive", "times", "human", "differences", "ability", "education", "skilled", "workers"], "domain_keywords": [], "technical_terms": ["Artificial", "Whether", "Communication Technology", "Information", "Machines", "Early", "Recent"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.03895", "topic": "artificial intelligence", "title": "Parameter efficient hybrid spiking-quantum convolutional neural network with surrogate gradient and quantum data-reupload", "authors": ["Luu Trong Nhan", "Luu Trung Duong", "Pham Ngoc Nam", "Truong Cong Thang"], "first_author": "Luu Trong Nhan", "abstract": "The rapid advancement of artificial intelligence (AI) and deep learning (DL) has catalyzed the emergence of several optimization-driven subfields, notably neuromorphic computing and quantum machine learning. Leveraging the differentiable nature of hybrid models, researchers have explored their potential to address complex problems through unified optimization strategies. One such development is the Spiking Quantum Neural Network (SQNN), which combines principles from spiking neural networks (SNNs) and quantum computing. However, existing SQNN implementations often depend on pretrained SNNs due to the non-differentiable nature of spiking activity and the limited scalability of current SNN encoders. In this work, we propose a novel architecture, Spiking-Quantum Data Re-upload Convolutional Neural Network (SQDR-CNN), that enables joint training of convolutional SNNs and quantum circuits within a single backpropagation framework. Unlike its predecessor, SQDR-CNN allow convergence to reasonable performance without the reliance of pretrained spiking encoder and subsetting datasets. We also clarified some theoretical foundations, testing new design using quantum data-reupload with different training algorithm-initialization and evaluate the performance of the proposed model under noisy simulated quantum environments. As a result, we were able to achieve 86% of the mean top-performing accuracy of the SOTA SNN baselines, yet uses only 0.5% of the smallest spiking model's parameters. Through this integration of neuromorphic and quantum paradigms, we aim to open new research directions and foster technological progress in multi-modal, learnable systems.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.NE"], "url": "http://arxiv.org/abs/2512.03895", "comments": "Work under review", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Parameter efficient hybrid spiking-quantum convolutional neural network with surrogate gradient and quantum data-reupload\nResearch Topic: artificial intelligence\nAuthors: Luu Trong Nhan, Luu Trung Duong, Pham Ngoc Nam, Truong Cong Thang\nAbstract: The rapid advancement of artificial intelligence (AI) and deep learning (DL) has catalyzed the emergence of several optimization-driven subfields, notably neuromorphic computing and quantum machine learning. Leveraging the differentiable nature of hybrid models, researchers have explored their potential to address complex problems through unified optimization strategies. One such development is the Spiking Quantum Neural Network (SQNN), which combines principles from spiking neural networks (SNNs) and quantum computing. However, existing SQNN implementations often depend on pretrained SNNs due to the non-differentiable nature of spiking activity and the limited scalability of current SNN encoders. In this work, we propose a novel architecture, Spiking-Quantum Data Re-upload Convolutional Neural Network (SQDR-CNN), that enables joint training of convolutional SNNs and quantum circuits within a single backpropagation framework. Unlike its predecessor, SQDR-CNN allow convergence to reasonable performance without the reliance of pretrained spiking encoder and subsetting datasets. We also clarified some theoretical foundations, testing new design using quantum data-reupload with different training algorithm-initialization and evaluate the performance of the proposed model under noisy simulated quantum environments. As a result, we were able to achieve 86% of the mean top-performing accuracy of the SOTA SNN baselines, yet uses only 0.5% of the smallest spiking model's parameters. Through this integration of neuromorphic and quantum paradigms, we aim to open new research directions and foster technological progress in multi-modal, learnable systems.\nCategories: cs.NE\nPublication Date: 2025-12-03\nPaper ID: 2512.03895\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 121, "abstract_length": 1670, "total_text_length": 1792, "word_count": 235, "sentence_count": 11}, "basic_keywords": ["quantum", "spiking", "neural", "snns", "learning", "optimization", "neuromorphic", "computing"], "domain_keywords": [], "technical_terms": ["However", "Quantum Data Re", "Spiking Quantum Neural Network", "Unlike", "Through", "Convolutional Neural Network", "Spiking", "Leveraging"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03887", "topic": "artificial intelligence", "title": "A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)", "authors": ["Saurav Prateek"], "first_author": "Saurav Prateek", "abstract": "The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow. The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation. We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03887", "comments": "16 pages, 6 figures, 4 tables. Code available at: https://github.com/SauravP97/Static-Deep-Research", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\nResearch Topic: artificial intelligence\nAuthors: Saurav Prateek\nAbstract: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.   The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.   We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03887\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 104, "abstract_length": 1669, "total_text_length": 1774, "word_count": 233, "sentence_count": 13}, "basic_keywords": ["research", "static", "deep", "depth", "agent", "breadth", "large", "language"], "domain_keywords": [], "technical_terms": ["Evaluation", "Independent", "Tree", "Worker", "Static", "Deep", "Breadth", "Adaptive Criteria"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03879", "topic": "artificial intelligence", "title": "Hybrid Temporal-8-Bit Spike Coding for Spiking Neural Network Surrogate Training", "authors": ["Luu Trong Nhan", "Luu Trung Duong", "Pham Ngoc Nam", "Truong Cong Thang"], "first_author": "Luu Trong Nhan", "abstract": "Spiking neural networks (SNNs) have emerged as a promising direction in both computational neuroscience and artificial intelligence, offering advantages such as strong biological plausibility and low energy consumption on neuromorphic hardware. Despite these benefits, SNNs still face challenges in achieving state-of-the-art performance on vision tasks. Recent work has shown that hybrid rate-temporal coding strategies (particularly those incorporating bit-plane representations of images into traditional rate coding schemes) can significantly improve performance when trained with surrogate backpropagation. Motivated by these findings, this study proposes a hybrid temporal-bit spike coding method that integrates bit-plane decompositions with temporal coding principles. Through extensive experiments across multiple computer vision benchmarks, we demonstrate that blending bit-plane information with temporal coding yields competitive, and in some cases improved, performance compared to established spike-coding techniques. To the best of our knowledge, this is the first work to introduce a hybrid temporal-bit coding scheme specifically designed for surrogate gradient training of SNNs.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.NE"], "url": "http://arxiv.org/abs/2512.03879", "comments": "Work under review", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Hybrid Temporal-8-Bit Spike Coding for Spiking Neural Network Surrogate Training\nResearch Topic: artificial intelligence\nAuthors: Luu Trong Nhan, Luu Trung Duong, Pham Ngoc Nam, Truong Cong Thang\nAbstract: Spiking neural networks (SNNs) have emerged as a promising direction in both computational neuroscience and artificial intelligence, offering advantages such as strong biological plausibility and low energy consumption on neuromorphic hardware. Despite these benefits, SNNs still face challenges in achieving state-of-the-art performance on vision tasks. Recent work has shown that hybrid rate-temporal coding strategies (particularly those incorporating bit-plane representations of images into traditional rate coding schemes) can significantly improve performance when trained with surrogate backpropagation. Motivated by these findings, this study proposes a hybrid temporal-bit spike coding method that integrates bit-plane decompositions with temporal coding principles. Through extensive experiments across multiple computer vision benchmarks, we demonstrate that blending bit-plane information with temporal coding yields competitive, and in some cases improved, performance compared to established spike-coding techniques. To the best of our knowledge, this is the first work to introduce a hybrid temporal-bit coding scheme specifically designed for surrogate gradient training of SNNs.\nCategories: cs.NE\nPublication Date: 2025-12-03\nPaper ID: 2512.03879\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1196, "total_text_length": 1277, "word_count": 162, "sentence_count": 7}, "basic_keywords": ["coding", "temporal", "snns", "performance", "hybrid", "plane", "vision", "work"], "domain_keywords": ["computer vision"], "technical_terms": ["Despite", "Through", "Motivated", "Recent", "Spiking"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03864", "topic": "artificial intelligence", "title": "Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment", "authors": ["Danny Hoang", "Anandkumar Patel", "Ruimen Chen", "Rajiv Malhotra", "Farhad Imani"], "first_author": "Danny Hoang", "abstract": "Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200 for training and 175 to 1000 for inference. Furthermore, HDC reduces training times by 200 and inference times by 300 to 600, showcasing its potential for energy-efficient smart manufacturing.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SC"], "url": "http://arxiv.org/abs/2512.03864", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment\nResearch Topic: artificial intelligence\nAuthors: Danny Hoang, Anandkumar Patel, Ruimen Chen, Rajiv Malhotra, Farhad Imani\nAbstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\\times$ for training and 175 to 1000$\\times$ for inference. Furthermore, HDC reduces training times by 200$\\times$ and inference times by 300 to 600$\\times$, showcasing its potential for energy-efficient smart manufacturing.\nCategories: cs.LG, cs.AI, cs.PF, cs.SC\nPublication Date: 2025-12-03\nPaper ID: 2512.03864\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 679, "total_text_length": 759, "word_count": 102, "sentence_count": 5}, "basic_keywords": ["energy", "smart", "consumption", "models", "manufacturing", "accuracy", "training", "inference"], "domain_keywords": [], "technical_terms": ["Computing", "Furthermore", "Smart"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03861", "topic": "artificial intelligence", "title": "Scalable Decision Focused Learning via Online Trainable Surrogates", "authors": ["Gaetano Signorelli", "Michele Lombardi"], "first_author": "Gaetano Signorelli", "abstract": "Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03861", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Scalable Decision Focused Learning via Online Trainable Surrogates\nResearch Topic: artificial intelligence\nAuthors: Gaetano Signorelli, Michele Lombardi\nAbstract: Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03861\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1126, "total_text_length": 1193, "word_count": 171, "sentence_count": 8}, "basic_keywords": ["decision", "loss", "method", "optimization", "using", "estimators", "cost", "function"], "domain_keywords": [], "technical_terms": ["Using", "Furthermore", "Decision Focused Learning", "Unlike", "Decision", "Recent"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03854", "topic": "artificial intelligence", "title": "Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population", "authors": ["Peshawa J. Muhammad Ali", "Navin Vincent", "Saman S. Abdulla", "Han N. Mohammed Fadhl", "Anders Blilie", "Kelvin Szolnoky", "Julia Anna Mielcarz", "Xiaoyi Ji", "Kimmo Kartasalo", "Abdulbasit K. Al-Talabani", "Nita Mulliqi"], "first_author": "Peshawa J. Muhammad Ali", "abstract": "Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.03854", "comments": "13 pages, 2 figures and 1 table", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156367", "embedding_text": "Paper Title: Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population\nResearch Topic: artificial intelligence\nAuthors: Peshawa J. Muhammad Ali, Navin Vincent, Saman S. Abdulla, Han N. Mohammed Fadhl, Anders Blilie, Kelvin Szolnoky, Julia Anna Mielcarz, Xiaoyi Ji, Kimmo Kartasalo, Abdulbasit K. Al-Talabani, Nita Mulliqi\nAbstract: Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.\nCategories: cs.CV\nPublication Date: 2025-12-03\nPaper ID: 2512.03854\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1236, "total_text_length": 1329, "word_count": 183, "sentence_count": 12}, "basic_keywords": ["pathology", "populations", "models", "dataset", "slides", "scanner", "under", "artificial"], "domain_keywords": [], "technical_terms": ["International Society", "Artificial", "Consequently", "Scanning", "Bioimage Archive", "Publicly", "Middle East", "Erbil"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03848", "topic": "artificial intelligence", "title": "PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation", "authors": ["Hania Ghouse", "Maryam Alsharqi", "Farhad R. Nezami", "Muzammil Behzad"], "first_author": "Hania Ghouse", "abstract": "Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03848", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation\nResearch Topic: artificial intelligence\nAuthors: Hania Ghouse, Maryam Alsharqi, Farhad R. Nezami, Muzammil Behzad\nAbstract: Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03848\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 125, "abstract_length": 1210, "total_text_length": 1336, "word_count": 164, "sentence_count": 7}, "basic_keywords": ["cardiac", "across", "classification", "framework", "task", "analysis", "anatomical", "segmentation"], "domain_keywords": [], "technical_terms": ["Unlike", "Cardiac"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03847", "topic": "artificial intelligence", "title": "DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training", "authors": ["Dingwei Zhu", "Zhiheng Xi", "Shihan Dou", "Yuhui Wang", "Sixian Li", "Junjie Ye", "Honglin Guo", "Shichun Liu", "Chenhao Huang", "Yajie Yang", "Junlin Shang", "Senjie Jin", "Ming Zhang", "Jiazheng Zhang", "Caishuang Huang", "Yunke Zhang", "Demei Yan", "Yuran Wang", "Tao Gui"], "first_author": "Dingwei Zhu", "abstract": "Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03847", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training\nResearch Topic: artificial intelligence\nAuthors: Dingwei Zhu, Zhiheng Xi, Shihan Dou, Yuhui Wang, Sixian Li, Junjie Ye, Honglin Guo, Shichun Liu, Chenhao Huang, Yajie Yang, Junlin Shang, Senjie Jin, Ming Zhang, Jiazheng Zhang, Caishuang Huang, Yunke Zhang, Demei Yan, Yuran Wang, Tao Gui\nAbstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03847\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1336, "total_text_length": 1420, "word_count": 184, "sentence_count": 11}, "basic_keywords": ["supervision", "training", "real", "noisy", "generalization", "dvpo", "value", "risk"], "domain_keywords": [], "technical_terms": ["Bellman", "Distributional Value Modeling", "While", "Across", "Policy Optimization", "Risk", "Reinforcement"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03795", "topic": "artificial intelligence", "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving", "authors": ["Jia Hu", "Zhexi Lian", "Xuerun Yan", "Ruiang Bi", "Dou Shen", "Yu Ruan", "Haoran Wang"], "first_author": "Jia Hu", "abstract": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.AI"], "url": "http://arxiv.org/abs/2512.03795", "comments": "17 pages, 18 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving\nResearch Topic: artificial intelligence\nAuthors: Jia Hu, Zhexi Lian, Xuerun Yan, Ruiang Bi, Dou Shen, Yu Ruan, Haoran Wang\nAbstract: Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.\nCategories: cs.RO, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03795\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1903, "total_text_length": 2004, "word_count": 263, "sentence_count": 18}, "basic_keywords": ["social", "interaction", "mpcformer", "dynamics", "driving", "approach", "based", "achieves"], "domain_keywords": [], "technical_terms": ["Autonomous Driving", "Open", "Transformer", "Results", "Reinforcement Learning", "Close"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03794", "topic": "artificial intelligence", "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition", "authors": ["Zichuan Lin", "Yicheng Liu", "Yang Yang", "Lvfang Tao", "Deheng Ye"], "first_author": "Zichuan Lin", "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.03794", "comments": "15 pages, 9 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition\nResearch Topic: artificial intelligence\nAuthors: Zichuan Lin, Yicheng Liu, Yang Yang, Lvfang Tao, Deheng Ye\nAbstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.\nCategories: cs.CV, cs.AI, cs.CL, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03794\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1742, "total_text_length": 1820, "word_count": 234, "sentence_count": 11}, "basic_keywords": ["visual", "tokens", "adaptvision", "efficient", "tool", "learning", "vision", "vlms"], "domain_keywords": [], "technical_terms": ["Based", "Inspired", "While", "Decoupled Turn Policy Optimization", "Vision", "Comprehensive", "Language Models", "Central"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03783", "topic": "artificial intelligence", "title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "authors": ["Dongchao Yang", "Songxiang Liu", "Disong Wang", "Yuanyuan Wang", "Guanglu Wan", "Helen Meng"], "first_author": "Dongchao Yang", "abstract": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["cs.AI", "cs.SD"], "url": "http://arxiv.org/abs/2512.03783", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Dongchao Yang, Songxiang Liu, Disong Wang, Yuanyuan Wang, Guanglu Wan, Helen Meng\nAbstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.\nCategories: cs.AI, cs.SD\nPublication Date: 2025-12-03\nPaper ID: 2512.03783\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1196, "total_text_length": 1269, "word_count": 158, "sentence_count": 8}, "basic_keywords": ["reasoning", "adaptive", "text", "omni", "framework", "multimodal", "behaviors", "model"], "domain_keywords": [], "technical_terms": ["However", "Tuning", "Omni", "Adaptive Reinforcement Learning", "Adaptive Supervised Fine", "Adaptive", "Recent", "Experimental"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03772", "topic": "artificial intelligence", "title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control", "authors": ["Gabriele Fadini", "Deepak Ingole", "Tong Duy Son", "Alisa Rupenyan"], "first_author": "Gabriele Fadini", "abstract": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.AI", "eess.SY"], "url": "http://arxiv.org/abs/2512.03772", "comments": "6 pages, 7 figures, 3 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control\nResearch Topic: artificial intelligence\nAuthors: Gabriele Fadini, Deepak Ingole, Tong Duy Son, Alisa Rupenyan\nAbstract: This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.\nCategories: cs.RO, cs.AI, eess.SY\nPublication Date: 2025-12-03\nPaper ID: 2512.03772\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1016, "total_text_length": 1110, "word_count": 142, "sentence_count": 9}, "basic_keywords": ["real", "torque", "model", "time", "controller", "parameters", "high", "dimensional"], "domain_keywords": [], "technical_terms": ["Aligned Subspace", "Bayesian Optimization", "Nonlinear Model Predictive Control", "Sparse Axis", "Moreover"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03771", "topic": "artificial intelligence", "title": "In-Context Representation Hijacking", "authors": ["Itay Yona", "Amir Sarid", "Michael Karasik", "Yossi Gandelsman"], "first_author": "Itay Yona", "abstract": "We introduce , a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., \"How to build a carrot?\") are internally interpreted as disallowed instructions (e.g., \"How to build a bomb?\"), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "url": "http://arxiv.org/abs/2512.03771", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: In-Context Representation Hijacking\nResearch Topic: artificial intelligence\nAuthors: Itay Yona, Amir Sarid, Michael Karasik, Yossi Gandelsman\nAbstract: We introduce $\\textbf{Doublespeak}$, a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., \"How to build a carrot?\") are internally interpreted as disallowed instructions (e.g., \"How to build a bomb?\"), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.\nCategories: cs.CL, cs.AI, cs.CR, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03771\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 35, "abstract_length": 1310, "total_text_length": 1346, "word_count": 187, "sentence_count": 19}, "basic_keywords": ["harmful", "context", "representation", "attack", "benign", "llms", "bomb", "token"], "domain_keywords": [], "technical_terms": ["Llama", "Doublespeak", "Instruct"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03767", "topic": "artificial intelligence", "title": "CaFTRA: Frequency-Domain Correlation-Aware Feedback-Free MIMO Transmission and Resource Allocation for 6G and Beyond", "authors": ["Bo Qian", "Hanlin Wu", "Jiacheng Chen", "Yunting Xu", "Xiaoyu Wang", "Haibo Zhou", "Yusheng Ji"], "first_author": "Bo Qian", "abstract": "The fundamental design of wireless systems toward AI-native 6G and beyond is driven by the need for ever-increasing demand of mobile data traffic, extreme spectral efficiency, and adaptability across diverse service scenarios. To overcome the limitations posed by feedback-based multiple-input and multiple-output (MIMO) transmission, we propose a novel frequency-domain Correlation-aware Feedback-free MIMO Transmission and Resource Allocation (CaFTRA) framework tailored for fully-decoupled radio access networks (FD-RAN) to meet the emerging requirements of AI-Native 6G and beyond. By leveraging artificial intelligence (AI), CaFTRA effectively eliminates real-time uplink feedback by predicting channel state information (CSI) based solely on user geolocation. We introduce a Learnable Queries-driven Transformer Network for CSI mapping from user geolocation, which utilizes multi-head attention and learnable query embeddings to accurately capture frequency-domain correlations among resource blocks (RBs), thereby significantly improving the precision of CSI prediction. Once base stations (BSs) adopt feedback-free transmission, their downlink transmission coverage can be significantly expanded due to the elimination of frequent uplink feedback. To enable efficient resource scheduling under such extensive-coverage scenarios, we apply a low-complexity many-to-one matching theory-based algorithm for efficient multi-BS association and multi-RB resource allocation, which is proven to converge to a stable matching within limited iterations. Simulation results demonstrate that CaFTRA achieves stable matching convergence and significant gains in spectral efficiency and user fairness compared to 5G, underscoring its potential value for 6G standardization efforts.", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["eess.SY", "cs.IT"], "url": "http://arxiv.org/abs/2512.03767", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: CaFTRA: Frequency-Domain Correlation-Aware Feedback-Free MIMO Transmission and Resource Allocation for 6G and Beyond\nResearch Topic: artificial intelligence\nAuthors: Bo Qian, Hanlin Wu, Jiacheng Chen, Yunting Xu, Xiaoyu Wang, Haibo Zhou, Yusheng Ji\nAbstract: The fundamental design of wireless systems toward AI-native 6G and beyond is driven by the need for ever-increasing demand of mobile data traffic, extreme spectral efficiency, and adaptability across diverse service scenarios. To overcome the limitations posed by feedback-based multiple-input and multiple-output (MIMO) transmission, we propose a novel frequency-domain Correlation-aware Feedback-free MIMO Transmission and Resource Allocation (CaFTRA) framework tailored for fully-decoupled radio access networks (FD-RAN) to meet the emerging requirements of AI-Native 6G and beyond. By leveraging artificial intelligence (AI), CaFTRA effectively eliminates real-time uplink feedback by predicting channel state information (CSI) based solely on user geolocation. We introduce a Learnable Queries-driven Transformer Network for CSI mapping from user geolocation, which utilizes multi-head attention and learnable query embeddings to accurately capture frequency-domain correlations among resource blocks (RBs), thereby significantly improving the precision of CSI prediction. Once base stations (BSs) adopt feedback-free transmission, their downlink transmission coverage can be significantly expanded due to the elimination of frequent uplink feedback. To enable efficient resource scheduling under such extensive-coverage scenarios, we apply a low-complexity many-to-one matching theory-based algorithm for efficient multi-BS association and multi-RB resource allocation, which is proven to converge to a stable matching within limited iterations. Simulation results demonstrate that CaFTRA achieves stable matching convergence and significant gains in spectral efficiency and user fairness compared to 5G, underscoring its potential value for 6G standardization efforts.\nCategories: eess.SY, cs.IT\nPublication Date: 2025-12-03\nPaper ID: 2512.03767\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1775, "total_text_length": 1892, "word_count": 237, "sentence_count": 8}, "basic_keywords": ["feedback", "transmission", "resource", "based", "caftra", "user", "multi", "matching"], "domain_keywords": [], "technical_terms": ["Simulation", "Transformer Network", "Correlation", "Once", "Learnable Queries", "Native", "Transmission", "Resource Allocation"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03762", "topic": "artificial intelligence", "title": "RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design", "authors": ["Jiawei Xu", "Feng-Feng Wei", "Wei-Neng Chen"], "first_author": "Jiawei Xu", "abstract": "Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03762", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design\nResearch Topic: artificial intelligence\nAuthors: Jiawei Xu, Feng-Feng Wei, Wei-Neng Chen\nAbstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03762\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1610, "total_text_length": 1677, "word_count": 210, "sentence_count": 12}, "basic_keywords": ["role", "roco", "term", "based", "multi", "explorer", "exploiter", "both"], "domain_keywords": [], "technical_terms": ["These", "Automatic Heuristic Design", "Based System", "Agent Role", "Large Language Models", "Multi", "Experimental"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03759", "topic": "artificial intelligence", "title": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective", "authors": ["Jingyang Ou", "Jiaqi Han", "Minkai Xu", "Shaoxuan Xu", "Jianwen Xie", "Stefano Ermon", "Yi Wu", "Chongxuan Li"], "first_author": "Jingyang Ou", "abstract": "Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03759", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective\nResearch Topic: artificial intelligence\nAuthors: Jingyang Ou, Jiaqi Han, Minkai Xu, Shaoxuan Xu, Jianwen Xie, Stefano Ermon, Yi Wu, Chongxuan Li\nAbstract: Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.\nCategories: cs.CL, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03759\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1354, "total_text_length": 1429, "word_count": 177, "sentence_count": 11}, "basic_keywords": ["level", "token", "sequence", "autoregressive", "models", "dllms", "espo", "effective"], "domain_keywords": [], "technical_terms": ["Extensive", "Policy Optimization", "Countdown", "Reinforcement Learning", "Sequence"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03751", "topic": "artificial intelligence", "title": "Research on Brain Tumor Classification Method Based on Improved ResNet34 Network", "authors": ["Yufeng Li", "Wenchao Zhao", "Bo Dang", "Weimin Wang"], "first_author": "Yufeng Li", "abstract": "Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03751", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: Research on Brain Tumor Classification Method Based on Improved ResNet34 Network\nResearch Topic: artificial intelligence\nAuthors: Yufeng Li, Wenchao Zhao, Bo Dang, Weimin Wang\nAbstract: Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03751\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1314, "total_text_length": 1395, "word_count": 200, "sentence_count": 11}, "basic_keywords": ["network", "classification", "accuracy", "model", "image", "brain", "tumor", "improved"], "domain_keywords": ["feature extraction", "image classification"], "technical_terms": ["However", "Inception", "Previously", "Furthermore", "Even", "Therefore"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03739", "topic": "artificial intelligence", "title": "Penalty-Free SDDP: Feasibility Cuts for Robust Multi-Stage Stochastic Optimization in Energy Planning", "authors": ["Guilherme Freitas", "Luiz Carlos da Costa Junior", "Tiago Andrade", "Alexandre Street"], "first_author": "Guilherme Freitas", "abstract": "Multi-stage decision problems under uncertainty can be efficiently solved with the Stochastic Dual Dynamic Programming (SDDP) algorithm. However, traditional implementations require all stage problems to be feasible. Feasibility is usually enforced by adding slack variables and penalizing them in the objective function, a process that depends on case-specific calibration and often distorts the economic interpretation of results. This paper proposes the Penalty-Free SDDP, an extension that introduces a Future Feasibility Function alongside the traditional Future Cost Function. The new recursion handles infeasibilities automatically, distinguishing between temporary and truly infeasible cases, and propagates feasibility information across stages through dedicated feasibility cuts. The approach was validated in a large-scale deterministic case inspired by the Brazilian hydrothermal system, achieving equivalent feasibility to the benchmark solution while eliminating miscalibrated artificial penalties. Results confirm its robustness and practicality as a foundation for future stochastic, multi-stage applications.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["math.OC"], "url": "http://arxiv.org/abs/2512.03739", "comments": "8 pages, 1 figure, 1 table", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: Penalty-Free SDDP: Feasibility Cuts for Robust Multi-Stage Stochastic Optimization in Energy Planning\nResearch Topic: artificial intelligence\nAuthors: Guilherme Freitas, Luiz Carlos da Costa Junior, Tiago Andrade, Alexandre Street\nAbstract: Multi-stage decision problems under uncertainty can be efficiently solved with the Stochastic Dual Dynamic Programming (SDDP) algorithm. However, traditional implementations require all stage problems to be feasible. Feasibility is usually enforced by adding slack variables and penalizing them in the objective function, a process that depends on case-specific calibration and often distorts the economic interpretation of results. This paper proposes the Penalty-Free SDDP, an extension that introduces a Future Feasibility Function alongside the traditional Future Cost Function. The new recursion handles infeasibilities automatically, distinguishing between temporary and truly infeasible cases, and propagates feasibility information across stages through dedicated feasibility cuts. The approach was validated in a large-scale deterministic case inspired by the Brazilian hydrothermal system, achieving equivalent feasibility to the benchmark solution while eliminating miscalibrated artificial penalties. Results confirm its robustness and practicality as a foundation for future stochastic, multi-stage applications.\nCategories: math.OC\nPublication Date: 2025-12-03\nPaper ID: 2512.03739\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1125, "total_text_length": 1227, "word_count": 153, "sentence_count": 8}, "basic_keywords": ["feasibility", "stage", "function", "future", "multi", "problems", "stochastic", "sddp"], "domain_keywords": [], "technical_terms": ["Stochastic Dual Dynamic Programming", "However", "Brazilian", "Feasibility", "Penalty", "Results", "Future Cost Function", "Free"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03730", "topic": "artificial intelligence", "title": "Out-of-the-box: Black-box Causal Attacks on Object Detectors", "authors": ["Melane Navaratnarajah", "David A. Kelly", "Hana Chockler"], "first_author": "Melane Navaratnarajah", "abstract": "Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03730", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:49.156874", "embedding_text": "Paper Title: Out-of-the-box: Black-box Causal Attacks on Object Detectors\nResearch Topic: artificial intelligence\nAuthors: Melane Navaratnarajah, David A. Kelly, Hana Chockler\nAbstract: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03730\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 60, "abstract_length": 1472, "total_text_length": 1533, "word_count": 221, "sentence_count": 14}, "basic_keywords": ["attacks", "blackcatt", "object", "detectors", "black", "causal", "pixels", "times"], "domain_keywords": [], "technical_terms": ["More", "Adversarial", "Insights", "Existing"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03728", "topic": "artificial intelligence", "title": "AI/ML in 3GPP 5G Advanced - Services and Architecture", "authors": ["Pradnya Taksande", "Shwetha Kiran", "Pranav Jha", "Prasanna Chaporkar"], "first_author": "Pradnya Taksande", "abstract": "The 3rd Generation Partnership Project (3GPP), the standards body for mobile networks, is in the final phase of Release 19 standardization and is beginning Release 20. Artificial Intelligence/ Machine Learning (AI/ML) has brought about a paradigm shift in technology and it is being adopted across industries and verticals. 3GPP has been integrating AI/ML into the 5G advanced system since Release 18. This paper focuses on the AI/ML related technological advancements and features introduced in Release 19 within the Service and System Aspects (SA) Technical specifications group of 3GPP. The advancements relate to two paradigms: (i) enhancements that AI/ML brought to the 5G advanced system (AI for network), e.g. resource optimization, and (ii) enhancements that were made to the 5G system to support AI/ML applications (Network for AI), e.g. image recognition.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.ET", "cs.AI"], "url": "http://arxiv.org/abs/2512.03728", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: AI/ML in 3GPP 5G Advanced - Services and Architecture\nResearch Topic: artificial intelligence\nAuthors: Pradnya Taksande, Shwetha Kiran, Pranav Jha, Prasanna Chaporkar\nAbstract: The 3rd Generation Partnership Project (3GPP), the standards body for mobile networks, is in the final phase of Release 19 standardization and is beginning Release 20. Artificial Intelligence/ Machine Learning (AI/ML) has brought about a paradigm shift in technology and it is being adopted across industries and verticals. 3GPP has been integrating AI/ML into the 5G advanced system since Release 18. This paper focuses on the AI/ML related technological advancements and features introduced in Release 19 within the Service and System Aspects (SA) Technical specifications group of 3GPP. The advancements relate to two paradigms: (i) enhancements that AI/ML brought to the 5G advanced system (AI for network), e.g. resource optimization, and (ii) enhancements that were made to the 5G system to support AI/ML applications (Network for AI), e.g. image recognition.\nCategories: cs.ET, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03728\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 53, "abstract_length": 865, "total_text_length": 919, "word_count": 139, "sentence_count": 10}, "basic_keywords": ["release", "system", "brought", "advanced", "advancements", "enhancements", "network", "generation"], "domain_keywords": [], "technical_terms": ["Generation Partnership Project", "Release", "Service", "Network", "Artificial Intelligence", "Technical", "System Aspects", "Machine Learning"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03722", "topic": "artificial intelligence", "title": "Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks", "authors": ["Lingyi Cai", "Wenjie Fu", "Yuxi Huang", "Ruichen Zhang", "Yinqiu Liu", "Jiawen Kang", "Zehui Xiong", "Tao Jiang", "Dusit Niyato", "Xianbin Wang", "Shiwen Mao", "Xuemin Shen"], "first_author": "Lingyi Cai", "abstract": "Reinforcement Learning (RL) has shown remarkable success in enabling adaptive and data-driven optimization for various applications in wireless networks. However, classical RL suffers from limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. Large Language Models (LLMs) have emerged as a transformative Artificial Intelligence (AI) paradigm with exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which have demonstrated strong potential to enhance classical RL. This paper serves as a comprehensive tutorial on LLM-enhanced RL for wireless networks. We propose a taxonomy to categorize the roles of LLMs into four critical functions: state perceiver, reward designer, decision-maker, and generator. Then, we review existing studies exploring how each role of LLMs enhances different stages of the RL pipeline. Moreover, we provide a series of case studies to illustrate how to design and apply LLM-enhanced RL in low-altitude economy networking, vehicular networks, and space-air-ground integrated networks. Finally, we conclude with a discussion on potential future directions for LLM-enhanced RL and offer insights into its future development in wireless networks.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.NI"], "url": "http://arxiv.org/abs/2512.03722", "comments": "30 pages, 12 figures, survey paper", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks\nResearch Topic: artificial intelligence\nAuthors: Lingyi Cai, Wenjie Fu, Yuxi Huang, Ruichen Zhang, Yinqiu Liu, Jiawen Kang, Zehui Xiong, Tao Jiang, Dusit Niyato, Xianbin Wang, Shiwen Mao, Xuemin Shen\nAbstract: Reinforcement Learning (RL) has shown remarkable success in enabling adaptive and data-driven optimization for various applications in wireless networks. However, classical RL suffers from limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. Large Language Models (LLMs) have emerged as a transformative Artificial Intelligence (AI) paradigm with exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which have demonstrated strong potential to enhance classical RL. This paper serves as a comprehensive tutorial on LLM-enhanced RL for wireless networks. We propose a taxonomy to categorize the roles of LLMs into four critical functions: state perceiver, reward designer, decision-maker, and generator. Then, we review existing studies exploring how each role of LLMs enhances different stages of the RL pipeline. Moreover, we provide a series of case studies to illustrate how to design and apply LLM-enhanced RL in low-altitude economy networking, vehicular networks, and space-air-ground integrated networks. Finally, we conclude with a discussion on potential future directions for LLM-enhanced RL and offer insights into its future development in wireless networks.\nCategories: cs.NI\nPublication Date: 2025-12-03\nPaper ID: 2512.03722\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1292, "total_text_length": 1379, "word_count": 182, "sentence_count": 9}, "basic_keywords": ["networks", "wireless", "llms", "enhanced", "learning", "classical", "generalization", "potential"], "domain_keywords": [], "technical_terms": ["However", "Finally", "Artificial Intelligence", "Large Language Models", "Reinforcement Learning", "Moreover", "Then"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03720", "topic": "artificial intelligence", "title": "Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs", "authors": ["Tengyun Ma", "Jiaqi Yao", "Daojing He", "Shihao Peng", "Yu Li", "Shaohui Liu", "Zhuotao Tian"], "first_author": "Tengyun Ma", "abstract": "Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CR", "cs.AI"], "url": "http://arxiv.org/abs/2512.03720", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs\nResearch Topic: artificial intelligence\nAuthors: Tengyun Ma, Jiaqi Yao, Daojing He, Shihao Peng, Yu Li, Shaohui Liu, Zhuotao Tian\nAbstract: Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.\nCategories: cs.CR, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03720\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1364, "total_text_length": 1440, "word_count": 172, "sentence_count": 10}, "basic_keywords": ["instruction", "cahl", "vulnerabilities", "models", "tool", "completion", "attack", "model"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Tool", "Completion Attack", "Large Language Models", "Aware Hierarchical Learning", "Completion", "Context"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03719", "topic": "artificial intelligence", "title": "Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing", "authors": ["Seyed Mohammad Azimi-Abarghouyi", "Carlo Fischione", "Kaibin Huang"], "first_author": "Seyed Mohammad Azimi-Abarghouyi", "abstract": "Over-the-Air Federated Learning (AirFL) is an emerging paradigm that tightly integrates wireless signal processing and distributed machine learning to enable scalable AI at the network edge. By leveraging the superposition property of wireless signals, AirFL performs communication and model aggregation of the learning process simultaneously, significantly reducing latency, bandwidth, and energy consumption. This article offers a tutorial treatment of AirFL, presenting a novel classification into three design approaches: CSIT-aware, blind, and weighted AirFL. We provide a comprehensive guide to theoretical foundations, performance analysis, complexity considerations, practical limitations, and prospective research directions.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.IT", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03719", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing\nResearch Topic: artificial intelligence\nAuthors: Seyed Mohammad Azimi-Abarghouyi, Carlo Fischione, Kaibin Huang\nAbstract: Over-the-Air Federated Learning (AirFL) is an emerging paradigm that tightly integrates wireless signal processing and distributed machine learning to enable scalable AI at the network edge. By leveraging the superposition property of wireless signals, AirFL performs communication and model aggregation of the learning process simultaneously, significantly reducing latency, bandwidth, and energy consumption. This article offers a tutorial treatment of AirFL, presenting a novel classification into three design approaches: CSIT-aware, blind, and weighted AirFL. We provide a comprehensive guide to theoretical foundations, performance analysis, complexity considerations, practical limitations, and prospective research directions.\nCategories: cs.IT, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03719\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 734, "total_text_length": 812, "word_count": 100, "sentence_count": 5}, "basic_keywords": ["airfl", "learning", "wireless", "over", "federated", "emerging", "paradigm", "tightly"], "domain_keywords": [], "technical_terms": ["Over", "Air Federated Learning"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03718", "topic": "artificial intelligence", "title": "Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity", "authors": ["Robert Ganian", "Hung P. Hoang", "Simon Wietheger"], "first_author": "Robert Ganian", "abstract": "We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.DS", "cs.AI"], "url": "http://arxiv.org/abs/2512.03718", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity\nResearch Topic: artificial intelligence\nAuthors: Robert Ganian, Hung P. Hoang, Simon Wietheger\nAbstract: We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most $k$ values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices.\nCategories: cs.DS, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03718\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 854, "total_text_length": 932, "word_count": 129, "sentence_count": 5}, "basic_keywords": ["problem", "fair", "means", "clustering", "fixed", "parameter", "algorithm", "instances"], "domain_keywords": [], "technical_terms": ["While"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.75, "quality_tier": "medium"}
{"paper_id": "2512.03709", "topic": "artificial intelligence", "title": "The Effect of High-Speed Rail Connectivity on Capital Market Earnings Forecast Error: Evidence from the Chinese Stock Market", "authors": ["Shilong Han"], "first_author": "Shilong Han", "abstract": "This study examines how China's high-speed rail (HSR) expansion affects analyst earnings forecast errors from an economic information friction perspective. Using firm-year panel data from 2008-2019, a period that covers HSR's early introduction and rapid nationwide rollout, the findings show that analysts' relative earnings forecast errors (RFE) decline significantly only after firms' cities become connected by high-speed rail. The placebo test, which artificially shifts HSR connectivity 3 years earlier than the actual opening year, yields an insignificant DID coefficient, rejecting the possibility that forecast errors were improving before the infrastructure shock. This supports the conclusion that forecast error reduction is linked to real geographic accessibility improvements rather than coincidence, pre-existing trends, or analyst anticipation. Economically, the study highlights that HSR reduces analysts' costs of gathering private, incremental information, particularly soft information obtained via plant or management visits. The rail network does not directly alter firms' internal capital allocation or earnings generation paths, but it lowers spatial barriers to information collection, enabling analysts to update EPS expectations under reduced travel friction. This work provides intuitive evidence that geography and mobility improvements contribute to forecasting accuracy in China's emerging, decentralized capital market corridors, and it encourages future research to consider transport accessibility as an exogenous information cost shock rather than an internal firm-capital shock.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["q-fin.GN"], "url": "http://arxiv.org/abs/2512.03709", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: The Effect of High-Speed Rail Connectivity on Capital Market Earnings Forecast Error: Evidence from the Chinese Stock Market\nResearch Topic: artificial intelligence\nAuthors: Shilong Han\nAbstract: This study examines how China's high-speed rail (HSR) expansion affects analyst earnings forecast errors from an economic information friction perspective. Using firm-year panel data from 2008-2019, a period that covers HSR's early introduction and rapid nationwide rollout, the findings show that analysts' relative earnings forecast errors (RFE) decline significantly only after firms' cities become connected by high-speed rail. The placebo test, which artificially shifts HSR connectivity 3 years earlier than the actual opening year, yields an insignificant DID coefficient, rejecting the possibility that forecast errors were improving before the infrastructure shock. This supports the conclusion that forecast error reduction is linked to real geographic accessibility improvements rather than coincidence, pre-existing trends, or analyst anticipation. Economically, the study highlights that HSR reduces analysts' costs of gathering private, incremental information, particularly soft information obtained via plant or management visits. The rail network does not directly alter firms' internal capital allocation or earnings generation paths, but it lowers spatial barriers to information collection, enabling analysts to update EPS expectations under reduced travel friction. This work provides intuitive evidence that geography and mobility improvements contribute to forecasting accuracy in China's emerging, decentralized capital market corridors, and it encourages future research to consider transport accessibility as an exogenous information cost shock rather than an internal firm-capital shock.\nCategories: q-fin.GN\nPublication Date: 2025-12-03\nPaper ID: 2512.03709\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 124, "abstract_length": 1614, "total_text_length": 1739, "word_count": 230, "sentence_count": 8}, "basic_keywords": ["information", "forecast", "rail", "earnings", "errors", "analysts", "shock", "capital"], "domain_keywords": [], "technical_terms": ["Using", "Economically", "China"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.03696", "topic": "artificial intelligence", "title": "Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns", "authors": ["Mohammad Doost", "Mohammad Manthouri"], "first_author": "Mohammad Doost", "abstract": "We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03696", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns\nResearch Topic: artificial intelligence\nAuthors: Mohammad Doost, Mohammad Manthouri\nAbstract: We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03696\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1431, "total_text_length": 1510, "word_count": 181, "sentence_count": 9}, "basic_keywords": ["quantum", "topological", "qtgnn", "graph", "financial", "fraud", "learning", "framework"], "domain_keywords": [], "technical_terms": ["Simulations", "Elliptic", "Optimized", "Rigorous"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03682", "topic": "artificial intelligence", "title": "Knowing oneself with and through AI: From self-tracking to chatbots", "authors": ["Lucy Osler"], "first_author": "Lucy Osler", "abstract": "This chapter examines how algorithms and artificial intelligence are transforming our practices of self-knowledge, self-understanding, and self-narration. Drawing on frameworks from distributed cognition, I analyse three key domains where AI shapes how and what we come to know about ourselves: self-tracking applications, technologically-distributed autobiographical memories, and narrative co-construction with Large Language Models (LLMs). While self-tracking devices promise enhanced self-knowledge through quantified data, they also impose particular frameworks that can crowd out other forms of self-understanding and promote self-optimization. Digital technologies increasingly serve as repositories for our autobiographical memories and self-narratives, offering benefits such as detailed record-keeping and scaffolding during difficult periods, but also creating vulnerabilities to algorithmic manipulation. Finally, conversational AI introduces new possibilities for interactive narrative construction that mimics interpersonal dialogue. While LLMs can provide valuable support for self-exploration, they also present risks of narrative deference and the construction of self-narratives that are detached from reality.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CY"], "url": "http://arxiv.org/abs/2512.03682", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Knowing oneself with and through AI: From self-tracking to chatbots\nResearch Topic: artificial intelligence\nAuthors: Lucy Osler\nAbstract: This chapter examines how algorithms and artificial intelligence are transforming our practices of self-knowledge, self-understanding, and self-narration. Drawing on frameworks from distributed cognition, I analyse three key domains where AI shapes how and what we come to know about ourselves: self-tracking applications, technologically-distributed autobiographical memories, and narrative co-construction with Large Language Models (LLMs). While self-tracking devices promise enhanced self-knowledge through quantified data, they also impose particular frameworks that can crowd out other forms of self-understanding and promote self-optimization. Digital technologies increasingly serve as repositories for our autobiographical memories and self-narratives, offering benefits such as detailed record-keeping and scaffolding during difficult periods, but also creating vulnerabilities to algorithmic manipulation. Finally, conversational AI introduces new possibilities for interactive narrative construction that mimics interpersonal dialogue. While LLMs can provide valuable support for self-exploration, they also present risks of narrative deference and the construction of self-narratives that are detached from reality.\nCategories: cs.CY\nPublication Date: 2025-12-03\nPaper ID: 2512.03682\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 1228, "total_text_length": 1296, "word_count": 157, "sentence_count": 7}, "basic_keywords": ["self", "narrative", "construction", "knowledge", "understanding", "frameworks", "distributed", "tracking"], "domain_keywords": [], "technical_terms": ["While", "Finally", "Large Language Models", "Drawing", "Digital"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03671", "topic": "artificial intelligence", "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context", "authors": ["Beatrice Savoldi", "Giuseppe Attanasio", "Olga Gorodetskaya", "Marta Marchiori Manerba", "Elisa Bassignana", "Silvia Casola", "Matteo Negri", "Tommaso Caselli", "Luisa Bentivogli", "Alan Ramponi", "Arianna Muti", "Nicoletta Balbo", "Debora Nozza"], "first_author": "Beatrice Savoldi", "abstract": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.03671", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context\nResearch Topic: artificial intelligence\nAuthors: Beatrice Savoldi, Giuseppe Attanasio, Olga Gorodetskaya, Marta Marchiori Manerba, Elisa Bassignana, Silvia Casola, Matteo Negri, Tommaso Caselli, Luisa Bentivogli, Alan Ramponi, Arianna Muti, Nicoletta Balbo, Debora Nozza\nAbstract: The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.\nCategories: cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.03671\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1477, "total_text_length": 1570, "word_count": 223, "sentence_count": 9}, "basic_keywords": ["genai", "adoption", "digital", "literacy", "technologies", "particularly", "while", "risk"], "domain_keywords": [], "technical_terms": ["Italian", "While", "Crucially", "Italy", "Artificial Intelligence", "Overall", "Moreover"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03666", "topic": "artificial intelligence", "title": "ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos", "authors": ["Qi'ao Xu", "Tianwen Qian", "Yuqian Fu", "Kailing Li", "Yang Jiao", "Jiacheng Zhang", "Xiaoling Wang", "Liang He"], "first_author": "Qi'ao Xu", "abstract": "A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce , the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) , which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) , where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) , where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: {https://github.com/qaxuDev/ToG-Bench}..", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03666", "comments": "26 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos\nResearch Topic: artificial intelligence\nAuthors: Qi'ao Xu, Tianwen Qian, Yuqian Fu, Kailing Li, Yang Jiao, Jiacheng Zhang, Xiaoling Wang, Liang He\nAbstract: A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \\textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \\textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \\textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \\textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \\href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03666\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1673, "total_text_length": 1745, "word_count": 217, "sentence_count": 10}, "basic_keywords": ["task", "grounding", "objects", "object", "oriented", "embodied", "stvg", "bench"], "domain_keywords": [], "technical_terms": ["Extensive", "Spatio", "Despite", "Bench", "Built", "Data", "Temporal Video Grounding"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03661", "topic": "artificial intelligence", "title": "Dynamically Scaled Activation Steering", "authors": ["Alex Ferrando", "Xavier Suau", "Jordi Gonzàlez", "Pau Rodriguez"], "first_author": "Alex Ferrando", "abstract": "Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03661", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Dynamically Scaled Activation Steering\nResearch Topic: artificial intelligence\nAuthors: Alex Ferrando, Xavier Suau, Jordi Gonzàlez, Pau Rodriguez\nAbstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03661\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 38, "abstract_length": 1344, "total_text_length": 1383, "word_count": 184, "sentence_count": 10}, "basic_keywords": ["steering", "dsas", "method", "existing", "activation", "behavior", "toxicity", "mitigation"], "domain_keywords": [], "technical_terms": ["However", "Finally", "Pareto", "Dynamically Scaled Activation Steering", "When", "Activation"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03640", "topic": "artificial intelligence", "title": "MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms", "authors": ["Jiahao Zhang", "Xiao Zhao", "Guangyu Gao"], "first_author": "Jiahao Zhang", "abstract": "Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03640", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms\nResearch Topic: artificial intelligence\nAuthors: Jiahao Zhang, Xiao Zhao, Guangyu Gao\nAbstract: Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03640\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 113, "abstract_length": 1854, "total_text_length": 1968, "word_count": 251, "sentence_count": 13}, "basic_keywords": ["small", "spatial", "detection", "remote", "sensing", "selection", "mksnet", "attention"], "domain_keywords": ["object detection", "attention mechanisms"], "technical_terms": ["These", "However", "Kernel Selection Network", "Additionally", "Furthermore", "Empirical", "Kernel Selection", "Simultaneously"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04131", "topic": "artificial intelligence", "title": "Artificial Intelligence / Human Intelligence: Who Controls Whom?", "authors": ["Charlotte Jacquemot"], "first_author": "Charlotte Jacquemot", "abstract": "Using the example of the film 2001: A Space Odyssey, this chapter illustrates the challenges posed by an AI capable of making decisions that go against human interests. But are human decisions always rational and ethical? In reality, the cognitive decision-making process is influenced by cognitive biases that affect our behavior and choices. AI not only reproduces these biases, but can also exploit them, with the potential to shape our decisions and judgments. Behind IA algorithms, there are sometimes individuals who show little concern for fundamental rights and impose their own rules. To address the ethical and societal challenges raised by AI and its governance, the regulation of digital platforms and education are keys levers. Regulation must reflect ethical, legal, and political choices, while education must strengthen digital literacy and teach people to make informed and critical choices when facing digital technologies.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CY", "cs.AI"], "url": "http://arxiv.org/abs/2512.04131", "comments": "in French language", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Artificial Intelligence / Human Intelligence: Who Controls Whom?\nResearch Topic: artificial intelligence\nAuthors: Charlotte Jacquemot\nAbstract: Using the example of the film 2001: A Space Odyssey, this chapter illustrates the challenges posed by an AI capable of making decisions that go against human interests. But are human decisions always rational and ethical? In reality, the cognitive decision-making process is influenced by cognitive biases that affect our behavior and choices. AI not only reproduces these biases, but can also exploit them, with the potential to shape our decisions and judgments. Behind IA algorithms, there are sometimes individuals who show little concern for fundamental rights and impose their own rules. To address the ethical and societal challenges raised by AI and its governance, the regulation of digital platforms and education are keys levers. Regulation must reflect ethical, legal, and political choices, while education must strengthen digital literacy and teach people to make informed and critical choices when facing digital technologies.\nCategories: cs.CY, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.04131\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 941, "total_text_length": 1006, "word_count": 150, "sentence_count": 9}, "basic_keywords": ["decisions", "ethical", "choices", "digital", "challenges", "making", "human", "cognitive"], "domain_keywords": [], "technical_terms": ["Regulation", "Using", "Behind", "Space Odyssey"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03634", "topic": "artificial intelligence", "title": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment", "authors": ["Ahmad Aghaebrahimian"], "first_author": "Ahmad Aghaebrahimian", "abstract": "Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.03634", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment\nResearch Topic: artificial intelligence\nAuthors: Ahmad Aghaebrahimian\nAbstract: Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03634\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1075, "total_text_length": 1152, "word_count": 150, "sentence_count": 9}, "basic_keywords": ["factual", "language", "domains", "clinical", "evaluation", "consistency", "propose", "assessment"], "domain_keywords": [], "technical_terms": ["Additionally", "Unlike", "Existing", "Large Language Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03627", "topic": "artificial intelligence", "title": "MemVerse: Multimodal Memory for Lifelong Learning Agents", "authors": ["Junming Liu", "Yifei Sun", "Weihua Cheng", "Haodong Lei", "Yirong Chen", "Licheng Wen", "Xuemeng Yang", "Daocheng Fu", "Pinlong Cai", "Nianchen Deng", "Yi Yu", "Shuyue Hu", "Botian Shi", "Ding Wang"], "first_author": "Junming Liu", "abstract": "Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03627", "comments": "11 pages, 2 figures, 2 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: MemVerse: Multimodal Memory for Lifelong Learning Agents\nResearch Topic: artificial intelligence\nAuthors: Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, Ding Wang\nAbstract: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03627\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 56, "abstract_length": 1265, "total_text_length": 1322, "word_count": 156, "sentence_count": 8}, "basic_keywords": ["memory", "multimodal", "memverse", "agents", "long", "term", "remember", "experiences"], "domain_keywords": [], "technical_terms": ["Without", "Extensive", "Despite"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03623", "topic": "artificial intelligence", "title": "The promising potential of vision language models for the generation of textual weather forecasts", "authors": ["Edward C. C. Steele", "Dinesh Mane", "Emilio Monti", "Luis Orus", "Rebecca Chantrill-Cheyette", "Matthew Couch", "Kirstine I. Dale", "Simon Eaton", "Govindarajan Rangarajan", "Amir Majlesi", "Steven Ramsdale", "Michael Sharpe", "Craig Smith", "Jonathan Smith", "Rebecca Yates", "Holly Ellis", "Charles Ewen"], "first_author": "Edward C. C. Steele", "abstract": "Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "url": "http://arxiv.org/abs/2512.03623", "comments": "7 pages, 2 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: The promising potential of vision language models for the generation of textual weather forecasts\nResearch Topic: artificial intelligence\nAuthors: Edward C. C. Steele, Dinesh Mane, Emilio Monti, Luis Orus, Rebecca Chantrill-Cheyette, Matthew Couch, Kirstine I. Dale, Simon Eaton, Govindarajan Rangarajan, Amir Majlesi, Steven Ramsdale, Michael Sharpe, Craig Smith, Jonathan Smith, Rebecca Yates, Holly Ellis, Charles Ewen\nAbstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.\nCategories: cs.LG, cs.AI, physics.ao-ph\nPublication Date: 2025-12-03\nPaper ID: 2512.03623\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 525, "total_text_length": 623, "word_count": 83, "sentence_count": 4}, "basic_keywords": ["promising", "weather", "despite", "capability", "multimodal", "foundation", "models", "application"], "domain_keywords": [], "technical_terms": ["These", "Shipping Forecast", "Despite"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03620", "topic": "artificial intelligence", "title": "SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting", "authors": ["Hanxiu Zhang", "Yue Zheng"], "first_author": "Hanxiu Zhang", "abstract": "The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.03620", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting\nResearch Topic: artificial intelligence\nAuthors: Hanxiu Zhang, Yue Zheng\nAbstract: The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.\nCategories: cs.CR, cs.AI, cs.CL, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03620\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1196, "total_text_length": 1273, "word_count": 159, "sentence_count": 8}, "basic_keywords": ["based", "self", "protection", "while", "fingerprinting", "false", "attacks", "weight"], "domain_keywords": [], "technical_terms": ["Intellectual Property", "Experimental", "While", "Large Language Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03608", "topic": "artificial intelligence", "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing", "authors": ["Lishuo Deng", "Shaojie Xu", "Jinwu Chen", "Changwei Yan", "Jiajie Wang", "Zhe Jiang", "Weiwei Shan"], "first_author": "Lishuo Deng", "abstract": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties. We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\)/1.94\\(\\)/2.05\\(\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AR", "cs.AI", "cs.ET"], "url": "http://arxiv.org/abs/2512.03608", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing\nResearch Topic: artificial intelligence\nAuthors: Lishuo Deng, Shaojie Xu, Jinwu Chen, Changwei Yan, Jiajie Wang, Zhe Jiang, Weiwei Shan\nAbstract: Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.   We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.\nCategories: cs.AR, cs.AI, cs.ET\nPublication Date: 2025-12-03\nPaper ID: 2512.03608\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1826, "total_text_length": 1920, "word_count": 260, "sentence_count": 14}, "basic_keywords": ["flash", "cache", "dram", "kvnand", "weight", "context", "llms", "cost"], "domain_keywords": [], "technical_terms": ["Attempts", "However", "These", "Evaluations", "Deploying", "Recent"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03607", "topic": "artificial intelligence", "title": "DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization", "authors": ["Yusen Wu", "Xiaotie Deng"], "first_author": "Yusen Wu", "abstract": "This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints. Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03607", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization\nResearch Topic: artificial intelligence\nAuthors: Yusen Wu, Xiaotie Deng\nAbstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.   Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03607\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 132, "abstract_length": 1816, "total_text_length": 1949, "word_count": 230, "sentence_count": 13}, "basic_keywords": ["framework", "business", "optimization", "real", "economic", "unstructured", "constraints", "retail"], "domain_keywords": [], "technical_terms": ["Addressing", "Finally", "Then"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03582", "topic": "artificial intelligence", "title": "Fine-grained Narrative Classification in Biased News Articles", "authors": ["Zeba Afroz", "Harsh Vardhan", "Pawan Bhakuni", "Aanchal Punia", "Rajdeep Kumar", "Md. Shad Akhtar"], "first_author": "Zeba Afroz", "abstract": "Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.03582", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Fine-grained Narrative Classification in Biased News Articles\nResearch Topic: artificial intelligence\nAuthors: Zeba Afroz, Harsh Vardhan, Pawan Bhakuni, Aanchal Punia, Rajdeep Kumar, Md. Shad Akhtar\nAbstract: Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03582\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 1534, "total_text_length": 1596, "word_count": 197, "sentence_count": 12}, "basic_keywords": ["persuasive", "narrative", "fine", "grained", "classification", "ideological", "article", "bias"], "domain_keywords": [], "technical_terms": ["Each", "Narratives", "Farmers", "They", "Indian"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03578", "topic": "artificial intelligence", "title": "When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate", "authors": ["Florent Forest", "Amaury Wei", "Olga Fink"], "first_author": "Florent Forest", "abstract": "Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data. To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03578", "comments": "12 pages, 5 figures, 4 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate\nResearch Topic: artificial intelligence\nAuthors: Florent Forest, Amaury Wei, Olga Fink\nAbstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.   To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03578\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 119, "abstract_length": 1738, "total_text_length": 1858, "word_count": 243, "sentence_count": 13}, "basic_keywords": ["time", "series", "predictions", "concepts", "tser", "features", "regression", "input"], "domain_keywords": [], "technical_terms": ["However", "Predictions", "Each", "Post", "Time", "Time Series", "Recently", "Mask"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03571", "topic": "artificial intelligence", "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "authors": ["Zhening Li", "Armando Solar-Lezama", "Yisong Yue", "Stephan Zheng"], "first_author": "Zhening Li", "abstract": "We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce \"probabilistic angelic nondeterminism\" (\"PAN\"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "cs.LG", "cs.PL"], "url": "http://arxiv.org/abs/2512.03571", "comments": "65 pages, 2 figures, published in NeurIPS 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths\nResearch Topic: artificial intelligence\nAuthors: Zhening Li, Armando Solar-Lezama, Yisong Yue, Stephan Zheng\nAbstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce \"probabilistic angelic nondeterminism\" (\"PAN\"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.\nCategories: cs.AI, cs.LG, cs.PL\nPublication Date: 2025-12-03\nPaper ID: 2512.03571\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 911, "total_text_length": 991, "word_count": 139, "sentence_count": 8}, "basic_keywords": ["agent", "programming", "workflow", "inference", "time", "introduce", "search", "programmer"], "domain_keywords": [], "technical_terms": ["Current", "Python"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03570", "topic": "artificial intelligence", "title": "Machine Learning to Predict Slot Usage in TSCH Wireless Sensor Networks", "authors": ["Stefano Scanzio", "Gabriele Formis", "Tullio Facchinetti", "Gianluca Cena"], "first_author": "Stefano Scanzio", "abstract": "Wireless sensor networks (WSNs) are employed across a wide range of industrial applications where ultra-low power consumption is a critical prerequisite. At the same time, these systems must maintain a certain level of determinism to ensure reliable and predictable operation. In this view, time slotted channel hopping (TSCH) is a communication technology that meets both conditions, making it an attractive option for its usage in industrial WSNs. This work proposes the use of machine learning to learn the traffic pattern generated in networks based on the TSCH protocol, in order to turn nodes into a deep sleep state when no transmission is planned and thus to improve the energy efficiency of the WSN. The ability of machine learning models to make good predictions at different network levels in a typical tree network topology was analyzed in depth, showing how their capabilities degrade while approaching the root of the tree. The application of these models on simulated data based on an accurate modeling of wireless sensor nodes indicates that the investigated algorithms can be suitably used to further and substantially reduce the power consumption of a TSCH network.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.NI", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03570", "comments": "preprint accepted, 8 pages, 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: Machine Learning to Predict Slot Usage in TSCH Wireless Sensor Networks\nResearch Topic: artificial intelligence\nAuthors: Stefano Scanzio, Gabriele Formis, Tullio Facchinetti, Gianluca Cena\nAbstract: Wireless sensor networks (WSNs) are employed across a wide range of industrial applications where ultra-low power consumption is a critical prerequisite. At the same time, these systems must maintain a certain level of determinism to ensure reliable and predictable operation. In this view, time slotted channel hopping (TSCH) is a communication technology that meets both conditions, making it an attractive option for its usage in industrial WSNs. This work proposes the use of machine learning to learn the traffic pattern generated in networks based on the TSCH protocol, in order to turn nodes into a deep sleep state when no transmission is planned and thus to improve the energy efficiency of the WSN. The ability of machine learning models to make good predictions at different network levels in a typical tree network topology was analyzed in depth, showing how their capabilities degrade while approaching the root of the tree. The application of these models on simulated data based on an accurate modeling of wireless sensor nodes indicates that the investigated algorithms can be suitably used to further and substantially reduce the power consumption of a TSCH network.\nCategories: cs.NI, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03570\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1183, "total_text_length": 1255, "word_count": 198, "sentence_count": 7}, "basic_keywords": ["tsch", "network", "wireless", "sensor", "networks", "wsns", "industrial", "power"], "domain_keywords": [], "technical_terms": ["Wireless"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03563", "topic": "artificial intelligence", "title": "State Space Models for Bioacoustics: A comparative Evaluation with Transformers", "authors": ["Chengyu Tang", "Sanjeev Baskiyar"], "first_author": "Chengyu Tang", "abstract": "In this study, we evaluate the efficacy of the Mamba model in the field of bioacoustics. We first pretrain a Mamba-based audio large language model (LLM) on a large corpus of audio data using self-supervised learning. We fine-tune and evaluate BioMamba on the BEANS benchmark, a collection of diverse bioacoustic tasks including classification and detection, and compare its performance and efficiency with multiple baseline models, including AVES, a state-of-the-art Transformer-based model. The results show that BioMamba achieves comparable performance with AVES while consumption significantly less VRAM, demonstrating its potential in this domain.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.SD", "cs.AI"], "url": "http://arxiv.org/abs/2512.03563", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.433983", "embedding_text": "Paper Title: State Space Models for Bioacoustics: A comparative Evaluation with Transformers\nResearch Topic: artificial intelligence\nAuthors: Chengyu Tang, Sanjeev Baskiyar\nAbstract: In this study, we evaluate the efficacy of the Mamba model in the field of bioacoustics. We first pretrain a Mamba-based audio large language model (LLM) on a large corpus of audio data using self-supervised learning. We fine-tune and evaluate BioMamba on the BEANS benchmark, a collection of diverse bioacoustic tasks including classification and detection, and compare its performance and efficiency with multiple baseline models, including AVES, a state-of-the-art Transformer-based model. The results show that BioMamba achieves comparable performance with AVES while consumption significantly less VRAM, demonstrating its potential in this domain.\nCategories: cs.SD, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03563\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 652, "total_text_length": 732, "word_count": 102, "sentence_count": 5}, "basic_keywords": ["model", "evaluate", "mamba", "based", "audio", "large", "biomamba", "including"], "domain_keywords": [], "technical_terms": ["Transformer", "Mamba"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.4}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03560", "topic": "artificial intelligence", "title": "Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks", "authors": ["Gianni Molinari", "Fabio Ciravegna"], "first_author": "Gianni Molinari", "abstract": "Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "cs.MA"], "url": "http://arxiv.org/abs/2512.03560", "comments": "11 pages, 1 figure, 2 tables, Workshop AAAI 2026 agentic AI Benchmarks and Applications for Enterprise Tasks", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks\nResearch Topic: artificial intelligence\nAuthors: Gianni Molinari, Fabio Ciravegna\nAbstract: Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.\nCategories: cs.AI, cs.MA\nPublication Date: 2025-12-03\nPaper ID: 2512.03560\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1784, "total_text_length": 1880, "word_count": 249, "sentence_count": 11}, "basic_keywords": ["react", "agent", "context", "diverse", "large", "tool", "approach", "execution"], "domain_keywords": [], "technical_terms": ["Large Reasoning Model", "Despite", "Crucially", "Furthermore", "Proxy", "Second", "Execution Agent", "First"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03553", "topic": "artificial intelligence", "title": "Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching", "authors": ["Wei Chee Yew", "Hailun Xu", "Sanjay Saha", "Xiaotian Fan", "Hiok Hian Ong", "David Yuchen Wang", "Kanchan Sarkar", "Zhenheng Yang", "Danhui Guan"], "first_author": "Wei Chee Yew", "abstract": "Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03553", "comments": "Accepted at KDD 2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching\nResearch Topic: artificial intelligence\nAuthors: Wei Chee Yew, Hailun Xu, Sanjay Saha, Xiaotian Fan, Hiok Hian Ong, David Yuchen Wang, Kanchan Sarkar, Zhenheng Yang, Danhui Guan\nAbstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03553\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1188, "total_text_length": 1305, "word_count": 171, "sentence_count": 8}, "basic_keywords": ["multimodal", "content", "moderation", "large", "scale", "violations", "both", "user"], "domain_keywords": [], "technical_terms": ["These", "Content", "Multimodal", "Large"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03549", "topic": "artificial intelligence", "title": "PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks", "authors": ["Yuki Orimo", "Iori Kurata", "Hodaka Mori", "Ryuhei Okuno", "Ryohto Sawada", "Daisuke Okanohara"], "first_author": "Yuki Orimo", "abstract": "We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03549", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks\nResearch Topic: artificial intelligence\nAuthors: Yuki Orimo, Iori Kurata, Hodaka Mori, Ryuhei Okuno, Ryohto Sawada, Daisuke Okanohara\nAbstract: We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03549\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1291, "total_text_length": 1383, "word_count": 177, "sentence_count": 9}, "basic_keywords": ["parc", "self", "agent", "tasks", "feedback", "science", "execution", "computational"], "domain_keywords": [], "technical_terms": ["These", "In Kaggle"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6000000000000001}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03548", "topic": "artificial intelligence", "title": "A Learning-based Control Methodology for Transitioning VTOL UAVs", "authors": ["Zexin Lin", "Yebin Zhong", "Hanwen Wan", "Jiu Cheng", "Zhenglong Sun", "Xiaoqiang Ji"], "first_author": "Zexin Lin", "abstract": "Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.AI"], "url": "http://arxiv.org/abs/2512.03548", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: A Learning-based Control Methodology for Transitioning VTOL UAVs\nResearch Topic: artificial intelligence\nAuthors: Zexin Lin, Yebin Zhong, Hanwen Wan, Jiu Cheng, Zhenglong Sun, Xiaoqiang Ji\nAbstract: Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.\nCategories: cs.RO, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03548\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 996, "total_text_length": 1061, "word_count": 140, "sentence_count": 6}, "basic_keywords": ["transition", "control", "development", "during", "position", "controller", "method", "poses"], "domain_keywords": [], "technical_terms": ["Landing Unmanned Aerial Vehicle", "Besides", "Transition", "Current", "Vertical Take"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03542", "topic": "artificial intelligence", "title": "V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention", "authors": ["Nan Sun", "Zhenyu Zhang", "Xixun Lin", "Kun Wang", "Yanmin Shang", "Naibin Gu", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Haifeng Wang", "Yanan Cao"], "first_author": "Nan Sun", "abstract": "Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on \"how to intervene\" but overlooking the prerequisite \"when to intervene\", which leads to the \"over-intervention\" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03542", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention\nResearch Topic: artificial intelligence\nAuthors: Nan Sun, Zhenyu Zhang, Xixun Lin, Kun Wang, Yanmin Shang, Naibin Gu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang, Yanan Cao\nAbstract: Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on \"how to intervene\" but overlooking the prerequisite \"when to intervene\", which leads to the \"over-intervention\" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03542\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 107, "abstract_length": 1355, "total_text_length": 1463, "word_count": 186, "sentence_count": 7}, "basic_keywords": ["visual", "neglect", "hallucinations", "language", "models", "mllms", "vision", "input"], "domain_keywords": [], "technical_terms": ["Extensive", "Visual Recall Intervenor", "Existing", "Visual Neglect Detector", "Multimodal Large Language Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03541", "topic": "artificial intelligence", "title": "Toward AI-Ready Medical Imaging Data", "authors": ["Milen Nikolov", "Edilberto Amorim", "J Harry Caufield", "Nayoon Gim", "Nomi L Harris", "Jared Houghtaling", "Xiang Li", "Danielle Morrison", "Anaïs Rameau", "Jamie Shaffer", "Hari Trivedi", "Monica C Munoz-Torres"], "first_author": "Milen Nikolov", "abstract": "Medical imaging data plays a vital role in disease diagnosis, monitoring, and clinical research discovery. Biomedical data managers and clinical researchers must navigate a complex landscape of medical imaging infrastructure, input/output tools and data reliability workflow configurations taking months to operationalize. While standard formats exist for medical imaging data, standard operating procedures (SOPs) for data management are lacking. These data management SOPs are key for developing Findable, Accessible, Interoperable, and Reusable (FAIR) data, a prerequisite for AI-ready datasets. The National Institutes of Health (NIH) Bridge to Artificial Intelligence (Bridge2AI) Standards Working Group members and domain-expert stakeholders from the Bridge2AI Grand Challenges teams developed data management SOPs for the Digital Imaging and Communications in Medicine (DICOM) format. We describe novel SOPs applying to both static and cutting edge video imaging modalities. We emphasize steps required for centralized data aggregation, validation, and de-identification, including a review of new defacing methods for facial DICOM scans, anticipating adversarial AI/ML data re-identification methods. Data management vignettes based on Bridge2AI datasets include example parameters for efficient capture of a wide modality spectrum, including datasets from new ophthalmology retinal scans DICOM modalities.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["q-bio.OT"], "url": "http://arxiv.org/abs/2512.03541", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Toward AI-Ready Medical Imaging Data\nResearch Topic: artificial intelligence\nAuthors: Milen Nikolov, Edilberto Amorim, J Harry Caufield, Nayoon Gim, Nomi L Harris, Jared Houghtaling, Xiang Li, Danielle Morrison, Anaïs Rameau, Jamie Shaffer, Hari Trivedi, Monica C Munoz-Torres\nAbstract: Medical imaging data plays a vital role in disease diagnosis, monitoring, and clinical research discovery. Biomedical data managers and clinical researchers must navigate a complex landscape of medical imaging infrastructure, input/output tools and data reliability workflow configurations taking months to operationalize.   While standard formats exist for medical imaging data, standard operating procedures (SOPs) for data management are lacking. These data management SOPs are key for developing Findable, Accessible, Interoperable, and Reusable (FAIR) data, a prerequisite for AI-ready datasets.   The National Institutes of Health (NIH) Bridge to Artificial Intelligence (Bridge2AI) Standards Working Group members and domain-expert stakeholders from the Bridge2AI Grand Challenges teams developed data management SOPs for the Digital Imaging and Communications in Medicine (DICOM) format.   We describe novel SOPs applying to both static and cutting edge video imaging modalities. We emphasize steps required for centralized data aggregation, validation, and de-identification, including a review of new defacing methods for facial DICOM scans, anticipating adversarial AI/ML data re-identification methods. Data management vignettes based on Bridge2AI datasets include example parameters for efficient capture of a wide modality spectrum, including datasets from new ophthalmology retinal scans DICOM modalities.\nCategories: q-bio.OT\nPublication Date: 2025-12-03\nPaper ID: 2512.03541\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 36, "abstract_length": 1414, "total_text_length": 1451, "word_count": 190, "sentence_count": 9}, "basic_keywords": ["data", "imaging", "sops", "management", "medical", "datasets", "dicom", "clinical"], "domain_keywords": ["medical imaging"], "technical_terms": ["These", "Biomedical", "Interoperable", "While", "Communications", "Reusable", "Medical", "Artificial Intelligence"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03540", "topic": "artificial intelligence", "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation", "authors": ["Ruoxuan Zhang", "Bin Wen", "Hongxia Xie", "Yi Yao", "Songhan Zuo", "Jian-Yu Jiang-Lin", "Hong-Han Shuai", "Wen-Huang Cheng"], "first_author": "Ruoxuan Zhang", "abstract": "Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03540", "comments": "Accepted by ACM Multimedia 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation\nResearch Topic: artificial intelligence\nAuthors: Ruoxuan Zhang, Bin Wen, Hongxia Xie, Yi Yao, Songhan Zuo, Jian-Yu Jiang-Lin, Hong-Han Shuai, Wen-Huang Cheng\nAbstract: Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03540\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1554, "total_text_length": 1643, "word_count": 209, "sentence_count": 8}, "basic_keywords": ["step", "recipe", "image", "illustration", "instructions", "framework", "cooking", "both"], "domain_keywords": [], "technical_terms": ["While", "Cooking", "Regional Control", "Flexible", "Cross", "Additionally", "Step", "Step Consistency Control"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03534", "topic": "artificial intelligence", "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation", "authors": ["Subin Kim", "Sangwoo Mo", "Mamshad Nayeem Rizve", "Yiran Xu", "Difan Liu", "Jinwoo Shin", "Tobias Hinz"], "first_author": "Subin Kim", "abstract": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03534", "comments": "Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation\nResearch Topic: artificial intelligence\nAuthors: Subin Kim, Sangwoo Mo, Mamshad Nayeem Rizve, Yiran Xu, Difan Liu, Jinwoo Shin, Tobias Hinz\nAbstract: Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03534\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1508, "total_text_length": 1589, "word_count": 217, "sentence_count": 15}, "basic_keywords": ["prompt", "visuals", "alignment", "generated", "text", "visual", "generation", "inference"], "domain_keywords": [], "technical_terms": ["Extensive", "These", "Scaling", "Achieving", "Visualizations", "Inference", "Prompt Redesign"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03528", "topic": "artificial intelligence", "title": "Multi-Agent Reinforcement Learning with Communication-Constrained Priors", "authors": ["Guang Yang", "Tianpei Yang", "Jingwen Qiao", "Yanqing Wu", "Jing Huo", "Xingguo Chen", "Yang Gao"], "first_author": "Guang Yang", "abstract": "Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI", "cs.MA"], "url": "http://arxiv.org/abs/2512.03528", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Multi-Agent Reinforcement Learning with Communication-Constrained Priors\nResearch Topic: artificial intelligence\nAuthors: Guang Yang, Tianpei Yang, Jingwen Qiao, Yanqing Wu, Jing Huo, Xingguo Chen, Yang Gao\nAbstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.\nCategories: cs.AI, cs.MA\nPublication Date: 2025-12-03\nPaper ID: 2512.03528\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1081, "total_text_length": 1154, "word_count": 144, "sentence_count": 8}, "basic_keywords": ["communication", "learning", "multi", "agent", "scenarios", "lossy", "constrained", "messages"], "domain_keywords": [], "technical_terms": ["However", "Based", "Communication", "Existing", "Finally", "Additionally"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03519", "topic": "artificial intelligence", "title": "Left shifting analysis of Human-Autonomous Team interactions to analyse risks of autonomy in high-stakes AI systems", "authors": ["Ben Larwood", "Oliver J. Sutton", "Callum Cockburn"], "first_author": "Ben Larwood", "abstract": "Developing high-stakes autonomous systems that include Artificial Intelligence (AI) components is complex; the consequences of errors can be catastrophic, yet it is challenging to plan for all operational cases. In stressful scenarios for the human operator, such as short decision-making timescales, the risk of failures is exacerbated. A lack of understanding of AI failure modes obstructs this and so blocks the robust implementation of applications of AI in smart systems. This prevents early risk identification, leading to increased time, risk and cost of projects. A key tenet of Systems Engineering and acquisition engineering is centred around a \"left-shift\" in test and evaluation activities to earlier in the system lifecycle, to allow for \"accelerated delivery of [systems] that work\". We argue it is therefore essential that this shift includes the analysis of AI failure cases as part of the design stages of the system life cycle. Our proposed framework enables the early characterisation of risks emerging from human-autonomy teaming (HAT) in operational contexts. The cornerstone of this is a new analysis of AI failure modes, built on the seminal modelling of human-autonomy teams laid out by LaMonica et al., 2022. Using the analysis of the interactions between human and autonomous systems and exploring the failure modes within each aspect, our approach provides a way to systematically identify human-AI interactions risks across the operational domain of the system of interest. The understanding of the emergent behaviour enables increased robustness of the system, for which the analysis should be undertaken over the whole scope of its operational design domain. This approach is illustrated through an example use case for an AI assistant supporting a Command & Control (C2) System.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.HC", "eess.SY"], "url": "http://arxiv.org/abs/2512.03519", "comments": "Published in: IfSE Annual Systems Engineering Conference Proceedings 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Left shifting analysis of Human-Autonomous Team interactions to analyse risks of autonomy in high-stakes AI systems\nResearch Topic: artificial intelligence\nAuthors: Ben Larwood, Oliver J. Sutton, Callum Cockburn\nAbstract: Developing high-stakes autonomous systems that include Artificial Intelligence (AI) components is complex; the consequences of errors can be catastrophic, yet it is challenging to plan for all operational cases. In stressful scenarios for the human operator, such as short decision-making timescales, the risk of failures is exacerbated. A lack of understanding of AI failure modes obstructs this and so blocks the robust implementation of applications of AI in smart systems. This prevents early risk identification, leading to increased time, risk and cost of projects.   A key tenet of Systems Engineering and acquisition engineering is centred around a \"left-shift\" in test and evaluation activities to earlier in the system lifecycle, to allow for \"accelerated delivery of [systems] that work\". We argue it is therefore essential that this shift includes the analysis of AI failure cases as part of the design stages of the system life cycle. Our proposed framework enables the early characterisation of risks emerging from human-autonomy teaming (HAT) in operational contexts. The cornerstone of this is a new analysis of AI failure modes, built on the seminal modelling of human-autonomy teams laid out by LaMonica et al., 2022. Using the analysis of the interactions between human and autonomous systems and exploring the failure modes within each aspect, our approach provides a way to systematically identify human-AI interactions risks across the operational domain of the system of interest. The understanding of the emergent behaviour enables increased robustness of the system, for which the analysis should be undertaken over the whole scope of its operational design domain. This approach is illustrated through an example use case for an AI assistant supporting a Command & Control (C2) System.\nCategories: cs.HC, eess.SY\nPublication Date: 2025-12-03\nPaper ID: 2512.03519\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 115, "abstract_length": 1809, "total_text_length": 1925, "word_count": 292, "sentence_count": 13}, "basic_keywords": ["systems", "human", "system", "operational", "failure", "analysis", "risk", "modes"], "domain_keywords": [], "technical_terms": ["Using", "Command", "System", "Artificial Intelligence", "Control", "Systems Engineering", "Developing"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03514", "topic": "artificial intelligence", "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval", "authors": ["Adithya S Kolavi", "Vyoman Jain"], "first_author": "Adithya S Kolavi", "abstract": "Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "url": "http://arxiv.org/abs/2512.03514", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: M3DR: Towards Universal Multilingual Multimodal Document Retrieval\nResearch Topic: artificial intelligence\nAuthors: Adithya S Kolavi, Vyoman Jain\nAbstract: Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.\nCategories: cs.IR, cs.AI, cs.CL, cs.CV\nPublication Date: 2025-12-03\nPaper ID: 2512.03514\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1372, "total_text_length": 1439, "word_count": 169, "sentence_count": 10}, "basic_keywords": ["across", "multilingual", "document", "retrieval", "languages", "cross", "models", "multimodal"], "domain_keywords": [], "technical_terms": ["However", "Multilingual Multimodal Document Retrieval", "English", "Using", "Multimodal"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03512", "topic": "artificial intelligence", "title": "Physics-Driven Learning Framework for Tomographic Tactile Sensing", "authors": ["Xuanxuan Yang", "Xiuyang Zhang", "Haofeng Chen", "Gang Ma", "Xiaojie Wang"], "first_author": "Xuanxuan Yang", "abstract": "Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03512", "comments": "7pages,7figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Physics-Driven Learning Framework for Tomographic Tactile Sensing\nResearch Topic: artificial intelligence\nAuthors: Xuanxuan Yang, Xiuyang Zhang, Haofeng Chen, Gang Ma, Xiaojie Wang\nAbstract: Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03512\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1214, "total_text_length": 1280, "word_count": 162, "sentence_count": 7}, "basic_keywords": ["phydnn", "tactile", "forward", "sensing", "shape", "nonlinear", "artifacts", "contact"], "domain_keywords": [], "technical_terms": ["Extensive", "Electrical"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03499", "topic": "artificial intelligence", "title": "NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang"], "first_author": "Renqi Chen", "abstract": "The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.03499", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation\nResearch Topic: artificial intelligence\nAuthors: Renqi Chen, Haoyang Su, Shixiang Tang\nAbstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.\nCategories: cs.CV, cs.AI, cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.03499\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 108, "abstract_length": 1504, "total_text_length": 1613, "word_count": 213, "sentence_count": 11}, "basic_keywords": ["lora", "model", "encoder", "semantic", "peft", "visual", "foundation", "image"], "domain_keywords": [], "technical_terms": ["However", "Tuning", "Parameter", "Despite", "Efficient Fine", "Neural Architecture Search", "Various Experiments", "Furthermore"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03497", "topic": "artificial intelligence", "title": "Cell-cell communication inference and analysis: biological mechanisms, computational approaches, and future opportunities", "authors": ["Xiangzheng Cheng", "Haili Huang", "Ye Su", "Qing Nie", "Xiufen Zou", "Suoqin Jin"], "first_author": "Xiangzheng Cheng", "abstract": "In multicellular organisms, cells coordinate their activities through cell-cell communication (CCC), which are crucial for development, tissue homeostasis, and disease progression. Recent advances in single-cell and spatial omics technologies provide unprecedented opportunities to systematically infer and analyze CCC from these omics data, either by integrating prior knowledge of ligand-receptor interactions (LRIs) or through de novo approaches. A variety of computational methods have been developed, focusing on methodological innovations, accurate modeling of complex signaling mechanisms, and investigation of broader biological questions. These advances have greatly enhanced our ability to analyze CCC and generate biological hypotheses. Here, we introduce the biological mechanisms and modeling strategies of CCC, and provide a focused overview of more than 140 computational methods for inferring CCC from single-cell and spatial transcriptomic data, emphasizing the diversity in methodological frameworks and biological questions. Finally, we discuss the current challenges and future opportunities in this rapidly evolving field.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["q-bio.QM", "cs.AI", "q-bio.CB"], "url": "http://arxiv.org/abs/2512.03497", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Cell-cell communication inference and analysis: biological mechanisms, computational approaches, and future opportunities\nResearch Topic: artificial intelligence\nAuthors: Xiangzheng Cheng, Haili Huang, Ye Su, Qing Nie, Xiufen Zou, Suoqin Jin\nAbstract: In multicellular organisms, cells coordinate their activities through cell-cell communication (CCC), which are crucial for development, tissue homeostasis, and disease progression. Recent advances in single-cell and spatial omics technologies provide unprecedented opportunities to systematically infer and analyze CCC from these omics data, either by integrating prior knowledge of ligand-receptor interactions (LRIs) or through de novo approaches. A variety of computational methods have been developed, focusing on methodological innovations, accurate modeling of complex signaling mechanisms, and investigation of broader biological questions. These advances have greatly enhanced our ability to analyze CCC and generate biological hypotheses. Here, we introduce the biological mechanisms and modeling strategies of CCC, and provide a focused overview of more than 140 computational methods for inferring CCC from single-cell and spatial transcriptomic data, emphasizing the diversity in methodological frameworks and biological questions. Finally, we discuss the current challenges and future opportunities in this rapidly evolving field.\nCategories: q-bio.QM, cs.AI, q-bio.CB\nPublication Date: 2025-12-03\nPaper ID: 2512.03497\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 121, "abstract_length": 1143, "total_text_length": 1265, "word_count": 160, "sentence_count": 7}, "basic_keywords": ["cell", "biological", "through", "advances", "single", "spatial", "omics", "provide"], "domain_keywords": [], "technical_terms": ["These", "Here", "Recent", "Finally"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04130", "topic": "artificial intelligence", "title": "Controlled Tension Forecasting: Quantifying Cross-Probe Biases in $ω_0ω_a$CDM", "authors": ["Seokcheon Lee"], "first_author": "Seokcheon Lee", "abstract": "Recent analyses combining DESI DR2 BAO, Planck CMB, and Pantheon+ SNe have reported mild but intriguing deviations from the LambdaCDM model. A central challenge is to determine whether these deviations reflect genuine dynamical dark energy behavior or instead arise from cross-probe inconsistencies, prior choices, or mismatches in likelihood construction. Previous work demonstrated that imposing a biased supernova-motivated prior on Omega_{m0} can artificially displace the BAO-inferred w_0,w_a values from the LambdaCDM expectation. A complementary pedagogic study further showed that the differing degeneracy geometries of BAO, CMB, and SNe can generate apparent dark energy evolution even when the underlying cosmology is exactly LambdaCDM. In this manuscript, we develop a controlled tension injection framework that provides a systematic means of quantifying how probe-level tensions influence inferred dark energy parameters. Self-consistent BAO, CMB, and SNe mock datasets are augmented with parameterized shifts in (Omega_{m0}, H_0), supernova absolute calibration, and the BAO sound-horizon scale . The resulting datasets are analyzed through a unified MCMC pipeline, enabling a direct assessment of how these controlled tensions propagate into biases in (w_0, w_a) and the pivot equation-of-state parameter w_p. This forecasting framework provides practical guidance for identifying probe combinations that are most susceptible to spurious signatures of dynamical dark energy, and helps ensure robust multi-probe inference in forthcoming precision surveys.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["astro-ph.CO"], "url": "http://arxiv.org/abs/2512.04130", "comments": "31 pages, 9 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Controlled Tension Forecasting: Quantifying Cross-Probe Biases in $ω_0ω_a$CDM\nResearch Topic: artificial intelligence\nAuthors: Seokcheon Lee\nAbstract: Recent analyses combining DESI DR2 BAO, Planck CMB, and Pantheon+ SNe have reported mild but intriguing deviations from the LambdaCDM model. A central challenge is to determine whether these deviations reflect genuine dynamical dark energy behavior or instead arise from cross-probe inconsistencies, prior choices, or mismatches in likelihood construction. Previous work demonstrated that imposing a biased supernova-motivated prior on Omega_{m0} can artificially displace the BAO-inferred w_0,w_a values from the LambdaCDM expectation. A complementary pedagogic study further showed that the differing degeneracy geometries of BAO, CMB, and SNe can generate apparent dark energy evolution even when the underlying cosmology is exactly LambdaCDM. In this manuscript, we develop a controlled tension injection framework that provides a systematic means of quantifying how probe-level tensions influence inferred dark energy parameters. Self-consistent BAO, CMB, and SNe mock datasets are augmented with parameterized shifts in (Omega_{m0}, H_0), supernova absolute calibration, and the BAO sound-horizon scale $r_d$. The resulting datasets are analyzed through a unified MCMC pipeline, enabling a direct assessment of how these controlled tensions propagate into biases in (w_0, w_a) and the pivot equation-of-state parameter w_p. This forecasting framework provides practical guidance for identifying probe combinations that are most susceptible to spurious signatures of dynamical dark energy, and helps ensure robust multi-probe inference in forthcoming precision surveys.\nCategories: astro-ph.CO\nPublication Date: 2025-12-03\nPaper ID: 2512.04130\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1569, "total_text_length": 1647, "word_count": 220, "sentence_count": 9}, "basic_keywords": ["dark", "energy", "probe", "lambdacdm", "deviations", "dynamical", "prior", "supernova"], "domain_keywords": [], "technical_terms": ["Planck", "Pantheon", "Previous", "Recent", "Self"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03476", "topic": "artificial intelligence", "title": "ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms", "authors": ["Juan Diego Toscano", "Daniel T. Chen", "George Em Karniadakis"], "first_author": "Juan Diego Toscano", "abstract": "Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' () from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code () to generate scientific rewards (). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of . Furthermore, collaborative ``human-in-the-loop\" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.NA", "physics.comp-ph"], "url": "http://arxiv.org/abs/2512.03476", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms\nResearch Topic: artificial intelligence\nAuthors: Juan Diego Toscano, Daniel T. Chen, George Em Karniadakis\nAbstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop\" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.\nCategories: cs.LG, cs.AI, cs.MA, math.NA, physics.comp-ph\nPublication Date: 2025-12-03\nPaper ID: 2512.03476\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1514, "total_text_length": 1586, "word_count": 200, "sentence_count": 15}, "basic_keywords": ["scientific", "computational", "implementation", "scic", "sciml", "athena", "agentic", "numerical"], "domain_keywords": [], "technical_terms": ["These", "Universal Approximation", "Scientific Machine Learning", "Informed", "Autonomous Lab", "Physics", "Hierarchical Evolutionary Numerical Algorithms", "Furthermore"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03466", "topic": "artificial intelligence", "title": "AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation", "authors": ["Xavier Cadet", "Edward Koh", "Peter Chin"], "first_author": "Xavier Cadet", "abstract": "Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.MA", "cs.AI"], "url": "http://arxiv.org/abs/2512.03466", "comments": "Accepted at NeurIPS MTI-LLM 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation\nResearch Topic: artificial intelligence\nAuthors: Xavier Cadet, Edward Koh, Peter Chin\nAbstract: Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.\nCategories: cs.MA, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03466\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 58, "abstract_length": 1188, "total_text_length": 1247, "word_count": 170, "sentence_count": 8}, "basic_keywords": ["feedback", "multi", "agent", "puzzle", "turn", "open", "asympuzl", "communication"], "domain_keywords": [], "technical_terms": ["These", "Using", "Large Language Model", "Each", "Claude"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03463", "topic": "artificial intelligence", "title": "Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models", "authors": ["Shojiro Yamabe", "Futa Waseda", "Daiki Shiono", "Tsubasa Takahashi"], "first_author": "Shojiro Yamabe", "abstract": "Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.03463", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models\nResearch Topic: artificial intelligence\nAuthors: Shojiro Yamabe, Futa Waseda, Daiki Shiono, Tsubasa Takahashi\nAbstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.\nCategories: cs.CV, cs.AI, cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.03463\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 114, "abstract_length": 1734, "total_text_length": 1849, "word_count": 266, "sentence_count": 13}, "basic_keywords": ["text", "image", "training", "images", "cost", "models", "centric", "data"], "domain_keywords": [], "technical_terms": ["However", "While", "Across", "Unlike", "Overall", "Recent", "Printed Image", "Text"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03460", "topic": "artificial intelligence", "title": "Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study", "authors": ["Johnny Peng", "Thanh Tung Khuat", "Ellen Otte", "Katarzyna Musial", "Bogdan Gabrys"], "first_author": "Johnny Peng", "abstract": "In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG"], "url": "http://arxiv.org/abs/2512.03460", "comments": "This is a pre-print for submitting to computers & chemical engineering journal", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study\nResearch Topic: artificial intelligence\nAuthors: Johnny Peng, Thanh Tung Khuat, Ellen Otte, Katarzyna Musial, Bogdan Gabrys\nAbstract: In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.\nCategories: q-bio.QM, cs.AI, cs.CE, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03460\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 96, "abstract_length": 1835, "total_text_length": 1932, "word_count": 255, "sentence_count": 10}, "basic_keywords": ["learning", "process", "cell", "time", "batch", "monitoring", "limited", "data"], "domain_keywords": [], "technical_terms": ["However", "Additionally", "Raman"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03454", "topic": "artificial intelligence", "title": "Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles", "authors": ["Haicheng Liao", "Huanming Shen", "Bonan Wang", "Yongkang Li", "Yihong Tang", "Chengyue Wang", "Dingyi Zhuang", "Kehua Chen", "Hai Yang", "Chengzhong Xu", "Zhenning Li"], "first_author": "Haicheng Liao", "abstract": "Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03454", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\nResearch Topic: artificial intelligence\nAuthors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\nAbstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03454\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1474, "total_text_length": 1564, "word_count": 204, "sentence_count": 9}, "basic_keywords": ["spatial", "states", "autonomous", "grounding", "scene", "world", "thinkdeeper", "future"], "domain_keywords": ["autonomous vehicles"], "technical_terms": ["Interpreting", "Extensive", "Existing", "Retrieval", "Augmented Generation", "Notably", "Thought", "Aware World Model"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03451", "topic": "artificial intelligence", "title": "GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers", "authors": ["Zhiye Song", "Steve Dai", "Ben Keller", "Brucek Khailany"], "first_author": "Zhiye Song", "abstract": "Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications. We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve and speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03451", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers\nResearch Topic: artificial intelligence\nAuthors: Zhiye Song, Steve Dai, Ben Keller, Brucek Khailany\nAbstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.   We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\\times$ and $2.37\\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).\nCategories: cs.CV, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03451\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 106, "abstract_length": 1213, "total_text_length": 1320, "word_count": 173, "sentence_count": 16}, "basic_keywords": ["video", "generation", "model", "models", "free", "guidance", "superior", "each"], "domain_keywords": [], "technical_terms": ["Despite", "Each", "Transformer", "Through", "Diffusion"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03445", "topic": "artificial intelligence", "title": "Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation", "authors": ["Xieji Li", "Siyuan Yan", "Yingsheng Liu", "H. Peter Soyer", "Monika Janda", "Victoria Mar", "Zongyuan Ge"], "first_author": "Xieji Li", "abstract": "Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03445", "comments": "10 pages. Under Review", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\nResearch Topic: artificial intelligence\nAuthors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\nAbstract: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03445\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1446, "total_text_length": 1547, "word_count": 184, "sentence_count": 11}, "basic_keywords": ["medical", "image", "data", "knowledge", "pretraining", "learning", "text", "pairs"], "domain_keywords": [], "technical_terms": ["However", "Aspect Knowledge", "Agent", "Vision", "Second", "Ontology", "First", "Enhanced"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03442", "topic": "artificial intelligence", "title": "PretrainZero: Reinforcement Active Pretraining", "authors": ["Xingrun Xing", "Zhiyuan Fan", "Jie Lou", "Guoqi Li", "Jiajun Zhang", "Debing Zhang"], "first_author": "Xingrun Xing", "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.03442", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: PretrainZero: Reinforcement Active Pretraining\nResearch Topic: artificial intelligence\nAuthors: Xingrun Xing, Zhiyuan Fan, Jie Lou, Guoqi Li, Jiajun Zhang, Debing Zhang\nAbstract: Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.\nCategories: cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.03442\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 46, "abstract_length": 1645, "total_text_length": 1692, "word_count": 219, "sentence_count": 14}, "basic_keywords": ["general", "models", "learning", "reasoning", "pretrainzero", "pretraining", "reinforcement", "active"], "domain_keywords": [], "technical_terms": ["Active", "Verification", "Wikipedia", "Mimicking", "Base", "Recent", "Self"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03438", "topic": "artificial intelligence", "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents", "authors": ["Reuben Tan", "Baolin Peng", "Zhengyuan Yang", "Hao Cheng", "Oier Mees", "Theodore Zhao", "Andrea Tupini", "Isar Meijier", "Qianhui Wu", "Yuncong Yang", "Lars Liden", "Yu Gu", "Sheng Zhang", "Xiaodong Liu", "Lijuan Wang", "Marc Pollefeys", "Yong Jae Lee", "Jianfeng Gao"], "first_author": "Reuben Tan", "abstract": "Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03438", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Multimodal Reinforcement Learning with Agentic Verifier for AI Agents\nResearch Topic: artificial intelligence\nAuthors: Reuben Tan, Baolin Peng, Zhengyuan Yang, Hao Cheng, Oier Mees, Theodore Zhao, Andrea Tupini, Isar Meijier, Qianhui Wu, Yuncong Yang, Lars Liden, Yu Gu, Sheng Zhang, Xiaodong Liu, Lijuan Wang, Marc Pollefeys, Yong Jae Lee, Jianfeng Gao\nAbstract: Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03438\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1706, "total_text_length": 1776, "word_count": 250, "sentence_count": 10}, "basic_keywords": ["agentic", "reasoning", "based", "reward", "models", "mmrl", "rewards", "scoring"], "domain_keywords": [], "technical_terms": ["However", "Finally", "Agentic", "Argos", "Grounded", "Critically", "Richer", "Agentic Reward"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03429", "topic": "artificial intelligence", "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations", "authors": ["Raul Steinmetz", "Fabio Demo Rosa", "Victor Augusto Kich", "Jair Augusto Bottega", "Ricardo Bedin Grando", "Daniel Fernando Tello Gamarra"], "first_author": "Raul Steinmetz", "abstract": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.AI"], "url": "http://arxiv.org/abs/2512.03429", "comments": "Accepted for publication in the Journal of Intelligent and Fuzzy Systems", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations\nResearch Topic: artificial intelligence\nAuthors: Raul Steinmetz, Fabio Demo Rosa, Victor Augusto Kich, Jair Augusto Bottega, Ricardo Bedin Grando, Daniel Fernando Tello Gamarra\nAbstract: Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.\nCategories: cs.RO, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03429\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1425, "total_text_length": 1510, "word_count": 197, "sentence_count": 8}, "basic_keywords": ["model", "navigation", "lidar", "high", "free", "based", "latent", "using"], "domain_keywords": [], "technical_terms": ["These", "Multi", "Experiments", "Conventional", "Autonomous", "Reinforcement Learning", "Layer Perceptron Variational Autoencoder"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03413", "topic": "artificial intelligence", "title": "BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents", "authors": ["Shu Wang", "Yingli Zhou", "Yixiang Fang"], "first_author": "Shu Wang", "abstract": "As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.IR", "cs.AI"], "url": "http://arxiv.org/abs/2512.03413", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\nResearch Topic: artificial intelligence\nAuthors: Shu Wang, Yingli Zhou, Yixiang Fang\nAbstract: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.\nCategories: cs.IR, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03413\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1543, "total_text_length": 1660, "word_count": 223, "sentence_count": 8}, "basic_keywords": ["documents", "performance", "retrieval", "information", "hierarchical", "structure", "method", "task"], "domain_keywords": [], "technical_terms": ["Extensive", "Existing", "Retrieval", "Augmented Generation", "Large Language Models", "Information Foraging Theory", "Specifically", "Leveraging"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03406", "topic": "artificial intelligence", "title": "Why Some Seek AI, Others Seek Therapists: Mental Health in the Age of Generative AI", "authors": ["Junsang Park", "Sarah Brown", "Sharon Lynn Chu"], "first_author": "Junsang Park", "abstract": "As generative artificial intelligence (GAI) enters the mental health landscape, questions arise about how individuals weigh AI tools against human therapists. Drawing on the Health Belief Model (HBM), this study examined belief-based predictors of intention to use GAI and therapists across two populations: a university sample (N = 1,155) and a nationally representative adult sample (N = 651). Using repeated-measures ANOVA and LASSO regression, we found that therapists were consistently valued for emotional, relational, and personalization benefits, while GAI was favored for accessibility and affordability. Yet structural advantages alone did not predict adoption; emotional benefit and personalization emerged as decisive factors. Adoption patterns diverged across groups: students treated GAI as a complement, whereas national adults approached it as a substitute. Concerns about privacy and reliability constrained GAI use in both groups. These findings extend HBM to multi-modality contexts and highlight design implications for trustworthy, emotionally resonant digital mental health tools.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.HC"], "url": "http://arxiv.org/abs/2512.03406", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Why Some Seek AI, Others Seek Therapists: Mental Health in the Age of Generative AI\nResearch Topic: artificial intelligence\nAuthors: Junsang Park, Sarah Brown, Sharon Lynn Chu\nAbstract: As generative artificial intelligence (GAI) enters the mental health landscape, questions arise about how individuals weigh AI tools against human therapists. Drawing on the Health Belief Model (HBM), this study examined belief-based predictors of intention to use GAI and therapists across two populations: a university sample (N = 1,155) and a nationally representative adult sample (N = 651). Using repeated-measures ANOVA and LASSO regression, we found that therapists were consistently valued for emotional, relational, and personalization benefits, while GAI was favored for accessibility and affordability. Yet structural advantages alone did not predict adoption; emotional benefit and personalization emerged as decisive factors. Adoption patterns diverged across groups: students treated GAI as a complement, whereas national adults approached it as a substitute. Concerns about privacy and reliability constrained GAI use in both groups. These findings extend HBM to multi-modality contexts and highlight design implications for trustworthy, emotionally resonant digital mental health tools.\nCategories: cs.HC\nPublication Date: 2025-12-03\nPaper ID: 2512.03406\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1102, "total_text_length": 1186, "word_count": 165, "sentence_count": 8}, "basic_keywords": ["health", "therapists", "mental", "about", "tools", "belief", "across", "sample"], "domain_keywords": [], "technical_terms": ["These", "Using", "Adoption", "Concerns", "Health Belief Model", "Drawing"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03400", "topic": "artificial intelligence", "title": "Better World Models Can Lead to Better Post-Training Performance", "authors": ["Prakhar Gupta", "Henry Conklin", "Sarah-Jane Leslie", "Andrew Lee"], "first_author": "Prakhar Gupta", "abstract": "In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03400", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Better World Models Can Lead to Better Post-Training Performance\nResearch Topic: artificial intelligence\nAuthors: Prakhar Gupta, Henry Conklin, Sarah-Jane Leslie, Andrew Lee\nAbstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03400\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 1170, "total_text_length": 1235, "word_count": 164, "sentence_count": 8}, "basic_keywords": ["world", "representations", "state", "training", "model", "explicit", "modeling", "affect"], "domain_keywords": [], "technical_terms": ["Rubik", "Group Relative Policy Optimization", "Transformers", "More", "Cube"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03394", "topic": "artificial intelligence", "title": "VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing", "authors": ["Hamed Poursiami", "Shay Snyder", "Guojing Cong", "Thomas Potok", "Maryam Parsa"], "first_author": "Hamed Poursiami", "abstract": "Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI", "cs.NE"], "url": "http://arxiv.org/abs/2512.03394", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing\nResearch Topic: artificial intelligence\nAuthors: Hamed Poursiami, Shay Snyder, Guojing Cong, Thomas Potok, Maryam Parsa\nAbstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.\nCategories: cs.LG, cs.AI, cs.NE\nPublication Date: 2025-12-03\nPaper ID: 2512.03394\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1582, "total_text_length": 1669, "word_count": 219, "sentence_count": 9}, "basic_keywords": ["graph", "gnns", "performance", "message", "passing", "high", "vector", "while"], "domain_keywords": [], "technical_terms": ["While", "Spike Diffusion", "Furthermore", "Hyperdimensional Computing", "Graph", "Vector Symbolic Architectures", "Without", "Associative Message Passing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03383", "topic": "artificial intelligence", "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs", "authors": ["Hung-Yueh Chiang", "Chi-Chih Chang", "Yu-Chen Lu", "Chien-Yu Lin", "Kai-Chiang Wu", "Mohamed S. Abdelfattah", "Diana Marculescu"], "first_author": "Hung-Yueh Chiang", "abstract": "Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03383", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs\nResearch Topic: artificial intelligence\nAuthors: Hung-Yueh Chiang, Chi-Chih Chang, Yu-Chen Lu, Chien-Yu Lin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu\nAbstract: Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03383\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1539, "total_text_length": 1615, "word_count": 220, "sentence_count": 14}, "basic_keywords": ["models", "quantization", "device", "framework", "uniql", "pruning", "ssms", "weight"], "domain_keywords": [], "technical_terms": ["Nemotron", "Resource", "Bamba", "Transformers", "Deploying", "State Space Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03369", "topic": "artificial intelligence", "title": "FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting", "authors": ["Nan Zhou", "Huandong Wang", "Jiahao Li", "Han Li", "Yali Song", "Qiuhua Wang", "Yong Li", "Xinlei Chen"], "first_author": "Nan Zhou", "abstract": "Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03369", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting\nResearch Topic: artificial intelligence\nAuthors: Nan Zhou, Huandong Wang, Jiahao Li, Han Li, Yali Song, Qiuhua Wang, Yong Li, Xinlei Chen\nAbstract: Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03369\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 104, "abstract_length": 1650, "total_text_length": 1755, "word_count": 218, "sentence_count": 19}, "basic_keywords": ["firesentry", "fire", "dataset", "benchmark", "wildfire", "video", "mask", "firediff"], "domain_keywords": [], "technical_terms": ["However", "Dataset", "Benchmark", "Fine", "Collected", "Building"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03360", "topic": "artificial intelligence", "title": "From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation", "authors": ["Qingchuan Li", "Mingyue Cheng", "Zirui Liu", "Daoyu Wang", "Yuting Zeng", "Tongxuan Liu"], "first_author": "Qingchuan Li", "abstract": "Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.03360", "comments": "Accepted by AAAI2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation\nResearch Topic: artificial intelligence\nAuthors: Qingchuan Li, Mingyue Cheng, Zirui Liu, Daoyu Wang, Yuting Zeng, Tongxuan Liu\nAbstract: Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.\nCategories: cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.03360\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1511, "total_text_length": 1613, "word_count": 206, "sentence_count": 11}, "basic_keywords": ["reasoning", "logical", "language", "hblr", "translation", "core", "natural", "step"], "domain_keywords": [], "technical_terms": ["Hypothesis", "However", "Extensive", "Backward Logical Reasoning", "Despite", "Logical", "First", "Order Logic"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03345", "topic": "artificial intelligence", "title": "HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration", "authors": ["Seunghoi Kim", "Henry F. J. Tregidgo", "Chen Jin", "Matteo Figini", "Daniel C. Alexander"], "first_author": "Seunghoi Kim", "abstract": "Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03345", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration\nResearch Topic: artificial intelligence\nAuthors: Seunghoi Kim, Henry F. J. Tregidgo, Chen Jin, Matteo Figini, Daniel C. Alexander\nAbstract: Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03345\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1676, "total_text_length": 1774, "word_count": 221, "sentence_count": 11}, "basic_keywords": ["hallucinations", "hallucination", "restoration", "image", "hallugen", "models", "incorrect", "safety"], "domain_keywords": ["medical imaging"], "technical_terms": ["Using", "Feature Evaluation", "Semantic Hallucination Assessment", "Generative", "Together", "Progress"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03343", "topic": "artificial intelligence", "title": "Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning", "authors": ["Darshan Fofadiya"], "first_author": "Darshan Fofadiya", "abstract": "Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning . While scaling model size mitigates this , the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.03343", "comments": "Code available at https://github.com/DarshanFofadiya/idea-gated-transformers/tree/main", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning\nResearch Topic: artificial intelligence\nAuthors: Darshan Fofadiya\nAbstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \\citep{holtzman2019curious}. While scaling model size mitigates this \\citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03343\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1294, "total_text_length": 1386, "word_count": 181, "sentence_count": 10}, "basic_keywords": ["generation", "idea", "language", "trained", "drift", "planning", "while", "model"], "domain_keywords": [], "technical_terms": ["Concept Vector", "Idea Head", "Experiments", "Finance", "While", "Gated Transformer", "Autoregressive Language Models", "Topic Drift"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03339", "topic": "artificial intelligence", "title": "ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography", "authors": ["Yeganeh Ghamary", "Victoria Wu", "Hooman Vaseli", "Christina Luong", "Teresa Tsang", "Siavash Bigdeli", "Purang Abolmaesumi"], "first_author": "Yeganeh Ghamary", "abstract": "Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.672.68 to 79.642.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03339", "comments": "11 pages, Accepted in IMIMIC Workshop at MICCAI 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography\nResearch Topic: artificial intelligence\nAuthors: Yeganeh Ghamary, Victoria Wu, Hooman Vaseli, Christina Luong, Teresa Tsang, Siavash Bigdeli, Purang Abolmaesumi\nAbstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\\pm$2.68 to 79.64$\\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF\nCategories: cs.CV, cs.AI, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03339\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1413, "total_text_length": 1530, "word_count": 197, "sentence_count": 16}, "basic_keywords": ["proposed", "model", "cardiac", "making", "process", "learning", "methods", "prediction"], "domain_keywords": [], "technical_terms": ["However", "Ejection", "Additionally", "Traditionally", "Prototype Angular Separation", "Most", "Some"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03336", "topic": "artificial intelligence", "title": "Single-Round Scalable Analytic Federated Learning", "authors": ["Alan T. L. Bacellar", "Mustafa Munir", "Felipe M. G. França", "Priscila M. V. Lima", "Radu Marculescu", "Lizy K. John"], "first_author": "Alan T. L. Bacellar", "abstract": "Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI", "stat.ML"], "url": "http://arxiv.org/abs/2512.03336", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.435007", "embedding_text": "Paper Title: Single-Round Scalable Analytic Federated Learning\nResearch Topic: artificial intelligence\nAuthors: Alan T. L. Bacellar, Mustafa Munir, Felipe M. G. França, Priscila M. V. Lima, Radu Marculescu, Lizy K. John\nAbstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.\nCategories: cs.LG, cs.AI, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03336\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 49, "abstract_length": 1020, "total_text_length": 1070, "word_count": 140, "sentence_count": 9}, "basic_keywords": ["linear", "single", "round", "safle", "federated", "high", "data", "analytic"], "domain_keywords": [], "technical_terms": ["Analytic", "Federated Learning", "Subsequent", "Empirically"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03324", "topic": "artificial intelligence", "title": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs", "authors": ["Ngoc Bui", "Shubham Sharma", "Simran Lamba", "Saumitra Mishra", "Rex Ying"], "first_author": "Ngoc Bui", "abstract": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03324", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs\nResearch Topic: artificial intelligence\nAuthors: Ngoc Bui, Shubham Sharma, Simran Lamba, Saumitra Mishra, Rex Ying\nAbstract: Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03324\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1814, "total_text_length": 1884, "word_count": 252, "sentence_count": 11}, "basic_keywords": ["memory", "retention", "long", "tokens", "inference", "cache", "trim", "gate"], "domain_keywords": [], "technical_terms": ["Tokens", "Beyond", "Existing", "Across", "Each", "Qualitative", "Remarkably", "Memory"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03318", "topic": "artificial intelligence", "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "authors": ["Chandler Smith", "Marwa Abdulhai", "Manfred Diaz", "Marko Tesic", "Rakshit S. Trivedi", "Alexander Sasha Vezhnevets", "Lewis Hammond", "Jesse Clifton", "Minsuk Chang", "Edgar A. Duéñez-Guzmán", "John P. Agapiou", "Jayd Matyas", "Danny Karmon", "Akash Kundu", "Aliaksei Korshuk", "Ananya Ananya", "Arrasy Rahman", "Avinaash Anand Kulandaivel", "Bain McHale", "Beining Zhang", "Buyantuev Alexander", "Carlos Saith Rodriguez Rojas", "Caroline Wang", "Chetan Talele", "Chenao Liu", "Chichen Lin", "Diana Riazi", "Di Yang Shi", "Emanuel Tewolde", "Elizaveta Tennant", "Fangwei Zhong", "Fuyang Cui", "Gang Zhao", "Gema Parreño Piqueras", "Hyeonggeun Yun", "Ilya Makarov", "Jiaxun Cui", "Jebish Purbey", "Jim Dilkes", "Jord Nguyen", "Lingyun Xiao", "Luis Felipe Giraldo", "Manuela Chacon-Chamorro", "Manuel Sebastian Rios Beltran", "Marta Emili García Segura", "Mengmeng Wang", "Mogtaba Alim", "Nicanor Quijano", "Nico Schiavone", "Olivia Macmillan-Scott", "Oswaldo Peña", "Peter Stone", "Ram Mohan Rao Kadiyala", "Rolando Fernandez", "Ruben Manrique", "Sunjia Lu", "Sheila A. McIlraith", "Shamika Dhuri", "Shuqing Shi", "Siddhant Gupta", "Sneheel Sarangi", "Sriram Ganapathi Subramanian", "Taehun Cha", "Toryn Q. Klassen", "Wenming Tu", "Weijian Fan", "Wu Ruiyang", "Xue Feng", "Yali Du", "Yang Liu", "Yiding Wang", "Yipeng Kang", "Yoonchang Sung", "Yuxuan Chen", "Zhaowei Zhang", "Zhihan Wang", "Zhiqiang Wu", "Ziang Chen", "Zilong Zheng", "Zixia Jia", "Ziyan Wang", "Dylan Hadfield-Menell", "Natasha Jaques", "Tim Baarslag", "Jose Hernandez-Orallo", "Joel Z. Leibo"], "first_author": "Chandler Smith", "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03318", "comments": "Published at NeurIPS Datasets and Benchmarks 2025, 10 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia\nResearch Topic: artificial intelligence\nAuthors: Chandler Smith, Marwa Abdulhai, Manfred Diaz, Marko Tesic, Rakshit S. Trivedi, Alexander Sasha Vezhnevets, Lewis Hammond, Jesse Clifton, Minsuk Chang, Edgar A. Duéñez-Guzmán, John P. Agapiou, Jayd Matyas, Danny Karmon, Akash Kundu, Aliaksei Korshuk, Ananya Ananya, Arrasy Rahman, Avinaash Anand Kulandaivel, Bain McHale, Beining Zhang, Buyantuev Alexander, Carlos Saith Rodriguez Rojas, Caroline Wang, Chetan Talele, Chenao Liu, Chichen Lin, Diana Riazi, Di Yang Shi, Emanuel Tewolde, Elizaveta Tennant, Fangwei Zhong, Fuyang Cui, Gang Zhao, Gema Parreño Piqueras, Hyeonggeun Yun, Ilya Makarov, Jiaxun Cui, Jebish Purbey, Jim Dilkes, Jord Nguyen, Lingyun Xiao, Luis Felipe Giraldo, Manuela Chacon-Chamorro, Manuel Sebastian Rios Beltran, Marta Emili García Segura, Mengmeng Wang, Mogtaba Alim, Nicanor Quijano, Nico Schiavone, Olivia Macmillan-Scott, Oswaldo Peña, Peter Stone, Ram Mohan Rao Kadiyala, Rolando Fernandez, Ruben Manrique, Sunjia Lu, Sheila A. McIlraith, Shamika Dhuri, Shuqing Shi, Siddhant Gupta, Sneheel Sarangi, Sriram Ganapathi Subramanian, Taehun Cha, Toryn Q. Klassen, Wenming Tu, Weijian Fan, Wu Ruiyang, Xue Feng, Yali Du, Yang Liu, Yiding Wang, Yipeng Kang, Yoonchang Sung, Yuxuan Chen, Zhaowei Zhang, Zhihan Wang, Zhiqiang Wu, Ziang Chen, Zilong Zheng, Zixia Jia, Ziyan Wang, Dylan Hadfield-Menell, Natasha Jaques, Tim Baarslag, Jose Hernandez-Orallo, Joel Z. Leibo\nAbstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.\nCategories: cs.AI\nPublication Date: 2025-12-03\nPaper ID: 2512.03318\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1218, "total_text_length": 1319, "word_count": 176, "sentence_count": 7}, "basic_keywords": ["agents", "capabilities", "ability", "agent", "language", "social", "situations", "based"], "domain_keywords": [], "technical_terms": ["These", "Concordia Contest", "Concordia", "Large Language Model"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03317", "topic": "artificial intelligence", "title": "NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction", "authors": ["Thomas Monninger", "Zihan Zhang", "Steffen Staab", "Sihao Ding"], "first_author": "Thomas Monninger", "abstract": "Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "url": "http://arxiv.org/abs/2512.03317", "comments": "Accepted to 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2026)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction\nResearch Topic: artificial intelligence\nAuthors: Thomas Monninger, Zihan Zhang, Steffen Staab, Sihao Ding\nAbstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion\nCategories: cs.CV, cs.AI, cs.LG, cs.RO\nPublication Date: 2025-12-03\nPaper ID: 2512.03317\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1810, "total_text_length": 1908, "word_count": 251, "sentence_count": 15}, "basic_keywords": ["maps", "navigation", "online", "data", "construction", "diffusion", "fidelity", "autonomous"], "domain_keywords": [], "technical_terms": ["However", "Instead", "What", "Traditionally", "Accurate", "Navigation"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03316", "topic": "artificial intelligence", "title": "Symmetry Breaking of Current Response in Disordered Exclusion Processes", "authors": ["Issei Sakai", "Takuma Akimoto"], "first_author": "Issei Sakai", "abstract": "The bias-reversal symmetry -- where reversing an external bias inverts the current without changing its magnitude -- is a hallmark of nonequilibrium transport. While this property holds in homogeneous systems such as the asymmetric simple exclusion process, how disorder and its interplay with particle interactions affect this symmetry has remained unclear. Here, we establish a general criterion showing that the bias-reversal symmetry holds if and only if the local left-right bond-bias ratio is spatially uniform. Analytical and numerical analyses reveal that bond disorder preserves the symmetry beyond linear response, whereas site disorder breaks it through an interplay between heterogeneity and particle interactions. Our results demonstrate how environmental disorder and interparticle interactions cooperate to generate asymmetric transport, thereby providing a unified theoretical framework relevant to transport through biological and artificial nanochannels.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn"], "url": "http://arxiv.org/abs/2512.03316", "comments": "6 pages, 3 figures + 7 pages of supplemental material", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Symmetry Breaking of Current Response in Disordered Exclusion Processes\nResearch Topic: artificial intelligence\nAuthors: Issei Sakai, Takuma Akimoto\nAbstract: The bias-reversal symmetry -- where reversing an external bias inverts the current without changing its magnitude -- is a hallmark of nonequilibrium transport. While this property holds in homogeneous systems such as the asymmetric simple exclusion process, how disorder and its interplay with particle interactions affect this symmetry has remained unclear. Here, we establish a general criterion showing that the bias-reversal symmetry holds if and only if the local left-right bond-bias ratio is spatially uniform. Analytical and numerical analyses reveal that bond disorder preserves the symmetry beyond linear response, whereas site disorder breaks it through an interplay between heterogeneity and particle interactions. Our results demonstrate how environmental disorder and interparticle interactions cooperate to generate asymmetric transport, thereby providing a unified theoretical framework relevant to transport through biological and artificial nanochannels.\nCategories: cond-mat.stat-mech, cond-mat.dis-nn\nPublication Date: 2025-12-03\nPaper ID: 2512.03316\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 972, "total_text_length": 1044, "word_count": 139, "sentence_count": 6}, "basic_keywords": ["bias", "symmetry", "disorder", "transport", "interactions", "reversal", "holds", "asymmetric"], "domain_keywords": [], "technical_terms": ["Here", "Analytical", "While"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03309", "topic": "artificial intelligence", "title": "Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates", "authors": ["Aniruddha Bora", "Shixuan Zhang", "Khemraj Shukla", "Bryce Harrop", "George Em. Karniadakis", "L. Ruby Leung"], "first_author": "Aniruddha Bora", "abstract": "Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI", "math-ph"], "url": "http://arxiv.org/abs/2512.03309", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates\nResearch Topic: artificial intelligence\nAuthors: Aniruddha Bora, Shixuan Zhang, Khemraj Shukla, Bryce Harrop, George Em. Karniadakis, L. Ruby Leung\nAbstract: Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.\nCategories: cs.LG, cs.AI, math-ph\nPublication Date: 2025-12-02\nPaper ID: 2512.03309\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1495, "total_text_length": 1573, "word_count": 197, "sentence_count": 10}, "basic_keywords": ["model", "bias", "simulations", "states", "earth", "system", "correction", "limited"], "domain_keywords": [], "technical_terms": ["Earth", "Inception", "Traditional", "Energy Exascale Earth System Model", "Coarse", "Both", "Trained", "Building"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03307", "topic": "artificial intelligence", "title": "Robust Tabular Foundation Models", "authors": ["Matthew Peroni", "Franck Le", "Vadim Sheinin"], "first_author": "Matthew Peroni", "abstract": "The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03307", "comments": "Shaping Responsible Synthetic Data in the Era of Foundation Models, AAAI 2026", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Robust Tabular Foundation Models\nResearch Topic: artificial intelligence\nAuthors: Matthew Peroni, Franck Le, Vadim Sheinin\nAbstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03307\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 32, "abstract_length": 1405, "total_text_length": 1438, "word_count": 201, "sentence_count": 9}, "basic_keywords": ["performance", "tfms", "data", "synthetic", "datasets", "model", "adversarial", "training"], "domain_keywords": [], "technical_terms": ["These", "Prior", "Random Forests", "Applied", "Building", "Robust Tabular Foundation Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03300", "topic": "artificial intelligence", "title": "HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction", "authors": ["Pengfei Hu", "Fan Ming", "Xiaoxue Han", "Chang Lu", "Yue Ning", "Dan Lu"], "first_author": "Pengfei Hu", "abstract": "Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03300", "comments": "Accepted by AAAI 2026 workshop (oral) on AI for Environmental Science", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction\nResearch Topic: artificial intelligence\nAuthors: Pengfei Hu, Fan Ming, Xiaoxue Han, Chang Lu, Yue Ning, Dan Lu\nAbstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03300\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1355, "total_text_length": 1446, "word_count": 177, "sentence_count": 9}, "basic_keywords": ["domain", "reservoir", "inflow", "reservoirs", "metadata", "learning", "invariant", "hydrological"], "domain_keywords": [], "technical_terms": ["However", "Upper Colorado River Basin", "Domain", "Experiment", "Spatial", "Deep", "During"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03298", "topic": "artificial intelligence", "title": "Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction", "authors": ["Echo Diyun LU", "Charles Findling", "Marianne Clausel", "Alessandro Leite", "Wei Gong", "Pierric Kersaudy"], "first_author": "Echo Diyun LU", "abstract": "Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03298", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction\nResearch Topic: artificial intelligence\nAuthors: Echo Diyun LU, Charles Findling, Marianne Clausel, Alessandro Leite, Wei Gong, Pierric Kersaudy\nAbstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03298\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 131, "abstract_length": 776, "total_text_length": 908, "word_count": 110, "sentence_count": 5}, "basic_keywords": ["regime", "uncertainty", "point", "accuracy", "switching", "conformal", "model", "transitions"], "domain_keywords": [], "technical_terms": ["Across", "Dropout", "Gaussian", "Regime", "Adaptive Conformal Inference", "Deep Switching State Space Models"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03293", "topic": "artificial intelligence", "title": "Prior preferences in active inference agents: soft, hard, and goal shaping", "authors": ["Filippo Torresan", "Ryota Kanai", "Manuel Baltieri"], "first_author": "Filippo Torresan", "abstract": "Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.AI", "q-bio.NC"], "url": "http://arxiv.org/abs/2512.03293", "comments": "41 pages, 23 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Prior preferences in active inference agents: soft, hard, and goal shaping\nResearch Topic: artificial intelligence\nAuthors: Filippo Torresan, Ryota Kanai, Manuel Baltieri\nAbstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).\nCategories: cs.AI, q-bio.NC\nPublication Date: 2025-12-02\nPaper ID: 2512.03293\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1308, "total_text_length": 1383, "word_count": 200, "sentence_count": 13}, "basic_keywords": ["inference", "agent", "distribution", "preference", "learning", "agents", "goal", "active"], "domain_keywords": [], "technical_terms": ["Leibler", "Active", "Kullback"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03289", "topic": "artificial intelligence", "title": "DAWZY: A New Addition to AI powered \"Human in the Loop\" Music Co-creation", "authors": ["Aaron C Elkins", "Sanchit Singh", "Adrian Kieback", "Sawyer Blankenship", "Uyiosa Philip Amadasun", "Aman Chadha"], "first_author": "Aaron C Elkins", "abstract": "Digital Audio Workstations (DAWs) offer fine control, but mapping high-level intent (e.g., \"warm the vocals\") to low-level edits breaks creative flow. Existing artificial intelligence (AI) music generators are typically one-shot, limiting opportunities for iterative development and human contribution. We present DAWZY, an open-source assistant that turns natural-language (text/voice/hum) requests into reversible actions in REAPER. DAWZY keeps the DAW as the creative hub with a minimal GUI and voice-first interface. DAWZY uses LLM-based code generation as a novel way to significantly reduce the time users spend familiarizing themselves with large interfaces, replacing hundreds of buttons and drop-downs with a chat box. DAWZY also uses three Model Context Protocol tools for live state queries, parameter adjustment, and AI beat generation. It maintains grounding by refreshing state before mutation and ensures safety and reversibility with atomic scripts and undo. In evaluations, DAWZY performed reliably on common production tasks and was rated positively by users across Usability, Control, Learning, Collaboration, and Enjoyment.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.HC"], "url": "http://arxiv.org/abs/2512.03289", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: DAWZY: A New Addition to AI powered \"Human in the Loop\" Music Co-creation\nResearch Topic: artificial intelligence\nAuthors: Aaron C Elkins, Sanchit Singh, Adrian Kieback, Sawyer Blankenship, Uyiosa Philip Amadasun, Aman Chadha\nAbstract: Digital Audio Workstations (DAWs) offer fine control, but mapping high-level intent (e.g., \"warm the vocals\") to low-level edits breaks creative flow. Existing artificial intelligence (AI) music generators are typically one-shot, limiting opportunities for iterative development and human contribution. We present DAWZY, an open-source assistant that turns natural-language (text/voice/hum) requests into reversible actions in REAPER. DAWZY keeps the DAW as the creative hub with a minimal GUI and voice-first interface. DAWZY uses LLM-based code generation as a novel way to significantly reduce the time users spend familiarizing themselves with large interfaces, replacing hundreds of buttons and drop-downs with a chat box. DAWZY also uses three Model Context Protocol tools for live state queries, parameter adjustment, and AI beat generation. It maintains grounding by refreshing state before mutation and ensures safety and reversibility with atomic scripts and undo. In evaluations, DAWZY performed reliably on common production tasks and was rated positively by users across Usability, Control, Learning, Collaboration, and Enjoyment.\nCategories: cs.HC\nPublication Date: 2025-12-02\nPaper ID: 2512.03289\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1143, "total_text_length": 1217, "word_count": 171, "sentence_count": 11}, "basic_keywords": ["dawzy", "control", "level", "creative", "voice", "uses", "generation", "users"], "domain_keywords": [], "technical_terms": ["Usability", "Collaboration", "Existing", "Digital Audio Workstations", "Control", "Model Context Protocol", "Learning", "Enjoyment"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03280", "topic": "artificial intelligence", "title": "BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark", "authors": ["Nicholas Sung", "Steven Spreizer", "Mohamed Elrefaie", "Matthew C. Jones", "Faez Ahmed"], "first_author": "Nicholas Sung", "abstract": "Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03280", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark\nResearch Topic: artificial intelligence\nAuthors: Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Matthew C. Jones, Faez Ahmed\nAbstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03280\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1733, "total_text_length": 1814, "word_count": 237, "sentence_count": 10}, "basic_keywords": ["inverse", "dataset", "model", "aerodynamic", "reproducible", "design", "blendednet", "benchmark"], "domain_keywords": [], "technical_terms": ["Using", "Finally", "Despite", "Transolver", "Graph Neural Operator Transformer", "Transformer"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03278", "topic": "artificial intelligence", "title": "Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases", "authors": ["Michael Theologitis", "Dan Suciu"], "first_author": "Michael Theologitis", "abstract": "In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claimsoften about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databasestypically a few hundred rowsthat conveniently fit within an LLM's context window. In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact datasetthe standard benchmark for fact verification over structured dataThucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.DB", "cs.AI"], "url": "http://arxiv.org/abs/2512.03278", "comments": "Accepted at AAAI 2026 Workshop on LLM-based Multi-Agent Systems (LaMAS)", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases\nResearch Topic: artificial intelligence\nAuthors: Michael Theologitis, Dan Suciu\nAbstract: In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\\unicode{x2014}$typically a few hundred rows$\\unicode{x2014}$that conveniently fit within an LLM's context window.   In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\\unicode{x2014}$the standard benchmark for fact verification over structured data$\\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).\nCategories: cs.DB, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03278\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1652, "total_text_length": 1742, "word_count": 252, "sentence_count": 16}, "basic_keywords": ["verification", "structured", "thucy", "public", "about", "verified", "against", "data"], "domain_keywords": [], "technical_terms": ["Every", "Importantly", "When", "Still", "Most", "Thucy", "Building"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03272", "topic": "artificial intelligence", "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?", "authors": ["Zhiyuan He", "Dingmin Wang"], "first_author": "Zhiyuan He", "abstract": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03272", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: When Do Symbolic Solvers Enhance Reasoning in Large Language Models?\nResearch Topic: artificial intelligence\nAuthors: Zhiyuan He, Dingmin Wang\nAbstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.\nCategories: cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03272\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1186, "total_text_length": 1255, "word_count": 171, "sentence_count": 9}, "basic_keywords": ["reasoning", "symbolic", "solver", "performance", "integrated", "llms", "models", "tasks"], "domain_keywords": [], "technical_terms": ["However", "Thought", "When", "Large Reasoning Models", "Chains", "Zebra"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03257", "topic": "artificial intelligence", "title": "PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery", "authors": ["Mark Moussa", "Andre Williams", "Seth Roffe", "Douglas Morton"], "first_author": "Mark Moussa", "abstract": "Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical. We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency. Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03257", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery\nResearch Topic: artificial intelligence\nAuthors: Mark Moussa, Andre Williams, Seth Roffe, Douglas Morton\nAbstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.   We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.   Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.\nCategories: cs.CV, cs.AI, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03257\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 107, "abstract_length": 1418, "total_text_length": 1526, "word_count": 193, "sentence_count": 9}, "basic_keywords": ["fire", "time", "detection", "real", "onboard", "wildfire", "airborne", "missions"], "domain_keywords": ["convolutional neural networks (cnns)"], "technical_terms": ["Using", "Experimental", "Transformer", "Convolutional Neural Networks", "Rapid", "Multispectral", "Airborne Simulator"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03248", "topic": "artificial intelligence", "title": "Learning Network Sheaves for AI-native Semantic Communication", "authors": ["Enrico Grimaldi", "Mario Edoardo Pandolfo", "Gabriele D'Acunto", "Sergio Barbarossa", "Paolo Di Lorenzo"], "first_author": "Enrico Grimaldi", "abstract": "Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.MA", "cs.AI", "cs.LG", "eess.SP"], "url": "http://arxiv.org/abs/2512.03248", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Learning Network Sheaves for AI-native Semantic Communication\nResearch Topic: artificial intelligence\nAuthors: Enrico Grimaldi, Mario Edoardo Pandolfo, Gabriele D'Acunto, Sergio Barbarossa, Paolo Di Lorenzo\nAbstract: Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.\nCategories: cs.MA, cs.AI, cs.LG, eess.SP\nPublication Date: 2025-12-02\nPaper ID: 2512.03248\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 1287, "total_text_length": 1349, "word_count": 181, "sentence_count": 8}, "basic_keywords": ["semantic", "agents", "communication", "space", "learning", "challenge", "exchange", "latent"], "domain_keywords": [], "technical_terms": ["Experiments", "Recent"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03244", "topic": "artificial intelligence", "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning", "authors": ["Salman Rahman", "Sruthi Gorantla", "Arpit Gupta", "Swastik Roy", "Nanyun Peng", "Yang Liu"], "first_author": "Salman Rahman", "abstract": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI", "cs.CL"], "url": "http://arxiv.org/abs/2512.03244", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning\nResearch Topic: artificial intelligence\nAuthors: Salman Rahman, Sruthi Gorantla, Arpit Gupta, Swastik Roy, Nanyun Peng, Yang Liu\nAbstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.\nCategories: cs.LG, cs.AI, cs.CL\nPublication Date: 2025-12-02\nPaper ID: 2512.03244\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 1534, "total_text_length": 1614, "word_count": 213, "sentence_count": 14}, "basic_keywords": ["reward", "ground", "truth", "training", "stage", "process", "models", "step"], "domain_keywords": [], "technical_terms": ["Process", "Using", "Math"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03238", "topic": "artificial intelligence", "title": "How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy", "authors": ["Natalia Ponomareva", "Zheng Xu", "H. Brendan McMahan", "Peter Kairouz", "Lucas Rosenblatt", "Vincent Cohen-Addad", "Cristóbal Guzmán", "Ryan McKenna", "Galen Andrew", "Alex Bie", "Da Yu", "Alex Kurakin", "Morteza Zadimoghaddam", "Sergei Vassilvitskii", "Andreas Terzis"], "first_author": "Natalia Ponomareva", "abstract": "High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, , refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization. In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CR", "cs.AI", "cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03238", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy\nResearch Topic: artificial intelligence\nAuthors: Natalia Ponomareva, Zheng Xu, H. Brendan McMahan, Peter Kairouz, Lucas Rosenblatt, Vincent Cohen-Addad, Cristóbal Guzmán, Ryan McKenna, Galen Andrew, Alex Bie, Da Yu, Alex Kurakin, Morteza Zadimoghaddam, Sergei Vassilvitskii, Andreas Terzis\nAbstract: High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \\emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.   In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.\nCategories: cs.CR, cs.AI, cs.LG, stat.ML\nPublication Date: 2025-12-02\nPaper ID: 2512.03238\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 96, "abstract_length": 1851, "total_text_length": 1948, "word_count": 297, "sentence_count": 12}, "basic_keywords": ["data", "privacy", "synthetic", "users", "user", "high", "quality", "needed"], "domain_keywords": [], "technical_terms": ["High", "However", "Additionally", "Differential Privacy", "Therefore"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03197", "topic": "artificial intelligence", "title": "InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation", "authors": ["Faezeh Faez", "Marzieh S. Tahaei", "Yaochen Hu", "Ali Pourranjbar", "Mahdi Biparva", "Mark Coates", "Yingxue Zhang"], "first_author": "Faezeh Faez", "abstract": "Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.03197", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation\nResearch Topic: artificial intelligence\nAuthors: Faezeh Faez, Marzieh S. Tahaei, Yaochen Hu, Ali Pourranjbar, Mahdi Biparva, Mark Coates, Yingxue Zhang\nAbstract: Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03197\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 106, "abstract_length": 1543, "total_text_length": 1650, "word_count": 208, "sentence_count": 8}, "basic_keywords": ["text", "pipeline", "llms", "data", "generation", "large", "models", "generate"], "domain_keywords": [], "technical_terms": ["These", "Within", "Large Language Models", "Many", "Experimental"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03196", "topic": "artificial intelligence", "title": "Ultra-Strong Gradient Diffusion MRI with Self-Supervised Learning for Prostate Cancer Characterization", "authors": ["Tanishq Patil", "Snigdha Sen", "Malwina Molendowska", "Kieran G. Foley", "Fabrizio Fasano", "Mara Cercignani", "Marco Palombo", "Paddy J. Slator", "Eleftheria Panagiotaki"], "first_author": "Tanishq Patil", "abstract": "Diffusion MRI (dMRI) enables non-invasive assessment of prostate microstructure but conventional metrics such as the Apparent Diffusion Coefficient in multiparametric MRI lack specificity to underlying histology. Integrating dMRI with the compartment-based biophysical VERDICT (Vascular, Extracellular, and Restricted Diffusion for Cytometry in Tumours) framework offers richer microstructural insights, though clinical gradient systems (40-80 mT/m) suffer from poor signal-to-noise ratio (SNR) at stronger diffusion weightings due to prolonged echo times. Ultra-strong gradients (up to 300 mT/m) can mitigate these limitations by improving SNR and contrast-to-noise ratios (CNR) but their adoption has until recently been limited to research environments due to challenges with peripheral nerve stimulation thresholds and gradient non-uniformity. This study investigates whether physics-informed self-supervised VERDICT (ssVERDICT) fitting applied to ultra-strong gradients enhances prostate cancer characterization relative to current clinical acquisitions. We developed enhanced ssVERDICT fitting approaches using dense multilayer perceptron (Dense MLP) and convolutional U-Net architectures, benchmarking them against non-linear least-squares (NLLS) fitting and Diffusion Kurtosis Imaging across clinical- to ultra-strong gradient systems. Dense ssVERDICT at ultra-strong gradient notably outperformed NLLS VERDICT, boosting median CNR by 47%, cutting inter-patient Coefficient of Variation by 52%, and reducing pooled f_ic variation by 50%. Overall, it delivered the highest CNR, the most stable parameter estimates, and the clearest tumour-normal contrast compared with conventional methods and clinical gradient systems. These findings highlight the potential of advanced gradient systems and deep learning-based modelling to improve non-invasive prostate cancer characterization and reduce unnecessary biopsies.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["eess.IV", "cs.AI", "cs.LG"], "url": "http://arxiv.org/abs/2512.03196", "comments": "24 pages, 17 figures, 7 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Ultra-Strong Gradient Diffusion MRI with Self-Supervised Learning for Prostate Cancer Characterization\nResearch Topic: artificial intelligence\nAuthors: Tanishq Patil, Snigdha Sen, Malwina Molendowska, Kieran G. Foley, Fabrizio Fasano, Mara Cercignani, Marco Palombo, Paddy J. Slator, Eleftheria Panagiotaki\nAbstract: Diffusion MRI (dMRI) enables non-invasive assessment of prostate microstructure but conventional metrics such as the Apparent Diffusion Coefficient in multiparametric MRI lack specificity to underlying histology. Integrating dMRI with the compartment-based biophysical VERDICT (Vascular, Extracellular, and Restricted Diffusion for Cytometry in Tumours) framework offers richer microstructural insights, though clinical gradient systems (40-80 mT/m) suffer from poor signal-to-noise ratio (SNR) at stronger diffusion weightings due to prolonged echo times. Ultra-strong gradients (up to 300 mT/m) can mitigate these limitations by improving SNR and contrast-to-noise ratios (CNR) but their adoption has until recently been limited to research environments due to challenges with peripheral nerve stimulation thresholds and gradient non-uniformity. This study investigates whether physics-informed self-supervised VERDICT (ssVERDICT) fitting applied to ultra-strong gradients enhances prostate cancer characterization relative to current clinical acquisitions. We developed enhanced ssVERDICT fitting approaches using dense multilayer perceptron (Dense MLP) and convolutional U-Net architectures, benchmarking them against non-linear least-squares (NLLS) fitting and Diffusion Kurtosis Imaging across clinical- to ultra-strong gradient systems. Dense ssVERDICT at ultra-strong gradient notably outperformed NLLS VERDICT, boosting median CNR by 47%, cutting inter-patient Coefficient of Variation by 52%, and reducing pooled f_ic variation by 50%. Overall, it delivered the highest CNR, the most stable parameter estimates, and the clearest tumour-normal contrast compared with conventional methods and clinical gradient systems. These findings highlight the potential of advanced gradient systems and deep learning-based modelling to improve non-invasive prostate cancer characterization and reduce unnecessary biopsies.\nCategories: eess.IV, cs.AI, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03196\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 102, "abstract_length": 1919, "total_text_length": 2022, "word_count": 248, "sentence_count": 9}, "basic_keywords": ["gradient", "diffusion", "clinical", "systems", "ultra", "strong", "prostate", "verdict"], "domain_keywords": [], "technical_terms": ["These", "Cytometry", "Coefficient", "Dense", "Extracellular", "Ultra", "Integrating", "Apparent Diffusion Coefficient"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03176", "topic": "artificial intelligence", "title": "Plantain: Plan-Answer Interleaved Reasoning", "authors": ["Anthony Liang", "Jonathan Berant", "Adam Fisch", "Abhimanyu Goyal", "Kalpesh Krishna", "Jacob Eisenstein"], "first_author": "Anthony Liang", "abstract": "Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard \"think-then-answer\" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03176", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Plantain: Plan-Answer Interleaved Reasoning\nResearch Topic: artificial intelligence\nAuthors: Anthony Liang, Jonathan Berant, Adam Fisch, Abhimanyu Goyal, Kalpesh Krishna, Jacob Eisenstein\nAbstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard \"think-then-answer\" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03176\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 1644, "total_text_length": 1688, "word_count": 246, "sentence_count": 10}, "basic_keywords": ["reasoning", "user", "time", "response", "answer", "plan", "first", "models"], "domain_keywords": [], "technical_terms": ["Answer Interleaving", "Plan", "Thought", "Reasoning", "Plantain"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03173", "topic": "artificial intelligence", "title": "Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping", "authors": ["Joan Nwatu", "Longju Bai", "Oana Ignat", "Rada Mihalcea"], "first_author": "Joan Nwatu", "abstract": "Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "url": "http://arxiv.org/abs/2512.03173", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping\nResearch Topic: artificial intelligence\nAuthors: Joan Nwatu, Longju Bai, Oana Ignat, Rada Mihalcea\nAbstract: Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.\nCategories: cs.CY, cs.AI, cs.CL, cs.CV\nPublication Date: 2025-12-02\nPaper ID: 2512.03173\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1339, "total_text_length": 1421, "word_count": 171, "sentence_count": 12}, "basic_keywords": ["objects", "income", "datasets", "contexts", "model", "culture", "frequently", "cultural"], "domain_keywords": [], "technical_terms": ["Culture", "Atlas", "Language", "Furthermore", "Vision", "Dollar Street", "Western", "Through"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03042", "topic": "artificial intelligence", "title": "PPTArena: A Benchmark for Agentic PowerPoint Editing", "authors": ["Michael Ofengenden", "Yunze Man", "Ziqi Pang", "Yu-Xiong Wang"], "first_author": "Michael Ofengenden", "abstract": "We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03042", "comments": "25 pages, 26 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: PPTArena: A Benchmark for Agentic PowerPoint Editing\nResearch Topic: artificial intelligence\nAuthors: Michael Ofengenden, Yunze Man, Ziqi Pang, Yu-Xiong Wang\nAbstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03042\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 52, "abstract_length": 1338, "total_text_length": 1391, "word_count": 180, "sentence_count": 7}, "basic_keywords": ["editing", "slide", "pptarena", "reliable", "slides", "text", "over", "edits"], "domain_keywords": [], "technical_terms": ["Each", "Building", "Despite"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03040", "topic": "artificial intelligence", "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation", "authors": ["Zeqi Xiao", "Yiwei Zhao", "Lingxiao Li", "Yushi Lan", "Yu Ning", "Rahul Garg", "Roshni Cooper", "Mohammad H. Taghavi", "Xingang Pan"], "first_author": "Zeqi Xiao", "abstract": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03040", "comments": "Project page at https://xizaoqu.github.io/video4spatial/", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation\nResearch Topic: artificial intelligence\nAuthors: Zeqi Xiao, Yiwei Zhao, Lingxiao Li, Yushi Lan, Yu Ning, Rahul Garg, Roshni Cooper, Mohammad H. Taghavi, Xingang Pan\nAbstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03040\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1093, "total_text_length": 1179, "word_count": 152, "sentence_count": 7}, "basic_keywords": ["video", "models", "scene", "spatial", "tasks", "generative", "visuospatial", "only"], "domain_keywords": [], "technical_terms": ["Taken", "Both"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03127", "topic": "artificial intelligence", "title": "Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra", "authors": ["Ziyu Xiong", "Yichi Zhang", "Foyez Alauddin", "Chu Xin Cheng", "Joon Soo An", "Mohammad R. Seyedsayamdost", "Ellen D. Zhong"], "first_author": "Ziyu Xiong", "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI", "physics.chem-ph"], "url": "http://arxiv.org/abs/2512.03127", "comments": "NeurIPS 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra\nResearch Topic: artificial intelligence\nAuthors: Ziyu Xiong, Yichi Zhang, Foyez Alauddin, Chu Xin Cheng, Joon Soo An, Mohammad R. Seyedsayamdost, Ellen D. Zhong\nAbstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.\nCategories: cs.LG, cs.AI, physics.chem-ph\nPublication Date: 2025-12-02\nPaper ID: 2512.03127\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1187, "total_text_length": 1269, "word_count": 170, "sentence_count": 10}, "basic_keywords": ["natural", "products", "spectra", "chefnmr", "chemical", "elucidation", "structure", "structures"], "domain_keywords": [], "technical_terms": ["Nuclear Magnetic Resonance", "Elucidation From", "Code"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03036", "topic": "artificial intelligence", "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation", "authors": ["Mengchen Zhang", "Qi Chen", "Tong Wu", "Zihan Liu", "Dahua Lin"], "first_author": "Mengchen Zhang", "abstract": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.03036", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation\nResearch Topic: artificial intelligence\nAuthors: Mengchen Zhang, Qi Chen, Tong Wu, Zihan Liu, Dahua Lin\nAbstract: Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03036\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 1496, "total_text_length": 1564, "word_count": 185, "sentence_count": 11}, "basic_keywords": ["audio", "video", "spatial", "binaural", "generation", "visaudio", "mono", "immersion"], "domain_keywords": [], "technical_terms": ["Existing", "Despite", "Project", "Furthermore", "Integrated", "Comprehensive"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03028", "topic": "artificial intelligence", "title": "SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control", "authors": ["Yuxuan Mu", "Ziyu Zhang", "Yi Shi", "Minami Matsumoto", "Kotaro Imamura", "Guy Tevet", "Chuan Guo", "Michael Taylor", "Chang Shu", "Pengcheng Xi", "Xue Bin Peng"], "first_author": "Yuxuan Mu", "abstract": "Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20", "publish_date": "2025-12-02", "update_date": "2025-12-03", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.RO"], "url": "http://arxiv.org/abs/2512.03028", "comments": "14 pages, 9 figures", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control\nResearch Topic: artificial intelligence\nAuthors: Yuxuan Mu, Ziyu Zhang, Yi Shi, Minami Matsumoto, Kotaro Imamura, Guy Tevet, Chuan Guo, Michael Taylor, Chang Shu, Pengcheng Xi, Xue Bin Peng\nAbstract: Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20\nCategories: cs.GR, cs.AI, cs.CV, cs.RO\nPublication Date: 2025-12-02\nPaper ID: 2512.03028\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1511, "total_text_length": 1590, "word_count": 219, "sentence_count": 12}, "basic_keywords": ["motion", "priors", "trained", "data", "adversarial", "learning", "tasks", "naturalistic"], "domain_keywords": [], "technical_terms": ["Video", "However", "Adversarial", "Once", "Furthermore", "Matching Motion Priors", "Data", "Score"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03026", "topic": "artificial intelligence", "title": "The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models", "authors": ["Saeid Jamshidi", "Kawser Wazed Nafi", "Arghavan Moradi Dakhel", "Negar Shahabi", "Foutse Khomh"], "first_author": "Saeid Jamshidi", "abstract": "The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.03026", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models\nResearch Topic: artificial intelligence\nAuthors: Saeid Jamshidi, Kawser Wazed Nafi, Arghavan Moradi Dakhel, Negar Shahabi, Foutse Khomh\nAbstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03026\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 1716, "total_text_length": 1804, "word_count": 235, "sentence_count": 10}, "basic_keywords": ["ethical", "moral", "mocop", "reasoning", "model", "behavior", "llms", "consistency"], "domain_keywords": [], "technical_terms": ["These", "Existing", "Turbo", "Moral Consistency Pipeline", "Furthermore", "Large Language Models"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03025", "topic": "artificial intelligence", "title": "LORE: A Large Generative Model for Search Relevance", "authors": ["Chenji Lu", "Zhuo Chen", "Hui Zhao", "Zhiyuan Zeng", "Gang Zhao", "Junjie Ren", "Ruicong Xu", "Haoran Li", "Songyan Liu", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "first_author": "Chenji Lu", "abstract": "Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.", "publish_date": "2025-12-02", "update_date": "2025-12-04", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.03025", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: LORE: A Large Generative Model for Search Relevance\nResearch Topic: artificial intelligence\nAuthors: Chenji Lu, Zhuo Chen, Hui Zhao, Zhiyuan Zeng, Gang Zhao, Junjie Ren, Ruicong Xu, Haoran Li, Songyan Liu, Pengjie Wang, Jian Xu, Bo Zheng\nAbstract: Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.\nCategories: cs.IR, cs.AI, cs.CL, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03025\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 1393, "total_text_length": 1445, "word_count": 191, "sentence_count": 16}, "basic_keywords": ["relevance", "lore", "capabilities", "online", "lifecycle", "training", "deployment", "insight"], "domain_keywords": [], "technical_terms": ["While", "Large Generative Model", "Contributions", "Thought", "Chain", "Achievement", "Deployed", "Insight"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03024", "topic": "artificial intelligence", "title": "TokenPowerBench: Benchmarking the Power Consumption of LLM Inference", "authors": ["Chenxu Niu", "Wei Zhang", "Jie Li", "Yongjian Zhao", "Tongyang Wang", "Xi Wang", "Yong Chen"], "first_author": "Chenxu Niu", "abstract": "Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.DC"], "url": "http://arxiv.org/abs/2512.03024", "comments": "Accepted by the AAAI'26 Conference Main Track", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: TokenPowerBench: Benchmarking the Power Consumption of LLM Inference\nResearch Topic: artificial intelligence\nAuthors: Chenxu Niu, Wei Zhang, Jie Li, Yongjian Zhao, Tongyang Wang, Xi Wang, Yong Chen\nAbstract: Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.\nCategories: cs.LG, cs.AI, cs.CY, cs.DC\nPublication Date: 2025-12-02\nPaper ID: 2512.03024\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1527, "total_text_length": 1596, "word_count": 220, "sentence_count": 9}, "basic_keywords": ["power", "inference", "model", "consumption", "tokenpowerbench", "services", "training", "measurement"], "domain_keywords": [], "technical_terms": ["These", "However", "Furthermore", "Large", "Llama", "Falcon", "Mistral", "Qwen"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03019", "topic": "artificial intelligence", "title": "Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge", "authors": ["Hamid Dadkhahi", "Firas Trabelsi", "Parker Riley", "Juraj Juraska", "Mehdi Mirzazadeh"], "first_author": "Hamid Dadkhahi", "abstract": "Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03019", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge\nResearch Topic: artificial intelligence\nAuthors: Hamid Dadkhahi, Firas Trabelsi, Parker Riley, Juraj Juraska, Mehdi Mirzazadeh\nAbstract: Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03019\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1093, "total_text_length": 1168, "word_count": 142, "sentence_count": 6}, "basic_keywords": ["aggregation", "thinking", "models", "pairwise", "preferences", "noisy", "self", "ties"], "domain_keywords": [], "technical_terms": ["These", "Terry", "Across", "Thinking Large Language Models", "Bradley", "Davidson"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03013", "topic": "artificial intelligence", "title": "In-Context Sync-LoRA for Portrait Video Editing", "authors": ["Sagi Polaczek", "Or Patashnik", "Ali Mahdavi-Amiri", "Daniel Cohen-Or"], "first_author": "Sagi Polaczek", "abstract": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.AI", "cs.GR"], "url": "http://arxiv.org/abs/2512.03013", "comments": "Project page: https://sagipolaczek.github.io/Sync-LoRA/", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: In-Context Sync-LoRA for Portrait Video Editing\nResearch Topic: artificial intelligence\nAuthors: Sagi Polaczek, Or Patashnik, Ali Mahdavi-Amiri, Daniel Cohen-Or\nAbstract: Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.\nCategories: cs.CV, cs.AI, cs.GR\nPublication Date: 2025-12-02\nPaper ID: 2512.03013\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 47, "abstract_length": 1599, "total_text_length": 1647, "word_count": 227, "sentence_count": 12}, "basic_keywords": ["frame", "videos", "appearance", "lora", "visual", "synchronization", "motion", "editing"], "domain_keywords": [], "technical_terms": ["These", "Editing", "Sync", "Trained"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03125", "topic": "artificial intelligence", "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models", "authors": ["Xiwen Wei", "Mustafa Munir", "Radu Marculescu"], "first_author": "Xiwen Wei", "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/abs/2512.03125", "comments": "NeurIPS 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models\nResearch Topic: artificial intelligence\nAuthors: Xiwen Wei, Mustafa Munir, Radu Marculescu\nAbstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git\nCategories: cs.LG, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03125\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1378, "total_text_length": 1474, "word_count": 178, "sentence_count": 10}, "basic_keywords": ["forgetting", "modal", "modality", "intra", "inter", "mode", "both", "modalities"], "domain_keywords": [], "technical_terms": ["Modality", "However", "Experiments", "While", "Codes", "Unified Multimodal Generative Models", "Unlike", "Decoupled Experts"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03005", "topic": "artificial intelligence", "title": "From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?", "authors": ["Dawei Li", "Abdullah Alnaibari", "Arslan Bisharat", "Manny Sandoval", "Deborah Hall", "Yasin Silva", "Huan Liu"], "first_author": "Dawei Li", "abstract": "The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03005", "comments": "Under review", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?\nResearch Topic: artificial intelligence\nAuthors: Dawei Li, Abdullah Alnaibari, Arslan Bisharat, Manny Sandoval, Deborah Hall, Yasin Silva, Huan Liu\nAbstract: The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.\nCategories: cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03005\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 1158, "total_text_length": 1238, "word_count": 169, "sentence_count": 9}, "basic_keywords": ["llms", "mediation", "online", "based", "large", "models", "both", "rapid"], "domain_keywords": [], "technical_terms": ["Experiments", "Reddit"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.6000000000000001}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03001", "topic": "artificial intelligence", "title": "Invasive Context Engineering to Control Large Language Models", "authors": ["Thomas Rivasseau"], "first_author": "Thomas Rivasseau", "abstract": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.AI"], "url": "http://arxiv.org/abs/2512.03001", "comments": "4 pages", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Invasive Context Engineering to Control Large Language Models\nResearch Topic: artificial intelligence\nAuthors: Thomas Rivasseau\nAbstract: Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.\nCategories: cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.03001\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 771, "total_text_length": 833, "word_count": 115, "sentence_count": 7}, "basic_keywords": ["context", "training", "control", "models", "long", "situations", "invasive", "engineering"], "domain_keywords": [], "technical_terms": ["Despite", "Thought", "Large Language Models", "Chain", "Current", "Invasive Context Engineering", "There"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.02987", "topic": "artificial intelligence", "title": "Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic", "authors": ["Muyu Pan", "Dheeraj Kodakandla", "Mahfuza Farooque"], "first_author": "Muyu Pan", "abstract": "Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CL", "cs.AI"], "url": "http://arxiv.org/abs/2512.02987", "comments": "IEEE ISNCC 2025", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic\nResearch Topic: artificial intelligence\nAuthors: Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque\nAbstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.\nCategories: cs.CL, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.02987\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1079, "total_text_length": 1177, "word_count": 150, "sentence_count": 8}, "basic_keywords": ["language", "into", "hallucinations", "model", "natural", "particularly", "llms", "translation"], "domain_keywords": [], "technical_terms": ["However", "English", "Conjunctive Normal Form", "Thus", "Recent"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.02978", "topic": "artificial intelligence", "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding", "authors": ["Paul Barbaste", "Olivier Oullier", "Xavier Vasques"], "first_author": "Paul Barbaste", "abstract": "Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["q-bio.NC", "cs.AI", "cs.HC", "cs.LG"], "url": "http://arxiv.org/abs/2512.02978", "comments": "28 pages, 8 figures, 2 tables", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding\nResearch Topic: artificial intelligence\nAuthors: Paul Barbaste, Olivier Oullier, Xavier Vasques\nAbstract: Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.\nCategories: q-bio.NC, cs.AI, cs.HC, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.02978\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1674, "total_text_length": 1791, "word_count": 221, "sentence_count": 12}, "basic_keywords": ["classification", "patterns", "participant", "variability", "spatial", "across", "datasets", "level"], "domain_keywords": [], "technical_terms": ["However", "Here", "Common Spatial Patterns", "Importantly", "Riemannian", "Unlike", "Covariance", "Robust"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.02966", "topic": "artificial intelligence", "title": "Lumos: Let there be Language Model System Certification", "authors": ["Isha Chaudhary", "Vedaant Jain", "Avaljot Singh", "Kavya Sachdeva", "Sayan Ranu", "Gagandeep Singh"], "first_author": "Isha Chaudhary", "abstract": "We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.PL", "cs.AI", "cs.MA"], "url": "http://arxiv.org/abs/2512.02966", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Lumos: Let there be Language Model System Certification\nResearch Topic: artificial intelligence\nAuthors: Isha Chaudhary, Vedaant Jain, Avaljot Singh, Kavya Sachdeva, Sayan Ranu, Gagandeep Singh\nAbstract: We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.\nCategories: cs.PL, cs.AI, cs.MA\nPublication Date: 2025-12-02\nPaper ID: 2512.02966\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 55, "abstract_length": 1693, "total_text_length": 1749, "word_count": 233, "sentence_count": 12}, "basic_keywords": ["lumos", "specifications", "first", "specifying", "certifying", "language", "safety", "framework"], "domain_keywords": [], "technical_terms": ["Using", "Language Model System", "Qwen", "Lumos"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.02942", "topic": "artificial intelligence", "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench", "authors": ["Lanxiang Hu", "Abhilash Shankarampeta", "Yixin Huang", "Zilin Dai", "Haoyang Yu", "Yujie Zhao", "Haoqiang Kang", "Daniel Zhao", "Tajana Rosing", "Hao Zhang"], "first_author": "Lanxiang Hu", "abstract": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: {github.com/hao-ai-lab/VideoScience}.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.02942", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench\nResearch Topic: artificial intelligence\nAuthors: Lanxiang Hu, Abhilash Shankarampeta, Yixin Huang, Zilin Dai, Haoyang Yu, Yujie Zhao, Haoqiang Kang, Daniel Zhao, Tajana Rosing, Hao Zhang\nAbstract: The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.02942\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1470, "total_text_length": 1568, "word_count": 194, "sentence_count": 11}, "basic_keywords": ["video", "scientific", "models", "understanding", "reasoning", "physical", "videoscience", "benchmark"], "domain_keywords": [], "technical_terms": ["However", "Using", "Spatio", "Temporal Continuity", "Each", "Prompt Consistency", "Bench", "Judge"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.02932", "topic": "artificial intelligence", "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis", "authors": ["Yancheng Zhang", "Guangyu Sun", "Chen Chen"], "first_author": "Yancheng Zhang", "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.AI"], "url": "http://arxiv.org/abs/2512.02932", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis\nResearch Topic: artificial intelligence\nAuthors: Yancheng Zhang, Guangyu Sun, Chen Chen\nAbstract: Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.\nCategories: cs.CV, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.02932\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1117, "total_text_length": 1215, "word_count": 154, "sentence_count": 8}, "basic_keywords": ["gaussian", "view", "splatting", "rendering", "high", "appearance", "multi", "geometric"], "domain_keywords": ["computer vision"], "technical_terms": ["Hybrid Gaussian Rasterization", "Decoupled Optimization", "Novel", "Extensive", "While", "Gaussian", "Adaptive Type Exchange", "Frequency"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04124", "topic": "artificial intelligence", "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models", "authors": ["Afshin Khadangi", "Hanna Marxen", "Amir Sartipi", "Igor Tchappi", "Gilbert Fridgen"], "first_author": "Afshin Khadangi", "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CY", "cs.AI"], "url": "http://arxiv.org/abs/2512.04124", "comments": "", "query_keywords": "artificial intelligence", "time_range": "week", "crawl_time": "2025-12-08T01:32:55.436032", "embedding_text": "Paper Title: When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models\nResearch Topic: artificial intelligence\nAuthors: Afshin Khadangi, Hanna Marxen, Amir Sartipi, Igor Tchappi, Gilbert Fridgen\nAbstract: Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.\nCategories: cs.CY, cs.AI\nPublication Date: 2025-12-02\nPaper ID: 2512.04124\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1908, "total_text_length": 2001, "word_count": 271, "sentence_count": 14}, "basic_keywords": ["gemini", "frontier", "models", "llms", "grok", "self", "stage", "therapy"], "domain_keywords": [], "technical_terms": ["Psychotherapy", "Using", "Therapy", "Grok", "Under", "Big Five", "Gemini", "Characterisation"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05116", "topic": "machine learning", "title": "Value Gradient Guidance for Flow Matching Alignment", "authors": ["Zhen Liu", "Tim Z. Xiao", "Carles Domingo-Enrich", "Weiyang Liu", "Dinghuai Zhang"], "first_author": "Zhen Liu", "abstract": "While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CV"], "url": "http://arxiv.org/abs/2512.05116", "comments": "Accepted at NeurIPS 2025; 26 pages, 20 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.213872", "embedding_text": "Paper Title: Value Gradient Guidance for Flow Matching Alignment\nResearch Topic: machine learning\nAuthors: Zhen Liu, Tim Z. Xiao, Carles Domingo-Enrich, Weiyang Liu, Dinghuai Zhang\nAbstract: While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.\nCategories: cs.LG, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05116\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 999, "total_text_length": 1051, "word_count": 146, "sentence_count": 6}, "basic_keywords": ["flow", "matching", "models", "method", "while", "popular", "effective", "adaptation"], "domain_keywords": [], "technical_terms": ["Flow", "Stable Diffusion", "While", "Empirically"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05114", "topic": "machine learning", "title": "Deep infant brain segmentation from multi-contrast MRI", "authors": ["Malte Hoffmann", "Lilla Zöllei", "Adrian V. Dalca"], "first_author": "Malte Hoffmann", "abstract": "Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CV", "eess.IV"], "url": "http://arxiv.org/abs/2512.05114", "comments": "8 pages, 8 figures, 1 table, website at https://w3id.org/babyseg, presented at the 2025 IEEE Asilomar Conference on Signals, Systems, and Computers", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.213872", "embedding_text": "Paper Title: Deep infant brain segmentation from multi-contrast MRI\nResearch Topic: machine learning\nAuthors: Malte Hoffmann, Lilla Zöllei, Adrian V. Dalca\nAbstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.\nCategories: cs.LG, cs.CV, eess.IV\nPublication Date: 2025-12-04\nPaper ID: 2512.05114\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 54, "abstract_length": 1399, "total_text_length": 1454, "word_count": 206, "sentence_count": 9}, "basic_keywords": ["segmentation", "images", "brain", "development", "infants", "young", "children", "imaging"], "domain_keywords": [], "technical_terms": ["Pediatric", "However", "Segmentation"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05106", "topic": "machine learning", "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation", "authors": ["Yu Zeng", "Charles Ochoa", "Mingyuan Zhou", "Vishal M. Patel", "Vitor Guizilini", "Rowan McAllister"], "first_author": "Yu Zeng", "abstract": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion -PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. -PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, -PD produces controllable, spatially aligned results. When applied to the CARLA simulator, -PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our {project page}.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.RO"], "url": "http://arxiv.org/abs/2512.05106", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation\nResearch Topic: machine learning\nAuthors: Yu Zeng, Charles Ochoa, Mingyuan Zhou, Vishal M. Patel, Vitor Guizilini, Rowan McAllister\nAbstract: Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. φ-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, φ-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, φ-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \\href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.\nCategories: cs.CV, cs.GR, cs.LG, cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.05106\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1359, "total_text_length": 1435, "word_count": 173, "sentence_count": 10}, "basic_keywords": ["image", "diffusion", "generation", "phase", "noise", "random", "while", "structure"], "domain_keywords": [], "technical_terms": ["Phase", "While", "Waymo", "Fourier", "Across", "Gaussian", "Frequency", "When"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05092", "topic": "machine learning", "title": "Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction", "authors": ["Vincent Pauline", "Tobias Höppe", "Kirill Neklyudov", "Alexander Tong", "Stefan Bauer", "Andrea Dittadi"], "first_author": "Vincent Pauline", "abstract": "Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in and continuous-time Markov chains (CTMCs) on finite alphabets -- and derive the associated Fokker--Planck and master equations. A common variational treatment yields the ELBO that underpins standard training losses. We make explicit how forward corruption choices -- Gaussian processes in continuous spaces and structured categorical transition kernels (uniform, masking/absorbing and more) in discrete spaces -- shape reverse dynamics and the ELBO. The presentation is layered for three audiences: newcomers seeking a self-contained intuitive introduction; diffusion practitioners wanting a global theoretical synthesis; and continuous-diffusion experts looking for an analogy-first path into discrete diffusion. The result is a unified roadmap to modern diffusion methodology across continuous domains and discrete sequences, highlighting a compact set of reusable proofs, identities, and core theoretical principles.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/abs/2512.05092", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction\nResearch Topic: machine learning\nAuthors: Vincent Pauline, Tobias Höppe, Kirill Neklyudov, Alexander Tong, Stefan Bauer, Andrea Dittadi\nAbstract: Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in $\\mathbb{R}^d$ and continuous-time Markov chains (CTMCs) on finite alphabets -- and derive the associated Fokker--Planck and master equations. A common variational treatment yields the ELBO that underpins standard training losses. We make explicit how forward corruption choices -- Gaussian processes in continuous spaces and structured categorical transition kernels (uniform, masking/absorbing and more) in discrete spaces -- shape reverse dynamics and the ELBO. The presentation is layered for three audiences: newcomers seeking a self-contained intuitive introduction; diffusion practitioners wanting a global theoretical synthesis; and continuous-diffusion experts looking for an analogy-first path into discrete diffusion. The result is a unified roadmap to modern diffusion methodology across continuous domains and discrete sequences, highlighting a compact set of reusable proofs, identities, and core theoretical principles.\nCategories: stat.ML, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05092\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1460, "total_text_length": 1547, "word_count": 201, "sentence_count": 8}, "basic_keywords": ["diffusion", "discrete", "continuous", "spaces", "time", "state", "self", "contained"], "domain_keywords": [], "technical_terms": ["Planck", "Markov", "Euclidean", "Gaussian", "Fokker", "Although"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05084", "topic": "machine learning", "title": "Gradient Descent with Provably Tuned Learning-rate Schedules", "authors": ["Dravyansh Sharma"], "first_author": "Dravyansh Sharma", "abstract": "Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.05084", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Gradient Descent with Provably Tuned Learning-rate Schedules\nResearch Topic: machine learning\nAuthors: Dravyansh Sharma\nAbstract: Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05084\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 60, "abstract_length": 1470, "total_text_length": 1531, "word_count": 215, "sentence_count": 11}, "basic_keywords": ["gradient", "tuning", "learning", "descent", "functions", "based", "work", "step"], "domain_keywords": [], "technical_terms": ["However", "Roughgarden", "Gradient", "Recent", "They", "Gupta"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05080", "topic": "machine learning", "title": "OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design", "authors": ["Ian Dunn", "Liv Toft", "Tyler Katz", "Juhi Gupta", "Riya Shah", "Ramith Hettiarachchi", "David R. Koes"], "first_author": "Ian Dunn", "abstract": "Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.05080", "comments": "Presented at the Machine Learning for Structural Biology Workshop, 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design\nResearch Topic: machine learning\nAuthors: Ian Dunn, Liv Toft, Tyler Katz, Juhi Gupta, Riya Shah, Ramith Hettiarachchi, David R. Koes\nAbstract: Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05080\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1190, "total_text_length": 1259, "word_count": 175, "sentence_count": 9}, "basic_keywords": ["design", "sbdd", "omtra", "structure", "protein", "methods", "modern", "workflows"], "domain_keywords": [], "technical_terms": ["Additionally", "Modern", "Structure", "Computational"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05070", "topic": "machine learning", "title": "Control Consistency Losses for Diffusion Bridges", "authors": ["Samuel Howard", "Nikolas Nüsken", "Jakiw Pidstrigach"], "first_author": "Samuel Howard", "abstract": "Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/abs/2512.05070", "comments": "Frontiers in Probabilistic Inference: Sampling Meets Learning Workshop at NeurIPS 2025 (Oral)", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Control Consistency Losses for Diffusion Bridges\nResearch Topic: machine learning\nAuthors: Samuel Howard, Nikolas Nüsken, Jakiw Pidstrigach\nAbstract: Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.\nCategories: stat.ML, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05070\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 48, "abstract_length": 497, "total_text_length": 546, "word_count": 78, "sentence_count": 4}, "basic_keywords": ["dynamics", "conditioned", "diffusion", "terminal", "simulating", "processes", "given", "initial"], "domain_keywords": [], "technical_terms": ["Simulating"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.7749999999999999, "quality_tier": "medium"}
{"paper_id": "2512.05069", "topic": "machine learning", "title": "Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection", "authors": ["Mohammad Arif Rasyidi", "Omar Alhussein", "Sami Muhaidat", "Ernesto Damiani"], "first_author": "Mohammad Arif Rasyidi", "abstract": "Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement, measurement approach, variational and non-variational formulations, and latent-space regularization. Experiments across three benchmark NIDS datasets show that HQC autoencoders can match or exceed classical performance in their best configurations, although they exhibit higher sensitivity to architectural decisions. Under zero-day evaluation, well-configured HQC models provide stronger and more stable generalization than classical and supervised baselines. Simulated gate-noise experiments reveal early performance degradation, indicating the need for noise-aware HQC designs. These results provide the first data-driven characterization of HQC autoencoder behavior for network intrusion detection and outline key factors that govern their practical viability. All experiment code and configurations are available at https://github.com/arasyi/hqcae-network-intrusion-detection.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CR", "quant-ph"], "url": "http://arxiv.org/abs/2512.05069", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection\nResearch Topic: machine learning\nAuthors: Mohammad Arif Rasyidi, Omar Alhussein, Sami Muhaidat, Ernesto Damiani\nAbstract: Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement, measurement approach, variational and non-variational formulations, and latent-space regularization. Experiments across three benchmark NIDS datasets show that HQC autoencoders can match or exceed classical performance in their best configurations, although they exhibit higher sensitivity to architectural decisions. Under zero-day evaluation, well-configured HQC models provide stronger and more stable generalization than classical and supervised baselines. Simulated gate-noise experiments reveal early performance degradation, indicating the need for noise-aware HQC designs. These results provide the first data-driven characterization of HQC autoencoder behavior for network intrusion detection and outline key factors that govern their practical viability. All experiment code and configurations are available at https://github.com/arasyi/hqcae-network-intrusion-detection.\nCategories: cs.LG, cs.CR, quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.05069\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 82, "abstract_length": 1255, "total_text_length": 1338, "word_count": 156, "sentence_count": 10}, "basic_keywords": ["intrusion", "detection", "quantum", "classical", "models", "first", "evaluation", "autoencoders"], "domain_keywords": [], "technical_terms": ["These", "Experiments", "Under", "Simulated", "Unsupervised"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05038", "topic": "machine learning", "title": "SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals", "authors": ["Cassandra Goldberg", "Chaehyeon Kim", "Adam Stein", "Eric Wong"], "first_author": "Cassandra Goldberg", "abstract": "Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.05038", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals\nResearch Topic: machine learning\nAuthors: Cassandra Goldberg, Chaehyeon Kim, Adam Stein, Eric Wong\nAbstract: Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05038\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 905, "total_text_length": 990, "word_count": 132, "sentence_count": 5}, "basic_keywords": ["concept", "model", "activations", "superactivator", "mechanism", "tokens", "vectors", "enhance"], "domain_keywords": [], "technical_terms": ["Concept", "Finally", "Mechanism"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05030", "topic": "machine learning", "title": "Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression", "authors": ["Xuan Li", "Samuel Bello"], "first_author": "Xuan Li", "abstract": "Accurate estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs) is crucial for both biomechanics research and clinical rehabilitation evaluation. In this study, we focus on insole-based GRF/GRM estimation and further validate our approach on a public walking dataset. We propose a Dual-Path Region-Guided Attention Network that integrates anatomy-inspired spatial priors and temporal priors into a region-level attention mechanism, while a complementary path captures context from the full sensor field. The two paths are trained jointly and their outputs are combined to produce the final GRF/GRM predictions. Conclusions: Our model outperforms strong baseline models, including CNN and CNN-LSTM architectures on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical ground reaction force on the public dataset. This demonstrates robust performance for ground reaction force and moment estimation.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "eess.SP"], "url": "http://arxiv.org/abs/2512.05030", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression\nResearch Topic: machine learning\nAuthors: Xuan Li, Samuel Bello\nAbstract: Accurate estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs) is crucial for both biomechanics research and clinical rehabilitation evaluation. In this study, we focus on insole-based GRF/GRM estimation and further validate our approach on a public walking dataset. We propose a Dual-Path Region-Guided Attention Network that integrates anatomy-inspired spatial priors and temporal priors into a region-level attention mechanism, while a complementary path captures context from the full sensor field. The two paths are trained jointly and their outputs are combined to produce the final GRF/GRM predictions. Conclusions: Our model outperforms strong baseline models, including CNN and CNN-LSTM architectures on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical ground reaction force on the public dataset. This demonstrates robust performance for ground reaction force and moment estimation.\nCategories: cs.LG, eess.SP\nPublication Date: 2025-12-04\nPaper ID: 2512.05030\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 989, "total_text_length": 1079, "word_count": 148, "sentence_count": 9}, "basic_keywords": ["estimation", "ground", "reaction", "dataset", "insole", "public", "path", "region"], "domain_keywords": [], "technical_terms": ["Dual", "Path Region", "Guided Attention Network", "Conclusions", "Accurate"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05021", "topic": "machine learning", "title": "HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition", "authors": ["Pham Thach Thanh Truc", "Dang Hoai Nam", "Huynh Tong Dang Khoa", "Vo Nguyen Le Duy"], "first_author": "Pham Thach Thanh Truc", "abstract": "Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.05021", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition\nResearch Topic: machine learning\nAuthors: Pham Thach Thanh Truc, Dang Hoai Nam, Huynh Tong Dang Khoa, Vo Nguyen Le Duy\nAbstract: Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.05021\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1240, "total_text_length": 1334, "word_count": 172, "sentence_count": 9}, "basic_keywords": ["limited", "data", "high", "writing", "existing", "address", "convtext", "model"], "domain_keywords": ["feature extraction"], "technical_terms": ["Handwritten Text Recognition", "Existing", "Additionally", "Evaluations", "Connectionist Temporal Classification", "Positional Encoding", "Convolutional Neural Network"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04985", "topic": "machine learning", "title": "Towards a unified framework for guided diffusion models", "authors": ["Yuchen Jiao", "Yuxin Chen", "Gen Li"], "first_author": "Yuchen Jiao", "abstract": "Guided or controlled data generation with diffusion models.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ML", "cs.LG", "math.ST"], "url": "http://arxiv.org/abs/2512.04985", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Towards a unified framework for guided diffusion models\nResearch Topic: machine learning\nAuthors: Yuchen Jiao, Yuxin Chen, Gen Li\nAbstract: Guided or controlled data generation with diffusion models\\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \\citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.\nCategories: stat.ML, cs.LG, math.ST\nPublication Date: 2025-12-04\nPaper ID: 2512.04985\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 55, "abstract_length": 1236, "total_text_length": 1292, "word_count": 169, "sentence_count": 9}, "basic_keywords": ["diffusion", "reward", "guided", "theoretical", "framework", "guidance", "models", "classifier"], "domain_keywords": [], "technical_terms": ["Numerical", "Aimed", "Despite", "When", "Guided"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04981", "topic": "machine learning", "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models", "authors": ["NaHyeon Park", "Namin An", "Kunhee Kim", "Soyeon Yoon", "Jiahao Huo", "Hyunjung Shim"], "first_author": "NaHyeon Park", "abstract": "Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.04981", "comments": "Project page: https://fairpro-t2i.github.io", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models\nResearch Topic: machine learning\nAuthors: NaHyeon Park, Namin An, Kunhee Kim, Soyeon Yoon, Jiahao Huo, Hyunjung Shim\nAbstract: Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04981\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 115, "abstract_length": 1335, "total_text_length": 1451, "word_count": 193, "sentence_count": 10}, "basic_keywords": ["image", "lvlm", "based", "system", "prompts", "models", "demographic", "bias"], "domain_keywords": [], "technical_terms": ["Experiments", "Image", "Large", "Through", "Qwen"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04980", "topic": "machine learning", "title": "Learning Causality for Longitudinal Data", "authors": ["Mouad EL Bouchattaoui"], "first_author": "Mouad EL Bouchattaoui", "abstract": "This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data. The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables. The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference. The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ML", "cs.IT", "cs.LG"], "url": "http://arxiv.org/abs/2512.04980", "comments": "PhD thesis manuscript", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Learning Causality for Longitudinal Data\nResearch Topic: machine learning\nAuthors: Mouad EL Bouchattaoui\nAbstract: This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.   The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.   The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.   The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.\nCategories: stat.ML, cs.IT, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04980\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 40, "abstract_length": 1728, "total_text_length": 1769, "word_count": 224, "sentence_count": 12}, "basic_keywords": ["latent", "causal", "contribution", "cdvae", "based", "observed", "inference", "time"], "domain_keywords": [], "technical_terms": ["Experiments", "Scalable Jacobian", "Individual Treatment Effects", "Contrastive Predictive Coding", "Jacobian", "Causal Dynamic Variational Autoencoder"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04974", "topic": "machine learning", "title": "Efficient Generative Transformer Operators For Million-Point PDEs", "authors": ["Armand Kassaï Koupaï", "Lise Le Boudec", "Patrick Gallinari"], "first_author": "Armand Kassaï Koupaï", "abstract": "We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dynamic unrolling, and task-specific design. ECHO addresses these challenges through three key innovations. (i) It employs a hierarchical convolutional encode-decode architecture that achieves a 100 spatio-temporal compression while preserving fidelity on mesh points. (ii) It incorporates a training and adaptation strategy that enables high-resolution PDE solution generation from sparse input grids. (iii) It adopts a generative modeling paradigm that learns complete trajectory segments, mitigating long-horizon error drift. The training strategy decouples representation learning from downstream task supervision, allowing the model to tackle multiple tasks such as trajectory generation, forward and inverse problems, and interpolation. The generative model further supports both conditional and unconditional generation. We demonstrate state-of-the-art performance on million-point simulations across diverse PDE systems featuring complex geometries, high-frequency dynamics, and long-term horizons.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04974", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Efficient Generative Transformer Operators For Million-Point PDEs\nResearch Topic: machine learning\nAuthors: Armand Kassaï Koupaï, Lise Le Boudec, Patrick Gallinari\nAbstract: We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dynamic unrolling, and task-specific design. ECHO addresses these challenges through three key innovations. (i) It employs a hierarchical convolutional encode-decode architecture that achieves a 100 $\\times$ spatio-temporal compression while preserving fidelity on mesh points. (ii) It incorporates a training and adaptation strategy that enables high-resolution PDE solution generation from sparse input grids. (iii) It adopts a generative modeling paradigm that learns complete trajectory segments, mitigating long-horizon error drift. The training strategy decouples representation learning from downstream task supervision, allowing the model to tackle multiple tasks such as trajectory generation, forward and inverse problems, and interpolation. The generative model further supports both conditional and unconditional generation. We demonstrate state-of-the-art performance on million-point simulations across diverse PDE systems featuring complex geometries, high-frequency dynamics, and long-term horizons.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04974\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1304, "total_text_length": 1370, "word_count": 169, "sentence_count": 10}, "basic_keywords": ["generation", "echo", "million", "point", "while", "grids", "error", "task"], "domain_keywords": [], "technical_terms": ["While"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04969", "topic": "machine learning", "title": "Rethinking the Use of Vision Transformers for AI-Generated Image Detection", "authors": ["NaHyeon Park", "Kunhee Kim", "Junsuk Choe", "Hyunjung Shim"], "first_author": "NaHyeon Park", "abstract": "Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.04969", "comments": "Code: https://github.com/nahyeonkaty/mold", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Rethinking the Use of Vision Transformers for AI-Generated Image Detection\nResearch Topic: machine learning\nAuthors: NaHyeon Park, Kunhee Kim, Junsuk Choe, Hyunjung Shim\nAbstract: Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04969\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1130, "total_text_length": 1205, "word_count": 159, "sentence_count": 8}, "basic_keywords": ["features", "detection", "generated", "layer", "layers", "image", "final", "performance"], "domain_keywords": [], "technical_terms": ["Extensive", "While", "Finally", "Motivated", "Rich", "Moreover"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04966", "topic": "machine learning", "title": "Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels", "authors": ["Guangming Liang", "Mingjie Yang", "Dongzhu Liu", "Paul Henderson", "Lajos Hanzo"], "first_author": "Guangming Liang", "abstract": "Accurate channel state information (CSI) underpins reliable and efficient wireless communication. However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments. By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates. In contrast to prior studies that rely on predefined channel models, we develop a data-driven framework that formulates the sensing-to-channel mapping as a cross-modal flow matching problem. The framework fuses multimodal features into a latent distribution within the channel domain, and learns a velocity field that continuously transforms the latent distribution toward the channel distribution. To make this formulation tractable and efficient, we reformulate the problem as an equivalent conditional flow matching objective and incorporate a modality alignment loss, while adopting low-latency inference mechanisms to enable real-time CSI estimation. In experiments, we build a procedural data generator based on Sionna and Blender to support realistic modeling of sensing scenes and wireless propagation. System-level evaluations demonstrate significant improvements over pilot- and sensing-based benchmarks in both channel estimation accuracy and spectral efficiency for the downstream beamforming task.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IT", "cs.LG", "eess.SP"], "url": "http://arxiv.org/abs/2512.04966", "comments": "13 pages, 13 figures, 40 references, submitted to IEEE for possible publication", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels\nResearch Topic: machine learning\nAuthors: Guangming Liang, Mingjie Yang, Dongzhu Liu, Paul Henderson, Lajos Hanzo\nAbstract: Accurate channel state information (CSI) underpins reliable and efficient wireless communication. However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments. By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates. In contrast to prior studies that rely on predefined channel models, we develop a data-driven framework that formulates the sensing-to-channel mapping as a cross-modal flow matching problem. The framework fuses multimodal features into a latent distribution within the channel domain, and learns a velocity field that continuously transforms the latent distribution toward the channel distribution. To make this formulation tractable and efficient, we reformulate the problem as an equivalent conditional flow matching objective and incorporate a modality alignment loss, while adopting low-latency inference mechanisms to enable real-time CSI estimation. In experiments, we build a procedural data generator based on Sionna and Blender to support realistic modeling of sensing scenes and wireless propagation. System-level evaluations demonstrate significant improvements over pilot- and sensing-based benchmarks in both channel estimation accuracy and spectral efficiency for the downstream beamforming task.\nCategories: cs.IT, cs.LG, eess.SP\nPublication Date: 2025-12-04\nPaper ID: 2512.04966\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 102, "abstract_length": 1548, "total_text_length": 1651, "word_count": 210, "sentence_count": 9}, "basic_keywords": ["channel", "sensing", "pilot", "estimation", "data", "distribution", "efficient", "wireless"], "domain_keywords": [], "technical_terms": ["However", "Blender", "System", "Doppler", "Accurate", "Sionna"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04954", "topic": "machine learning", "title": "Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows", "authors": ["Rajneil Baruah"], "first_author": "Rajneil Baruah", "abstract": "We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "hep-ex", "hep-ph", "physics.comp-ph", "physics.data-an"], "url": "http://arxiv.org/abs/2512.04954", "comments": "14 pages, 8 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows\nResearch Topic: machine learning\nAuthors: Rajneil Baruah\nAbstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.\nCategories: cs.LG, hep-ex, hep-ph, physics.comp-ph, physics.data-an\nPublication Date: 2025-12-04\nPaper ID: 2512.04954\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 884, "total_text_length": 974, "word_count": 134, "sentence_count": 7}, "basic_keywords": ["posterior", "base", "distributions", "modes", "present", "novel", "technique", "amortized"], "domain_keywords": [], "technical_terms": ["Normalizing Flows", "Gaussian Mixture Model"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04918", "topic": "machine learning", "title": "Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty", "authors": ["Kailiang Liu", "Ying Chen", "Ralf Borndörfer", "Thorsten Koch"], "first_author": "Kailiang Liu", "abstract": "Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04918", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty\nResearch Topic: machine learning\nAuthors: Kailiang Liu, Ying Chen, Ralf Borndörfer, Thorsten Koch\nAbstract: Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04918\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1724, "total_text_length": 1817, "word_count": 231, "sentence_count": 10}, "basic_keywords": ["policy", "scheduling", "multi", "problem", "under", "throughput", "urgent", "emergency"], "domain_keywords": [], "technical_terms": ["Markov", "Policy", "Proximal Policy Optimization", "Intraday", "Overall"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04912", "topic": "machine learning", "title": "A result relating convex n-widths to covering numbers with some applications to neural networks", "authors": ["Jonathan Baxter", "Peter Bartlett"], "first_author": "Jonathan Baxter", "abstract": "In general, approximating classes of functions defined over high-dimensional input spaces by linear combinations of a fixed set of basis functions or ``features'' is known to be hard. Typically, the worst-case error of the best basis set decays only as fast as , where is the number of basis functions and is the input dimension. However, there are many examples of high-dimensional pattern recognition problems (such as face recognition) where linear combinations of small sets of features do solve the problem well. Hence these function classes do not suffer from the ``curse of dimensionality'' associated with more general classes. It is natural then, to look for characterizations of high-dimensional function classes that nevertheless are approximated well by linear combinations of small sets of features. In this paper we give a general result relating the error of approximation of a function class to the covering number of its ``convex core''. For one-hidden-layer neural networks, covering numbers of the class of functions computed by a single hidden node upper bound the covering numbers of the convex core. Hence, using standard results we obtain upper bounds on the approximation rate of neural network classes.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04912", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: A result relating convex n-widths to covering numbers with some applications to neural networks\nResearch Topic: machine learning\nAuthors: Jonathan Baxter, Peter Bartlett\nAbstract: In general, approximating classes of functions defined over high-dimensional input spaces by linear combinations of a fixed set of basis functions or ``features'' is known to be hard. Typically, the worst-case error of the best basis set decays only as fast as $Θ\\(n^{-1/d}\\)$, where $n$ is the number of basis functions and $d$ is the input dimension. However, there are many examples of high-dimensional pattern recognition problems (such as face recognition) where linear combinations of small sets of features do solve the problem well. Hence these function classes do not suffer from the ``curse of dimensionality'' associated with more general classes. It is natural then, to look for characterizations of high-dimensional function classes that nevertheless are approximated well by linear combinations of small sets of features. In this paper we give a general result relating the error of approximation of a function class to the covering number of its ``convex core''. For one-hidden-layer neural networks, covering numbers of the class of functions computed by a single hidden node upper bound the covering numbers of the convex core. Hence, using standard results we obtain upper bounds on the approximation rate of neural network classes.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04912\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1227, "total_text_length": 1323, "word_count": 205, "sentence_count": 9}, "basic_keywords": ["classes", "functions", "general", "high", "dimensional", "linear", "combinations", "basis"], "domain_keywords": [], "technical_terms": ["However", "Typically", "Hence"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04909", "topic": "machine learning", "title": "PVLS: A Learning-based Parameter Prediction Technique for Variational Quantum Linear Solvers", "authors": ["Youla Yang"], "first_author": "Youla Yang", "abstract": "Variational Quantum Linear Solvers (VQLS) are a promising method for solving linear systems on near-term quantum devices. However, their performance is often limited by barren plateaus and inefficient parameter initialization, which significantly hinder trainability as the system size increases. In this work, we introduce PVLS, a learning-based parameter prediction framework that uses Graph Neural Networks (GNNs) to generate high-quality initial parameters for VQLS circuits. By leveraging structural information from the coefficient matrix, PVLS predicts expressive and scalable initializations that improve convergence and reduce optimization difficulty. Extensive experiments on matrix sizes ranging from 16 to 1024 show that PVLS provides up to a 2.6x speedup in optimization and requires fewer iterations while maintaining comparable solution accuracy. These results demonstrate the potential of machine-learning-guided initialization strategies for improving the practicality of hybrid quantum-classical algorithms in the NISQ era.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph"], "url": "http://arxiv.org/abs/2512.04909", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: PVLS: A Learning-based Parameter Prediction Technique for Variational Quantum Linear Solvers\nResearch Topic: machine learning\nAuthors: Youla Yang\nAbstract: Variational Quantum Linear Solvers (VQLS) are a promising method for solving linear systems on near-term quantum devices. However, their performance is often limited by barren plateaus and inefficient parameter initialization, which significantly hinder trainability as the system size increases. In this work, we introduce PVLS, a learning-based parameter prediction framework that uses Graph Neural Networks (GNNs) to generate high-quality initial parameters for VQLS circuits. By leveraging structural information from the coefficient matrix, PVLS predicts expressive and scalable initializations that improve convergence and reduce optimization difficulty. Extensive experiments on matrix sizes ranging from 16 to 1024 show that PVLS provides up to a 2.6x speedup in optimization and requires fewer iterations while maintaining comparable solution accuracy. These results demonstrate the potential of machine-learning-guided initialization strategies for improving the practicality of hybrid quantum-classical algorithms in the NISQ era.\nCategories: quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04909\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1041, "total_text_length": 1134, "word_count": 147, "sentence_count": 8}, "basic_keywords": ["quantum", "pvls", "linear", "vqls", "parameter", "initialization", "learning", "matrix"], "domain_keywords": [], "technical_terms": ["These", "However", "Extensive", "Graph Neural Networks", "Variational Quantum Linear Solvers"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04874", "topic": "machine learning", "title": "Shorting Dynamics and Structured Kernel Regularization", "authors": ["James Tian"], "first_author": "James Tian", "abstract": "This paper develops a nonlinear operator dynamic that progressively removes the influence of a prescribed feature subspace while retaining maximal structure elsewhere. The induced sequence of positive operators is monotone, admits an exact residual decomposition, and converges to the classical shorted operator. Transporting this dynamic to reproducing kernel Hilbert spaces yields a corresponding family of kernels that converges to the largest kernel dominated by the original one and annihilating the given subspace. In the finite-sample setting, the associated Gram operators inherit a structured residual decomposition that leads to a canonical form of kernel ridge regression and a principled way to enforce nuisance invariance. This gives a unified operator-analytic approach to invariant kernel construction and structured regularization in data analysis.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.FA", "cs.LG"], "url": "http://arxiv.org/abs/2512.04874", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Shorting Dynamics and Structured Kernel Regularization\nResearch Topic: machine learning\nAuthors: James Tian\nAbstract: This paper develops a nonlinear operator dynamic that progressively removes the influence of a prescribed feature subspace while retaining maximal structure elsewhere. The induced sequence of positive operators is monotone, admits an exact residual decomposition, and converges to the classical shorted operator. Transporting this dynamic to reproducing kernel Hilbert spaces yields a corresponding family of kernels that converges to the largest kernel dominated by the original one and annihilating the given subspace. In the finite-sample setting, the associated Gram operators inherit a structured residual decomposition that leads to a canonical form of kernel ridge regression and a principled way to enforce nuisance invariance. This gives a unified operator-analytic approach to invariant kernel construction and structured regularization in data analysis.\nCategories: math.FA, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04874\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 54, "abstract_length": 864, "total_text_length": 919, "word_count": 125, "sentence_count": 6}, "basic_keywords": ["kernel", "operator", "dynamic", "subspace", "operators", "residual", "decomposition", "converges"], "domain_keywords": [], "technical_terms": ["Hilbert", "Gram", "Transporting"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04865", "topic": "machine learning", "title": "Series of quasi-uniform scatterings with fast search, root systems and neural network classifications", "authors": ["Igor V. Netay"], "first_author": "Igor V. Netay", "abstract": "In this paper we describe an approach to construct large extendable collections of vectors in predefined spaces of given dimensions. These collections are useful for neural network latent space configuration and training. For classification problem with large or unknown number of classes this allows to construct classifiers without classification layer and extend the number of classes without retraining of network from the very beginning. The construction allows to create large well-spaced vector collections in spaces of minimal possible dimension. If the number of classes is known or approximately predictable, one can choose sufficient enough vector collection size. If one needs to significantly extend the number of classes, one can extend the collection in the same latent space, or to incorporate the collection into collection of higher dimensions with same spacing between vectors. Also, regular symmetric structure of constructed vector collections can significantly simplify problems of search for nearest cluster centers or embeddings in the latent space. Construction of vector collections is based on combinatorics and geometry of semi-simple Lie groups irreducible representations with highest weight.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.AG", "cs.LG", "math.RT"], "url": "http://arxiv.org/abs/2512.04865", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Series of quasi-uniform scatterings with fast search, root systems and neural network classifications\nResearch Topic: machine learning\nAuthors: Igor V. Netay\nAbstract: In this paper we describe an approach to construct large extendable collections of vectors in predefined spaces of given dimensions. These collections are useful for neural network latent space configuration and training. For classification problem with large or unknown number of classes this allows to construct classifiers without classification layer and extend the number of classes without retraining of network from the very beginning. The construction allows to create large well-spaced vector collections in spaces of minimal possible dimension. If the number of classes is known or approximately predictable, one can choose sufficient enough vector collection size. If one needs to significantly extend the number of classes, one can extend the collection in the same latent space, or to incorporate the collection into collection of higher dimensions with same spacing between vectors. Also, regular symmetric structure of constructed vector collections can significantly simplify problems of search for nearest cluster centers or embeddings in the latent space. Construction of vector collections is based on combinatorics and geometry of semi-simple Lie groups irreducible representations with highest weight.\nCategories: math.AG, cs.LG, math.RT\nPublication Date: 2025-12-04\nPaper ID: 2512.04865\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1222, "total_text_length": 1324, "word_count": 188, "sentence_count": 9}, "basic_keywords": ["collections", "number", "classes", "vector", "collection", "large", "latent", "space"], "domain_keywords": [], "technical_terms": ["These", "Also", "Construction"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04861", "topic": "machine learning", "title": "Concentration bounds for intrinsic dimension estimation using Gaussian kernels", "authors": ["Martin Andersson"], "first_author": "Martin Andersson", "abstract": "We prove finite-sample concentration and anti-concentration bounds for dimension estimation using Gaussian kernel sums. Our bounds provide explicit dependence on sample size, bandwidth, and local geometric and distributional parameters, characterizing precisely how regularity conditions govern statistical performance. We also propose a bandwidth selection heuristic using derivative information, which shows promise in numerical experiments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.ST", "stat.ML"], "url": "http://arxiv.org/abs/2512.04861", "comments": "24 pages, 8 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Concentration bounds for intrinsic dimension estimation using Gaussian kernels\nResearch Topic: machine learning\nAuthors: Martin Andersson\nAbstract: We prove finite-sample concentration and anti-concentration bounds for dimension estimation using Gaussian kernel sums. Our bounds provide explicit dependence on sample size, bandwidth, and local geometric and distributional parameters, characterizing precisely how regularity conditions govern statistical performance. We also propose a bandwidth selection heuristic using derivative information, which shows promise in numerical experiments.\nCategories: math.ST, stat.ML\nPublication Date: 2025-12-04\nPaper ID: 2512.04861\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 443, "total_text_length": 522, "word_count": 62, "sentence_count": 4}, "basic_keywords": ["sample", "concentration", "bounds", "using", "bandwidth", "prove", "finite", "anti"], "domain_keywords": [], "technical_terms": ["Gaussian"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.6000000000000001}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04839", "topic": "machine learning", "title": "Optimal Transport Event Representation for Anomaly Detection", "authors": ["Aditya Bhargava", "Tianji Cai", "Benjamin Nachman"], "first_author": "Aditya Bhargava", "abstract": "We introduce optimal transport (OT) as a physics-based intermediate event representation for weakly supervised anomaly detection. With only injection of resonant signals in the LHC Olympics benchmark datasets, the OT-augmented feature set achieves nearly twice the significance improvement of standard high-level observables, while end-to-end deep learning on low-level four-momenta struggles in the low-signal regime. The gains persist across signal types and classifiers, underscoring the value of structured representations in machine learning for anomaly detection.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["hep-ph", "hep-ex"], "url": "http://arxiv.org/abs/2512.04839", "comments": "8 pages, 5 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Optimal Transport Event Representation for Anomaly Detection\nResearch Topic: machine learning\nAuthors: Aditya Bhargava, Tianji Cai, Benjamin Nachman\nAbstract: We introduce optimal transport (OT) as a physics-based intermediate event representation for weakly supervised anomaly detection. With only $0.5\\%$ injection of resonant signals in the LHC Olympics benchmark datasets, the OT-augmented feature set achieves nearly twice the significance improvement of standard high-level observables, while end-to-end deep learning on low-level four-momenta struggles in the low-signal regime. The gains persist across signal types and classifiers, underscoring the value of structured representations in machine learning for anomaly detection.\nCategories: hep-ph, hep-ex\nPublication Date: 2025-12-04\nPaper ID: 2512.04839\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 60, "abstract_length": 569, "total_text_length": 630, "word_count": 81, "sentence_count": 4}, "basic_keywords": ["anomaly", "detection", "level", "learning", "signal", "introduce", "optimal", "transport"], "domain_keywords": [], "technical_terms": ["Olympics"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04832", "topic": "machine learning", "title": "Tokenizing Buildings: A Transformer for Layout Synthesis", "authors": ["Manuel Ladron de Guevara", "Jinmo Rhee", "Ardavan Bidgoli", "Vaidas Razgaitis", "Michael Bergin"], "first_author": "Manuel Ladron de Guevara", "abstract": "We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.GR", "cs.LG"], "url": "http://arxiv.org/abs/2512.04832", "comments": "8 pages, 1 page References, 4 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.214900", "embedding_text": "Paper Title: Tokenizing Buildings: A Transformer for Layout Synthesis\nResearch Topic: machine learning\nAuthors: Manuel Ladron de Guevara, Jinmo Rhee, Ardavan Bidgoli, Vaidas Razgaitis, Michael Bergin\nAbstract: We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.\nCategories: cs.CV, cs.GR, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04832\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 56, "abstract_length": 1135, "total_text_length": 1192, "word_count": 158, "sentence_count": 8}, "basic_keywords": ["feature", "room", "building", "transformer", "layout", "synthesis", "sets", "learns"], "domain_keywords": [], "technical_terms": ["Experiments", "Building Information Modeling", "Transformer", "Such", "Small Building Model", "Data", "Driven Entity Prediction", "Lastly"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04827", "topic": "machine learning", "title": "Contract-Driven QoE Auditing for Speech and Singing Services: From MOS Regression to Service Graphs", "authors": ["Wenzhang Du"], "first_author": "Wenzhang Du", "abstract": "Subjective mean opinion scores (MOS) remain the de-facto target for non-intrusive speech and singing quality assessment. However, MOS is a scalar that collapses heterogeneous user expectations, ignores service-level objectives, and is difficult to compare across deployment graphs. We propose a contract-driven QoE auditing framework: each service graph G is evaluated under a set of human-interpretable experience contracts C, yielding a contract-level satisfaction vector Q(G, C). We show that (i) classical MOS regression is a special case with a degenerate contract set, (ii) contract-driven quality is more stable than MOS under graph view transformations (e.g., pooling by system vs. by system type), and (iii) the effective sample complexity of learning contracts is governed by contract semantics rather than merely the dimensionality of C. We instantiate the framework on URGENT2024 MOS (6.9k speech utterances with raw rating vectors) and SingMOS v1 (7,981 singing clips; 80 systems). On URGENT, we train a contract-aware neural auditor on self-supervised WavLM embeddings; on SingMOS, we perform contract-driven graph auditing using released rating vectors and metadata without decoding audio. Empirically, our auditor matches strong MOS predictors in MOS accuracy while providing calibrated contract probabilities; on SingMOS, Q(G, C) exhibits substantially smaller cross-view drift than raw MOS and graph-only baselines; on URGENT, difficulty curves reveal that mis-specified \"simple\" contracts can be harder to learn than richer but better aligned contract sets.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.LG"], "url": "http://arxiv.org/abs/2512.04827", "comments": "11 pages, 3 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Contract-Driven QoE Auditing for Speech and Singing Services: From MOS Regression to Service Graphs\nResearch Topic: machine learning\nAuthors: Wenzhang Du\nAbstract: Subjective mean opinion scores (MOS) remain the de-facto target for non-intrusive speech and singing quality assessment. However, MOS is a scalar that collapses heterogeneous user expectations, ignores service-level objectives, and is difficult to compare across deployment graphs. We propose a contract-driven QoE auditing framework: each service graph G is evaluated under a set of human-interpretable experience contracts C, yielding a contract-level satisfaction vector Q(G, C). We show that (i) classical MOS regression is a special case with a degenerate contract set, (ii) contract-driven quality is more stable than MOS under graph view transformations (e.g., pooling by system vs. by system type), and (iii) the effective sample complexity of learning contracts is governed by contract semantics rather than merely the dimensionality of C. We instantiate the framework on URGENT2024 MOS (6.9k speech utterances with raw rating vectors) and SingMOS v1 (7,981 singing clips; 80 systems). On URGENT, we train a contract-aware neural auditor on self-supervised WavLM embeddings; on SingMOS, we perform contract-driven graph auditing using released rating vectors and metadata without decoding audio. Empirically, our auditor matches strong MOS predictors in MOS accuracy while providing calibrated contract probabilities; on SingMOS, Q(G, C) exhibits substantially smaller cross-view drift than raw MOS and graph-only baselines; on URGENT, difficulty curves reveal that mis-specified \"simple\" contracts can be harder to learn than richer but better aligned contract sets.\nCategories: cs.SD, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04827\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 99, "abstract_length": 1576, "total_text_length": 1676, "word_count": 237, "sentence_count": 12}, "basic_keywords": ["contract", "graph", "driven", "contracts", "singmos", "speech", "singing", "quality"], "domain_keywords": [], "technical_terms": ["Subjective", "However", "Empirically"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04781", "topic": "machine learning", "title": "Pick-to-Learn for Systems and Control: Data-driven Synthesis with State-of-the-art Safety Guarantees", "authors": ["Dario Paccagnan", "Daniel Marks", "Marco C. Campi", "Simone Garatti"], "first_author": "Dario Paccagnan", "abstract": "Data-driven methods have become paramount in modern systems and control problems characterized by growing levels of complexity. In safety-critical environments, deploying these methods requires rigorous guarantees, a need that has motivated much recent work at the interface of statistical learning and control. However, many existing approaches achieve this goal at the cost of sacrificing valuable data for testing and calibration, or by constraining the choice of learning algorithm, thus leading to suboptimal performances. In this paper, we describe Pick-to-Learn (P2L) for Systems and Control, a framework that allows any data-driven control method to be equipped with state-of-the-art safety and performance guarantees. P2L enables the use of all available data to jointly synthesize and certify the design, eliminating the need to set aside data for calibration or validation purposes. In presenting a comprehensive version of P2L for systems and control, this paper demonstrates its effectiveness across a range of core problems, including optimal control, reachability analysis, safe synthesis, and robust control. In many of these applications, P2L delivers designs and certificates that outperform commonly employed methods, and shows strong potential for broad applicability in diverse practical settings.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SY", "cs.LG"], "url": "http://arxiv.org/abs/2512.04781", "comments": "27 double-column pages, 18 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Pick-to-Learn for Systems and Control: Data-driven Synthesis with State-of-the-art Safety Guarantees\nResearch Topic: machine learning\nAuthors: Dario Paccagnan, Daniel Marks, Marco C. Campi, Simone Garatti\nAbstract: Data-driven methods have become paramount in modern systems and control problems characterized by growing levels of complexity. In safety-critical environments, deploying these methods requires rigorous guarantees, a need that has motivated much recent work at the interface of statistical learning and control. However, many existing approaches achieve this goal at the cost of sacrificing valuable data for testing and calibration, or by constraining the choice of learning algorithm, thus leading to suboptimal performances. In this paper, we describe Pick-to-Learn (P2L) for Systems and Control, a framework that allows any data-driven control method to be equipped with state-of-the-art safety and performance guarantees. P2L enables the use of all available data to jointly synthesize and certify the design, eliminating the need to set aside data for calibration or validation purposes. In presenting a comprehensive version of P2L for systems and control, this paper demonstrates its effectiveness across a range of core problems, including optimal control, reachability analysis, safe synthesis, and robust control. In many of these applications, P2L delivers designs and certificates that outperform commonly employed methods, and shows strong potential for broad applicability in diverse practical settings.\nCategories: eess.SY, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04781\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1318, "total_text_length": 1419, "word_count": 197, "sentence_count": 8}, "basic_keywords": ["control", "data", "methods", "systems", "driven", "problems", "safety", "guarantees"], "domain_keywords": [], "technical_terms": ["However", "Pick", "Learn", "Control", "Systems", "Data"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04771", "topic": "machine learning", "title": "Complementary Characterization of Agent-Based Models via Computational Mechanics and Diffusion Models", "authors": ["Roberto Garrone"], "first_author": "Roberto Garrone", "abstract": "This article extends the preprint \"Characterizing Agent-Based Model Dynamics via -Machines and Kolmogorov-Style Complexity\" by introducing diffusion models as orthogonal and complementary tools for characterizing the output of agent-based models (ABMs). Where -machines capture the predictive temporal structure and intrinsic computation of ABM-generated time series, diffusion models characterize high-dimensional cross-sectional distributions, learn underlying data manifolds, and enable synthetic generation of plausible population-level outcomes. We provide a formal analysis demonstrating that the two approaches operate on distinct mathematical domains -- processes vs. distributions -- and show that their combination yields a two-axis representation of ABM behavior based on temporal organization and distributional geometry. To our knowledge, this is the first framework to integrate computational mechanics with score-based generative modeling for the structural analysis of ABM outputs, thereby situating ABM characterization within the broader landscape of modern machine-learning methods for density estimation and intrinsic computation. The framework is validated using the same elder-caregiver ABM dataset introduced in the companion paper, and we provide precise definitions and propositions formalizing the mathematical complementarity between -machines and diffusion models. This establishes a principled methodology for jointly analyzing temporal predictability and high-dimensional distributional structure in complex simulation models.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.MA", "cs.LG"], "url": "http://arxiv.org/abs/2512.04771", "comments": "11 pages. Methods paper introducing a dual-domain framework for analyzing ABM dynamics. Companion temporal-analysis preprint: arXiv:2510.12729", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Complementary Characterization of Agent-Based Models via Computational Mechanics and Diffusion Models\nResearch Topic: machine learning\nAuthors: Roberto Garrone\nAbstract: This article extends the preprint \"Characterizing Agent-Based Model Dynamics via $ε$-Machines and Kolmogorov-Style Complexity\" by introducing diffusion models as orthogonal and complementary tools for characterizing the output of agent-based models (ABMs). Where $ε$-machines capture the predictive temporal structure and intrinsic computation of ABM-generated time series, diffusion models characterize high-dimensional cross-sectional distributions, learn underlying data manifolds, and enable synthetic generation of plausible population-level outcomes. We provide a formal analysis demonstrating that the two approaches operate on distinct mathematical domains -- processes vs. distributions -- and show that their combination yields a two-axis representation of ABM behavior based on temporal organization and distributional geometry. To our knowledge, this is the first framework to integrate computational mechanics with score-based generative modeling for the structural analysis of ABM outputs, thereby situating ABM characterization within the broader landscape of modern machine-learning methods for density estimation and intrinsic computation. The framework is validated using the same elder-caregiver ABM dataset introduced in the companion paper, and we provide precise definitions and propositions formalizing the mathematical complementarity between $ε$-machines and diffusion models. This establishes a principled methodology for jointly analyzing temporal predictability and high-dimensional distributional structure in complex simulation models.\nCategories: cs.MA, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04771\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1556, "total_text_length": 1658, "word_count": 203, "sentence_count": 8}, "basic_keywords": ["models", "based", "machines", "diffusion", "temporal", "characterizing", "agent", "structure"], "domain_keywords": [], "technical_terms": ["Machines", "Based Model Dynamics", "Characterizing Agent", "Kolmogorov", "Where", "Style Complexity"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04765", "topic": "machine learning", "title": "AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages", "authors": ["Pooja Singh", "Sandeep Kumar"], "first_author": "Pooja Singh", "abstract": "Large language models and multilingual machine translation (MT) systems increasingly drive access to information, yet many languages of the tribal communities remain effectively invisible in these technologies. This invisibility exacerbates existing structural inequities in education, governance, and digital participation. We present AdiBhashaa, a community-driven initiative that constructs the first open parallel corpora and baseline MT systems for four major Indian tribal languages-Bhili, Mundari, Gondi, and Santali. This work combines participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. In addition to reporting technical findings, we articulate how AdiBhashaa illustrates a possible model for more equitable AI research: it centers local expertise, builds capacity among early-career researchers from marginalized communities, and foregrounds human validation in the development of language technologies.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04765", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages\nResearch Topic: machine learning\nAuthors: Pooja Singh, Sandeep Kumar\nAbstract: Large language models and multilingual machine translation (MT) systems increasingly drive access to information, yet many languages of the tribal communities remain effectively invisible in these technologies. This invisibility exacerbates existing structural inequities in education, governance, and digital participation. We present AdiBhashaa, a community-driven initiative that constructs the first open parallel corpora and baseline MT systems for four major Indian tribal languages-Bhili, Mundari, Gondi, and Santali. This work combines participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. In addition to reporting technical findings, we articulate how AdiBhashaa illustrates a possible model for more equitable AI research: it centers local expertise, builds capacity among early-career researchers from marginalized communities, and foregrounds human validation in the development of language technologies.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04765\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1029, "total_text_length": 1124, "word_count": 141, "sentence_count": 6}, "basic_keywords": ["language", "models", "large", "systems", "languages", "tribal", "communities", "technologies"], "domain_keywords": [], "technical_terms": ["Gondi", "Santali", "Bhili", "Large", "Mundari", "Indian"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04763", "topic": "machine learning", "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems", "authors": ["Massimo Bini", "Ondrej Bohdal", "Umberto Michieli", "Zeynep Akata", "Mete Ozay", "Taha Ceritli"], "first_author": "Massimo Bini", "abstract": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operationsknowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10 larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60 larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CL", "cs.CV"], "url": "http://arxiv.org/abs/2512.04763", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: MemLoRA: Distilling Expert Adapters for On-Device Memory Systems\nResearch Topic: machine learning\nAuthors: Massimo Bini, Ondrej Bohdal, Umberto Michieli, Zeynep Akata, Mete Ozay, Taha Ceritli\nAbstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.\nCategories: cs.LG, cs.CL, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04763\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 1858, "total_text_length": 1923, "word_count": 246, "sentence_count": 19}, "basic_keywords": ["memory", "models", "visual", "based", "device", "memlora", "augmented", "language"], "domain_keywords": [], "technical_terms": ["Visual Question Answering", "However", "Additionally", "Following", "Large Language Models", "Vision", "Language Models", "Such"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04752", "topic": "machine learning", "title": "RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting", "authors": ["Siqi Wang", "Hailong Yang", "Junjie Zhu", "Xuezhu Wang", "Yufan Xu", "Depei Qian"], "first_author": "Siqi Wang", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with adaptive speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04752", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting\nResearch Topic: machine learning\nAuthors: Siqi Wang, Hailong Yang, Junjie Zhu, Xuezhu Wang, Yufan Xu, Depei Qian\nAbstract: Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with adaptive speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04752\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1450, "total_text_length": 1534, "word_count": 210, "sentence_count": 9}, "basic_keywords": ["generation", "stage", "rlhfspec", "rlhf", "execution", "speculative", "decoding", "sample"], "domain_keywords": [], "technical_terms": ["Moreover", "Specifically", "Reinforcement Learning", "Human Feedback"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04751", "topic": "machine learning", "title": "NAWOA-XGBoost: A Novel Model for Early Prediction of Academic Potential in Computer Science Students", "authors": ["Junhao Wei", "Yanzhao Gu", "Ran Zhang", "Mingjing Huang", "Jinhong Song", "Yanxiao Li", "Wenxuan Zhu", "Yapeng Wang", "Zikun Li", "Zhiwen Wang", "Xu Yang", "Ngai Cheong"], "first_author": "Junhao Wei", "abstract": "Whale Optimization Algorithm (WOA) suffers from limited global search ability, slow convergence, and tendency to fall into local optima, restricting its effectiveness in hyperparameter optimization for machine learning models. To address these issues, this study proposes a Nonlinear Adaptive Whale Optimization Algorithm (NAWOA), which integrates strategies such as Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor to enhance exploration, exploitation, and convergence stability. Experiments on 23 benchmark functions demonstrate NAWOA's superior optimization capability and robustness. Based on this optimizer, an NAWOA-XGBoost model was developed to predict academic potential using data from 495 Computer Science undergraduates at Macao Polytechnic University (2009-2019). Results show that NAWOA-XGBoost outperforms traditional XGBoost and WOA-XGBoost across key metrics, including Accuracy (0.8148), Macro F1 (0.8101), AUC (0.8932), and G-Mean (0.8172), demonstrating strong adaptability on multi-class imbalanced datasets.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CE"], "url": "http://arxiv.org/abs/2512.04751", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: NAWOA-XGBoost: A Novel Model for Early Prediction of Academic Potential in Computer Science Students\nResearch Topic: machine learning\nAuthors: Junhao Wei, Yanzhao Gu, Ran Zhang, Mingjing Huang, Jinhong Song, Yanxiao Li, Wenxuan Zhu, Yapeng Wang, Zikun Li, Zhiwen Wang, Xu Yang, Ngai Cheong\nAbstract: Whale Optimization Algorithm (WOA) suffers from limited global search ability, slow convergence, and tendency to fall into local optima, restricting its effectiveness in hyperparameter optimization for machine learning models. To address these issues, this study proposes a Nonlinear Adaptive Whale Optimization Algorithm (NAWOA), which integrates strategies such as Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor to enhance exploration, exploitation, and convergence stability. Experiments on 23 benchmark functions demonstrate NAWOA's superior optimization capability and robustness. Based on this optimizer, an NAWOA-XGBoost model was developed to predict academic potential using data from 495 Computer Science undergraduates at Macao Polytechnic University (2009-2019). Results show that NAWOA-XGBoost outperforms traditional XGBoost and WOA-XGBoost across key metrics, including Accuracy (0.8148), Macro F1 (0.8101), AUC (0.8932), and G-Mean (0.8172), demonstrating strong adaptability on multi-class imbalanced datasets.\nCategories: cs.CE\nPublication Date: 2025-12-04\nPaper ID: 2512.04751\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1118, "total_text_length": 1219, "word_count": 152, "sentence_count": 10}, "basic_keywords": ["optimization", "nawoa", "xgboost", "convergence", "whale", "algorithm", "nonlinear", "suffers"], "domain_keywords": [], "technical_terms": ["Experiments", "Based", "Computer Science", "Followers Foraging", "Triangular Hunting", "Whale Optimization Algorithm", "Nonlinear Adaptive Whale Optimization Algorithm", "Macao Polytechnic University"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04718", "topic": "machine learning", "title": "Heat transport in superionic materials via machine-learned molecular dynamics", "authors": ["Wenjiang Zhou", "Benrui Tang", "Zheyong Fan", "Federico Grasselli", "Stefano Baroni", "Bai Song"], "first_author": "Wenjiang Zhou", "abstract": "Precise modeling and understanding of heat transport in the superionic phase are of great interest. Although simulations combining Green-Kubo (GK) molecular dynamics with machine-learned potentials (MLPs) stand as a promising approach, substantial challenges remain due to the crucial impact of atomic diffusion. Here, we first show that the thermal conductivity () of superionic materials calculated via conventional GK integral of the energy flux varies notably with the MLP model. Subsequently, we highlight that reliable, model-independent values can be obtained by applying Onsager's reciprocal relations to correctly capture the coupled heat and mass transport. Remarkably, an anomalously invariant is observed over a wide temperature range, distinct from the characteristic trends in traditional crystals and glasses. Finally, we illustrate that conventional decompositions into kinetic, potential, and cross terms suffer from ambiguities in the physical interpretation, despite their mathematical rigor.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04718", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Heat transport in superionic materials via machine-learned molecular dynamics\nResearch Topic: machine learning\nAuthors: Wenjiang Zhou, Benrui Tang, Zheyong Fan, Federico Grasselli, Stefano Baroni, Bai Song\nAbstract: Precise modeling and understanding of heat transport in the superionic phase are of great interest. Although simulations combining Green-Kubo (GK) molecular dynamics with machine-learned potentials (MLPs) stand as a promising approach, substantial challenges remain due to the crucial impact of atomic diffusion. Here, we first show that the thermal conductivity ($κ$) of superionic materials calculated via conventional GK integral of the energy flux varies notably with the MLP model. Subsequently, we highlight that reliable, model-independent $κ$ values can be obtained by applying Onsager's reciprocal relations to correctly capture the coupled heat and mass transport. Remarkably, an anomalously invariant $κ$ is observed over a wide temperature range, distinct from the characteristic trends in traditional crystals and glasses. Finally, we illustrate that conventional $κ$ decompositions into kinetic, potential, and cross terms suffer from ambiguities in the physical interpretation, despite their mathematical rigor.\nCategories: cond-mat.mtrl-sci\nPublication Date: 2025-12-04\nPaper ID: 2512.04718\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1011, "total_text_length": 1089, "word_count": 146, "sentence_count": 7}, "basic_keywords": ["heat", "transport", "superionic", "conventional", "model", "precise", "modeling", "understanding"], "domain_keywords": [], "technical_terms": ["Onsager", "Finally", "Here", "Kubo", "Green", "Precise", "Remarkably", "Although"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04717", "topic": "machine learning", "title": "Accelerating discovery of infrared nonlinear optical materials with large shift current via high-throughput screening", "authors": ["Aiqin Yang", "Dian Jin", "Mingkang Liu", "Daye Zheng", "Qi Wang", "Qiangqiang Gu", "Jian-Hua Jiang"], "first_author": "Aiqin Yang", "abstract": "Discovering nonlinear optical (NLO) materials with strong shift current response, particularly in the infrared (IR) regime, is essential for next-generation optoelectronics yet remains highly challenging in both experiments and theory, which still largely relies on case by case studies. Here, we employ a high-throughput screening strategy, applying a multi-step filter to the Materials Project database (>154,000 materials), which yielded 2,519 candidate materials for detailed first-principle evaluation. From these calculations, we identify 32 NLO materials with strong shift current response ( > 100 ). Our work reveals that layered structures with symmetry and heavy -block elements (e.g. Te, Sb) exhibit apparent superiority in enhancing shift current. More importantly, 9 of these compounds show shift current response peaks in the IR region, with the strongest reaching 616 , holding significant application potential in fields such as IR photodetection, sensing, and energy harvesting. Beyond identifying promising candidates, this work establishes a comprehensive and high-quality first-principles dataset for NLO response, providing a solid foundation for future AI-driven screening and accelerated discovery of high-performance NLO materials, as demonstrated by a prototype machine-learning application.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04717", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Accelerating discovery of infrared nonlinear optical materials with large shift current via high-throughput screening\nResearch Topic: machine learning\nAuthors: Aiqin Yang, Dian Jin, Mingkang Liu, Daye Zheng, Qi Wang, Qiangqiang Gu, Jian-Hua Jiang\nAbstract: Discovering nonlinear optical (NLO) materials with strong shift current response, particularly in the infrared (IR) regime, is essential for next-generation optoelectronics yet remains highly challenging in both experiments and theory, which still largely relies on case by case studies. Here, we employ a high-throughput screening strategy, applying a multi-step filter to the Materials Project database (>154,000 materials), which yielded 2,519 candidate materials for detailed first-principle evaluation. From these calculations, we identify 32 NLO materials with strong shift current response ($σ$ > 100 $μA/V^2$). Our work reveals that layered structures with $C_{3v}$ symmetry and heavy $p$-block elements (e.g. Te, Sb) exhibit apparent superiority in enhancing shift current. More importantly, 9 of these compounds show shift current response peaks in the IR region, with the strongest reaching 616 $μA/V^2$, holding significant application potential in fields such as IR photodetection, sensing, and energy harvesting. Beyond identifying promising candidates, this work establishes a comprehensive and high-quality first-principles dataset for NLO response, providing a solid foundation for future AI-driven screening and accelerated discovery of high-performance NLO materials, as demonstrated by a prototype machine-learning application.\nCategories: cond-mat.mtrl-sci\nPublication Date: 2025-12-04\nPaper ID: 2512.04717\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 117, "abstract_length": 1316, "total_text_length": 1434, "word_count": 192, "sentence_count": 9}, "basic_keywords": ["materials", "shift", "current", "response", "high", "strong", "case", "screening"], "domain_keywords": [], "technical_terms": ["Materials Project", "Beyond", "Here", "Discovering", "More"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04703", "topic": "machine learning", "title": "Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement", "authors": ["Stefan Perko"], "first_author": "Stefan Perko", "abstract": "Gradient optimization algorithms using epochs, that is those based on stochastic gradient descent without replacement (SGDo), are predominantly used to train machine learning models in practice. However, the mathematical theory of SGDo and related algorithms remain underexplored compared to their \"with replacement\" and \"one-pass\" counterparts. In this article, we propose a stochastic, continuous-time approximation to SGDo with additive noise based on a Young differential equation driven by a stochastic process we call an \"epoched Brownian motion\". We show its usefulness by proving the almost sure convergence of the continuous-time approximation for strongly convex objectives and learning rate schedules of the form . Moreover, we compute an upper bound on the asymptotic rate of almost sure convergence, which is as good or better than previous results for SGDo.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "math.PR"], "url": "http://arxiv.org/abs/2512.04703", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement\nResearch Topic: machine learning\nAuthors: Stefan Perko\nAbstract: Gradient optimization algorithms using epochs, that is those based on stochastic gradient descent without replacement (SGDo), are predominantly used to train machine learning models in practice. However, the mathematical theory of SGDo and related algorithms remain underexplored compared to their \"with replacement\" and \"one-pass\" counterparts. In this article, we propose a stochastic, continuous-time approximation to SGDo with additive noise based on a Young differential equation driven by a stochastic process we call an \"epoched Brownian motion\". We show its usefulness by proving the almost sure convergence of the continuous-time approximation for strongly convex objectives and learning rate schedules of the form $u_t = \\frac{1}{(1+t)^β}, β\\in (0,1)$. Moreover, we compute an upper bound on the asymptotic rate of almost sure convergence, which is as good or better than previous results for SGDo.\nCategories: cs.LG, math.PR\nPublication Date: 2025-12-04\nPaper ID: 2512.04703\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 871, "total_text_length": 962, "word_count": 136, "sentence_count": 6}, "basic_keywords": ["sgdo", "stochastic", "gradient", "algorithms", "based", "replacement", "learning", "continuous"], "domain_keywords": [], "technical_terms": ["However", "Young", "Gradient", "Brownian", "Moreover"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04697", "topic": "machine learning", "title": "Continuous-time reinforcement learning for optimal switching over multiple regimes", "authors": ["Yijie Huang", "Mengge Li", "Xiang Yu", "Zhou Zhou"], "first_author": "Yijie Huang", "abstract": "This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.OC", "cs.LG", "q-fin.CP"], "url": "http://arxiv.org/abs/2512.04697", "comments": "Keywords: Optimal regime switching, multiple regimes, continuous-time reinforcement learning, system of HJB equations, policy improvement, policy iteration convergence", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Continuous-time reinforcement learning for optimal switching over multiple regimes\nResearch Topic: machine learning\nAuthors: Yijie Huang, Mengge Li, Xiang Yu, Zhou Zhou\nAbstract: This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.\nCategories: math.OC, cs.LG, q-fin.CP\nPublication Date: 2025-12-04\nPaper ID: 2512.04697\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 82, "abstract_length": 1095, "total_text_length": 1178, "word_count": 160, "sentence_count": 8}, "basic_keywords": ["policy", "formulation", "continuous", "time", "reinforcement", "learning", "optimal", "regimes"], "domain_keywords": [], "technical_terms": ["Jacobi", "Bellman", "Finally", "Markov", "Hamilton"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04696", "topic": "machine learning", "title": "Provable FDR Control for Deep Feature Selection: Deep MLPs and Beyond", "authors": ["Kazuma Sawaya"], "first_author": "Kazuma Sawaya", "abstract": "We develop a flexible feature selection framework based on deep neural networks that approximately controls the false discovery rate (FDR), a measure of Type-I error. The method applies to architectures whose first layer is fully connected. From the second layer onward, it accommodates multilayer perceptrons (MLPs) of arbitrary width and depth, convolutional and recurrent networks, attention mechanisms, residual connections, and dropout. The procedure also accommodates stochastic gradient descent with data-independent initializations and learning rates. To the best of our knowledge, this is the first work to provide a theoretical guarantee of FDR control for feature selection within such a general deep learning setting. Our analysis is built upon a multi-index data-generating model and an asymptotic regime in which the feature dimension diverges faster than the latent dimension , while the sample size, the number of training iterations, the network depth, and hidden layer widths are left unrestricted. Under this setting, we show that each coordinate of the gradient-based feature-importance vector admits a marginal normal approximation, thereby supporting the validity of asymptotic FDR control. As a theoretical limitation, we assume -right orthogonal invariance of the design matrix, and we discuss broader generalizations. We also present numerical experiments that underscore the theoretical findings.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ML", "cs.LG", "math.ST"], "url": "http://arxiv.org/abs/2512.04696", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Provable FDR Control for Deep Feature Selection: Deep MLPs and Beyond\nResearch Topic: machine learning\nAuthors: Kazuma Sawaya\nAbstract: We develop a flexible feature selection framework based on deep neural networks that approximately controls the false discovery rate (FDR), a measure of Type-I error. The method applies to architectures whose first layer is fully connected. From the second layer onward, it accommodates multilayer perceptrons (MLPs) of arbitrary width and depth, convolutional and recurrent networks, attention mechanisms, residual connections, and dropout. The procedure also accommodates stochastic gradient descent with data-independent initializations and learning rates. To the best of our knowledge, this is the first work to provide a theoretical guarantee of FDR control for feature selection within such a general deep learning setting.   Our analysis is built upon a multi-index data-generating model and an asymptotic regime in which the feature dimension $n$ diverges faster than the latent dimension $q^{*}$, while the sample size, the number of training iterations, the network depth, and hidden layer widths are left unrestricted. Under this setting, we show that each coordinate of the gradient-based feature-importance vector admits a marginal normal approximation, thereby supporting the validity of asymptotic FDR control. As a theoretical limitation, we assume $\\mathbf{B}$-right orthogonal invariance of the design matrix, and we discuss broader generalizations. We also present numerical experiments that underscore the theoretical findings.\nCategories: stat.ML, cs.LG, math.ST\nPublication Date: 2025-12-04\nPaper ID: 2512.04696\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1422, "total_text_length": 1492, "word_count": 212, "sentence_count": 10}, "basic_keywords": ["feature", "layer", "theoretical", "selection", "based", "deep", "networks", "first"], "domain_keywords": ["attention mechanisms"], "technical_terms": ["Under", "Type"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04695", "topic": "machine learning", "title": "TRINITY: An Evolved LLM Coordinator", "authors": ["Jinglue Xu", "Qi Sun", "Peter Schwendeman", "Stefan Nielsen", "Edoardo Cetin", "Yujin Tang"], "first_author": "Jinglue Xu", "abstract": "Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately B parameters) and a lightweight head (approximately K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04695", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: TRINITY: An Evolved LLM Coordinator\nResearch Topic: machine learning\nAuthors: Jinglue Xu, Qi Sun, Peter Schwendeman, Stefan Nielsen, Edoardo Cetin, Yujin Tang\nAbstract: Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04695\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 35, "abstract_length": 1464, "total_text_length": 1500, "word_count": 188, "sentence_count": 9}, "basic_keywords": ["coordinator", "trinity", "models", "lightweight", "language", "approximately", "parameters", "strategy"], "domain_keywords": [], "technical_terms": ["Experiments", "Thinker", "Combining", "Worker", "Covariance Matrix Adaptation Evolution Strategy", "Theoretical", "Verifier", "Trinity"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04690", "topic": "machine learning", "title": "Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting", "authors": ["Souhir Ben Amor", "Florian Ziel"], "first_author": "Souhir Ben Amor", "abstract": "We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/abs/2512.04690", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting\nResearch Topic: machine learning\nAuthors: Souhir Ben Amor, Florian Ziel\nAbstract: We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures.\nCategories: stat.ML, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04690\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 82, "abstract_length": 1199, "total_text_length": 1282, "word_count": 168, "sentence_count": 7}, "basic_keywords": ["linear", "model", "forecasting", "structures", "recurrent", "neural", "network", "electricity"], "domain_keywords": [], "technical_terms": ["Kalman", "European"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04663", "topic": "machine learning", "title": "Fermionic neural Gibbs states", "authors": ["Jannes Nys", "Juan Carrasquilla"], "first_author": "Jannes Nys", "abstract": "We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cond-mat.str-el", "cs.LG", "physics.comp-ph"], "url": "http://arxiv.org/abs/2512.04663", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Fermionic neural Gibbs states\nResearch Topic: machine learning\nAuthors: Jannes Nys, Juan Carrasquilla\nAbstract: We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.\nCategories: quant-ph, cond-mat.str-el, cs.LG, physics.comp-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04663\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 29, "abstract_length": 850, "total_text_length": 880, "word_count": 110, "sentence_count": 5}, "basic_keywords": ["neural", "fngs", "fermionic", "states", "finite", "temperature", "properties", "strongly"], "domain_keywords": [], "technical_terms": ["These", "Fermi", "Gibbs", "Applied", "Hubbard"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04644", "topic": "machine learning", "title": "Contract-Governed Training for Earth Observation: Observed Service Agreement Graphs and Coverage-Accuracy Trade-offs", "authors": ["Wenzhang Du"], "first_author": "Wenzhang Du", "abstract": "Earth observation (EO) models are frequently trained under implicit sampling policies that optimize global accuracy but provide no explicit guarantees on who (which regions, classes, or mission-critical strata) is being served throughout training. This paper introduces a contract-governed training paradigm for EO in which training samples are grouped into service contracts -- semantically meaningful units such as (dataset, region, rare-crop indicator) -- and each contract is assigned a target service share. We instantiate this paradigm as an Observed Service Agreement Graph (OSAG), a lightweight governance layer that (i) monitors contract-level exposure (coverage) during optimization, (ii) drives empirical coverage toward target shares via contract-normalized sampling weights, and (iii) exposes explicit accuracy-governance trade-offs through two knobs: a sampling mixture coefficient alpha and a contract-regularization weight lambda_C. We provide a compact theory in a toy setting: OSAG sampling concentrates empirical coverage to targets; coverage deviations upper-bound service-risk deviations; and contract design (coarse vs. fine) modulates governance cost. Experiments on AVIRIS hyperspectral scenes (Indian Pines plus Salinas) and multispectral Sentinel-2 EuroSAT demonstrate that OSAG can substantially reduce priority coverage error while maintaining global accuracy and improving high-priority accuracy. A EuroSAT coarse-vs-fine contract ablation further evidences how semantically refined contracts can reduce the accuracy cost per unit of governance improvement.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04644", "comments": "9 pages, 2 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Contract-Governed Training for Earth Observation: Observed Service Agreement Graphs and Coverage-Accuracy Trade-offs\nResearch Topic: machine learning\nAuthors: Wenzhang Du\nAbstract: Earth observation (EO) models are frequently trained under implicit sampling policies that optimize global accuracy but provide no explicit guarantees on who (which regions, classes, or mission-critical strata) is being served throughout training. This paper introduces a contract-governed training paradigm for EO in which training samples are grouped into service contracts -- semantically meaningful units such as (dataset, region, rare-crop indicator) -- and each contract is assigned a target service share. We instantiate this paradigm as an Observed Service Agreement Graph (OSAG), a lightweight governance layer that (i) monitors contract-level exposure (coverage) during optimization, (ii) drives empirical coverage toward target shares via contract-normalized sampling weights, and (iii) exposes explicit accuracy-governance trade-offs through two knobs: a sampling mixture coefficient alpha and a contract-regularization weight lambda_C. We provide a compact theory in a toy setting: OSAG sampling concentrates empirical coverage to targets; coverage deviations upper-bound service-risk deviations; and contract design (coarse vs. fine) modulates governance cost. Experiments on AVIRIS hyperspectral scenes (Indian Pines plus Salinas) and multispectral Sentinel-2 EuroSAT demonstrate that OSAG can substantially reduce priority coverage error while maintaining global accuracy and improving high-priority accuracy. A EuroSAT coarse-vs-fine contract ablation further evidences how semantically refined contracts can reduce the accuracy cost per unit of governance improvement.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04644\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1586, "total_text_length": 1703, "word_count": 217, "sentence_count": 8}, "basic_keywords": ["contract", "accuracy", "coverage", "sampling", "service", "governance", "training", "osag"], "domain_keywords": [], "technical_terms": ["Observed Service Agreement Graph", "Earth", "Experiments", "Salinas", "Indian Pines", "Sentinel"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04635", "topic": "machine learning", "title": "Federated Learning for Anomaly Detection in Maritime Movement Data", "authors": ["Anita Graser", "Axel Weißenfeld", "Clemens Heistracher", "Melitta Dragaschnig", "Peter Widhalm"], "first_author": "Anita Graser", "abstract": "This paper introduces M3fed, a novel solution for federated learning of movement anomaly detection models. This innovation has the potential to improve data privacy and reduce communication costs in machine learning for movement anomaly detection. We present the novel federated learning (FL) strategies employed to train M3fed, perform an example experiment with maritime AIS data, and evaluate the results with respect to communication costs and FL model quality by comparing classic centralized M3 and the new federated M3fed.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04635", "comments": "Accepted at MDM2024", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Federated Learning for Anomaly Detection in Maritime Movement Data\nResearch Topic: machine learning\nAuthors: Anita Graser, Axel Weißenfeld, Clemens Heistracher, Melitta Dragaschnig, Peter Widhalm\nAbstract: This paper introduces M3fed, a novel solution for federated learning of movement anomaly detection models. This innovation has the potential to improve data privacy and reduce communication costs in machine learning for movement anomaly detection. We present the novel federated learning (FL) strategies employed to train M3fed, perform an example experiment with maritime AIS data, and evaluate the results with respect to communication costs and FL model quality by comparing classic centralized M3 and the new federated M3fed.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04635\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 529, "total_text_length": 596, "word_count": 87, "sentence_count": 4}, "basic_keywords": ["federated", "learning", "novel", "movement", "anomaly", "detection", "data", "communication"], "domain_keywords": [], "technical_terms": [], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04625", "topic": "machine learning", "title": "Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective", "authors": ["Bowen Zheng", "Ran Cheng"], "first_author": "Bowen Zheng", "abstract": "In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CV"], "url": "http://arxiv.org/abs/2512.04625", "comments": "Accepted to IEEE TNNLS", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective\nResearch Topic: machine learning\nAuthors: Bowen Zheng, Ran Cheng\nAbstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.\nCategories: cs.LG, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04625\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 82, "abstract_length": 1565, "total_text_length": 1648, "word_count": 217, "sentence_count": 11}, "basic_keywords": ["knowledge", "distillation", "gdkd", "logit", "predictive", "distribution", "loss", "logits"], "domain_keywords": ["imagenet"], "technical_terms": ["However", "Then", "Cityscapes", "While", "Generalized Decoupled Knowledge Distillation", "First", "Utilizing", "Tiny"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04617", "topic": "machine learning", "title": "Score Matching for Estimating Finite Point Processes", "authors": ["Haoqun Cao", "Yixuan Zhang", "Feng Zhou"], "first_author": "Haoqun Cao", "abstract": "Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04617", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Score Matching for Estimating Finite Point Processes\nResearch Topic: machine learning\nAuthors: Haoqun Cao, Yixuan Zhang, Feng Zhou\nAbstract: Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04617\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 52, "abstract_length": 1516, "total_text_length": 1569, "word_count": 208, "sentence_count": 8}, "basic_keywords": ["score", "matching", "point", "processes", "temporal", "estimators", "finite", "properties"], "domain_keywords": [], "technical_terms": ["Score", "Experiments", "While", "Janossy"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04616", "topic": "machine learning", "title": "Standard audiogram classification from loudness scaling data using unsupervised, supervised, and explainable machine learning techniques", "authors": ["Chen Xu", "Lena Schell-Majoor", "Birger Kollmeier"], "first_author": "Chen Xu", "abstract": "To address the calibration and procedural challenges inherent in remote audiogram assessment for rehabilitative audiology, this study investigated whether calibration-independent adaptive categorical loudness scaling (ACALOS) data can be used to approximate individual audiograms by classifying listeners into standard Bisgaard audiogram types using machine learning. Three classes of machine learning approaches - unsupervised, supervised, and explainable - were evaluated. Principal component analysis (PCA) was performed to extract the first two principal components, which together explained more than 50 percent of the variance. Seven supervised multi-class classifiers were trained and compared, alongside unsupervised and explainable methods. Model development and evaluation used a large auditory reference database containing ACALOS data (N = 847). The PCA factor map showed substantial overlap between listeners, indicating that cleanly separating participants into six Bisgaard classes based solely on their loudness patterns is challenging. Nevertheless, the models demonstrated reasonable classification performance, with logistic regression achieving the highest accuracy among supervised approaches. These findings demonstrate that machine learning models can predict standard Bisgaard audiogram types, within certain limits, from calibration-independent loudness perception data, supporting potential applications in remote or resource-limited settings without requiring a traditional audiogram.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "physics.med-ph"], "url": "http://arxiv.org/abs/2512.04616", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Standard audiogram classification from loudness scaling data using unsupervised, supervised, and explainable machine learning techniques\nResearch Topic: machine learning\nAuthors: Chen Xu, Lena Schell-Majoor, Birger Kollmeier\nAbstract: To address the calibration and procedural challenges inherent in remote audiogram assessment for rehabilitative audiology, this study investigated whether calibration-independent adaptive categorical loudness scaling (ACALOS) data can be used to approximate individual audiograms by classifying listeners into standard Bisgaard audiogram types using machine learning. Three classes of machine learning approaches - unsupervised, supervised, and explainable - were evaluated. Principal component analysis (PCA) was performed to extract the first two principal components, which together explained more than 50 percent of the variance. Seven supervised multi-class classifiers were trained and compared, alongside unsupervised and explainable methods. Model development and evaluation used a large auditory reference database containing ACALOS data (N = 847). The PCA factor map showed substantial overlap between listeners, indicating that cleanly separating participants into six Bisgaard classes based solely on their loudness patterns is challenging. Nevertheless, the models demonstrated reasonable classification performance, with logistic regression achieving the highest accuracy among supervised approaches. These findings demonstrate that machine learning models can predict standard Bisgaard audiogram types, within certain limits, from calibration-independent loudness perception data, supporting potential applications in remote or resource-limited settings without requiring a traditional audiogram.\nCategories: cs.SD, physics.med-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04616\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 136, "abstract_length": 1511, "total_text_length": 1648, "word_count": 202, "sentence_count": 9}, "basic_keywords": ["audiogram", "calibration", "loudness", "data", "bisgaard", "machine", "learning", "supervised"], "domain_keywords": [], "technical_terms": ["These", "Nevertheless", "Seven", "Bisgaard", "Model", "Principal", "Three"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04601", "topic": "machine learning", "title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space", "authors": ["Joey Hong", "Kang Liu", "Zhan Ling", "Jiecao Chen", "Sergey Levine"], "first_author": "Joey Hong", "abstract": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CL"], "url": "http://arxiv.org/abs/2512.04601", "comments": "22 pages, 4 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space\nResearch Topic: machine learning\nAuthors: Joey Hong, Kang Liu, Zhan Ling, Jiecao Chen, Sergey Levine\nAbstract: Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.\nCategories: cs.LG, cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04601\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1746, "total_text_length": 1824, "word_count": 261, "sentence_count": 9}, "basic_keywords": ["language", "training", "policy", "tasks", "natural", "agents", "policies", "actions"], "domain_keywords": [], "technical_terms": ["However", "Furthermore", "Large", "Critic", "Natural Language Actor"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04596", "topic": "machine learning", "title": "QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction", "authors": ["Guanchen Du", "Jianlong Xu", "Wei Wei"], "first_author": "Guanchen Du", "abstract": "Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. This dependency introduces severe scalability bottlenecks and limits performance when explicit connections are sparse or corrupted by noise. To address these challenges, this paper introduces , a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against data sparsity and observational noise.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04596", "comments": "Preprint submitted to IEEE Transactions on Services Computing", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction\nResearch Topic: machine learning\nAuthors: Guanchen Du, Jianlong Xu, Wei Wei\nAbstract: Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. This dependency introduces severe scalability bottlenecks and limits performance when explicit connections are sparse or corrupted by noise. To address these challenges, this paper introduces \\emph{QoSDiff}, a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against data sparsity and observational noise.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04596\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 144, "abstract_length": 1382, "total_text_length": 1527, "word_count": 179, "sentence_count": 10}, "basic_keywords": ["service", "user", "explicit", "noise", "data", "superior", "graph", "interaction"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Service", "Notably", "Graph Neural Networks", "Specifically", "Accurate Quality"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04590", "topic": "machine learning", "title": "Exploiting \\texttt{ftrace}'s \\texttt{function\\_graph} Tracer Features for Machine Learning: A Case Study on Encryption Detection", "authors": ["Kenan Begovic", "Abdulaziz Al-Ali", "Qutaibah Malluhi"], "first_author": "Kenan Begovic", "abstract": "This paper proposes using the Linux kernel ftrace framework, particularly the function graph tracer, to generate informative system level data for machine learning (ML) applications. Experiments on a real world encryption detection task demonstrate the efficacy of the proposed features across several learning algorithms. The learner faces the problem of detecting encryption activities across a large dataset of files, using function call traces and graph based features. Empirical results highlight an outstanding accuracy of 99.28 on the task at hand, underscoring the efficacy of features derived from the function graph tracer. The results were further validated in an additional experiment targeting a multilabel classification problem, in which running programs were identified from trace data. This work provides comprehensive methodologies for preprocessing raw trace data and extracting graph based features, offering significant advancements in applying ML to system behavior analysis, program identification, and anomaly detection. By bridging the gap between system tracing and ML, this paper paves the way for innovative solutions in performance monitoring and security analytics.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.CR"], "url": "http://arxiv.org/abs/2512.04590", "comments": "Conference paper presented at AICCSA 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Exploiting \\texttt{ftrace}'s \\texttt{function\\_graph} Tracer Features for Machine Learning: A Case Study on Encryption Detection\nResearch Topic: machine learning\nAuthors: Kenan Begovic, Abdulaziz Al-Ali, Qutaibah Malluhi\nAbstract: This paper proposes using the Linux kernel ftrace framework, particularly the function graph tracer, to generate informative system level data for machine learning (ML) applications. Experiments on a real world encryption detection task demonstrate the efficacy of the proposed features across several learning algorithms. The learner faces the problem of detecting encryption activities across a large dataset of files, using function call traces and graph based features. Empirical results highlight an outstanding accuracy of 99.28 on the task at hand, underscoring the efficacy of features derived from the function graph tracer. The results were further validated in an additional experiment targeting a multilabel classification problem, in which running programs were identified from trace data. This work provides comprehensive methodologies for preprocessing raw trace data and extracting graph based features, offering significant advancements in applying ML to system behavior analysis, program identification, and anomaly detection. By bridging the gap between system tracing and ML, this paper paves the way for innovative solutions in performance monitoring and security analytics.\nCategories: cs.LG, cs.CR\nPublication Date: 2025-12-04\nPaper ID: 2512.04590\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 128, "abstract_length": 1195, "total_text_length": 1324, "word_count": 181, "sentence_count": 9}, "basic_keywords": ["graph", "features", "function", "system", "data", "paper", "using", "tracer"], "domain_keywords": [], "technical_terms": ["Experiments", "Empirical", "Linux"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04571", "topic": "machine learning", "title": "Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift", "authors": ["Aditi Naiknaware", "Sanchit Singh", "Hajar Homayouni", "Salimeh Sekeh"], "first_author": "Aditi Naiknaware", "abstract": "Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04571", "comments": "22 pages, 12 figures, 72 subfigures, 6 tables", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift\nResearch Topic: machine learning\nAuthors: Aditi Naiknaware, Sanchit Singh, Hajar Homayouni, Salimeh Sekeh\nAbstract: Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04571\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 119, "abstract_length": 1208, "total_text_length": 1328, "word_count": 164, "sentence_count": 8}, "basic_keywords": ["scone", "temp", "temporal", "environments", "dynamic", "evolving", "while", "robustness"], "domain_keywords": [], "technical_terms": ["Experiments", "Existing", "Open", "Average Thresholded Confidence", "Temp"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04566", "topic": "machine learning", "title": "Reliable Statistical Guarantees for Conformal Predictors with Small Datasets", "authors": ["Miguel Sánchez-Domínguez", "Lucas Lacasa", "Javier de Vicente", "Gonzalo Rubio", "Eusebio Valero"], "first_author": "Miguel Sánchez-Domínguez", "abstract": "Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the reliability of the uncertainty model, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers reliable information about the coverage of a conformal predictor for small data sizes. We illustrate and validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "physics.data-an", "stat.ML"], "url": "http://arxiv.org/abs/2512.04566", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Reliable Statistical Guarantees for Conformal Predictors with Small Datasets\nResearch Topic: machine learning\nAuthors: Miguel Sánchez-Domínguez, Lucas Lacasa, Javier de Vicente, Gonzalo Rubio, Eusebio Valero\nAbstract: Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the reliability of the uncertainty model, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers reliable information about the coverage of a conformal predictor for small data sizes. We illustrate and validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.\nCategories: cs.LG, physics.data-an, stat.ML\nPublication Date: 2025-12-04\nPaper ID: 2512.04566\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1910, "total_text_length": 1987, "word_count": 277, "sentence_count": 7}, "basic_keywords": ["uncertainty", "surrogate", "models", "conformal", "guarantee", "coverage", "learning", "data"], "domain_keywords": [], "technical_terms": ["However", "After", "Surrogate"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04562", "topic": "machine learning", "title": "LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models", "authors": ["Siddharth Betala", "Samuel P. Gleason", "Ali Ramlaoui", "Andy Xu", "Georgia Channing", "Daniel Levy", "Clémentine Fourrier", "Nikita Kazeev", "Chaitanya K. Joshi", "Sékou-Oumar Kaba", "Félix Therrien", "Alex Hernandez-Garcia", "Rocío Mercado", "N. M. Anoop Krishnan", "Alexandre Duval"], "first_author": "Siddharth Betala", "abstract": "Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04562", "comments": "46 pages, 17 figures, 16 tables", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models\nResearch Topic: machine learning\nAuthors: Siddharth Betala, Samuel P. Gleason, Ali Ramlaoui, Andy Xu, Georgia Channing, Daniel Levy, Clémentine Fourrier, Nikita Kazeev, Chaitanya K. Joshi, Sékou-Oumar Kaba, Félix Therrien, Alex Hernandez-Garcia, Rocío Mercado, N. M. Anoop Krishnan, Alexandre Duval\nAbstract: Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04562\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1076, "total_text_length": 1153, "word_count": 157, "sentence_count": 7}, "basic_keywords": ["models", "generative", "materials", "evaluation", "model", "discovery", "lemat", "genbench"], "domain_keywords": [], "technical_terms": ["Altogether", "Generative", "Hugging Face", "Results"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04558", "topic": "machine learning", "title": "On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference", "authors": ["Yue Yu", "Qiwei Di", "Quanquan Gu", "Dongruo Zhou"], "first_author": "Yue Yu", "abstract": "Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of- (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04558", "comments": "45 pages, 6 figures, 3 tables", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference\nResearch Topic: machine learning\nAuthors: Yue Yu, Qiwei Di, Quanquan Gu, Dongruo Zhou\nAbstract: Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04558\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1013, "total_text_length": 1098, "word_count": 146, "sentence_count": 8}, "basic_keywords": ["sequential", "reward", "inference", "empirical", "policy", "standard", "filtered", "side"], "domain_keywords": [], "technical_terms": ["Test", "Despite"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04512", "topic": "machine learning", "title": "Adaptive Time-Domain Harmonic Control for Noise-Vibration-Harshness Reduction of Electric Drives", "authors": ["Klaus Herburger", "Fabian Jakob", "David Gänzle", "Maximilian Manderla", "Andrea Iannelli"], "first_author": "Klaus Herburger", "abstract": "Reducing Noise, Vibration, and Harshness (NVH) in electric drives is crucial for applications such as electric vehicle drivetrains and heat-pump compressors, where strict NVH requirements directly affect user satisfaction and component longevity. This work presents the integration of an adaptive time-domain harmonic controller into an existing electric-drive control loop to attenuate harmonic disturbances. Three control structures are proposed and analyzed, along with a modified parameter-estimation scheme that reduces computational effort while preserving estimation accuracy, making the method suitable for embedded real-time implementation. To cope with fast operating-point changes, a delta-learning approach combines adaptive control with a lookup-table-based feedforward estimator, ensuring fast convergence and robustness. The proposed controller architectures are validated through simulation and testbench experiments on a permanent-magnet synchronous machine drive, demonstrating substantial NVH reductions across operating conditions. The results confirm that time-domain adaptive harmonic control offers a practical and theoretically grounded solution for real-time NVH mitigation in electric drives.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SY"], "url": "http://arxiv.org/abs/2512.04512", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.215958", "embedding_text": "Paper Title: Adaptive Time-Domain Harmonic Control for Noise-Vibration-Harshness Reduction of Electric Drives\nResearch Topic: machine learning\nAuthors: Klaus Herburger, Fabian Jakob, David Gänzle, Maximilian Manderla, Andrea Iannelli\nAbstract: Reducing Noise, Vibration, and Harshness (NVH) in electric drives is crucial for applications such as electric vehicle drivetrains and heat-pump compressors, where strict NVH requirements directly affect user satisfaction and component longevity. This work presents the integration of an adaptive time-domain harmonic controller into an existing electric-drive control loop to attenuate harmonic disturbances. Three control structures are proposed and analyzed, along with a modified parameter-estimation scheme that reduces computational effort while preserving estimation accuracy, making the method suitable for embedded real-time implementation. To cope with fast operating-point changes, a delta-learning approach combines adaptive control with a lookup-table-based feedforward estimator, ensuring fast convergence and robustness. The proposed controller architectures are validated through simulation and testbench experiments on a permanent-magnet synchronous machine drive, demonstrating substantial NVH reductions across operating conditions. The results confirm that time-domain adaptive harmonic control offers a practical and theoretically grounded solution for real-time NVH mitigation in electric drives.\nCategories: eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04512\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 96, "abstract_length": 1218, "total_text_length": 1315, "word_count": 160, "sentence_count": 7}, "basic_keywords": ["electric", "time", "control", "adaptive", "harmonic", "drives", "domain", "controller"], "domain_keywords": [], "technical_terms": ["Reducing Noise", "Harshness", "Three", "Vibration"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04510", "topic": "machine learning", "title": "Optimal Scaling Quantum Interior Point Method for Linear Optimization", "authors": ["Mohammadhossein Mohammadisiahroudi", "Zeguan Wu", "Pouya Sampourmahani", "Jun-Kai You", "Tamás Terlaky"], "first_author": "Mohammadhossein Mohammadisiahroudi", "abstract": "The emergence of huge-scale, data-intensive linear optimization (LO) problems in applications such as machine learning has driven the need for more computationally efficient interior point methods (IPMs). While conventional IPMs are polynomial-time algorithms with rapid convergence, their per-iteration cost can be prohibitively high for dense large-scale LO problems. Quantum linear system solvers have shown potential in accelerating the solution of linear systems arising in IPMs. In this work, we introduce a novel almost-exact quantum IPM, where the Newton system is constructed and solved on a quantum computer, while solution updates occur on a classical machine. Additionally, all matrix-vector products are performed on the quantum hardware. This hybrid quantum-classical framework achieves an optimal worst-case scaling of for fully dense LO problems. To ensure high precision, despite the limited accuracy of quantum operations, we incorporate iterative refinement techniques both within and outside the proposed IPM iterations. The proposed algorithm has a quantum complexity of queries to QRAM and classical arithmetic operations. Our method outperforms the worst-case complexity of prior classical and quantum IPMs, offering a significant improvement in scalability and computational efficiency.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph"], "url": "http://arxiv.org/abs/2512.04510", "comments": "IEEE QCE 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Optimal Scaling Quantum Interior Point Method for Linear Optimization\nResearch Topic: machine learning\nAuthors: Mohammadhossein Mohammadisiahroudi, Zeguan Wu, Pouya Sampourmahani, Jun-Kai You, Tamás Terlaky\nAbstract: The emergence of huge-scale, data-intensive linear optimization (LO) problems in applications such as machine learning has driven the need for more computationally efficient interior point methods (IPMs). While conventional IPMs are polynomial-time algorithms with rapid convergence, their per-iteration cost can be prohibitively high for dense large-scale LO problems. Quantum linear system solvers have shown potential in accelerating the solution of linear systems arising in IPMs. In this work, we introduce a novel almost-exact quantum IPM, where the Newton system is constructed and solved on a quantum computer, while solution updates occur on a classical machine. Additionally, all matrix-vector products are performed on the quantum hardware. This hybrid quantum-classical framework achieves an optimal worst-case scaling of $\\mathcal{O}(n^2)$ for fully dense LO problems. To ensure high precision, despite the limited accuracy of quantum operations, we incorporate iterative refinement techniques both within and outside the proposed IPM iterations. The proposed algorithm has a quantum complexity of $\\mathcal{O}(n^{1.5} κ_A \\log(\\frac{1}ε))$ queries to QRAM and $\\mathcal{O}(n^2 \\log(\\frac{1}ε))$ classical arithmetic operations. Our method outperforms the worst-case complexity of prior classical and quantum IPMs, offering a significant improvement in scalability and computational efficiency.\nCategories: quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04510\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1310, "total_text_length": 1380, "word_count": 189, "sentence_count": 10}, "basic_keywords": ["quantum", "ipms", "classical", "linear", "problems", "scale", "machine", "while"], "domain_keywords": [], "technical_terms": ["Newton", "Additionally", "While", "Quantum"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04476", "topic": "machine learning", "title": "Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems", "authors": ["Zehao Fan", "Zhenyu Liu", "Yunzhen Liu", "Yayue Hou", "Hadjer Benmeziane", "Kaoutar El Maghraoui", "Liu Liu"], "first_author": "Zehao Fan", "abstract": "Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.AR"], "url": "http://arxiv.org/abs/2512.04476", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems\nResearch Topic: machine learning\nAuthors: Zehao Fan, Zhenyu Liu, Yunzhen Liu, Yayue Hou, Hadjer Benmeziane, Kaoutar El Maghraoui, Liu Liu\nAbstract: Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.\nCategories: cs.LG, cs.AR\nPublication Date: 2025-12-04\nPaper ID: 2512.04476\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1265, "total_text_length": 1339, "word_count": 179, "sentence_count": 10}, "basic_keywords": ["experts", "memory", "expert", "movement", "context", "system", "stage", "models"], "domain_keywords": [], "technical_terms": ["Experts", "Unlike", "Mixture"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04460", "topic": "machine learning", "title": "Relative Wavefront Error Correction Over a 2.4 km Free-Space Optical Link via Machine Learning", "authors": ["Nathan K. Long", "Benjamin P. Dix-Matthews", "Alex Frost", "John Wallis", "Ziqing Wang", "Kenneth J. Grant", "Robert Malaney"], "first_author": "Nathan K. Long", "abstract": "In coherent optical communication across turbulent atmospheric channels, reference beacons can be multiplexed with information-encoded signals during transmission. In this case, it is commonly assumed that the wavefront distortion of the two is equivalent. In contrast to this assumption, we present experimental evidence of relative wavefront errors (WFEs) between polarization-multiplexed reference beacons and signals, after passing through a 2.4 km atmospheric link. We develop machine learning-based wavefront correction algorithms to compensate for observed WFEs, via phase retrieval, resulting in up to a 2/3 reduction in the relative phase error variance. Further, we analyze the excess noise contributions from relative WFEs in the context of continuous-variable quantum key distribution (CV-QKD), where our findings suggest that if future CV-QKD implementations employ wavefront correction algorithms similar to those reported here, an order of magnitude increase in secure key rates may be forthcoming.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.optics"], "url": "http://arxiv.org/abs/2512.04460", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Relative Wavefront Error Correction Over a 2.4 km Free-Space Optical Link via Machine Learning\nResearch Topic: machine learning\nAuthors: Nathan K. Long, Benjamin P. Dix-Matthews, Alex Frost, John Wallis, Ziqing Wang, Kenneth J. Grant, Robert Malaney\nAbstract: In coherent optical communication across turbulent atmospheric channels, reference beacons can be multiplexed with information-encoded signals during transmission. In this case, it is commonly assumed that the wavefront distortion of the two is equivalent. In contrast to this assumption, we present experimental evidence of relative wavefront errors (WFEs) between polarization-multiplexed reference beacons and signals, after passing through a 2.4 km atmospheric link. We develop machine learning-based wavefront correction algorithms to compensate for observed WFEs, via phase retrieval, resulting in up to a 2/3 reduction in the relative phase error variance. Further, we analyze the excess noise contributions from relative WFEs in the context of continuous-variable quantum key distribution (CV-QKD), where our findings suggest that if future CV-QKD implementations employ wavefront correction algorithms similar to those reported here, an order of magnitude increase in secure key rates may be forthcoming.\nCategories: physics.optics\nPublication Date: 2025-12-04\nPaper ID: 2512.04460\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1013, "total_text_length": 1108, "word_count": 153, "sentence_count": 8}, "basic_keywords": ["wavefront", "relative", "wfes", "atmospheric", "reference", "beacons", "multiplexed", "signals"], "domain_keywords": [], "technical_terms": ["Further"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04434", "topic": "machine learning", "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks", "authors": ["Ali Rabeh", "Suresh Murugaiyan", "Adarsh Krishnamurthy", "Baskar Ganapathysubramanian"], "first_author": "Ali Rabeh", "abstract": "Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.flu-dyn", "cs.LG"], "url": "http://arxiv.org/abs/2512.04434", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks\nResearch Topic: machine learning\nAuthors: Ali Rabeh, Suresh Murugaiyan, Adarsh Krishnamurthy, Baskar Ganapathysubramanian\nAbstract: Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains $\\sim 5\\%$ relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking.\nCategories: physics.flu-dyn, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04434\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 959, "total_text_length": 1038, "word_count": 131, "sentence_count": 10}, "basic_keywords": ["geometry", "error", "flow", "parametric", "shapes", "fidelity", "fast", "generalizing"], "domain_keywords": [], "technical_terms": ["These", "Fast", "Code", "Deep Operator Network"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04396", "topic": "machine learning", "title": "Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering", "authors": ["Subrata Karmaker"], "first_author": "Subrata Karmaker", "abstract": "Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feature engineering, without relying on neural networks or context from parent comments. Using a 100,000-comment subsample of the Self-Annotated Reddit Corpus (SARC 2.0), I combine word-level and character-level TF-IDF features with simple stylistic indicators. Four models are evaluated: logistic regression, a linear SVM, multinomial Naive Bayes, and a random forest. Naive Bayes and logistic regression perform the strongest, achieving F1-scores around 0.57 for sarcastic comments. Although the lack of conversational context limits performance, the results offer a clear and reproducible baseline for sarcasm detection using lightweight and interpretable methods.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.04396", "comments": "11 pages, 2 figures, includes full Python code. Classical machine learning baseline for sarcasm detection on the SARC 2.0 dataset", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering\nResearch Topic: machine learning\nAuthors: Subrata Karmaker\nAbstract: Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feature engineering, without relying on neural networks or context from parent comments. Using a 100,000-comment subsample of the Self-Annotated Reddit Corpus (SARC 2.0), I combine word-level and character-level TF-IDF features with simple stylistic indicators. Four models are evaluated: logistic regression, a linear SVM, multinomial Naive Bayes, and a random forest. Naive Bayes and logistic regression perform the strongest, achieving F1-scores around 0.57 for sarcastic comments. Although the lack of conversational context limits performance, the results offer a clear and reproducible baseline for sarcasm detection using lightweight and interpretable methods.\nCategories: cs.CL, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04396\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 914, "total_text_length": 999, "word_count": 136, "sentence_count": 9}, "basic_keywords": ["sarcasm", "using", "detection", "methods", "context", "comments", "level", "logistic"], "domain_keywords": [], "technical_terms": ["Using", "Annotated Reddit Corpus", "Four", "Naive Bayes", "Sarcasm", "Although", "Self"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04392", "topic": "machine learning", "title": "Informative missingness and its implications in semi-supervised learning", "authors": ["Jinran Wu", "You-Gan Wang", "Geoffrey J. McLachlan"], "first_author": "Jinran Wu", "abstract": "Semi-supervised learning (SSL) constructs classifiers using both labelled and unlabelled data. It leverages information from labelled samples, whose acquisition is often costly or labour-intensive, together with unlabelled data to enhance prediction performance. This defines an incomplete-data problem, which statistically can be formulated within the likelihood framework for finite mixture models that can be fitted using the expectation-maximisation (EM) algorithm. Ideally, one would prefer a completely labelled sample, as one would anticipate that a labelled observation provides more information than an unlabelled one. However, when the mechanism governing label absence depends on the observed features or the class labels or both, the missingness indicators themselves contain useful information. In certain situations, the information gained from modelling the missing-label mechanism can even outweigh the loss due to missing labels, yielding a classifier with a smaller expected error than one based on a completely labelled sample analysed. This improvement arises particularly when class overlap is moderate, labelled data are sparse, and the missingness is informative. Modelling such informative missingness thus offers a coherent statistical framework that unifies likelihood-based inference with the behaviour of empirical SSL methods.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/abs/2512.04392", "comments": "1", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Informative missingness and its implications in semi-supervised learning\nResearch Topic: machine learning\nAuthors: Jinran Wu, You-Gan Wang, Geoffrey J. McLachlan\nAbstract: Semi-supervised learning (SSL) constructs classifiers using both labelled and unlabelled data. It leverages information from labelled samples, whose acquisition is often costly or labour-intensive, together with unlabelled data to enhance prediction performance. This defines an incomplete-data problem, which statistically can be formulated within the likelihood framework for finite mixture models that can be fitted using the expectation-maximisation (EM) algorithm. Ideally, one would prefer a completely labelled sample, as one would anticipate that a labelled observation provides more information than an unlabelled one. However, when the mechanism governing label absence depends on the observed features or the class labels or both, the missingness indicators themselves contain useful information. In certain situations, the information gained from modelling the missing-label mechanism can even outweigh the loss due to missing labels, yielding a classifier with a smaller expected error than one based on a completely labelled sample analysed. This improvement arises particularly when class overlap is moderate, labelled data are sparse, and the missingness is informative. Modelling such informative missingness thus offers a coherent statistical framework that unifies likelihood-based inference with the behaviour of empirical SSL methods.\nCategories: stat.ML, cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04392\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1355, "total_text_length": 1428, "word_count": 191, "sentence_count": 9}, "basic_keywords": ["labelled", "data", "information", "unlabelled", "missingness", "using", "both", "likelihood"], "domain_keywords": [], "technical_terms": ["Semi", "Modelling", "Ideally", "However"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04388", "topic": "machine learning", "title": "Learning to Orchestrate Agents in Natural Language with the Conductor", "authors": ["Stefan Nielsen", "Edoardo Cetin", "Peter Schwendeman", "Qi Sun", "Jinglue Xu", "Yujin Tang"], "first_author": "Stefan Nielsen", "abstract": "Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04388", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Learning to Orchestrate Agents in Natural Language with the Conductor\nResearch Topic: machine learning\nAuthors: Stefan Nielsen, Edoardo Cetin, Peter Schwendeman, Qi Sun, Jinglue Xu, Yujin Tang\nAbstract: Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04388\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1407, "total_text_length": 1477, "word_count": 200, "sentence_count": 8}, "basic_keywords": ["llms", "conductor", "powerful", "coordination", "strategies", "agent", "worker", "through"], "domain_keywords": [], "technical_terms": ["Conductor", "Powerful", "Furthermore", "More", "Our Conductor"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04371", "topic": "machine learning", "title": "Constructive Approximation under Carleman's Condition, with Applications to Smoothed Analysis", "authors": ["Frederic Koehler", "Beining Wu"], "first_author": "Frederic Koehler", "abstract": "A classical result of Carleman, based on the theory of quasianalytic functions, shows that polynomials are dense in for any such that the moments do not grow too rapidly as . In this work, we develop a fairly tight quantitative analogue of the underlying Denjoy-Carleman theorem via complex analysis, and show that this allows for nonasymptotic control of the rate of approximation by polynomials for any smooth function with polynomial growth at infinity. In many cases, this allows us to establish approximation-theoretic results for functions over general classes of distributions (e.g., multivariate sub-Gaussian or sub-exponential distributions) which were previously known only in special cases. As one application, we show that the Paley--Wiener class of functions bandlimited to admits superexponential rates of approximation over all strictly sub-exponential distributions, which leads to a new characterization of the class. As another application, we solve an open problem recently posed by Chandrasekaran, Klivans, Kontonis, Meka and Stavropoulos on the smoothed analysis of learning, and also obtain quantitative improvements to their main results and applications.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.PR", "cs.LG", "math.FA", "math.ST", "stat.ML"], "url": "http://arxiv.org/abs/2512.04371", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:01.216986", "embedding_text": "Paper Title: Constructive Approximation under Carleman's Condition, with Applications to Smoothed Analysis\nResearch Topic: machine learning\nAuthors: Frederic Koehler, Beining Wu\nAbstract: A classical result of Carleman, based on the theory of quasianalytic functions, shows that polynomials are dense in $L^2(μ)$ for any $μ$ such that the moments $\\int x^k dμ$ do not grow too rapidly as $k \\to \\infty$. In this work, we develop a fairly tight quantitative analogue of the underlying Denjoy-Carleman theorem via complex analysis, and show that this allows for nonasymptotic control of the rate of approximation by polynomials for any smooth function with polynomial growth at infinity. In many cases, this allows us to establish $L^2$ approximation-theoretic results for functions over general classes of distributions (e.g., multivariate sub-Gaussian or sub-exponential distributions) which were previously known only in special cases. As one application, we show that the Paley--Wiener class of functions bandlimited to $[-Ω,Ω]$ admits superexponential rates of approximation over all strictly sub-exponential distributions, which leads to a new characterization of the class. As another application, we solve an open problem recently posed by Chandrasekaran, Klivans, Kontonis, Meka and Stavropoulos on the smoothed analysis of learning, and also obtain quantitative improvements to their main results and applications.\nCategories: math.PR, cs.LG, math.FA, math.ST, stat.ML\nPublication Date: 2025-12-04\nPaper ID: 2512.04371\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1178, "total_text_length": 1272, "word_count": 180, "sentence_count": 8}, "basic_keywords": ["functions", "approximation", "distributions", "carleman", "polynomials", "quantitative", "analysis", "show"], "domain_keywords": [], "technical_terms": ["Carleman", "Denjoy", "Meka", "Gaussian", "Wiener", "Paley", "Kontonis", "Chandrasekaran"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04354", "topic": "machine learning", "title": "SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction", "authors": ["April S. Liang", "Fatemeh Amrollahi", "Yixing Jiang", "Conor K. Corbin", "Grace Y. E. Kim", "David Mui", "Trevor Crowell", "Aakash Acharya", "Sreedevi Mony", "Soumya Punnathanam", "Jack McKeown", "Margaret Smith", "Steven Lin", "Arnold Milstein", "Kevin Schulman", "Jason Hom", "Michael A. Pfeffer", "Tho D. Pham", "David Svec", "Weihan Chu", "Lisa Shieh", "Christopher Sharp", "Stephen P. Ma", "Jonathan H. Chen"], "first_author": "April S. Liang", "abstract": "Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG", "cs.HC"], "url": "http://arxiv.org/abs/2512.04354", "comments": "22 pages, 5 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction\nResearch Topic: machine learning\nAuthors: April S. Liang, Fatemeh Amrollahi, Yixing Jiang, Conor K. Corbin, Grace Y. E. Kim, David Mui, Trevor Crowell, Aakash Acharya, Sreedevi Mony, Soumya Punnathanam, Jack McKeown, Margaret Smith, Steven Lin, Arnold Milstein, Kevin Schulman, Jason Hom, Michael A. Pfeffer, Tho D. Pham, David Svec, Weihan Chu, Lisa Shieh, Christopher Sharp, Stephen P. Ma, Jonathan H. Chen\nAbstract: Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.\nCategories: cs.LG, cs.HC\nPublication Date: 2025-12-04\nPaper ID: 2512.04354\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 114, "abstract_length": 1697, "total_text_length": 1812, "word_count": 237, "sentence_count": 11}, "basic_keywords": ["testing", "clinical", "repetitive", "laboratory", "smartalert", "results", "implementation", "model"], "domain_keywords": [], "technical_terms": ["Education", "Repetitive", "August", "Results", "Implementation", "March"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04351", "topic": "machine learning", "title": "Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models", "authors": ["Manh Nguyen", "Sunil Gupta", "Hung Le"], "first_author": "Manh Nguyen", "abstract": "Detecting when large language models (LLMs) are uncertain is critical for building reliable systems, yet existing methods are overly complicated, relying on brittle semantic clustering or internal states. We introduce , a simple, parameter-free, fully model-agnostic uncertainty metric that measures the radial dispersion of sampled generations in embedding space. A lightweight probability-weighted variant further incorporates the model's own token probabilities when available, outperforming different nine strong baselines. Moroever, RDS naturally extends to per-sample scoring, enabling applications such as best-of- selection and confidence-based filtering. Across four challenging free-form QA datasets and multiple LLMs, our metrics achieve state-of-the-art hallucination detection and answer selection performance, while remaining robust and scalable with respect to sample size and embedding choice.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04351", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models\nResearch Topic: machine learning\nAuthors: Manh Nguyen, Sunil Gupta, Hung Le\nAbstract: Detecting when large language models (LLMs) are uncertain is critical for building reliable systems, yet existing methods are overly complicated, relying on brittle semantic clustering or internal states. We introduce \\textbf{Radial Dispersion Score (RDS)}, a simple, parameter-free, fully model-agnostic uncertainty metric that measures the radial dispersion of sampled generations in embedding space. A lightweight probability-weighted variant further incorporates the model's own token probabilities when available, outperforming different nine strong baselines. Moroever, RDS naturally extends to per-sample scoring, enabling applications such as best-of-$N$ selection and confidence-based filtering. Across four challenging free-form QA datasets and multiple LLMs, our metrics achieve state-of-the-art hallucination detection and answer selection performance, while remaining robust and scalable with respect to sample size and embedding choice.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04351\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 909, "total_text_length": 1005, "word_count": 129, "sentence_count": 6}, "basic_keywords": ["llms", "free", "model", "embedding", "sample", "selection", "detecting", "large"], "domain_keywords": [], "technical_terms": ["Moroever", "Detecting", "Across"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04341", "topic": "machine learning", "title": "Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism", "authors": ["Tianwei Ni", "Esther Derman", "Vineet Jain", "Vincent Taboga", "Siamak Ravanbakhsh", "Pierre-Luc Bacon"], "first_author": "Tianwei Ni", "abstract": "Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting planning horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale the principle to realistic tasks, identifying key design choices, such as layer normalization in the world model and adaptive long-horizon planning, that mitigate compounding error and value overestimation. These yield our practical algorithm, Neubay, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, Neubay generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with planning horizons of several hundred steps, challenging common belief. Finally, we characterize when Neubay is preferable to conservatism, laying the foundation for a new direction in offline and model-based RL.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04341", "comments": "Preprint (52 pages, 15 figures)", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism\nResearch Topic: machine learning\nAuthors: Tianwei Ni, Esther Derman, Vineet Jain, Vincent Taboga, Siamak Ravanbakhsh, Pierre-Luc Bacon\nAbstract: Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting planning horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale the principle to realistic tasks, identifying key design choices, such as layer normalization in the world model and adaptive long-horizon planning, that mitigate compounding error and value overestimation. These yield our practical algorithm, Neubay, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, Neubay generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with planning horizons of several hundred steps, challenging common belief. Finally, we characterize when Neubay is preferable to conservatism, laying the foundation for a new direction in offline and model-based RL.\nCategories: cs.LG\nPublication Date: 2025-12-04\nPaper ID: 2512.04341\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1364, "total_text_length": 1441, "word_count": 191, "sentence_count": 10}, "basic_keywords": ["conservatism", "offline", "planning", "principle", "bayesian", "neubay", "horizons", "world"], "domain_keywords": [], "technical_terms": ["Popular", "These", "Bayesianism", "Notably", "Rather", "Finally", "Neubay", "Bayesian"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04338", "topic": "machine learning", "title": "One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises", "authors": ["Biagio Montaruli", "Luca Compagna", "Serena Elisa Ponta", "Davide Balzarotti"], "first_author": "Biagio Montaruli", "abstract": "The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance). We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages. We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives. Overall, we uncovered 346 malicious packages, now reported to the community.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CR", "cs.LG"], "url": "http://arxiv.org/abs/2512.04338", "comments": "Proceedings of the 2025 Annual Computer Security Applications Conference (ACSAC' 25), December 8-12, 2025, Honolulu, Hawaii, USA", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises\nResearch Topic: machine learning\nAuthors: Biagio Montaruli, Luca Compagna, Serena Elisa Ponta, Davide Balzarotti\nAbstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).   We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.   We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.   Overall, we uncovered 346 malicious packages, now reported to the community.\nCategories: cs.CR, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04338\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 99, "abstract_length": 1860, "total_text_length": 1960, "word_count": 281, "sentence_count": 17}, "basic_keywords": ["packages", "detector", "pypi", "false", "enterprise", "malicious", "robust", "robustness"], "domain_keywords": [], "technical_terms": ["These", "Python", "Combining", "Current", "Overall"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04332", "topic": "machine learning", "title": "Data-regularized Reinforcement Learning for Diffusion Models at Scale", "authors": ["Haotian Ye", "Kaiwen Zheng", "Jiashu Xu", "Puheng Li", "Huayu Chen", "Jiaqi Han", "Sheng Liu", "Qinsheng Zhang", "Hanzi Mao", "Zekun Hao", "Prithvijit Chattopadhyay", "Dinghao Yang", "Liang Feng", "Maosheng Liao", "Junjie Bai", "Ming-Yu Liu", "James Zou", "Stefano Ermon"], "first_author": "Haotian Ye", "abstract": "Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04332", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Data-regularized Reinforcement Learning for Diffusion Models at Scale\nResearch Topic: machine learning\nAuthors: Haotian Ye, Kaiwen Zheng, Jiashu Xu, Puheng Li, Huayu Chen, Jiaqi Han, Sheng Liu, Qinsheng Zhang, Hanzi Mao, Zekun Hao, Prithvijit Chattopadhyay, Dinghao Yang, Liang Feng, Maosheng Liao, Junjie Bai, Ming-Yu Liu, James Zou, Stefano Ermon\nAbstract: Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04332\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1176, "total_text_length": 1246, "word_count": 162, "sentence_count": 8}, "basic_keywords": ["diffusion", "human", "reward", "ddrl", "reinforcement", "learning", "hacking", "over"], "domain_keywords": [], "technical_terms": ["Diffusion Reinforcement Learning", "Aligning", "Data", "Most", "Theoretically", "Empirically"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04326", "topic": "machine learning", "title": "A review on fundamental bounds and estimators for photometry and astrometry of celestial point sources using array detectors, from first principles", "authors": ["Sebastián Espinosa", "Rene A. Mendez", "Jorge F. Silva", "Marcos Orchard"], "first_author": "Sebastián Espinosa", "abstract": "Precise astrometric and photometric measurements of celestial point sources are fundamental to modern astronomy. These measurements, used to determine object positions, motions, and fluxes, are based on observational models that have evolved from empirical centroiding rules to rigorous probabilistic formulations at the pixel level. This review summarizes key contributions that formalized this transition and analyzes seminal works addressing both the theoretical limits and the empirical performance of estimators. Central to these developments is the derivation of fundamental bounds, such as the Cram r-Rao Lower Bound (CRLB), and the assessment of widely used estimators, including Maximum Likelihood (ML), Least Squares (LS), and Weighted Least Squares (WLS). These studies show that, while the CRLB sets a theoretical benchmark, practical estimators achieve it only under specific signal-to-noise ratio (SNR) regimes, with notable discrepancies in high-SNR conditions. Moreover, recent results demonstrate that jointly estimating source flux and background significantly improves photometric precision compared to sequential approaches. Looking ahead, the increasing complexity of astronomical surveys, driven by massive data volumes, dynamic observational conditions, and the integration of machine learning, poses new challenges to reliable inference. In this context, tools from statistical theory, including performance bounds and theoretically grounded estimators, remain critical to guide algorithm design and ensure robust astrometric and photometric pipelines.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["astro-ph.IM"], "url": "http://arxiv.org/abs/2512.04326", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: A review on fundamental bounds and estimators for photometry and astrometry of celestial point sources using array detectors, from first principles\nResearch Topic: machine learning\nAuthors: Sebastián Espinosa, Rene A. Mendez, Jorge F. Silva, Marcos Orchard\nAbstract: Precise astrometric and photometric measurements of celestial point sources are fundamental to modern astronomy. These measurements, used to determine object positions, motions, and fluxes, are based on observational models that have evolved from empirical centroiding rules to rigorous probabilistic formulations at the pixel level. This review summarizes key contributions that formalized this transition and analyzes seminal works addressing both the theoretical limits and the empirical performance of estimators. Central to these developments is the derivation of fundamental bounds, such as the Cramér-Rao Lower Bound (CRLB), and the assessment of widely used estimators, including Maximum Likelihood (ML), Least Squares (LS), and Weighted Least Squares (WLS). These studies show that, while the CRLB sets a theoretical benchmark, practical estimators achieve it only under specific signal-to-noise ratio (SNR) regimes, with notable discrepancies in high-SNR conditions. Moreover, recent results demonstrate that jointly estimating source flux and background significantly improves photometric precision compared to sequential approaches. Looking ahead, the increasing complexity of astronomical surveys, driven by massive data volumes, dynamic observational conditions, and the integration of machine learning, poses new challenges to reliable inference. In this context, tools from statistical theory, including performance bounds and theoretically grounded estimators, remain critical to guide algorithm design and ensure robust astrometric and photometric pipelines.\nCategories: astro-ph.IM\nPublication Date: 2025-12-03\nPaper ID: 2512.04326\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 147, "abstract_length": 1576, "total_text_length": 1724, "word_count": 228, "sentence_count": 9}, "basic_keywords": ["estimators", "photometric", "astrometric", "measurements", "fundamental", "used", "observational", "empirical"], "domain_keywords": [], "technical_terms": ["These", "Cram", "Weighted Least Squares", "Least Squares", "Looking", "Maximum Likelihood", "Precise", "Central"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04320", "topic": "machine learning", "title": "VLCs: Managing Parallelism with Virtualized Libraries", "authors": ["Yineng Yan", "William Ruys", "Hochan Lee", "Ian Henriksen", "Arthur Peters", "Sean Stephens", "Bozhi You", "Henrique Fingler", "Martin Burtscher", "Milos Gligoric", "Keshav Pingali", "Mattan Erez", "George Biros", "Christopher J. Rossbach"], "first_author": "Yineng Yan", "abstract": "As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible. We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process. In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.DC", "cs.OS"], "url": "http://arxiv.org/abs/2512.04320", "comments": "Research Paper accepted to the ACM Symposium on Cloud Computing (SoCC'25)", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: VLCs: Managing Parallelism with Virtualized Libraries\nResearch Topic: machine learning\nAuthors: Yineng Yan, William Ruys, Hochan Lee, Ian Henriksen, Arthur Peters, Sean Stephens, Bozhi You, Henrique Fingler, Martin Burtscher, Milos Gligoric, Keshav Pingali, Mattan Erez, George Biros, Christopher J. Rossbach\nAbstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.   We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.   In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.\nCategories: cs.DC, cs.OS\nPublication Date: 2025-12-03\nPaper ID: 2512.04320\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 53, "abstract_length": 1121, "total_text_length": 1175, "word_count": 168, "sentence_count": 11}, "basic_keywords": ["libraries", "vlcs", "library", "parallel", "composition", "encapsulate", "resources", "using"], "domain_keywords": [], "technical_terms": ["Virtual Library Contexts", "However", "Using", "Experiments", "Prior", "Python"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04310", "topic": "machine learning", "title": "RNNs perform task computations by dynamically warping neural representations", "authors": ["Arthur Pellegrino", "Angus Chadwick"], "first_author": "Arthur Pellegrino", "abstract": "Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such \"neural representations.\" In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "math.DG", "math.DS", "q-bio.NC"], "url": "http://arxiv.org/abs/2512.04310", "comments": "NeurIPS 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: RNNs perform task computations by dynamically warping neural representations\nResearch Topic: machine learning\nAuthors: Arthur Pellegrino, Angus Chadwick\nAbstract: Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such \"neural representations.\" In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations.\nCategories: cs.LG, math.DG, math.DS, q-bio.NC\nPublication Date: 2025-12-03\nPaper ID: 2512.04310\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 955, "total_text_length": 1032, "word_count": 142, "sentence_count": 8}, "basic_keywords": ["geometry", "neural", "perform", "computations", "networks", "data", "characterising", "representations"], "domain_keywords": [], "technical_terms": ["Here", "Riemannian", "Analysing", "Hence"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04299", "topic": "machine learning", "title": "When do spectral gradient updates help in deep learning?", "authors": ["Damek Davis", "Dmitriy Drusvyatskiy"], "first_author": "Damek Davis", "abstract": "Spectral gradient methods, such as the recently popularized Muon optimizer, are a promising alternative to standard Euclidean gradient descent for training deep neural networks and transformers, but it is still unclear in which regimes they are expected to perform better. We propose a simple layerwise condition that predicts when a spectral update yields a larger decrease in the loss than a Euclidean gradient step. This condition compares, for each parameter block, the squared nuclear-to-Frobenius ratio of the gradient to the stable rank of the incoming activations. To understand when this condition may be satisfied, we first prove that post-activation matrices have low stable rank at Gaussian initialization in random feature regression, feedforward networks, and transformer blocks. In spiked random feature models we then show that, after a short burn-in, the Euclidean gradient's nuclear-to-Frobenius ratio grows with the data dimension while the stable rank of the activations remains bounded, so the predicted advantage of spectral updates scales with dimension. We validate these predictions in synthetic regression experiments and in NanoGPT-scale language model training, where we find that intermediate activations have low-stable-rank throughout training and the corresponding gradients maintain large nuclear-to-Frobenius ratios. Together, these results identify conditions for spectral gradient methods, such as Muon, to be effective in training deep networks and transformers.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "math.OC", "stat.ML"], "url": "http://arxiv.org/abs/2512.04299", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: When do spectral gradient updates help in deep learning?\nResearch Topic: machine learning\nAuthors: Damek Davis, Dmitriy Drusvyatskiy\nAbstract: Spectral gradient methods, such as the recently popularized Muon optimizer, are a promising alternative to standard Euclidean gradient descent for training deep neural networks and transformers, but it is still unclear in which regimes they are expected to perform better. We propose a simple layerwise condition that predicts when a spectral update yields a larger decrease in the loss than a Euclidean gradient step. This condition compares, for each parameter block, the squared nuclear-to-Frobenius ratio of the gradient to the stable rank of the incoming activations. To understand when this condition may be satisfied, we first prove that post-activation matrices have low stable rank at Gaussian initialization in random feature regression, feedforward networks, and transformer blocks. In spiked random feature models we then show that, after a short burn-in, the Euclidean gradient's nuclear-to-Frobenius ratio grows with the data dimension while the stable rank of the activations remains bounded, so the predicted advantage of spectral updates scales with dimension. We validate these predictions in synthetic regression experiments and in NanoGPT-scale language model training, where we find that intermediate activations have low-stable-rank throughout training and the corresponding gradients maintain large nuclear-to-Frobenius ratios. Together, these results identify conditions for spectral gradient methods, such as Muon, to be effective in training deep networks and transformers.\nCategories: cs.LG, math.OC, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.04299\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 56, "abstract_length": 1499, "total_text_length": 1556, "word_count": 220, "sentence_count": 9}, "basic_keywords": ["gradient", "spectral", "training", "stable", "rank", "euclidean", "networks", "condition"], "domain_keywords": [], "technical_terms": ["Frobenius", "Euclidean", "Gaussian", "Muon", "Together", "Spectral"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04296", "topic": "machine learning", "title": "GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers", "authors": ["Malyaban Bal", "Abhronil Sengupta"], "first_author": "Malyaban Bal", "abstract": "Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.NE"], "url": "http://arxiv.org/abs/2512.04296", "comments": "Under Review", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers\nResearch Topic: machine learning\nAuthors: Malyaban Bal, Abhronil Sengupta\nAbstract: Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.\nCategories: cs.LG, cs.NE\nPublication Date: 2025-12-03\nPaper ID: 2512.04296\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 122, "abstract_length": 1478, "total_text_length": 1601, "word_count": 204, "sentence_count": 8}, "basic_keywords": ["peft", "parameters", "noise", "hardware", "efficient", "model", "large", "trained"], "domain_keywords": [], "technical_terms": ["Medium", "Parameter", "Across", "Under", "Gaussian", "Activation Shared Parameterization", "Building"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04291", "topic": "machine learning", "title": "Scaling MPI Applications on Aurora", "authors": ["Huda Ibeid", "Anthony-Trung Nguyen", "Aditya Nishtala", "Premanand Sakarda", "Larry Kaplan", "Nilakantan Mahadevan", "Michael Woodacre", "Victor Anisimov", "Kalyan Kumaran", "JaeHyuk Kwack", "Vitali Morozov", "Servesh Muralidharan", "Scott Parker"], "first_author": "Huda Ibeid", "abstract": "The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.DC"], "url": "http://arxiv.org/abs/2512.04291", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Scaling MPI Applications on Aurora\nResearch Topic: machine learning\nAuthors: Huda Ibeid, Anthony-Trung Nguyen, Aditya Nishtala, Premanand Sakarda, Larry Kaplan, Nilakantan Mahadevan, Michael Woodacre, Victor Anisimov, Kalyan Kumaran, JaeHyuk Kwack, Vitali Morozov, Servesh Muralidharan, Scott Parker\nAbstract: The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.\nCategories: cs.DC\nPublication Date: 2025-12-03\nPaper ID: 2512.04291\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 34, "abstract_length": 1740, "total_text_length": 1775, "word_count": 281, "sentence_count": 10}, "basic_keywords": ["system", "aurora", "intel", "performance", "nodes", "slingshot", "fabric", "currently"], "domain_keywords": [], "technical_terms": ["Rosetta", "Cassini", "Argonne National Laboratory", "Intel Data Center Max Series", "Intel Xeon Max Series", "Xeon", "Additionally", "Intel"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04283", "topic": "machine learning", "title": "Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint", "authors": ["Fan Jia", "Yuhao Huang", "Shih-Hsin Wang", "Cristina Garcia-Cardona", "Andrea L. Bertozzi", "Bao Wang"], "first_author": "Fan Jia", "abstract": "Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.04283", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint\nResearch Topic: machine learning\nAuthors: Fan Jia, Yuhao Huang, Shih-Hsin Wang, Cristina Garcia-Cardona, Andrea L. Bertozzi, Bao Wang\nAbstract: Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04283\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1255, "total_text_length": 1330, "word_count": 176, "sentence_count": 8}, "basic_keywords": ["flow", "image", "restoration", "model", "resulting", "matching", "models", "plug"], "domain_keywords": [], "technical_terms": ["Flow", "However", "Lipschitz", "Numerical"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04282", "topic": "machine learning", "title": "Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer", "authors": ["Tasmiah Haque", "Srinjoy Das"], "first_author": "Tasmiah Haque", "abstract": "Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.04282", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer\nResearch Topic: machine learning\nAuthors: Tasmiah Haque, Srinjoy Das\nAbstract: Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04282\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 96, "abstract_length": 1613, "total_text_length": 1710, "word_count": 215, "sentence_count": 9}, "basic_keywords": ["inference", "normalizing", "flows", "stochastic", "time", "based", "diverse", "video"], "domain_keywords": [], "technical_terms": ["These", "Experiments", "While", "Real", "Stochastic Normalizing Flows", "Normalizing Flows", "Gated Recurrent Unit", "Markov Chain Monte Carlo"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04264", "topic": "machine learning", "title": "Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness", "authors": ["Long Dang", "Thushari Hapuarachchi", "Kaiqi Xiong", "Jing Lin"], "first_author": "Long Dang", "abstract": "Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.CV"], "url": "http://arxiv.org/abs/2512.04264", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness\nResearch Topic: machine learning\nAuthors: Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, Jing Lin\nAbstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.\nCategories: cs.LG, cs.CV\nPublication Date: 2025-12-03\nPaper ID: 2512.04264\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1816, "total_text_length": 1909, "word_count": 263, "sentence_count": 16}, "basic_keywords": ["model", "data", "training", "robustness", "adversarial", "learning", "activation", "centralized"], "domain_keywords": [], "technical_terms": ["Experiments", "Adversarial", "Rectified", "Furthermore", "Most", "Then"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04260", "topic": "machine learning", "title": "Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks", "authors": ["Gaoning Pan", "Yiming Tao", "Qinying Wang", "Chunming Wu", "Mingde Hu", "Yizhi Ren", "Shouling Ji"], "first_author": "Gaoning Pan", "abstract": "Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CR"], "url": "http://arxiv.org/abs/2512.04260", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks\nResearch Topic: machine learning\nAuthors: Gaoning Pan, Yiming Tao, Qinying Wang, Chunming Wu, Mingde Hu, Yizhi Ren, Shouling Ji\nAbstract: Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.\nCategories: cs.CR\nPublication Date: 2025-12-03\nPaper ID: 2512.04260\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1198, "total_text_length": 1288, "word_count": 170, "sentence_count": 7}, "basic_keywords": ["memory", "exploitation", "vulnerabilities", "structures", "host", "environments", "guest", "cross"], "domain_keywords": [], "technical_terms": ["Domain Attacks", "Based", "Existing", "Hypervisors", "Cross", "Address Space Layout Randomization"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04257", "topic": "machine learning", "title": "Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification", "authors": ["Mansour Essgaer", "Khamis Massud", "Rabia Al Mamlook", "Najah Ghmaid"], "first_author": "Mansour Essgaer", "abstract": "This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.04257", "comments": "13 pages, 8 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification\nResearch Topic: machine learning\nAuthors: Mansour Essgaer, Khamis Massud, Rabia Al Mamlook, Najah Ghmaid\nAbstract: This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.\nCategories: cs.CL, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04257\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1679, "total_text_length": 1761, "word_count": 233, "sentence_count": 15}, "basic_keywords": ["dialect", "gram", "naive", "bayes", "libyan", "classification", "using", "study"], "domain_keywords": [], "technical_terms": ["Matthew", "Multinomial Naive Bayes", "Cohen", "Preprocessing", "Bernoulli Naive Bayes", "Additional", "Logistic Regression", "Naive Bayes"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04242", "topic": "machine learning", "title": "Characterizing Defect Dynamics in Silicon Carbide Using Symmetry-Adapted Collective Variables and Machine Learning Interatomic Potentials", "authors": ["Soumajit Dutta", "Cunzhi Zhang", "Gustavo Perez Lemus", "Juan J. de Pablo", "Francois Gygi", "Giulia Galli", "Andrew L. Ferguson"], "first_author": "Soumajit Dutta", "abstract": "Silicon carbide (SiC) divacancies are attractive candidates for spin defect qubits possessing long coherence times and optical addressability. The high activation barriers associated with SiC defect formation and motion pose challenges for their study by first-principles molecular dynamics. In this work, we develop and deploy machine learning interatomic potentials (MLIPs) to accelerate defect dynamics simulations while retaining ab initio accuracy. We employ an active learning strategy comprising symmetry-adapted collective variable discovery and enhanced sampling to compile configurationally diverse training data, calculation of energies and forces using density functional theory (DFT), and training of an E(3)-equivariant MLIP based on the Allegro model. The trained MLIP reproduces DFT-level accuracy in defect transition activation free energy barriers, enables the efficient and stable simulation of multi-defect 216-atom supercells, and permits an analysis of the temperature dependence of defect thermodynamic stability and formation/annihilation kinetics to propose an optimal annealing temperature to maximally stabilize VV divacancies.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cond-mat.mtrl-sci", "physics.chem-ph"], "url": "http://arxiv.org/abs/2512.04242", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Characterizing Defect Dynamics in Silicon Carbide Using Symmetry-Adapted Collective Variables and Machine Learning Interatomic Potentials\nResearch Topic: machine learning\nAuthors: Soumajit Dutta, Cunzhi Zhang, Gustavo Perez Lemus, Juan J. de Pablo, Francois Gygi, Giulia Galli, Andrew L. Ferguson\nAbstract: Silicon carbide (SiC) divacancies are attractive candidates for spin defect qubits possessing long coherence times and optical addressability. The high activation barriers associated with SiC defect formation and motion pose challenges for their study by first-principles molecular dynamics. In this work, we develop and deploy machine learning interatomic potentials (MLIPs) to accelerate defect dynamics simulations while retaining ab initio accuracy. We employ an active learning strategy comprising symmetry-adapted collective variable discovery and enhanced sampling to compile configurationally diverse training data, calculation of energies and forces using density functional theory (DFT), and training of an E(3)-equivariant MLIP based on the Allegro model. The trained MLIP reproduces DFT-level accuracy in defect transition activation free energy barriers, enables the efficient and stable simulation of multi-defect 216-atom supercells, and permits an analysis of the temperature dependence of defect thermodynamic stability and formation/annihilation kinetics to propose an optimal annealing temperature to maximally stabilize VV divacancies.\nCategories: cond-mat.mtrl-sci, physics.chem-ph\nPublication Date: 2025-12-03\nPaper ID: 2512.04242\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 137, "abstract_length": 1155, "total_text_length": 1293, "word_count": 165, "sentence_count": 6}, "basic_keywords": ["defect", "divacancies", "activation", "barriers", "formation", "dynamics", "learning", "accuracy"], "domain_keywords": [], "technical_terms": ["Allegro", "Silicon"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04241", "topic": "machine learning", "title": "Covering Relations in the Poset of Combinatorial Neural Codes", "authors": ["R. Amzi Jeffs", "Trong-Thuc Trang"], "first_author": "R. Amzi Jeffs", "abstract": "A combinatorial neural code is a subset of the power set on , in which each represents a neuron and each element (codeword) represents the co-firing event of some neurons. Consider a space , simulating an animal's environment, and a collection of open subsets of . Each simulates a place field which is a specific region where a place cell is active. Then, the code of in is defined as . If a neural code for some and , we say has a realization of open subsets of some space . Although every combinatorial neural code obviously has a realization by some open subsets, determining whether it has a realization by some open convex subsets remains unsolved. Many studies attempted to tackle this decision problem, but only partial results were achieved. In fact, a previous study showed that the decision problem of convex neural codes is NP-hard. Furthermore, the authors of this study conjectured that every convex neural code can be realized as a minor of a neural code arising from a representable oriented matroid, which can lead to an equivalence between convex and polytope convex neural codes. Even though this conjecture has been confirmed in dimension two, its validity in higher dimensions is still unknown. To advance the investigation of this conjecture, we provide a complete characterization of the covering relations within the poset of neural codes.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["math.CO", "q-bio.NC"], "url": "http://arxiv.org/abs/2512.04241", "comments": "To appear in Proceedings of the 4th NeurIPS Workshop on Symmetry and Geometry in Neural Representations, Proceedings of Machine Learning Research", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: Covering Relations in the Poset of Combinatorial Neural Codes\nResearch Topic: machine learning\nAuthors: R. Amzi Jeffs, Trong-Thuc Trang\nAbstract: A combinatorial neural code is a subset of the power set $2^{[n]}$ on $[n]=\\{1,\\dots, n\\}$, in which each $1\\leq i\\leq n$ represents a neuron and each element (codeword) represents the co-firing event of some neurons. Consider a space $X\\subseteq\\mathbb{R}^d$, simulating an animal's environment, and a collection $\\mathcal{U}=\\{U_1,\\dots,U_n\\}$ of open subsets of $X$. Each $U_i\\subseteq X$ simulates a place field which is a specific region where a place cell $i$ is active. Then, the code of $\\mathcal{U}$ in $X$ is defined as $\\text{code}(\\mathcal{U},X)=\\left\\{σ\\subseteq[n]\\bigg|\\bigcap_{i\\inσ} U_i\\setminus\\bigcup_{j\\notinσ}U_j\\neq\\varnothing\\right\\}$. If a neural code $\\mathcal{C}=\\text{code}(\\mathcal{U},X)$ for some $X$ and $\\mathcal{U}$, we say $\\mathcal{C}$ has a realization of open subsets of some space $X$. Although every combinatorial neural code obviously has a realization by some open subsets, determining whether it has a realization by some open convex subsets remains unsolved. Many studies attempted to tackle this decision problem, but only partial results were achieved. In fact, a previous study showed that the decision problem of convex neural codes is NP-hard. Furthermore, the authors of this study conjectured that every convex neural code can be realized as a minor of a neural code arising from a representable oriented matroid, which can lead to an equivalence between convex and polytope convex neural codes. Even though this conjecture has been confirmed in dimension two, its validity in higher dimensions is still unknown. To advance the investigation of this conjecture, we provide a complete characterization of the covering relations within the poset $\\mathbf{P_{Code}}$ of neural codes.\nCategories: math.CO, q-bio.NC\nPublication Date: 2025-12-03\nPaper ID: 2512.04241\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 1363, "total_text_length": 1425, "word_count": 237, "sentence_count": 12}, "basic_keywords": ["neural", "code", "convex", "open", "subsets", "each", "realization", "codes"], "domain_keywords": [], "technical_terms": ["Each", "Consider", "Furthermore", "Many", "Although", "Even", "Then"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04235", "topic": "machine learning", "title": "A Benchmark Study of Classical and Dual Polynomial Regression (DPR)-Based Probability Density Estimation Technique", "authors": ["Shantanu Sarkar", "Mousumi Sinha", "Dexter Cahoy"], "first_author": "Shantanu Sarkar", "abstract": "The probability density function (PDF) plays a central role in statistical and machine learning modeling. Real-world data often deviates from Gaussian assumptions, exhibiting skewness and exponential decay. To evaluate how well different density estimation methods capture such irregularities, we generated six unimodal datasets from diverse distributions that reflect real-world anomalies. These were compared using parametric methods (Pearson Type I and Normal distribution) as well as non-parametric approaches, including histograms, kernel density estimation (KDE), and our proposed method. To accelerate computation, we implemented GPU-based versions of KDE (tKDE) and histogram estimation (tHDE) in TensorFlow, both of which outperform Python SciPy's KDE. Prior work demonstrated the use of piecewise modeling for density estimation, such as local polynomial regression; however, these methods are computationally intensive. Based on the concept of piecewise modeling, we developed a computationally efficient model, the Dual Polynomial Regression (DPR) method, which leverages tKDE or tHDE for training. DPR employs the piecewise strategy to split the PDF at its mode and fit polynomial regressions to the left and right halves independently, enabling better capture of the asymmetric shape of the unimodal distribution. We used the Mean Squared Error (MSE), Jensen-Shannon Divergence (JSD), and Pearson's correlation coefficient, with reference to the baseline PDF, to validate accuracy. We verified normalization using Area Under the Curve (AUC) and computational overhead via execution time. Validation on real-world systolic and diastolic data from 300,000 unique patients shows that the DPR of order 4, trained with tKDE, offers the best balance between accuracy and computational overhead.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["stat.CO"], "url": "http://arxiv.org/abs/2512.04235", "comments": "Currently under review at Computational Statistics Journal", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: A Benchmark Study of Classical and Dual Polynomial Regression (DPR)-Based Probability Density Estimation Technique\nResearch Topic: machine learning\nAuthors: Shantanu Sarkar, Mousumi Sinha, Dexter Cahoy\nAbstract: The probability density function (PDF) plays a central role in statistical and machine learning modeling. Real-world data often deviates from Gaussian assumptions, exhibiting skewness and exponential decay. To evaluate how well different density estimation methods capture such irregularities, we generated six unimodal datasets from diverse distributions that reflect real-world anomalies. These were compared using parametric methods (Pearson Type I and Normal distribution) as well as non-parametric approaches, including histograms, kernel density estimation (KDE), and our proposed method. To accelerate computation, we implemented GPU-based versions of KDE (tKDE) and histogram estimation (tHDE) in TensorFlow, both of which outperform Python SciPy's KDE. Prior work demonstrated the use of piecewise modeling for density estimation, such as local polynomial regression; however, these methods are computationally intensive. Based on the concept of piecewise modeling, we developed a computationally efficient model, the Dual Polynomial Regression (DPR) method, which leverages tKDE or tHDE for training. DPR employs the piecewise strategy to split the PDF at its mode and fit polynomial regressions to the left and right halves independently, enabling better capture of the asymmetric shape of the unimodal distribution. We used the Mean Squared Error (MSE), Jensen-Shannon Divergence (JSD), and Pearson's correlation coefficient, with reference to the baseline PDF, to validate accuracy. We verified normalization using Area Under the Curve (AUC) and computational overhead via execution time. Validation on real-world systolic and diastolic data from 300,000 unique patients shows that the DPR of order 4, trained with tKDE, offers the best balance between accuracy and computational overhead.\nCategories: stat.CO\nPublication Date: 2025-12-03\nPaper ID: 2512.04235\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 114, "abstract_length": 1802, "total_text_length": 1917, "word_count": 264, "sentence_count": 12}, "basic_keywords": ["density", "estimation", "modeling", "real", "world", "methods", "tkde", "piecewise"], "domain_keywords": [], "technical_terms": ["These", "Based", "Pearson Type", "Prior", "Python", "Dual Polynomial Regression", "Real", "Shannon Divergence"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04223", "topic": "machine learning", "title": "ActVAE: Modelling human activity schedules with a deep conditional generative approach", "authors": ["Fred Shone", "Tim Hillel"], "first_author": "Fred Shone", "abstract": "Modelling the complexity and diversity of human activity scheduling behaviour is inherently challenging. We demonstrate a deep conditional-generative machine learning approach for the modelling of realistic activity schedules depending on input labels such as an individual's age, employment status, or other information relevant to their scheduling. We combine (i) a structured latent generative approach, with (ii) a conditional approach, through a novel Conditional VAE architecture. This allows for the rapid generation of precise and realistic schedules for different input labels. We extensively evaluate model capabilities using a joint density estimation framework and several case studies. We additionally show that our approach has practical data and computational requirements, and can be deployed within new and existing demand modelling frameworks. We evaluate the importance of generative capability more generally, by comparing our combined approach to (i) a purely generative model without conditionality, and (ii) a purely conditional model which outputs the most likely schedule given the input labels. This comparison highlights the usefulness of explicitly modelling the randomness of complex and diverse human behaviours using deep generative approaches.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04223", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.327419", "embedding_text": "Paper Title: ActVAE: Modelling human activity schedules with a deep conditional generative approach\nResearch Topic: machine learning\nAuthors: Fred Shone, Tim Hillel\nAbstract: Modelling the complexity and diversity of human activity scheduling behaviour is inherently challenging. We demonstrate a deep conditional-generative machine learning approach for the modelling of realistic activity schedules depending on input labels such as an individual's age, employment status, or other information relevant to their scheduling. We combine (i) a structured latent generative approach, with (ii) a conditional approach, through a novel Conditional VAE architecture. This allows for the rapid generation of precise and realistic schedules for different input labels. We extensively evaluate model capabilities using a joint density estimation framework and several case studies. We additionally show that our approach has practical data and computational requirements, and can be deployed within new and existing demand modelling frameworks. We evaluate the importance of generative capability more generally, by comparing our combined approach to (i) a purely generative model without conditionality, and (ii) a purely conditional model which outputs the most likely schedule given the input labels. This comparison highlights the usefulness of explicitly modelling the randomness of complex and diverse human behaviours using deep generative approaches.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04223\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1275, "total_text_length": 1362, "word_count": 187, "sentence_count": 9}, "basic_keywords": ["generative", "approach", "modelling", "conditional", "input", "labels", "model", "human"], "domain_keywords": [], "technical_terms": ["Conditional", "Modelling"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04165", "topic": "machine learning", "title": "Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity", "authors": ["Noa Rubin", "Orit Davidovich", "Zohar Ringel"], "first_author": "Noa Rubin", "abstract": "Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning effects revolve around networks with one or two trainable layers or deep linear networks. Furthermore, even under such limiting settings, predictions often appear in the form of high-dimensional non-linear equations, which require computationally intensive numerical solutions. Given the many details that go into defining a deep learning problem, this analytical complexity is a significant and often unavoidable challenge. Here, we propose a powerful heuristic route for predicting the data and width scales at which various patterns of feature learning emerge. This form of scale analysis is considerably simpler than such exact theories and reproduces the scaling exponents of various known results. In addition, we make novel predictions on complex toy architectures, such as three-layer non-linear networks and attention heads, thus extending the scope of first-principle theories of deep learning.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.04165", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity\nResearch Topic: machine learning\nAuthors: Noa Rubin, Orit Davidovich, Zohar Ringel\nAbstract: Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning effects revolve around networks with one or two trainable layers or deep linear networks. Furthermore, even under such limiting settings, predictions often appear in the form of high-dimensional non-linear equations, which require computationally intensive numerical solutions. Given the many details that go into defining a deep learning problem, this analytical complexity is a significant and often unavoidable challenge. Here, we propose a powerful heuristic route for predicting the data and width scales at which various patterns of feature learning emerge. This form of scale analysis is considerably simpler than such exact theories and reproduces the scaling exponents of various known results. In addition, we make novel predictions on complex toy architectures, such as three-layer non-linear networks and attention heads, thus extending the scope of first-principle theories of deep learning.\nCategories: cs.LG, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.04165\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1122, "total_text_length": 1215, "word_count": 176, "sentence_count": 8}, "basic_keywords": ["learning", "deep", "networks", "feature", "theories", "linear", "rich", "predictions"], "domain_keywords": [], "technical_terms": ["Here", "Current", "Furthermore", "Given"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04148", "topic": "machine learning", "title": "How large can lepton mixing be?", "authors": ["J. de Blas", "C. Giuliano", "G. Guedes", "R. Sánchez López", "J. Santiago"], "first_author": "J. de Blas", "abstract": "We show that, contrary to common expectations, the observed charged leptons can have a substantial mixing with new heavier fermions, at the level of 20. This can happen, in the language of effective theories, when the effect of mixing with heavier fermions vanishes at tree level in operators of mass-dimension 6 (or it is suppressed by the small charged lepton masses), a cancellation that can be naturally ensured by symmetries. Using a model that realizes this scenario we consider all current direct and indirect constraints and show that experimental constraints on the mixing are so mild that, given the current direct limit on the mass of the heavy fermions, theoretical considerations become the leading current constraints on the mixing. We also estimate the sensitivity to the mixing at future experiments, including the high-luminosity phase of the LHC and, most notably, the FCC-ee, and FCC-hh. We find a pattern in which the reach of direct searches in hadron machines makes theoretical considerations lead the limits while the precision of lepton machines can beat these theoretical considerations. We find that the FCC can finally reach per mille precision in the mixing squared of the charged leptons", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["hep-ph"], "url": "http://arxiv.org/abs/2512.04148", "comments": "38 pages, 14 figures. Matching results are given in ancilliary files", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: How large can lepton mixing be?\nResearch Topic: machine learning\nAuthors: J. de Blas, C. Giuliano, G. Guedes, R. Sánchez López, J. Santiago\nAbstract: We show that, contrary to common expectations, the observed charged leptons can have a substantial mixing with new heavier fermions, at the level of 20$\\%$. This can happen, in the language of effective theories, when the effect of mixing with heavier fermions vanishes at tree level in operators of mass-dimension 6 (or it is suppressed by the small charged lepton masses), a cancellation that can be naturally ensured by symmetries. Using a model that realizes this scenario we consider all current direct and indirect constraints and show that experimental constraints on the mixing are so mild that, given the current direct limit on the mass of the heavy fermions, theoretical considerations become the leading current constraints on the mixing. We also estimate the sensitivity to the mixing at future experiments, including the high-luminosity phase of the LHC and, most notably, the FCC-ee, and FCC-hh. We find a pattern in which the reach of direct searches in hadron machines makes theoretical considerations lead the limits while the precision of lepton machines can beat these theoretical considerations. We find that the FCC can finally reach per mille precision in the mixing squared of the charged leptons\nCategories: hep-ph\nPublication Date: 2025-12-03\nPaper ID: 2512.04148\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 31, "abstract_length": 1216, "total_text_length": 1248, "word_count": 200, "sentence_count": 7}, "basic_keywords": ["mixing", "charged", "fermions", "current", "direct", "constraints", "theoretical", "considerations"], "domain_keywords": [], "technical_terms": ["Using"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.7749999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04149", "topic": "machine learning", "title": "Enhancing next token prediction based pre-training for jet foundation models", "authors": ["Joschka Birk", "Anna Hallin", "Gregor Kasieczka", "Nikol Madzharova", "Ian Pang", "David Shih"], "first_author": "Joschka Birk", "abstract": "Next token prediction is an attractive pre-training task for jet foundation models, in that it is simulation free and enables excellent generative capabilities that can transfer across datasets. Here we study multiple improvements to next token prediction, building on the initial work of OmniJet-. Instead of tokenizing particles and subsequently only using the token-ID as the model input for both the generative and the classification task, we adopt a hybrid setup, which allows us to use continuous feature vectors as model input while only using token-IDs in the next token prediction target. Secondly, we explore a combined pre-training strategy that combines masked particle modeling and generative learning objectives. Taken together, these changes greatly improve the performance in downstream classification tasks without any loss in generative performance.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["hep-ph", "cs.LG", "hep-ex", "physics.data-an"], "url": "http://arxiv.org/abs/2512.04149", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Enhancing next token prediction based pre-training for jet foundation models\nResearch Topic: machine learning\nAuthors: Joschka Birk, Anna Hallin, Gregor Kasieczka, Nikol Madzharova, Ian Pang, David Shih\nAbstract: Next token prediction is an attractive pre-training task for jet foundation models, in that it is simulation free and enables excellent generative capabilities that can transfer across datasets. Here we study multiple improvements to next token prediction, building on the initial work of OmniJet-$α$. Instead of tokenizing particles and subsequently only using the token-ID as the model input for both the generative and the classification task, we adopt a hybrid setup, which allows us to use continuous feature vectors as model input while only using token-IDs in the next token prediction target. Secondly, we explore a combined pre-training strategy that combines masked particle modeling and generative learning objectives. Taken together, these changes greatly improve the performance in downstream classification tasks without any loss in generative performance.\nCategories: hep-ph, cs.LG, hep-ex, physics.data-an\nPublication Date: 2025-12-03\nPaper ID: 2512.04149\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 867, "total_text_length": 944, "word_count": 136, "sentence_count": 6}, "basic_keywords": ["token", "generative", "next", "prediction", "training", "task", "only", "using"], "domain_keywords": [], "technical_terms": ["Here", "Instead", "Next", "Secondly", "Taken"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04145", "topic": "machine learning", "title": "Minuet: A Diffusion Autoencoder for Compact Semantic Compression of Multi-Band Galaxy Images", "authors": ["Alexander T. Gagliano", "Yunyi Shen", "V. A. Villar"], "first_author": "Alexander T. Gagliano", "abstract": "The Vera C. Rubin Observatory is slated to observe nearly 20 billion galaxies during its decade-long Legacy Survey of Space and Time. The rich imaging data it collects will be an invaluable resource for probing galaxy evolution across cosmic time, characterizing the host galaxies of transient phenomena, and identifying novel populations of anomalous systems. While machine learning models have shown promise for extracting galaxy features from multi-band astronomical imaging, the large dimensionality of the learned latent space presents a challenge for mechanistic interpretability studies. In this work, we present Minuet, a low-dimensional diffusion autoencoder for multi-band galaxy imaging. Minuet is trained to reconstruct 72x72-pixel image cutouts of 6M galaxies within from the Dark Energy Camera Legacy Survey using only five latent dimensions. By using a diffusion model conditioned on the transformer-based autoencoder's output for image reconstruction, we achieve semantically-meaningful latent representations of galaxy images while still allowing for high-fidelity, probabilistic reconstructions. We train a series of binary classifiers on Minuet's latent features to quantify their connection to morphological labels from Galaxy Zoo, and a conditional flow to produce posterior distributions of SED-derived redshifts, stellar masses, and star-formation rates. We further show the value of Minuet for nearest neighbor searches in the learned latent space. Minuet provides strong evidence for the low intrinsic dimensionality of galaxy imaging, and introduces a class of astrophysical models that produce highly compact representations for diverse science goals.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["astro-ph.IM", "astro-ph.GA"], "url": "http://arxiv.org/abs/2512.04145", "comments": "27 pages, 15 figures, to be submitted to ApJ. Comments welcome", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Minuet: A Diffusion Autoencoder for Compact Semantic Compression of Multi-Band Galaxy Images\nResearch Topic: machine learning\nAuthors: Alexander T. Gagliano, Yunyi Shen, V. A. Villar\nAbstract: The Vera C. Rubin Observatory is slated to observe nearly 20 billion galaxies during its decade-long Legacy Survey of Space and Time. The rich imaging data it collects will be an invaluable resource for probing galaxy evolution across cosmic time, characterizing the host galaxies of transient phenomena, and identifying novel populations of anomalous systems. While machine learning models have shown promise for extracting galaxy features from multi-band astronomical imaging, the large dimensionality of the learned latent space presents a challenge for mechanistic interpretability studies. In this work, we present Minuet, a low-dimensional diffusion autoencoder for multi-band galaxy imaging. Minuet is trained to reconstruct 72x72-pixel $grz$ image cutouts of 6M galaxies within $z<1$ from the Dark Energy Camera Legacy Survey using only five latent dimensions. By using a diffusion model conditioned on the transformer-based autoencoder's output for image reconstruction, we achieve semantically-meaningful latent representations of galaxy images while still allowing for high-fidelity, probabilistic reconstructions. We train a series of binary classifiers on Minuet's latent features to quantify their connection to morphological labels from Galaxy Zoo, and a conditional flow to produce posterior distributions of SED-derived redshifts, stellar masses, and star-formation rates. We further show the value of Minuet for nearest neighbor searches in the learned latent space. Minuet provides strong evidence for the low intrinsic dimensionality of galaxy imaging, and introduces a class of astrophysical models that produce highly compact representations for diverse science goals.\nCategories: astro-ph.IM, astro-ph.GA\nPublication Date: 2025-12-03\nPaper ID: 2512.04145\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1678, "total_text_length": 1771, "word_count": 244, "sentence_count": 11}, "basic_keywords": ["galaxy", "latent", "minuet", "imaging", "galaxies", "space", "legacy", "survey"], "domain_keywords": [], "technical_terms": ["The Vera", "Space", "Legacy Survey", "While", "Minuet", "Time", "Dark Energy Camera Legacy Survey", "Galaxy Zoo"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04075", "topic": "machine learning", "title": "On distance and velocity estimation in cosmology", "authors": ["Adi Nusser"], "first_author": "Adi Nusser", "abstract": "Scatter in distance indicators introduces two conceptually distinct systematic biases when reconstructing peculiar velocity fields from redshifts and distances. The first is distance Malmquist bias (dMB) that affects individual distance estimates and can in principle be approximately corrected. The second is velocity Malmquist bias (vMB) that arises when constructing continuous velocity fields from scattered distance measurements: random scatter places galaxies at noisy spatial positions, introducing spurious velocity gradients that persist even when distances are corrected for dMB. Considering the Tully-Fisher relation as a concrete example, both inverse and forward formulations yield unbiased individual peculiar velocities for galaxies with the same true distance (the forward relation requires a selection-dependent correction), but neither eliminates vMB when galaxies are placed at their inferred distances. We develop a modified Wiener filter that properly encodes correlations between directly observed distance and true distance through the conditional probability , accounting for the distribution of true distances sampled by galaxies at observed distance . Nonetheless, this modified filter yields suppressed amplitude estimates. Since machine learning autoencoders converge to the Wiener filter for Gaussian fields, they are unlikely to significantly improve velocity field estimation. We therefore argue that optimal reconstruction places galaxies at their observed redshifts rather than inferred distances; an approach effective when distance errors exceed , a condition satisfied for most galaxies in typical surveys beyond the nearby volume.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["astro-ph.CO", "astro-ph.GA"], "url": "http://arxiv.org/abs/2512.04075", "comments": "21 pages, 6 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: On distance and velocity estimation in cosmology\nResearch Topic: machine learning\nAuthors: Adi Nusser\nAbstract: Scatter in distance indicators introduces two conceptually distinct systematic biases when reconstructing peculiar velocity fields from redshifts and distances. The first is distance Malmquist bias (dMB) that affects individual distance estimates and can in principle be approximately corrected. The second is velocity Malmquist bias (vMB) that arises when constructing continuous velocity fields from scattered distance measurements: random scatter places galaxies at noisy spatial positions, introducing spurious velocity gradients that persist even when distances are corrected for dMB. Considering the Tully-Fisher relation as a concrete example, both inverse and forward formulations yield unbiased individual peculiar velocities for galaxies with the same true distance (the forward relation requires a selection-dependent correction), but neither eliminates vMB when galaxies are placed at their inferred distances. We develop a modified Wiener filter that properly encodes correlations between directly observed distance $d$ and true distance $r$ through the conditional probability $P(r|d)$, accounting for the distribution of true distances sampled by galaxies at observed distance $d$. Nonetheless, this modified filter yields suppressed amplitude estimates. Since machine learning autoencoders converge to the Wiener filter for Gaussian fields, they are unlikely to significantly improve velocity field estimation. We therefore argue that optimal reconstruction places galaxies at their observed redshifts rather than inferred distances; an approach effective when distance errors exceed $σ_v/H_0$, a condition satisfied for most galaxies in typical surveys beyond the nearby volume.\nCategories: astro-ph.CO, astro-ph.GA\nPublication Date: 2025-12-03\nPaper ID: 2512.04075\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 48, "abstract_length": 1667, "total_text_length": 1716, "word_count": 230, "sentence_count": 9}, "basic_keywords": ["distance", "galaxies", "velocity", "distances", "fields", "true", "filter", "observed"], "domain_keywords": [], "technical_terms": ["Fisher", "Since", "Gaussian", "Tully", "Wiener", "Malmquist", "Considering", "Scatter"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04068", "topic": "machine learning", "title": "Learning Steerable Clarification Policies with Collaborative Self-play", "authors": ["Jonathan Berant", "Maximillian Chen", "Adam Fisch", "Reza Aghajani", "Fantine Huot", "Mirella Lapata", "Jacob Eisenstein"], "first_author": "Jonathan Berant", "abstract": "To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04068", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Learning Steerable Clarification Policies with Collaborative Self-play\nResearch Topic: machine learning\nAuthors: Jonathan Berant, Maximillian Chen, Adam Fisch, Reza Aghajani, Fantine Huot, Mirella Lapata, Jacob Eisenstein\nAbstract: To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04068\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 1352, "total_text_length": 1423, "word_count": 217, "sentence_count": 9}, "basic_keywords": ["user", "cost", "reward", "ambiguous", "policy", "managing", "uncertainty", "determine"], "domain_keywords": [], "technical_terms": ["However", "Importantly", "Training", "Reinforced Self", "Given", "Moreover"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04062", "topic": "machine learning", "title": "Eval Factsheets: A Structured Framework for Documenting AI Evaluations", "authors": ["Florian Bordes", "Candace Ross", "Justine T Kao", "Evangelia Spiliopoulou", "Adina Williams"], "first_author": "Florian Bordes", "abstract": "The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04062", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Eval Factsheets: A Structured Framework for Documenting AI Evaluations\nResearch Topic: machine learning\nAuthors: Florian Bordes, Candace Ross, Justine T Kao, Evangelia Spiliopoulou, Adina Williams\nAbstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04062\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 1330, "total_text_length": 1401, "word_count": 176, "sentence_count": 13}, "basic_keywords": ["evaluation", "benchmarks", "documentation", "eval", "factsheets", "reproducibility", "transparency", "structured"], "domain_keywords": [], "technical_terms": ["However", "Datasheets", "Model Cards", "Structure", "Scope", "What", "Eval Factsheets", "Through"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04058", "topic": "machine learning", "title": "Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap", "authors": ["Shashaank Khanna", "Matthew Pusey", "Roger Colbeck"], "first_author": "Shashaank Khanna", "abstract": "The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["quant-ph", "cs.LG", "math.ST"], "url": "http://arxiv.org/abs/2512.04058", "comments": "5 pages, 3 figures, 1 table", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap\nResearch Topic: machine learning\nAuthors: Shashaank Khanna, Matthew Pusey, Roger Colbeck\nAbstract: The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.\nCategories: quant-ph, cs.LG, math.ST\nPublication Date: 2025-12-03\nPaper ID: 2512.04058\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 900, "total_text_length": 1001, "word_count": 154, "sentence_count": 7}, "basic_keywords": ["causal", "quantum", "correlations", "exist", "structure", "structures", "bell", "cannot"], "domain_keywords": [], "technical_terms": ["Bell", "Here"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04051", "topic": "machine learning", "title": "Convergence for Discrete Parameter Updates", "authors": ["Paul Wilson", "Fabio Zanasi", "George Constantinides"], "first_author": "Paul Wilson", "abstract": "Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "math.OC"], "url": "http://arxiv.org/abs/2512.04051", "comments": "opt-ml 2025 workshop at NeurIPS", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Convergence for Discrete Parameter Updates\nResearch Topic: machine learning\nAuthors: Paul Wilson, Fabio Zanasi, George Constantinides\nAbstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.\nCategories: cs.LG, math.OC\nPublication Date: 2025-12-03\nPaper ID: 2512.04051\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 42, "abstract_length": 701, "total_text_length": 744, "word_count": 97, "sentence_count": 6}, "basic_keywords": ["training", "discrete", "models", "updates", "update", "rule", "modern", "deep"], "domain_keywords": [], "technical_terms": ["Modern", "Quantised"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04045", "topic": "machine learning", "title": "Machine Learning Pipeline for Denoising Low Signal-To-Noise Ratio and Out-of-Distribution Transmission Electron Microscopy Datasets", "authors": ["Brian Lee", "Meng Li", "Judith C Yang", "Dmitri N Zakharov", "Xiaohui Qu"], "first_author": "Brian Lee", "abstract": "High-resolution transmission electron microscopy (HRTEM) is crucial for observing material's structural and morphological evolution at Angstrom scales, but the electron beam can alter these processes. Devices such as CMOS-based direct-electron detectors operating in electron-counting mode can be utilized to substantially reduce the electron dosage. However, the resulting images often lead to low signal-to-noise ratio, which requires frame integration that sacrifices temporal resolution. Several machine learning (ML) models have been recently developed to successfully denoise HRTEM images. Yet, these models are often computationally expensive and their inference speeds on GPUs are outpaced by the imaging speed of advanced detectors, precluding in situ analysis. Furthermore, the performance of these denoising models on datasets with imaging conditions that deviate from the training datasets have not been evaluated. To mitigate these gaps, we propose a new self-supervised ML denoising pipeline specifically designed for time-series HRTEM images. This pipeline integrates a blind-spot convolution neural network with pre-processing and post-processing steps including drift correction and low-pass filtering. Results demonstrate that our model outperforms various other ML and non-ML denoising methods in noise reduction and contrast enhancement, leading to improved visual clarity of atomic features. Additionally, the model is drastically faster than U-Net-based ML models and demonstrates excellent out-of-distribution generalization. The model's computational inference speed is in the order of milliseconds per image, rendering it suitable for application in in-situ HRTEM experiments.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04045", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Machine Learning Pipeline for Denoising Low Signal-To-Noise Ratio and Out-of-Distribution Transmission Electron Microscopy Datasets\nResearch Topic: machine learning\nAuthors: Brian Lee, Meng Li, Judith C Yang, Dmitri N Zakharov, Xiaohui Qu\nAbstract: High-resolution transmission electron microscopy (HRTEM) is crucial for observing material's structural and morphological evolution at Angstrom scales, but the electron beam can alter these processes. Devices such as CMOS-based direct-electron detectors operating in electron-counting mode can be utilized to substantially reduce the electron dosage. However, the resulting images often lead to low signal-to-noise ratio, which requires frame integration that sacrifices temporal resolution. Several machine learning (ML) models have been recently developed to successfully denoise HRTEM images. Yet, these models are often computationally expensive and their inference speeds on GPUs are outpaced by the imaging speed of advanced detectors, precluding in situ analysis. Furthermore, the performance of these denoising models on datasets with imaging conditions that deviate from the training datasets have not been evaluated. To mitigate these gaps, we propose a new self-supervised ML denoising pipeline specifically designed for time-series HRTEM images. This pipeline integrates a blind-spot convolution neural network with pre-processing and post-processing steps including drift correction and low-pass filtering. Results demonstrate that our model outperforms various other ML and non-ML denoising methods in noise reduction and contrast enhancement, leading to improved visual clarity of atomic features. Additionally, the model is drastically faster than U-Net-based ML models and demonstrates excellent out-of-distribution generalization. The model's computational inference speed is in the order of milliseconds per image, rendering it suitable for application in in-situ HRTEM experiments.\nCategories: cond-mat.mtrl-sci\nPublication Date: 2025-12-03\nPaper ID: 2512.04045\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 131, "abstract_length": 1701, "total_text_length": 1833, "word_count": 239, "sentence_count": 12}, "basic_keywords": ["electron", "hrtem", "models", "images", "denoising", "model", "resolution", "based"], "domain_keywords": [], "technical_terms": ["High", "However", "Several", "Devices", "Additionally", "Furthermore", "Results", "Angstrom"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04034", "topic": "machine learning", "title": "Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions", "authors": ["Hong Yang", "Devroop Kar", "Qi Yu", "Alex Ororbia", "Travis Desell"], "first_author": "Hong Yang", "abstract": "Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04034", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions\nResearch Topic: machine learning\nAuthors: Hong Yang, Devroop Kar, Qi Yu, Alex Ororbia, Travis Desell\nAbstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04034\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1467, "total_text_length": 1553, "word_count": 202, "sentence_count": 13}, "basic_keywords": ["domain", "single", "information", "failure", "models", "learning", "catastrophic", "trained"], "domain_keywords": ["transfer learning"], "technical_terms": ["Fano", "Domain Bench", "While"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04024", "topic": "machine learning", "title": "Predicting parameters of a model cuprate superconductor using machine learning", "authors": ["V. A. Ulitko", "D. N. Yasinskaya", "S. A. Bezzubin", "A. A. Koshelev", "Y. D. Panov"], "first_author": "V. A. Ulitko", "abstract": "The computational complexity of calculating phase diagrams for multi-parameter models significantly limits the ability to select parameters that correspond to experimental data. This work presents a machine learning method for solving the inverse problem - forecasting the parameters of a model Hamiltonian for a cuprate superconductor based on its phase diagram. A comparative study of three deep learning architectures was conducted: VGG, ResNet, and U-Net. The latter was adapted for regression tasks and demonstrated the best performance. Training the U-Net model was performed on an extensive dataset of phase diagrams calculated within the mean-field approximation, followed by validation on data obtained using a semi-classical heat bath algorithm for Monte Carlo simulations. It is shown that the model accurately predicts all considered Hamiltonian parameters, and areas of low prediction accuracy correspond to regions of parametric insensitivity in the phase diagrams. This allows for the extraction of physically interpretable patterns and validation of the significance of parameters for the system. The results confirm the promising potential of applying machine learning to analyze complex physical models in condensed matter physics.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["physics.comp-ph", "cond-mat.stat-mech", "cond-mat.supr-con"], "url": "http://arxiv.org/abs/2512.04024", "comments": "22 pages, 8 figures, 2 tables", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Predicting parameters of a model cuprate superconductor using machine learning\nResearch Topic: machine learning\nAuthors: V. A. Ulitko, D. N. Yasinskaya, S. A. Bezzubin, A. A. Koshelev, Y. D. Panov\nAbstract: The computational complexity of calculating phase diagrams for multi-parameter models significantly limits the ability to select parameters that correspond to experimental data. This work presents a machine learning method for solving the inverse problem - forecasting the parameters of a model Hamiltonian for a cuprate superconductor based on its phase diagram. A comparative study of three deep learning architectures was conducted: VGG, ResNet, and U-Net. The latter was adapted for regression tasks and demonstrated the best performance. Training the U-Net model was performed on an extensive dataset of phase diagrams calculated within the mean-field approximation, followed by validation on data obtained using a semi-classical heat bath algorithm for Monte Carlo simulations. It is shown that the model accurately predicts all considered Hamiltonian parameters, and areas of low prediction accuracy correspond to regions of parametric insensitivity in the phase diagrams. This allows for the extraction of physically interpretable patterns and validation of the significance of parameters for the system. The results confirm the promising potential of applying machine learning to analyze complex physical models in condensed matter physics.\nCategories: physics.comp-ph, cond-mat.stat-mech, cond-mat.supr-con\nPublication Date: 2025-12-03\nPaper ID: 2512.04024\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1249, "total_text_length": 1328, "word_count": 187, "sentence_count": 9}, "basic_keywords": ["phase", "parameters", "diagrams", "learning", "model", "models", "correspond", "data"], "domain_keywords": [], "technical_terms": ["Monte Carlo", "Hamiltonian", "Training"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04022", "topic": "machine learning", "title": "Non-Linear Determinants of Pedestrian Injury Severity: Evidence from Administrative Data in Great Britain", "authors": ["Yifei Tong"], "first_author": "Yifei Tong", "abstract": "This study investigates the non-linear determinants of pedestrian injury severity using administrative data from Great Britain's 2023 STATS19 dataset. To address inherent data-quality challenges, including missing information and substantial class imbalance, we employ a rigorous preprocessing pipeline utilizing mode imputation and Synthetic Minority Over-sampling (SMOTE). We utilize non-parametric ensemble methods (Random Forest and XGBoost) to capture complex interactions and heterogeneity often missed by linear models, while Shapley Additive Explanations are employed to ensure interpretability and isolate marginal feature effects. Our analysis reveals that vehicle count, speed limits, lighting, and road surface conditions are the primary predictors of severity, with police attendance and junction characteristics further distinguishing severe collisions. Spatially, while pedestrian risk is concentrated in dense urban Local Authority Districts (LADs), we identify that certain rural LADs experience disproportionately severe outcomes conditional on a collision occurring. These findings underscore the value of combining spatial analysis with interpretable machine learning to guide geographically targeted speed management, infrastructure investment, and enforcement strategies.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CY"], "url": "http://arxiv.org/abs/2512.04022", "comments": "18 pages, 7 figures. Code and data available at: https://github.com/1TSHARUKA/Pedestrian_Safety_Analysis", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Non-Linear Determinants of Pedestrian Injury Severity: Evidence from Administrative Data in Great Britain\nResearch Topic: machine learning\nAuthors: Yifei Tong\nAbstract: This study investigates the non-linear determinants of pedestrian injury severity using administrative data from Great Britain's 2023 STATS19 dataset. To address inherent data-quality challenges, including missing information and substantial class imbalance, we employ a rigorous preprocessing pipeline utilizing mode imputation and Synthetic Minority Over-sampling (SMOTE). We utilize non-parametric ensemble methods (Random Forest and XGBoost) to capture complex interactions and heterogeneity often missed by linear models, while Shapley Additive Explanations are employed to ensure interpretability and isolate marginal feature effects. Our analysis reveals that vehicle count, speed limits, lighting, and road surface conditions are the primary predictors of severity, with police attendance and junction characteristics further distinguishing severe collisions. Spatially, while pedestrian risk is concentrated in dense urban Local Authority Districts (LADs), we identify that certain rural LADs experience disproportionately severe outcomes conditional on a collision occurring. These findings underscore the value of combining spatial analysis with interpretable machine learning to guide geographically targeted speed management, infrastructure investment, and enforcement strategies.\nCategories: cs.CY\nPublication Date: 2025-12-03\nPaper ID: 2512.04022\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 105, "abstract_length": 1293, "total_text_length": 1399, "word_count": 173, "sentence_count": 7}, "basic_keywords": ["linear", "pedestrian", "severity", "data", "while", "analysis", "speed", "severe"], "domain_keywords": [], "technical_terms": ["Great Britain", "Random Forest", "These", "Synthetic Minority Over", "Spatially", "Local Authority Districts", "Shapley Additive Explanations"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04014", "topic": "machine learning", "title": "Construction of irreducible integrity basis for anisotropic hyperelasticity via structural tensors", "authors": ["Brain M. Riemer", "Jörg Brummund", "Karl A. Kalina", "Abel H. G. Milor", "Franz Dammaß", "Markus Kästner"], "first_author": "Brain M. Riemer", "abstract": "We present a straightforward analytical-numerical methodology for determining polynomially complete and irreducible scalar-valued invariant sets for anisotropic hyperelasticity. By applying the proposed technique, we obtain irreducible integrity bases for all common anisotropies in hyperelasticity via the structural tensor concept, i.e., invariants are formed from a measure of deformation (symmetric 2nd order tensor) and a set of structural tensors describing the material's symmetry. Our work covers results for the 11 types of anisotropy that arise from the classical 7 crystal systems, as well as findings for 4 additional non-crystal anisotropies derived from the cylindrical, spherical, and icosahedral symmetry systems. Polynomial completeness and irreducibility of the proposed integrity bases are proven using the Molien series and, in addition, with established results for scalar-valued invariant sets from the literature. Furthermore, we derive relationships between a set of multiple structural tensors that specify a symmetry group and a description using only a single structural tensor. Both can be used to construct irreducible integrity bases by applying the proposed analytical-numerical method. The provided invariant sets are of great importance for modeling anisotropic materials via the structural tensor concept using both classical models as well as modern approaches based on machine learning. Alongside the results presented, this article also aims to provide an introductory overview of the complex field of modeling anisotropic materials.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04014", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Construction of irreducible integrity basis for anisotropic hyperelasticity via structural tensors\nResearch Topic: machine learning\nAuthors: Brain M. Riemer, Jörg Brummund, Karl A. Kalina, Abel H. G. Milor, Franz Dammaß, Markus Kästner\nAbstract: We present a straightforward analytical-numerical methodology for determining polynomially complete and irreducible scalar-valued invariant sets for anisotropic hyperelasticity. By applying the proposed technique, we obtain irreducible integrity bases for all common anisotropies in hyperelasticity via the structural tensor concept, i.e., invariants are formed from a measure of deformation (symmetric 2nd order tensor) and a set of structural tensors describing the material's symmetry. Our work covers results for the 11 types of anisotropy that arise from the classical 7 crystal systems, as well as findings for 4 additional non-crystal anisotropies derived from the cylindrical, spherical, and icosahedral symmetry systems. Polynomial completeness and irreducibility of the proposed integrity bases are proven using the Molien series and, in addition, with established results for scalar-valued invariant sets from the literature. Furthermore, we derive relationships between a set of multiple structural tensors that specify a symmetry group and a description using only a single structural tensor. Both can be used to construct irreducible integrity bases by applying the proposed analytical-numerical method. The provided invariant sets are of great importance for modeling anisotropic materials via the structural tensor concept using both classical models as well as modern approaches based on machine learning. Alongside the results presented, this article also aims to provide an introductory overview of the complex field of modeling anisotropic materials.\nCategories: cond-mat.mtrl-sci\nPublication Date: 2025-12-03\nPaper ID: 2512.04014\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 98, "abstract_length": 1570, "total_text_length": 1669, "word_count": 228, "sentence_count": 11}, "basic_keywords": ["structural", "tensor", "irreducible", "invariant", "sets", "anisotropic", "proposed", "integrity"], "domain_keywords": [], "technical_terms": ["Polynomial", "Alongside", "Furthermore", "Molien", "Both"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04008", "topic": "machine learning", "title": "Efficient Public Verification of Private ML via Regularization", "authors": ["Zoë Ruha Bell", "Anvith Thudi", "Olive Franzese-McLaughlin", "Nicolas Papernot", "Shafi Goldwasser"], "first_author": "Zoë Ruha Bell", "abstract": "Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.CR"], "url": "http://arxiv.org/abs/2512.04008", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Efficient Public Verification of Private ML via Regularization\nResearch Topic: machine learning\nAuthors: Zoë Ruha Bell, Anvith Thudi, Olive Franzese-McLaughlin, Nicolas Papernot, Shafi Goldwasser\nAbstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.\nCategories: cs.LG, cs.CR\nPublication Date: 2025-12-03\nPaper ID: 2512.04008\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1145, "total_text_length": 1208, "word_count": 180, "sentence_count": 9}, "basic_keywords": ["privacy", "training", "utility", "guarantees", "compute", "optimal", "trade", "offs"], "domain_keywords": [], "technical_terms": ["However", "Here", "Training", "Crucially"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04006", "topic": "machine learning", "title": "Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics", "authors": ["Connall Garrod", "Jonathan P. Keating", "Christos Thrampoulidis"], "first_author": "Connall Garrod", "abstract": "Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "math.OC", "stat.ML"], "url": "http://arxiv.org/abs/2512.04006", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics\nResearch Topic: machine learning\nAuthors: Connall Garrod, Jonathan P. Keating, Christos Thrampoulidis\nAbstract: Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.\nCategories: cs.LG, math.OC, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.04006\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 1356, "total_text_length": 1444, "word_count": 198, "sentence_count": 8}, "basic_keywords": ["convex", "dynamics", "loss", "neural", "training", "squared", "models", "linear"], "domain_keywords": [], "technical_terms": ["Cross", "Lyapunov", "Hadamard Initialization"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04004", "topic": "machine learning", "title": "Physics-Embedded Gaussian Process for Traffic State Estimation", "authors": ["Yanlin Chen", "Kehua Chen", "Yinhai Wang"], "first_author": "Yanlin Chen", "abstract": "Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04004", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Physics-Embedded Gaussian Process for Traffic State Estimation\nResearch Topic: machine learning\nAuthors: Yanlin Chen, Kehua Chen, Yinhai Wang\nAbstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04004\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1730, "total_text_length": 1793, "word_count": 229, "sentence_count": 13}, "basic_keywords": ["pegp", "traffic", "physical", "sparse", "data", "methods", "uncertainty", "physics"], "domain_keywords": [], "technical_terms": ["These", "However", "Embedded Gaussian Process", "Experiments", "Pure", "Physics", "Gaussian", "Traffic"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03994", "topic": "machine learning", "title": "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs", "authors": ["Oren Rachmil", "Roy Betser", "Itay Gershon", "Omer Hofman", "Nitay Yakoby", "Yuval Meron", "Idan Yankelev", "Asaf Shabtai", "Yuval Elovici", "Roman Vainshtein"], "first_author": "Oren Rachmil", "abstract": "Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03994", "comments": "Accepted to the AAAI 2026 Deployable AI (DAI) Workshop", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs\nResearch Topic: machine learning\nAuthors: Oren Rachmil, Roy Betser, Itay Gershon, Omer Hofman, Nitay Yakoby, Yuval Meron, Idan Yankelev, Asaf Shabtai, Yuval Elovici, Roman Vainshtein\nAbstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03994\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 1748, "total_text_length": 1828, "word_count": 236, "sentence_count": 12}, "basic_keywords": ["policy", "llms", "detection", "models", "organizational", "policies", "organizations", "legal"], "domain_keywords": [], "technical_terms": ["Beyond", "Aligning", "Inspired", "Existing", "Euclidean", "Code"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03991", "topic": "machine learning", "title": "When to Say \"Hi\" - Learn to Open a Conversation with an in-the-wild Dataset", "authors": ["Michael Schiffmann", "Felix Struth", "Sabina Jeschke", "Anja Richert"], "first_author": "Michael Schiffmann", "abstract": "The social capabilities of socially interactive agents (SIA) are a key to successful and smooth interactions between the user and the SIA. A successful start of the interaction is one of the essential factors for satisfying SIA interactions. For a service and information task in which the SIA helps with information, e.g. about the location, it is an important skill to master the opening of the conversation and to recognize which interlocutor opens the conversation and when. We are therefore investigating the extent to which the opening of the conversation can be trained using the user's body language as an input for machine learning to ensure smooth conversation starts for the interaction. In this paper we propose the Interaction Initiation System (IIS) which we developed, trained and validated using an in-the-wild data set. In a field test at the Deutsches Museum Bonn, a Furhat robot from Furhat Robotics was used as a service and information point. Over the period of use we collected the data of = 201 single user interactions for the training of the algorithms. We can show that the IIS, achieves a performance that allows the conclusion that this system is able to determine the greeting period and the opener of the interaction.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.HC", "cs.RO"], "url": "http://arxiv.org/abs/2512.03991", "comments": "6 pages, 3 figures, 5 tables. This paper has been accepted for publication at IEEE ROMAN 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: When to Say \"Hi\" - Learn to Open a Conversation with an in-the-wild Dataset\nResearch Topic: machine learning\nAuthors: Michael Schiffmann, Felix Struth, Sabina Jeschke, Anja Richert\nAbstract: The social capabilities of socially interactive agents (SIA) are a key to successful and smooth interactions between the user and the SIA. A successful start of the interaction is one of the essential factors for satisfying SIA interactions. For a service and information task in which the SIA helps with information, e.g. about the location, it is an important skill to master the opening of the conversation and to recognize which interlocutor opens the conversation and when. We are therefore investigating the extent to which the opening of the conversation can be trained using the user's body language as an input for machine learning to ensure smooth conversation starts for the interaction. In this paper we propose the Interaction Initiation System (IIS) which we developed, trained and validated using an in-the-wild data set. In a field test at the Deutsches Museum Bonn, a Furhat robot from Furhat Robotics was used as a service and information point. Over the period of use we collected the data of \\textit{N} = 201 single user interactions for the training of the algorithms. We can show that the IIS, achieves a performance that allows the conclusion that this system is able to determine the greeting period and the opener of the interaction.\nCategories: cs.HC, cs.RO\nPublication Date: 2025-12-03\nPaper ID: 2512.03991\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1247, "total_text_length": 1323, "word_count": 220, "sentence_count": 11}, "basic_keywords": ["interaction", "conversation", "interactions", "user", "information", "successful", "smooth", "service"], "domain_keywords": [], "technical_terms": ["Furhat", "Over", "Deutsches Museum Bonn", "Furhat Robotics", "Interaction Initiation System"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03974", "topic": "machine learning", "title": "Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions", "authors": ["Paul Fuchs", "Julija Zavadlav"], "first_author": "Paul Fuchs", "abstract": "Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["physics.comp-ph", "cs.LG"], "url": "http://arxiv.org/abs/2512.03974", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions\nResearch Topic: machine learning\nAuthors: Paul Fuchs, Julija Zavadlav\nAbstract: Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.\nCategories: physics.comp-ph, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03974\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1575, "total_text_length": 1662, "word_count": 213, "sentence_count": 11}, "basic_keywords": ["experimental", "approach", "learning", "reference", "temperatures", "solid", "foundational", "machine"], "domain_keywords": [], "technical_terms": ["However", "Often", "Molecular Dynamics", "Foundational Machine Learning Potentials", "Titanium", "Differentiable Trajectory Reweighting", "Thus", "They"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03967", "topic": "machine learning", "title": "Technical Report on Text Dataset Distillation", "authors": ["Keith Ando Ogawa", "Bruno Lopes Yamamoto", "Lucas Lauton de Alcantara", "Victor Zacarias", "Edson Bollis", "Lucas Pellicer", "Rosimeire Pereira Costa", "Anna Helena Reali Costa", "Artur Jordao"], "first_author": "Keith Ando Ogawa", "abstract": "In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03967", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Technical Report on Text Dataset Distillation\nResearch Topic: machine learning\nAuthors: Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Victor Zacarias, Edson Bollis, Lucas Pellicer, Rosimeire Pereira Costa, Anna Helena Reali Costa, Artur Jordao\nAbstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03967\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 45, "abstract_length": 1192, "total_text_length": 1238, "word_count": 179, "sentence_count": 7}, "basic_keywords": ["distillation", "dataset", "text", "vision", "into", "synthetic", "methods", "models"], "domain_keywords": [], "technical_terms": ["Several", "While", "Text", "Despite"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03966", "topic": "machine learning", "title": "A CMOS+X Spiking Neuron With On-Chip Machine Learning", "authors": ["Steven Louis", "Matthew Blake Abramson", "Hannah Bradley", "Cody Trevillian", "Gene David Nelson", "Andrei Slavin", "Artem Litvinenko", "Jason Gorski", "Ilya N. Krivorotov", "Darrin Hanna", "Vasyl Tyberkevych"], "first_author": "Steven Louis", "abstract": "We present the design and numerical simulation of a spiking neuron capable of on-chip machine learning. Built within the CMOS+X framework, the spiking neuron consists of an NMOS transistor combined with a magnetic tunnel junction (MTJ). This NMOS+MTJ unit, when simulated in the industry-standard circuit simulation software LTspice, reproduces multiple functions of a biological neuron, including threshold spiking, latency, refractory periods, synaptic integration, inhibition, and adaptation. These behaviors arise from the intrinsic magnetization dynamics of the MTJ and do not require any additional control circuitry. By interconnecting the NMOS+MTJ neurons, we construct a model of an analog multilayer network that learns through spike-timing-dependent weight updates derived from a gradient-descent rule, with both training and inference modeled in the analog domain. The simulated CMOS+X network achieves reliable spike propagation and successful training on a nonlinear task, indicating a feasible path toward compact, low-power, in-memory neuromorphic hardware for edge applications.", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["physics.app-ph"], "url": "http://arxiv.org/abs/2512.03966", "comments": "12 pages, 7 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: A CMOS+X Spiking Neuron With On-Chip Machine Learning\nResearch Topic: machine learning\nAuthors: Steven Louis, Matthew Blake Abramson, Hannah Bradley, Cody Trevillian, Gene David Nelson, Andrei Slavin, Artem Litvinenko, Jason Gorski, Ilya N. Krivorotov, Darrin Hanna, Vasyl Tyberkevych\nAbstract: We present the design and numerical simulation of a spiking neuron capable of on-chip machine learning. Built within the CMOS+X framework, the spiking neuron consists of an NMOS transistor combined with a magnetic tunnel junction (MTJ). This NMOS+MTJ unit, when simulated in the industry-standard circuit simulation software LTspice, reproduces multiple functions of a biological neuron, including threshold spiking, latency, refractory periods, synaptic integration, inhibition, and adaptation. These behaviors arise from the intrinsic magnetization dynamics of the MTJ and do not require any additional control circuitry. By interconnecting the NMOS+MTJ neurons, we construct a model of an analog multilayer network that learns through spike-timing-dependent weight updates derived from a gradient-descent rule, with both training and inference modeled in the analog domain. The simulated CMOS+X network achieves reliable spike propagation and successful training on a nonlinear task, indicating a feasible path toward compact, low-power, in-memory neuromorphic hardware for edge applications.\nCategories: physics.app-ph\nPublication Date: 2025-12-03\nPaper ID: 2512.03966\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 53, "abstract_length": 1095, "total_text_length": 1149, "word_count": 156, "sentence_count": 7}, "basic_keywords": ["spiking", "neuron", "nmos", "simulation", "cmos", "simulated", "analog", "network"], "domain_keywords": [], "technical_terms": ["These", "Built"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03962", "topic": "machine learning", "title": "Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction", "authors": ["Evan Bell", "Shijun Liang", "Ismail Alkhouri", "Saiprasad Ravishankar"], "first_author": "Evan Bell", "abstract": "Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["eess.IV", "cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.03962", "comments": "6 pages, 8 figures, 2025 Asilomar Conference on Signals, Systems, and Computers. Code is available at github.com/evanbell02/Tada-DIP/", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction\nResearch Topic: machine learning\nAuthors: Evan Bell, Shijun Liang, Ismail Alkhouri, Saiprasad Ravishankar\nAbstract: Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.\nCategories: eess.IV, cs.CV, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03962\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 833, "total_text_length": 912, "word_count": 119, "sentence_count": 6}, "basic_keywords": ["reconstruction", "image", "method", "tada", "network", "problems", "fully", "produces"], "domain_keywords": ["3d reconstruction"], "technical_terms": ["However", "Experiments", "Deep Image Prior", "Tada"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03947", "topic": "machine learning", "title": "Data-Dependent Complexity of First-Order Methods for Binary Classification", "authors": ["Matthew Hough", "Stephen A. Vavasis"], "first_author": "Matthew Hough", "abstract": "Large-scale problems in data science are often modeled with optimization, and the optimization model is usually solved with first-order methods that may converge at a sublinear rate. Therefore, it is of interest to terminate the optimization algorithm as soon as the underlying data science task is accomplished. We consider FISTA for solving two binary classification problems: the ellipsoid separation problem (ESP), and the soft-margin support-vector machine (SVM). For the ESP, we cast the dual second-order cone program into a form amenable to FISTA and show that the FISTA residual converges to the infimal displacement vector of the primal-dual hybrid gradient (PDHG) algorithm, that directly encodes a separating hyperplane. We further derive a data-dependent iteration upper bound scaling as , where is the minimal perturbation that destroys separability. For the SVM, we propose a strongly-concave perturbed dual that admits efficient FISTA updates under a linear time projection scheme, and with our parameter choices, the objective has small condition number, enabling rapid convergence. We prove that, under a reasonable data model, early-stopped iterates identify well-classified points and yield a hyperplane that exactly separates them, where the accuracy required of the dual iterate is governed by geometric properties of the data. In particular, the proposed early-stopping criteria diminish the need for hard-to-select tolerance-based stopping conditions. Our numerical experiments on ESP instances derived from MNIST data and on soft-margin SVM benchmarks indicate competitive runtimes and substantial speedups from stopping early.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["math.OC"], "url": "http://arxiv.org/abs/2512.03947", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Data-Dependent Complexity of First-Order Methods for Binary Classification\nResearch Topic: machine learning\nAuthors: Matthew Hough, Stephen A. Vavasis\nAbstract: Large-scale problems in data science are often modeled with optimization, and the optimization model is usually solved with first-order methods that may converge at a sublinear rate. Therefore, it is of interest to terminate the optimization algorithm as soon as the underlying data science task is accomplished. We consider FISTA for solving two binary classification problems: the ellipsoid separation problem (ESP), and the soft-margin support-vector machine (SVM). For the ESP, we cast the dual second-order cone program into a form amenable to FISTA and show that the FISTA residual converges to the infimal displacement vector of the primal-dual hybrid gradient (PDHG) algorithm, that directly encodes a separating hyperplane. We further derive a data-dependent iteration upper bound scaling as $\\mathcal{O}(1/δ_{\\mathcal{A}}^2)$, where $δ_{\\mathcal{A}}$ is the minimal perturbation that destroys separability. For the SVM, we propose a strongly-concave perturbed dual that admits efficient FISTA updates under a linear time projection scheme, and with our parameter choices, the objective has small condition number, enabling rapid convergence. We prove that, under a reasonable data model, early-stopped iterates identify well-classified points and yield a hyperplane that exactly separates them, where the accuracy required of the dual iterate is governed by geometric properties of the data. In particular, the proposed early-stopping criteria diminish the need for hard-to-select tolerance-based stopping conditions. Our numerical experiments on ESP instances derived from MNIST data and on soft-margin SVM benchmarks indicate competitive runtimes and substantial speedups from stopping early.\nCategories: math.OC\nPublication Date: 2025-12-03\nPaper ID: 2512.03947\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1652, "total_text_length": 1727, "word_count": 244, "sentence_count": 10}, "basic_keywords": ["data", "fista", "dual", "optimization", "early", "stopping", "problems", "science"], "domain_keywords": [], "technical_terms": ["Therefore", "Large"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03946", "topic": "machine learning", "title": "When are novel methods for analyzing complex chemical mixtures in epidemiology beneficial?", "authors": ["Nate Wiecha", "Emily Griffith", "Brian J. Reich", "Jane A. Hoppin"], "first_author": "Nate Wiecha", "abstract": "Estimating the health impacts of exposure to a mixture of chemicals poses many statistical challenges: multiple correlated exposure variables, moderate to high dimensionality, and possible nonlinear and interactive health effects of mixture components. Reviews of chemical mixture methods aim to help researchers select a statistical method suited to their goals and data, but examinations of empirical performance have emphasized novel methods purpose-built for analyzing complex chemical mixtures, or other more advanced methods, over more general methods which are widely used in many application domains. We conducted a broad experimental comparison, across simulated scenarios, of both more general methods (such as generalized linear models) and novel methods (such as Bayesian Kernel Machine Regression) designed to study chemical mixtures. We assessed methods based on their ability to control Type I error rate, maximize power, provide interpretable results, and make accurate predictions. We find that when there is moderate correlation between mixture components and the exposure-response function does not have complicated interactions, or when mixture components have opposite effects, general methods are preferred over novel ones. With highly interactive exposure-response functions or highly correlated exposures, novel methods provide important benefits. We provide a comprehensive summary of when different methods are most suitable.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["stat.ME"], "url": "http://arxiv.org/abs/2512.03946", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: When are novel methods for analyzing complex chemical mixtures in epidemiology beneficial?\nResearch Topic: machine learning\nAuthors: Nate Wiecha, Emily Griffith, Brian J. Reich, Jane A. Hoppin\nAbstract: Estimating the health impacts of exposure to a mixture of chemicals poses many statistical challenges: multiple correlated exposure variables, moderate to high dimensionality, and possible nonlinear and interactive health effects of mixture components. Reviews of chemical mixture methods aim to help researchers select a statistical method suited to their goals and data, but examinations of empirical performance have emphasized novel methods purpose-built for analyzing complex chemical mixtures, or other more advanced methods, over more general methods which are widely used in many application domains. We conducted a broad experimental comparison, across simulated scenarios, of both more general methods (such as generalized linear models) and novel methods (such as Bayesian Kernel Machine Regression) designed to study chemical mixtures. We assessed methods based on their ability to control Type I error rate, maximize power, provide interpretable results, and make accurate predictions. We find that when there is moderate correlation between mixture components and the exposure-response function does not have complicated interactions, or when mixture components have opposite effects, general methods are preferred over novel ones. With highly interactive exposure-response functions or highly correlated exposures, novel methods provide important benefits. We provide a comprehensive summary of when different methods are most suitable.\nCategories: stat.ME\nPublication Date: 2025-12-03\nPaper ID: 2512.03946\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1451, "total_text_length": 1542, "word_count": 211, "sentence_count": 9}, "basic_keywords": ["methods", "mixture", "exposure", "novel", "components", "chemical", "more", "general"], "domain_keywords": [], "technical_terms": ["Bayesian Kernel Machine Regression", "Estimating", "Reviews", "Type"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03945", "topic": "machine learning", "title": "Classification of User Satisfaction in HRI with Social Signals in the Wild", "authors": ["Michael Schiffmann", "Sabina Jeschke", "Anja Richert"], "first_author": "Michael Schiffmann", "abstract": "Socially interactive agents (SIAs) are being used in various scenarios and are nearing productive deployment. Evaluating user satisfaction with SIAs' performance is a key factor in designing the interaction between the user and SIA. Currently, subjective user satisfaction is primarily assessed manually through questionnaires or indirectly via system metrics. This study examines the automatic classification of user satisfaction through analysis of social signals, aiming to enhance both manual and autonomous evaluation methods for SIAs. During a field trial at the Deutsches Museum Bonn, a Furhat Robotics head was employed as a service and information hub, collecting an \"in-the-wild\" dataset. This dataset comprises 46 single-user interactions, including questionnaire responses and video data. Our method focuses on automatically classifying user satisfaction based on time series classification. We use time series of social signal metrics derived from the body pose, time series of facial expressions, and physical distance. This study compares three feature engineering approaches on different machine learning models. The results confirm the method's effectiveness in reliably identifying interactions with low user satisfaction without the need for manually annotated datasets. This approach offers significant potential for enhancing SIA performance and user experience through automated feedback mechanisms.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.HC", "cs.RO"], "url": "http://arxiv.org/abs/2512.03945", "comments": "15 pages, 3 figures. This paper has been accepted for publication at ICSR+AI 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Classification of User Satisfaction in HRI with Social Signals in the Wild\nResearch Topic: machine learning\nAuthors: Michael Schiffmann, Sabina Jeschke, Anja Richert\nAbstract: Socially interactive agents (SIAs) are being used in various scenarios and are nearing productive deployment. Evaluating user satisfaction with SIAs' performance is a key factor in designing the interaction between the user and SIA. Currently, subjective user satisfaction is primarily assessed manually through questionnaires or indirectly via system metrics. This study examines the automatic classification of user satisfaction through analysis of social signals, aiming to enhance both manual and autonomous evaluation methods for SIAs. During a field trial at the Deutsches Museum Bonn, a Furhat Robotics head was employed as a service and information hub, collecting an \"in-the-wild\" dataset. This dataset comprises 46 single-user interactions, including questionnaire responses and video data. Our method focuses on automatically classifying user satisfaction based on time series classification. We use time series of social signal metrics derived from the body pose, time series of facial expressions, and physical distance. This study compares three feature engineering approaches on different machine learning models. The results confirm the method's effectiveness in reliably identifying interactions with low user satisfaction without the need for manually annotated datasets. This approach offers significant potential for enhancing SIA performance and user experience through automated feedback mechanisms.\nCategories: cs.HC, cs.RO\nPublication Date: 2025-12-03\nPaper ID: 2512.03945\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1421, "total_text_length": 1496, "word_count": 206, "sentence_count": 12}, "basic_keywords": ["user", "satisfaction", "sias", "through", "time", "series", "performance", "manually"], "domain_keywords": [], "technical_terms": ["Evaluating", "Deutsches Museum Bonn", "Furhat Robotics", "During", "Socially", "Currently"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03928", "topic": "machine learning", "title": "Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization", "authors": ["Michele Alessi", "Alessio Ansuini", "Alex Rodriguez"], "first_author": "Michele Alessi", "abstract": "We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03928", "comments": "PriGM Workshop EurIPS 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization\nResearch Topic: machine learning\nAuthors: Michele Alessi, Alessio Ansuini, Alex Rodriguez\nAbstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03928\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 932, "total_text_length": 1030, "word_count": 137, "sentence_count": 7}, "basic_keywords": ["density", "prior", "divae", "data", "improves", "space", "learnable", "alignment"], "domain_keywords": [], "technical_terms": ["Density", "Standard", "Informed"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03923", "topic": "machine learning", "title": "Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations", "authors": ["Xiang Rao", "Yina Liu", "Yuxuan Shen"], "first_author": "Xiang Rao", "abstract": "Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "math.NA", "physics.comp-ph"], "url": "http://arxiv.org/abs/2512.03923", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations\nResearch Topic: machine learning\nAuthors: Xiang Rao, Yina Liu, Yuxuan Shen\nAbstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.\nCategories: cs.LG, math.NA, physics.comp-ph\nPublication Date: 2025-12-03\nPaper ID: 2512.03923\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1740, "total_text_length": 1831, "word_count": 218, "sentence_count": 8}, "basic_keywords": ["quantum", "high", "classical", "equation", "phase", "flow", "reservoir", "while"], "domain_keywords": [], "technical_terms": ["Traditional", "Cross", "Physics", "Classical Physics", "Cascade", "Leverett", "Informed Neural Networks", "Informed Neural Network"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03899", "topic": "machine learning", "title": "Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction", "authors": ["Janis Keck", "Lukas Silvester Barth", "Fatemeh", "Fahimi", "Parvaneh Joharinad", "Jürgen Jost"], "first_author": "Janis Keck", "abstract": "Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using ech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "math.AT", "stat.ML"], "url": "http://arxiv.org/abs/2512.03899", "comments": "47 pages (including appendix), 11 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.328458", "embedding_text": "Paper Title: Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction\nResearch Topic: machine learning\nAuthors: Janis Keck, Lukas Silvester Barth, Fatemeh, Fahimi, Parvaneh Joharinad, Jürgen Jost\nAbstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.\nCategories: cs.LG, math.AT, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03899\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1422, "total_text_length": 1512, "word_count": 208, "sentence_count": 8}, "basic_keywords": ["fuzzy", "simplicial", "sets", "umap", "probabilistic", "framework", "dimensionality", "reduction"], "domain_keywords": [], "technical_terms": ["However", "Leibler", "Kullback", "Boolean", "Fuzzy", "More", "Vietoris", "Rips"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03891", "topic": "machine learning", "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning", "authors": ["Ying-Kuan Tsai", "Yi-Ping Chen", "Vispi Karkaria", "Wei Chen"], "first_author": "Ying-Kuan Tsai", "abstract": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.LG"], "url": "http://arxiv.org/abs/2512.03891", "comments": "28 pages, 17 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning\nResearch Topic: machine learning\nAuthors: Ying-Kuan Tsai, Yi-Ping Chen, Vispi Karkaria, Wei Chen\nAbstract: Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.\nCategories: cs.RO, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03891\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 103, "abstract_length": 1903, "total_text_length": 2007, "word_count": 254, "sentence_count": 11}, "basic_keywords": ["systems", "control", "active", "suspension", "vehicle", "learning", "framework", "optimization"], "domain_keywords": [], "technical_terms": ["However", "Active", "Contributions", "Results", "Recent"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03882", "topic": "machine learning", "title": "Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models", "authors": ["Haidong Kang", "Wei Wu", "Hanling Wang"], "first_author": "Haidong Kang", "abstract": "Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03882", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models\nResearch Topic: machine learning\nAuthors: Haidong Kang, Wei Wu, Hanling Wang\nAbstract: Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03882\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1577, "total_text_length": 1670, "word_count": 240, "sentence_count": 13}, "basic_keywords": ["fscil", "attack", "methods", "learning", "classes", "human", "expert", "llms"], "domain_keywords": [], "technical_terms": ["Experiments", "Large Language Models", "Proximal Policy Optimization", "Grounded", "Previous", "Moreover"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04138", "topic": "machine learning", "title": "MechDetect: Detecting Data-Dependent Errors", "authors": ["Philipp Jung", "Nicholas Chandler", "Sebastian Jäger", "Felix Biessmann"], "first_author": "Philipp Jung", "abstract": "Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.DB", "cs.IR"], "url": "http://arxiv.org/abs/2512.04138", "comments": "International Conference on Data Science and Intelligent Systems (DSIS 2025)", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: MechDetect: Detecting Data-Dependent Errors\nResearch Topic: machine learning\nAuthors: Philipp Jung, Nicholas Chandler, Sebastian Jäger, Felix Biessmann\nAbstract: Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.\nCategories: cs.LG, cs.DB, cs.IR\nPublication Date: 2025-12-03\nPaper ID: 2512.04138\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 920, "total_text_length": 964, "word_count": 143, "sentence_count": 8}, "basic_keywords": ["error", "data", "errors", "mechanisms", "approaches", "detect", "investigate", "generation"], "domain_keywords": [], "technical_terms": ["Given", "While", "Data"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03880", "topic": "machine learning", "title": "Leveraging topological data analysis to estimate bone strength from micro-CT as a surrogate for advanced imaging", "authors": ["John Rick Manzanares", "Richard Leslie Abel", "Paweł Dłotko"], "first_author": "John Rick Manzanares", "abstract": "Accurate bone strength prediction is essential for assessing fracture risk, particularly in aging populations and individuals with osteoporosis. Bone imaging has evolved from X-rays and DXA to clinical computed tomography (CT), and now to advanced modalities such as high-resolution peripheral quantitative CT and synchrotron radiation CT, which offer unprecedented resolution of bone microarchitecture. However, analytical methods have not kept pace with these imaging advances. This study applied topological data analysis (TDA) to extract biomechanically relevant features from high-resolution bone images, offering a new framework for bone strength prediction. We extracted topological features, specifically those derived from persistent homology, and combined them with standard bone morphometric descriptors to train machine learning models for apparent strength prediction. Models based solely on topological features outperformed those using traditional morphometrics, highlighting TDA's ability to capture biomechanically relevant structure. In particular, internal voids, often dismissed as imaging noise, proved to be the most predictive. While limited by dataset size and class imbalance, these results suggest that TDA offers a promising approach for advancing osteoporosis risk assessment.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["q-bio.QM"], "url": "http://arxiv.org/abs/2512.03880", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Leveraging topological data analysis to estimate bone strength from micro-CT as a surrogate for advanced imaging\nResearch Topic: machine learning\nAuthors: John Rick Manzanares, Richard Leslie Abel, Paweł Dłotko\nAbstract: Accurate bone strength prediction is essential for assessing fracture risk, particularly in aging populations and individuals with osteoporosis. Bone imaging has evolved from X-rays and DXA to clinical computed tomography (CT), and now to advanced modalities such as high-resolution peripheral quantitative CT and synchrotron radiation CT, which offer unprecedented resolution of bone microarchitecture. However, analytical methods have not kept pace with these imaging advances. This study applied topological data analysis (TDA) to extract biomechanically relevant features from high-resolution bone images, offering a new framework for bone strength prediction. We extracted topological features, specifically those derived from persistent homology, and combined them with standard bone morphometric descriptors to train machine learning models for apparent strength prediction. Models based solely on topological features outperformed those using traditional morphometrics, highlighting TDA's ability to capture biomechanically relevant structure. In particular, internal voids, often dismissed as imaging noise, proved to be the most predictive. While limited by dataset size and class imbalance, these results suggest that TDA offers a promising approach for advancing osteoporosis risk assessment.\nCategories: q-bio.QM\nPublication Date: 2025-12-03\nPaper ID: 2512.03880\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 112, "abstract_length": 1304, "total_text_length": 1417, "word_count": 187, "sentence_count": 9}, "basic_keywords": ["bone", "strength", "prediction", "imaging", "resolution", "topological", "features", "risk"], "domain_keywords": [], "technical_terms": ["However", "Bone", "While", "Accurate", "Models"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03874", "topic": "machine learning", "title": "OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance", "authors": ["Lei Zhang", "Diwen Zheng", "Kaixin Bai", "Zhenshan Bing", "Zoltan-Csaba Marton", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "first_author": "Lei Zhang", "abstract": "Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.LG"], "url": "http://arxiv.org/abs/2512.03874", "comments": "Project Website: https://sites.google.com/view/omnidexvlg, 16 pages", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\nResearch Topic: machine learning\nAuthors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\nAbstract: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.\nCategories: cs.RO, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03874\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 133, "abstract_length": 1871, "total_text_length": 2005, "word_count": 252, "sentence_count": 8}, "basic_keywords": ["grasp", "generation", "semantics", "semantic", "dexterous", "taxonomy", "contact", "functional"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Vision Language Grasping", "Dexterous", "Building"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03851", "topic": "machine learning", "title": "Comparison of neural network training strategies for the simulation of dynamical systems", "authors": ["Paul Strasser", "Andreas Pfeffer", "Jakob Weber", "Markus Gurtner", "Andreas Körner"], "first_author": "Paul Strasser", "abstract": "Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/abs/2512.03851", "comments": "submitted to ECC", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Comparison of neural network training strategies for the simulation of dynamical systems\nResearch Topic: machine learning\nAuthors: Paul Strasser, Andreas Pfeffer, Jakob Weber, Markus Gurtner, Andreas Körner\nAbstract: Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.\nCategories: stat.ML, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03851\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 922, "total_text_length": 1011, "word_count": 133, "sentence_count": 8}, "basic_keywords": ["training", "parallel", "neural", "dynamical", "systems", "strategy", "simulation", "strategies"], "domain_keywords": [], "technical_terms": ["However", "Additionally", "Neural"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.03846", "topic": "machine learning", "title": "Fault-Tolerant Control of Steam Temperature in HRSG Superheater under Actuator Fault Using a Sliding Mode Observer and PINN", "authors": ["Mojtaba Fanoodi", "Farzaneh Abdollahi", "Mahdi Aliyari Shoorehdeli"], "first_author": "Mojtaba Fanoodi", "abstract": "This paper presents a novel fault-tolerant control framework for steam temperature regulation in Heat Recovery Steam Generators (HRSGs) subject to actuator faults. Addressing the critical challenge of valve degradation in superheater spray attemperators, we propose a synergistic architecture comprising three components: (1) a Sliding Mode Observer (SMO) for estimation of unmeasured thermal states, (2) a Physics-Informed Neural Network (PINN) for estimating multiplicative actuator faults using physical laws as constraints, and (3) a one-sided Sliding Mode Controller (SMC) that adapts to the estimated faults while minimizing excessive actuation. The key innovation lies in the framework of closed-loop physics-awareness, where the PINN continuously informs both the observer and controller about fault severity while preserving thermodynamic consistency. Rigorous uniform ultimate boundedness (UUB) is established via Lyapunov analysis under practical assumptions. Validated on real HRSG operational data, the framework demonstrates effective fault adaptation, reduced temperature overshoot, and maintains steam temperature within 1 C of the setpoint under valve effectiveness loss. This work bridges control theory and physics-guided machine learning to deliver a practically deployable solution for power plant resilience, with extensions applicable to thermal systems subject to multiplicative faults.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["eess.SY"], "url": "http://arxiv.org/abs/2512.03846", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Fault-Tolerant Control of Steam Temperature in HRSG Superheater under Actuator Fault Using a Sliding Mode Observer and PINN\nResearch Topic: machine learning\nAuthors: Mojtaba Fanoodi, Farzaneh Abdollahi, Mahdi Aliyari Shoorehdeli\nAbstract: This paper presents a novel fault-tolerant control framework for steam temperature regulation in Heat Recovery Steam Generators (HRSGs) subject to actuator faults. Addressing the critical challenge of valve degradation in superheater spray attemperators, we propose a synergistic architecture comprising three components: (1) a Sliding Mode Observer (SMO) for estimation of unmeasured thermal states, (2) a Physics-Informed Neural Network (PINN) for estimating multiplicative actuator faults using physical laws as constraints, and (3) a one-sided Sliding Mode Controller (SMC) that adapts to the estimated faults while minimizing excessive actuation.   The key innovation lies in the framework of closed-loop physics-awareness, where the PINN continuously informs both the observer and controller about fault severity while preserving thermodynamic consistency.   Rigorous uniform ultimate boundedness (UUB) is established via Lyapunov analysis under practical assumptions. Validated on real HRSG operational data, the framework demonstrates effective fault adaptation, reduced temperature overshoot, and maintains steam temperature within 1°C of the setpoint under valve effectiveness loss.   This work bridges control theory and physics-guided machine learning to deliver a practically deployable solution for power plant resilience, with extensions applicable to thermal systems subject to multiplicative faults.\nCategories: eess.SY\nPublication Date: 2025-12-03\nPaper ID: 2512.03846\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 123, "abstract_length": 1410, "total_text_length": 1534, "word_count": 203, "sentence_count": 7}, "basic_keywords": ["faults", "fault", "framework", "steam", "temperature", "physics", "control", "subject"], "domain_keywords": [], "technical_terms": ["Addressing", "Validated", "Physics", "Sliding Mode Observer", "Informed Neural Network", "Sliding Mode Controller", "Rigorous", "Lyapunov"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04135", "topic": "machine learning", "title": "Decoding Large Language Diffusion Models with Foreseeing Movement", "authors": ["Yichuan Mo", "Quan Chen", "Mingjie Li", "Zeming Wei", "Yisen Wang"], "first_author": "Yichuan Mo", "abstract": "Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04135", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Decoding Large Language Diffusion Models with Foreseeing Movement\nResearch Topic: machine learning\nAuthors: Yichuan Mo, Quan Chen, Mingjie Li, Zeming Wei, Yisen Wang\nAbstract: Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.04135\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1215, "total_text_length": 1281, "word_count": 169, "sentence_count": 8}, "basic_keywords": ["decoding", "models", "lldms", "inference", "critical", "performance", "tokens", "methods"], "domain_keywords": [], "technical_terms": ["Extensive", "Existing", "Furthermore", "Large Language Diffusion Models", "Foreseeing Decoding Method", "Acceleration"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03838", "topic": "machine learning", "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs", "authors": ["Michael Staniek", "Artem Sokolov", "Stefan Riezler"], "first_author": "Michael Staniek", "abstract": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.03838", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Training and Evaluation of Guideline-Based Medical Reasoning in LLMs\nResearch Topic: machine learning\nAuthors: Michael Staniek, Artem Sokolov, Stefan Riezler\nAbstract: Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.\nCategories: cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.03838\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1787, "total_text_length": 1856, "word_count": 259, "sentence_count": 9}, "basic_keywords": ["medical", "consensus", "prediction", "rules", "llms", "fine", "correctness", "learning"], "domain_keywords": [], "technical_terms": ["Machine", "Since", "Sepsis", "Consensus"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03837", "topic": "machine learning", "title": "Heatmap Pooling Network for Action Recognition from RGB Videos", "authors": ["Mengyuan Liu", "Jinfu Liu", "Yongkang Jiang", "Bin He"], "first_author": "Mengyuan Liu", "abstract": "Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.03837", "comments": "Final Version of IEEE Transactions on Pattern Analysis and Machine Intelligence", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Heatmap Pooling Network for Action Recognition from RGB Videos\nResearch Topic: machine learning\nAuthors: Mengyuan Liu, Jinfu Liu, Yongkang Jiang, Bin He\nAbstract: Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.\nCategories: cs.CV\nPublication Date: 2025-12-03\nPaper ID: 2512.03837\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1246, "total_text_length": 1309, "word_count": 179, "sentence_count": 9}, "basic_keywords": ["videos", "action", "features", "human", "recognition", "information", "pooled", "module"], "domain_keywords": [], "technical_terms": ["Extensive", "Smarthome", "Human", "Nevertheless", "Action", "Toyota"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03829", "topic": "machine learning", "title": "First Experimental Demonstration of Machine Learning-Based Tuning on the PSI Injector 2 Cyclotron", "authors": ["M. Haj Tahar", "W. Joho", "E. Solodko", "M. Bocchio", "S. Marquie", "M. Busch", "A. Barchetti", "J. Grillenberger", "J. Snuverink", "M. Schneider"], "first_author": "M. Haj Tahar", "abstract": "Reliable operation of high-power proton cyclotrons is a critical requirement for Accelerator Driven Systems (ADS) and other large-scale applications. Beam tuning in such machines is traditionally performed manually, a process that can be slow, non-optimal, and difficult to execute in the presence of faults or changing conditions. To address this, we developed and deployed a machine learning (ML) based tuning framework on the Injector 2 cyclotron at PSI, chosen as an ideal testbed for high-power operation. The system combined a tailored reinforcement learning algorithm with real-time diagnostics and control, and incorporated accelerator-physics inspired adaptations such as an overshoot strategy that reduced magnetic field settling times by nearly a factor of six. Over an extensive 12-day operational test campaign, relatively long in the context of real-time ML experiments, the ML agent successfully tuned the machine across multiple operating points, achieving convergence within hours and maintaining stable beam extraction with reduced losses. Beyond initial tuning, the system was also operated in evaluation mode overnight, where it autonomously monitored and corrected the machine to compensate for drifts, demonstrating robustness and long-term stability. Crucially, the learned policy generalized reliably from low-current training to higher-current operation, underscoring its scalability. These results constitute the first demonstration of ML-assisted tuning on a high-power cyclotron, with direct relevance to ADS-class drivers.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["physics.acc-ph", "hep-ex"], "url": "http://arxiv.org/abs/2512.03829", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: First Experimental Demonstration of Machine Learning-Based Tuning on the PSI Injector 2 Cyclotron\nResearch Topic: machine learning\nAuthors: M. Haj Tahar, W. Joho, E. Solodko, M. Bocchio, S. Marquie, M. Busch, A. Barchetti, J. Grillenberger, J. Snuverink, M. Schneider\nAbstract: Reliable operation of high-power proton cyclotrons is a critical requirement for Accelerator Driven Systems (ADS) and other large-scale applications. Beam tuning in such machines is traditionally performed manually, a process that can be slow, non-optimal, and difficult to execute in the presence of faults or changing conditions. To address this, we developed and deployed a machine learning (ML) based tuning framework on the Injector 2 cyclotron at PSI, chosen as an ideal testbed for high-power operation. The system combined a tailored reinforcement learning algorithm with real-time diagnostics and control, and incorporated accelerator-physics inspired adaptations such as an overshoot strategy that reduced magnetic field settling times by nearly a factor of six. Over an extensive 12-day operational test campaign, relatively long in the context of real-time ML experiments, the ML agent successfully tuned the machine across multiple operating points, achieving convergence within hours and maintaining stable beam extraction with reduced losses. Beyond initial tuning, the system was also operated in evaluation mode overnight, where it autonomously monitored and corrected the machine to compensate for drifts, demonstrating robustness and long-term stability. Crucially, the learned policy generalized reliably from low-current training to higher-current operation, underscoring its scalability. These results constitute the first demonstration of ML-assisted tuning on a high-power cyclotron, with direct relevance to ADS-class drivers.\nCategories: physics.acc-ph, hep-ex\nPublication Date: 2025-12-03\nPaper ID: 2512.03829\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1551, "total_text_length": 1649, "word_count": 226, "sentence_count": 9}, "basic_keywords": ["tuning", "operation", "high", "power", "machine", "accelerator", "beam", "learning"], "domain_keywords": [], "technical_terms": ["These", "Beyond", "Accelerator Driven Systems", "Beam", "Crucially", "Over", "Reliable", "Injector"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03819", "topic": "machine learning", "title": "Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission", "authors": ["Junlin Chang", "Yubo Han", "Hnag Yue", "John S Thompson", "Rongke Liu"], "first_author": "Junlin Chang", "abstract": "The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03819", "comments": "5 pages, 5 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission\nResearch Topic: machine learning\nAuthors: Junlin Chang, Yubo Han, Hnag Yue, John S Thompson, Rongke Liu\nAbstract: The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03819\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1054, "total_text_length": 1143, "word_count": 149, "sentence_count": 8}, "basic_keywords": ["point", "semantic", "cloud", "transmission", "folding", "signal", "noise", "bandwidth"], "domain_keywords": [], "technical_terms": ["Signal", "Noise Ratio", "Deep Joint Source", "Instead", "Chamfer Distance", "Peak Signal", "Channel Coding", "Results"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03818", "topic": "machine learning", "title": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology", "authors": ["Kylie L. Anglin", "Stephanie Milan", "Brittney Hernandez", "Claudia Ventura"], "first_author": "Kylie L. Anglin", "abstract": "Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.03818", "comments": "22 pages, 2 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology\nResearch Topic: machine learning\nAuthors: Kylie L. Anglin, Stephanie Milan, Brittney Hernandez, Claudia Ventura\nAbstract: Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.\nCategories: cs.CL\nPublication Date: 2025-12-03\nPaper ID: 2512.03818\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 141, "abstract_length": 1832, "total_text_length": 1974, "word_count": 267, "sentence_count": 11}, "basic_keywords": ["prompt", "performance", "engineering", "empirical", "training", "classification", "constructs", "prompting"], "domain_keywords": [], "technical_terms": ["However", "Based", "While", "Across", "Instead"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03817", "topic": "machine learning", "title": "HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English", "authors": ["Ahmed Nasser", "Marwan Mohamed", "Alaa Sherif", "Basmala Mahmoud", "Shereen Yehia", "Asmaa Saad", "Mariam S. El-Rahmany", "Ensaf H. Mohamed"], "first_author": "Ahmed Nasser", "abstract": "Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.03817", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English\nResearch Topic: machine learning\nAuthors: Ahmed Nasser, Marwan Mohamed, Alaa Sherif, Basmala Mahmoud, Shereen Yehia, Asmaa Saad, Mariam S. El-Rahmany, Ensaf H. Mohamed\nAbstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03817\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 879, "total_text_length": 974, "word_count": 132, "sentence_count": 9}, "basic_keywords": ["translation", "egyptian", "hieroglyphs", "ancient", "into", "english", "research", "dataset"], "domain_keywords": [], "technical_terms": ["English", "Egyptian", "Gardiner", "Deep", "Contour", "Morris Franken", "Translating"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03816", "topic": "machine learning", "title": "Log Probability Tracking of LLM APIs", "authors": ["Timothée Chauvin", "Erwan Le Merrer", "François Taïani", "Gilles Tredan"], "first_author": "Timothée Chauvin", "abstract": "When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.CR"], "url": "http://arxiv.org/abs/2512.03816", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Log Probability Tracking of LLM APIs\nResearch Topic: machine learning\nAuthors: Timothée Chauvin, Erwan Le Merrer, François Taïani, Gilles Tredan\nAbstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.\nCategories: cs.LG, cs.CR\nPublication Date: 2025-12-03\nPaper ID: 2512.03816\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 36, "abstract_length": 1004, "total_text_length": 1041, "word_count": 168, "sentence_count": 8}, "basic_keywords": ["model", "methods", "time", "existing", "audit", "apply", "apis", "while"], "domain_keywords": [], "technical_terms": ["When", "Existing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03807", "topic": "machine learning", "title": "Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics", "authors": ["Christos Kolomvakis", "Thomas Bobille", "Arnaud Vandaele", "Nicolas Gillis"], "first_author": "Christos Kolomvakis", "abstract": "Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging.", "publish_date": "2025-12-03", "update_date": "2025-12-04", "categories": ["cs.IR", "eess.SP", "math.OC", "stat.ML"], "url": "http://arxiv.org/abs/2512.03807", "comments": "24 pages, 12 tables, 3 figures, 2 typos corrected in v2, code and data available from https://gitlab.com/ckolomvakis/boolean-matrix-factorization-ip-and-heuristics", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics\nResearch Topic: machine learning\nAuthors: Christos Kolomvakis, Thomas Bobille, Arnaud Vandaele, Nicolas Gillis\nAbstract: Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging.\nCategories: cs.IR, eess.SP, math.OC, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03807\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1236, "total_text_length": 1321, "word_count": 197, "sentence_count": 9}, "basic_keywords": ["matrix", "boolean", "binary", "based", "factorization", "product", "factors", "algorithms"], "domain_keywords": ["computer vision"], "technical_terms": ["Unlike", "Boolean"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03805", "topic": "machine learning", "title": "Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA", "authors": ["Tai Nguyen", "Phong Le", "André Biedenkapp", "Carola Doerr", "Nguyen Dang"], "first_author": "Tai Nguyen", "abstract": "Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+(,))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03805", "comments": "arXiv admin note: text overlap with arXiv:2502.20265", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA\nResearch Topic: machine learning\nAuthors: Tai Nguyen, Phong Le, André Biedenkapp, Carola Doerr, Nguyen Dang\nAbstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03805\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 124, "abstract_length": 1899, "total_text_length": 2024, "word_count": 258, "sentence_count": 11}, "basic_keywords": ["learning", "ddqn", "policies", "optimization", "exploration", "reward", "hyperparameter", "algorithm"], "domain_keywords": [], "technical_terms": ["Numerous", "However", "Finally", "Reinforcement Learning", "Dynamic Algorithm Configuration"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03804", "topic": "machine learning", "title": "EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification", "authors": ["Hanhui Deng", "Xinglin Li", "Jie Luo", "Zhanpeng Jin", "Di Wu"], "first_author": "Hanhui Deng", "abstract": "Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03804", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification\nResearch Topic: machine learning\nAuthors: Hanhui Deng, Xinglin Li, Jie Luo, Zhanpeng Jin, Di Wu\nAbstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03804\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 96, "abstract_length": 1269, "total_text_length": 1366, "word_count": 187, "sentence_count": 10}, "basic_keywords": ["model", "data", "high", "diagnostic", "deep", "learning", "effectively", "existing"], "domain_keywords": [], "technical_terms": ["Electrocardiogram", "Specifically", "Unlike"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03787", "topic": "machine learning", "title": "Adaptive Identification and Modeling of Clinical Pathways with Process Mining", "authors": ["Francesco Vitale", "Nicola Mazzocca"], "first_author": "Francesco Vitale", "abstract": "Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03787", "comments": "Accepted to the 41st ACM/SIGAPP Symposium On Applied Computing (ACM SAC 2026)", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Adaptive Identification and Modeling of Clinical Pathways with Process Mining\nResearch Topic: machine learning\nAuthors: Francesco Vitale, Nicola Mazzocca\nAbstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03787\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1352, "total_text_length": 1430, "word_count": 201, "sentence_count": 12}, "basic_keywords": ["clinical", "pathways", "patient", "model", "treatment", "based", "phase", "knowledge"], "domain_keywords": [], "technical_terms": ["However", "Based", "Clinical", "Synthea", "They"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03786", "topic": "machine learning", "title": "Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach", "authors": ["Conor McCarthy", "Jan Peter van Zandwijk", "Marcel Worring", "Zeno Geradts"], "first_author": "Conor McCarthy", "abstract": "Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03786", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach\nResearch Topic: machine learning\nAuthors: Conor McCarthy, Jan Peter van Zandwijk, Marcel Worring, Zeno Geradts\nAbstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03786\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1084, "total_text_length": 1186, "word_count": 175, "sentence_count": 7}, "basic_keywords": ["activities", "present", "digital", "traces", "approach", "forensic", "into", "physical"], "domain_keywords": [], "technical_terms": ["Evaluating", "Smartphones"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03777", "topic": "machine learning", "title": "A comparison between initialization strategies for the infinite hidden Markov model", "authors": ["Federico P. Cortese", "Luca Rossini"], "first_author": "Federico P. Cortese", "abstract": "Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["stat.ME", "stat.AP", "stat.ML"], "url": "http://arxiv.org/abs/2512.03777", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: A comparison between initialization strategies for the infinite hidden Markov model\nResearch Topic: machine learning\nAuthors: Federico P. Cortese, Luca Rossini\nAbstract: Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.\nCategories: stat.ME, stat.AP, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03777\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 995, "total_text_length": 1079, "word_count": 144, "sentence_count": 6}, "basic_keywords": ["infinite", "hidden", "markov", "models", "framework", "initialization", "based", "provide"], "domain_keywords": [], "technical_terms": ["Despite", "Markov", "Results", "Infinite", "Bayesian", "Dirichlet"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03768", "topic": "machine learning", "title": "Deep Unfolding: Recent Developments, Theory, and Design Guidelines", "authors": ["Nir Shlezinger", "Santiago Segarra", "Yi Zhang", "Dvir Avrahami", "Zohar Davidov", "Tirza Routtenberg", "Yonina C. Eldar"], "first_author": "Nir Shlezinger", "abstract": "Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "eess.SP"], "url": "http://arxiv.org/abs/2512.03768", "comments": "under review for publication in the IEEE", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: Deep Unfolding: Recent Developments, Theory, and Design Guidelines\nResearch Topic: machine learning\nAuthors: Nir Shlezinger, Santiago Segarra, Yi Zhang, Dvir Avrahami, Zohar Davidov, Tirza Routtenberg, Yonina C. Eldar\nAbstract: Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.\nCategories: cs.LG, eess.SP\nPublication Date: 2025-12-03\nPaper ID: 2512.03768\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1506, "total_text_length": 1573, "word_count": 190, "sentence_count": 8}, "basic_keywords": ["optimization", "inference", "iterative", "theoretical", "deep", "unfolding", "algorithms", "provide"], "domain_keywords": [], "technical_terms": ["While", "Furthermore", "Deep", "Optimization", "Conversely"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03765", "topic": "machine learning", "title": "The Treasury Proof Ledger: A Cryptographic Framework for Accountable Bitcoin Treasuries", "authors": ["Jose E. Puente", "Carlos Puente"], "first_author": "Jose E. Puente", "abstract": "Public companies and institutional investors that hold Bitcoin face increasing pressure to show solvency, manage risk, and satisfy regulatory expectations without exposing internal wallet structures or trading strategies. This paper introduces the Treasury Proof Ledger (TPL), a Bitcoin-anchored logging framework for multi-domain Bitcoin treasuries that treats on-chain and off-chain exposures as a conserved state machine with an explicit fee sink. A TPL instance records proof-of-reserves snapshots, proof-of-transit receipts for movements between domains, and policy metadata, and it supports restricted views based on stakeholder permissions. We define an idealised TPL model, represent Bitcoin treasuries as multi-domain exposure vectors, and give deployment-level security notions including exposure soundness, policy completeness, non-equivocation, and privacy-compatible policy views. We then outline how practical, restricted forms of these guarantees can be achieved by combining standard proof-of-reserves and proof-of-transit techniques with hash-based commitments anchored on Bitcoin. The results are existence-type statements: they show which guarantees are achievable once economic and governance assumptions are set, without claiming that any current system already provides them. A stylised corporate-treasury example illustrates how TPL could support responsible transparency policies and future cross-institution checks consistent with Bitcoin's fixed monetary supply.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CR"], "url": "http://arxiv.org/abs/2512.03765", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:06.329494", "embedding_text": "Paper Title: The Treasury Proof Ledger: A Cryptographic Framework for Accountable Bitcoin Treasuries\nResearch Topic: machine learning\nAuthors: Jose E. Puente, Carlos Puente\nAbstract: Public companies and institutional investors that hold Bitcoin face increasing pressure to show solvency, manage risk, and satisfy regulatory expectations without exposing internal wallet structures or trading strategies. This paper introduces the Treasury Proof Ledger (TPL), a Bitcoin-anchored logging framework for multi-domain Bitcoin treasuries that treats on-chain and off-chain exposures as a conserved state machine with an explicit fee sink. A TPL instance records proof-of-reserves snapshots, proof-of-transit receipts for movements between domains, and policy metadata, and it supports restricted views based on stakeholder permissions. We define an idealised TPL model, represent Bitcoin treasuries as multi-domain exposure vectors, and give deployment-level security notions including exposure soundness, policy completeness, non-equivocation, and privacy-compatible policy views. We then outline how practical, restricted forms of these guarantees can be achieved by combining standard proof-of-reserves and proof-of-transit techniques with hash-based commitments anchored on Bitcoin. The results are existence-type statements: they show which guarantees are achievable once economic and governance assumptions are set, without claiming that any current system already provides them. A stylised corporate-treasury example illustrates how TPL could support responsible transparency policies and future cross-institution checks consistent with Bitcoin's fixed monetary supply.\nCategories: cs.CR\nPublication Date: 2025-12-03\nPaper ID: 2512.03765\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 1488, "total_text_length": 1576, "word_count": 198, "sentence_count": 8}, "basic_keywords": ["bitcoin", "proof", "policy", "show", "without", "treasury", "anchored", "multi"], "domain_keywords": [], "technical_terms": ["Bitcoin", "Public", "Treasury Proof Ledger"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03755", "topic": "machine learning", "title": "Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition", "authors": ["Stephen Law", "Tao Yang", "Nanjiang Chen", "Xuhui Lin"], "first_author": "Stephen Law", "abstract": "Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation (), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03755", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636284", "embedding_text": "Paper Title: Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition\nResearch Topic: machine learning\nAuthors: Stephen Law, Tao Yang, Nanjiang Chen, Xuhui Lin\nAbstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \\to B \\ne B \\to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03755\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 112, "abstract_length": 1582, "total_text_length": 1695, "word_count": 198, "sentence_count": 8}, "basic_keywords": ["urban", "origin", "spatial", "cognitive", "trajectory", "patterns", "representations", "asymmetries"], "domain_keywords": [], "technical_terms": ["Beijing", "Urban", "Results", "Three", "Xicheng District"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03750", "topic": "machine learning", "title": "Universally Converging Representations of Matter Across Scientific Foundation Models", "authors": ["Sathya Edamadaka", "Soojung Yang", "Ju Li", "Rafael Gómez-Bombarelli"], "first_author": "Sathya Edamadaka", "abstract": "Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.03750", "comments": "Oral spotlight at NeurIPS 2025 UniReps Workshop", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636284", "embedding_text": "Paper Title: Universally Converging Representations of Matter Across Scientific Foundation Models\nResearch Topic: machine learning\nAuthors: Sathya Edamadaka, Soojung Yang, Ju Li, Rafael Gómez-Bombarelli\nAbstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.\nCategories: cs.LG, cond-mat.mtrl-sci\nPublication Date: 2025-12-03\nPaper ID: 2512.03750\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1901, "total_text_length": 1986, "word_count": 265, "sentence_count": 10}, "basic_keywords": ["models", "representations", "scientific", "training", "representation", "different", "modalities", "similar"], "domain_keywords": [], "technical_terms": ["Machine", "However", "Here", "More", "Understanding", "Although", "Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03744", "topic": "machine learning", "title": "Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm", "authors": ["Xuhui Lin", "Qiuchen Lu"], "first_author": "Xuhui Lin", "abstract": "Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and \"false recovery,\" where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining \"structural irreversibility detection\" and \"energy landscape reconstruction\" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03744", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636284", "embedding_text": "Paper Title: Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm\nResearch Topic: machine learning\nAuthors: Xuhui Lin, Qiuchen Lu\nAbstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and \"false recovery,\" where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining \"structural irreversibility detection\" and \"energy landscape reconstruction\" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03744\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 126, "abstract_length": 1135, "total_text_length": 1262, "word_count": 148, "sentence_count": 8}, "basic_keywords": ["structural", "surface", "recovery", "extreme", "assessment", "indicators", "damage", "true"], "domain_keywords": [], "technical_terms": ["Hamiltonian", "Existing", "Urban", "London", "Analysis"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03743", "topic": "machine learning", "title": "Cross-embodied Co-design for Dexterous Hands", "authors": ["Kehlani Fay", "Darin Anthony Djapri", "Anya Zorin", "James Clinton", "Ali El Lahib", "Hao Su", "Michael T. Tolley", "Sha Yi", "Xiaolong Wang"], "first_author": "Kehlani Fay", "abstract": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.LG"], "url": "http://arxiv.org/abs/2512.03743", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636284", "embedding_text": "Paper Title: Cross-embodied Co-design for Dexterous Hands\nResearch Topic: machine learning\nAuthors: Kehlani Fay, Darin Anthony Djapri, Anya Zorin, James Clinton, Ali El Lahib, Hao Su, Michael T. Tolley, Sha Yi, Xiaolong Wang\nAbstract: Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.\nCategories: cs.RO, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03743\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 44, "abstract_length": 981, "total_text_length": 1026, "word_count": 141, "sentence_count": 8}, "basic_keywords": ["design", "dexterous", "control", "framework", "hand", "morphology", "manipulators", "tasks"], "domain_keywords": [], "technical_terms": ["Dexterous"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03736", "topic": "machine learning", "title": "Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control", "authors": ["Kenneth Stewart", "Samantha Chapin", "Roxana Leontie", "Carl Glen Henshaw"], "first_author": "Kenneth Stewart", "abstract": "Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.LG", "eess.SY"], "url": "http://arxiv.org/abs/2512.03736", "comments": "published at iSpaRo 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636284", "embedding_text": "Paper Title: Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control\nResearch Topic: machine learning\nAuthors: Kenneth Stewart, Samantha Chapin, Roxana Leontie, Carl Glen Henshaw\nAbstract: Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.\nCategories: cs.RO, cs.LG, eess.SY\nPublication Date: 2025-12-03\nPaper ID: 2512.03736\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 977, "total_text_length": 1094, "word_count": 142, "sentence_count": 7}, "basic_keywords": ["space", "control", "training", "learning", "orbit", "based", "astrobee", "enabling"], "domain_keywords": [], "technical_terms": ["Omniverse", "Using", "Monte Carlo", "Assembly", "International Space Station", "Space Servicing", "Manufacturing", "Astrobee"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03729", "topic": "machine learning", "title": "Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing", "authors": ["Samantha Chapin", "Kenneth Stewart", "Roxana Leontie", "Carl Glen Henshaw"], "first_author": "Samantha Chapin", "abstract": "The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.RO", "cs.LG", "eess.SY"], "url": "http://arxiv.org/abs/2512.03729", "comments": "iSpaRo 2025, Best Paper Award in Orbital Robotics", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636284", "embedding_text": "Paper Title: Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing\nResearch Topic: machine learning\nAuthors: Samantha Chapin, Kenneth Stewart, Roxana Leontie, Carl Glen Henshaw\nAbstract: The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.\nCategories: cs.RO, cs.LG, eess.SY\nPublication Date: 2025-12-03\nPaper ID: 2512.03729\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 125, "abstract_length": 1076, "total_text_length": 1202, "word_count": 160, "sentence_count": 6}, "basic_keywords": ["space", "free", "control", "reinforcement", "learning", "flyer", "apiary", "experiment"], "domain_keywords": [], "technical_terms": ["On Tuesday", "International Space Station", "Assembly Reinforcement", "Naval Research Laboratory", "Astrobee", "Proximal Policy Optimization", "Isaac Lab", "Autonomous Planning In"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03727", "topic": "machine learning", "title": "Colored Markov Random Fields for Probabilistic Topological Modeling", "authors": ["Lorenzo Marinucci", "Leonardo Di Nino", "Gabriele D'Acunto", "Mario Edoardo Pandolfo", "Paolo Di Lorenzo", "Sergio Barbarossa"], "first_author": "Lorenzo Marinucci", "abstract": "Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "url": "http://arxiv.org/abs/2512.03727", "comments": "Proceeding of 2025 Asilomar Conference on Signals, Systems, and Computers", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636284", "embedding_text": "Paper Title: Colored Markov Random Fields for Probabilistic Topological Modeling\nResearch Topic: machine learning\nAuthors: Lorenzo Marinucci, Leonardo Di Nino, Gabriele D'Acunto, Mario Edoardo Pandolfo, Paolo Di Lorenzo, Sergio Barbarossa\nAbstract: Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.\nCategories: stat.ML, cs.LG, eess.SP, stat.ME\nPublication Date: 2025-12-03\nPaper ID: 2512.03727\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 1164, "total_text_length": 1232, "word_count": 158, "sentence_count": 8}, "basic_keywords": ["variables", "topological", "pgms", "conditional", "dependencies", "random", "cmrfs", "among"], "domain_keywords": [], "technical_terms": ["Probabilistic Graphical Models", "Gaussian Markov Random Fields", "Gaussian", "Hodge", "Recent", "Colored Markov Random Fields"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03706", "topic": "machine learning", "title": "Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models", "authors": ["Vahid Nateghi", "Lara Neureither", "Selma Moqvist", "Carsten Hartmann", "Simon Olsson", "Feliks Nüske"], "first_author": "Vahid Nateghi", "abstract": "Coarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["physics.comp-ph", "cs.LG", "math.DS"], "url": "http://arxiv.org/abs/2512.03706", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models\nResearch Topic: machine learning\nAuthors: Vahid Nateghi, Lara Neureither, Selma Moqvist, Carsten Hartmann, Simon Olsson, Feliks Nüske\nAbstract: Coarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model.\nCategories: physics.comp-ph, cs.LG, math.DS\nPublication Date: 2025-12-03\nPaper ID: 2512.03706\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 107, "abstract_length": 1068, "total_text_length": 1176, "word_count": 158, "sentence_count": 7}, "basic_keywords": ["dynamics", "approach", "thermodynamic", "coarse", "model", "graining", "projection", "method"], "domain_keywords": [], "technical_terms": ["Langevin", "Using", "Zwanzig", "Finally", "Extended Dynamic Mode Decomposition", "Coarse", "Koopman", "Following"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03678", "topic": "machine learning", "title": "Feature-aware Modulation for Learning from Temporal Tabular Data", "authors": ["Hao-Run Cai", "Han-Jia Ye"], "first_author": "Hao-Run Cai", "abstract": "While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03678", "comments": "17 pages, 6 figures, 8 tables. NeurIPS 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Feature-aware Modulation for Learning from Temporal Tabular Data\nResearch Topic: machine learning\nAuthors: Hao-Run Cai, Han-Jia Ye\nAbstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03678\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 1266, "total_text_length": 1331, "word_count": 167, "sentence_count": 9}, "basic_keywords": ["temporal", "feature", "tabular", "shifts", "between", "models", "adaptability", "data"], "domain_keywords": [], "technical_terms": ["While", "Benchmark", "Crucially", "Static", "Motivated"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03656", "topic": "machine learning", "title": "Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting", "authors": ["Salim Khazem", "Houssam Kanso"], "first_author": "Salim Khazem", "abstract": "Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.CV"], "url": "http://arxiv.org/abs/2512.03656", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting\nResearch Topic: machine learning\nAuthors: Salim Khazem, Houssam Kanso\nAbstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.\nCategories: cs.LG, cs.CV\nPublication Date: 2025-12-03\nPaper ID: 2512.03656\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1415, "total_text_length": 1501, "word_count": 196, "sentence_count": 9}, "basic_keywords": ["forecasting", "cyclical", "temporal", "hybrid", "architectures", "calendar", "encodings", "term"], "domain_keywords": [], "technical_terms": ["These", "Using", "Accurate", "Results"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03653", "topic": "machine learning", "title": "Conditional updates of neural network weights for increased out of training performance", "authors": ["Jan Saynisch-Wagner", "Saran Rajendran Sari"], "first_author": "Jan Saynisch-Wagner", "abstract": "This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "physics.ao-ph", "physics.data-an"], "url": "http://arxiv.org/abs/2512.03653", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Conditional updates of neural network weights for increased out of training performance\nResearch Topic: machine learning\nAuthors: Jan Saynisch-Wagner, Saran Rajendran Sari\nAbstract: This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.\nCategories: cs.LG, physics.ao-ph, physics.data-an\nPublication Date: 2025-12-03\nPaper ID: 2512.03653\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 734, "total_text_length": 822, "word_count": 124, "sentence_count": 8}, "basic_keywords": ["neural", "data", "method", "network", "training", "application", "three", "reasonable"], "domain_keywords": [], "technical_terms": ["Extrapolate", "Choose", "Retrain"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03643", "topic": "machine learning", "title": "Optical Context Compression Is Just (Bad) Autoencoding", "authors": ["Ivan Yee Lee", "Cheng Yang", "Taylor Berg-Kirkpatrick"], "first_author": "Ivan Yee Lee", "abstract": "DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.03643", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Optical Context Compression Is Just (Bad) Autoencoding\nResearch Topic: machine learning\nAuthors: Ivan Yee Lee, Cheng Yang, Taylor Berg-Kirkpatrick\nAbstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding\nCategories: cs.CV, cs.CL, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03643\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 54, "abstract_length": 1132, "total_text_length": 1187, "word_count": 145, "sentence_count": 8}, "basic_keywords": ["vision", "compression", "based", "language", "reconstruction", "modeling", "deepseek", "text"], "domain_keywords": [], "technical_terms": ["Code", "Comparing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03642", "topic": "machine learning", "title": "Evaluation of Foundational Machine Learned Interatomic Potentials for Migration Barrier Predictions", "authors": ["Achinthya Krishna Bheemaguli", "Penghao Xiao", "Gopalakrishnan Sai Gautam"], "first_author": "Achinthya Krishna Bheemaguli", "abstract": "Fast, and accurate prediction of ionic migration barriers () is crucial for designing next-generation battery materials that combine high energy density with facile ion transport. Given the computational costs associated with estimating using conventional density functional theory (DFT) based nudged elastic band (NEB) calculations, we benchmark the accuracy in and geometry predictions of five foundational machine learned interatomic potentials (MLIPs), which can potentially accelerate predictions of ionic transport. Specifically, we assess the accuracy of MACE-MP-0, Orb-v3, SevenNet, CHGNet, and M3GNet models, coupled with the NEB framework, against DFT-NEB-calculated across a diverse set of battery-relevant chemistries and structures. Notably, MACE-MP-0 and Orb-v3 exhibit the lowest mean absolute errors in predictions across the entire dataset and over data points that are not outliers, respectively. Importantly, Orb-v3 and SevenNet classify `good' versus `bad' ionic conductors with an accuracy of 82\\%, based on a threshold of 500~meV, indicating their utility in high-throughput screening approaches. Notably, intermediate images generated by MACE-MP-0 and SevenNet provide better initial guesses relative to conventional interpolation techniques in 71\\% of structures, offering a practical route to accelerate subsequent DFT-NEB relaxations. Finally, we observe that accurate predictions by MLIPs are not correlated with accurate (local) geometry predictions. Our work establishes the use-cases, accuracies, and limitations of foundational MLIPs in estimating and should serve as a base for accelerating the discovery of novel ionic conductors for batteries and beyond.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.03642", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Evaluation of Foundational Machine Learned Interatomic Potentials for Migration Barrier Predictions\nResearch Topic: machine learning\nAuthors: Achinthya Krishna Bheemaguli, Penghao Xiao, Gopalakrishnan Sai Gautam\nAbstract: Fast, and accurate prediction of ionic migration barriers ($E_m$) is crucial for designing next-generation battery materials that combine high energy density with facile ion transport. Given the computational costs associated with estimating $E_m$ using conventional density functional theory (DFT) based nudged elastic band (NEB) calculations, we benchmark the accuracy in $E_m$ and geometry predictions of five foundational machine learned interatomic potentials (MLIPs), which can potentially accelerate predictions of ionic transport. Specifically, we assess the accuracy of MACE-MP-0, Orb-v3, SevenNet, CHGNet, and M3GNet models, coupled with the NEB framework, against DFT-NEB-calculated $E_m$ across a diverse set of battery-relevant chemistries and structures. Notably, MACE-MP-0 and Orb-v3 exhibit the lowest mean absolute errors in $E_m$ predictions across the entire dataset and over data points that are not outliers, respectively. Importantly, Orb-v3 and SevenNet classify `good' versus `bad' ionic conductors with an accuracy of $>$82\\%, based on a threshold $E_m$ of 500~meV, indicating their utility in high-throughput screening approaches. Notably, intermediate images generated by MACE-MP-0 and SevenNet provide better initial guesses relative to conventional interpolation techniques in $>$71\\% of structures, offering a practical route to accelerate subsequent DFT-NEB relaxations. Finally, we observe that accurate $E_m$ predictions by MLIPs are not correlated with accurate (local) geometry predictions. Our work establishes the use-cases, accuracies, and limitations of foundational MLIPs in estimating $E_m$ and should serve as a base for accelerating the discovery of novel ionic conductors for batteries and beyond.\nCategories: cond-mat.mtrl-sci\nPublication Date: 2025-12-03\nPaper ID: 2512.03642\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 99, "abstract_length": 1688, "total_text_length": 1788, "word_count": 238, "sentence_count": 9}, "basic_keywords": ["predictions", "ionic", "accurate", "accuracy", "mlips", "mace", "sevennet", "battery"], "domain_keywords": [], "technical_terms": ["Notably", "Finally", "Importantly", "Fast", "Given", "Specifically"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03637", "topic": "machine learning", "title": "AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning", "authors": ["Kohei Yamamoto", "Kosuke Okusa"], "first_author": "Kohei Yamamoto", "abstract": "Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while na ve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.SD", "cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03637", "comments": "11 pages, 4 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning\nResearch Topic: machine learning\nAuthors: Kohei Yamamoto, Kosuke Okusa\nAbstract: Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while naïve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.\nCategories: cs.SD, cs.LG, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03637\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1665, "total_text_length": 1752, "word_count": 222, "sentence_count": 12}, "basic_keywords": ["frequency", "aliasing", "patch", "aape", "high", "across", "audio", "self"], "domain_keywords": [], "technical_terms": ["Nyquist", "Transformer", "Frequency", "Patch Embedding", "Complementary", "Aliasing"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03635", "topic": "machine learning", "title": "Formal Analysis of the Sigmoid Function and Formal Proof of the Universal Approximation Theorem", "authors": ["Dustin Bryant", "Jim Woodcock", "Simon Foster"], "first_author": "Dustin Bryant", "abstract": "This paper presents a formalized analysis of the sigmoid function and a fully mechanized proof of the Universal Approximation Theorem (UAT) in Isabelle/HOL, a higher-order logic theorem prover. The sigmoid function plays a fundamental role in neural networks; yet, its formal properties, such as differentiability, higher-order derivatives, and limit behavior, have not previously been comprehensively mechanized in a proof assistant. We present a rigorous formalization of the sigmoid function, proving its monotonicity, smoothness, and higher-order derivatives. We provide a constructive proof of the UAT, demonstrating that neural networks with sigmoidal activation functions can approximate any continuous function on a compact interval. Our work identifies and addresses gaps in Isabelle/HOL's formal proof libraries and introduces simpler methods for reasoning about the limits of real functions. By exploiting theorem proving for AI verification, our work enhances trust in neural networks and contributes to the broader goal of verified and trustworthy machine learning.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LO", "cs.SE"], "url": "http://arxiv.org/abs/2512.03635", "comments": "1 figure", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Formal Analysis of the Sigmoid Function and Formal Proof of the Universal Approximation Theorem\nResearch Topic: machine learning\nAuthors: Dustin Bryant, Jim Woodcock, Simon Foster\nAbstract: This paper presents a formalized analysis of the sigmoid function and a fully mechanized proof of the Universal Approximation Theorem (UAT) in Isabelle/HOL, a higher-order logic theorem prover. The sigmoid function plays a fundamental role in neural networks; yet, its formal properties, such as differentiability, higher-order derivatives, and limit behavior, have not previously been comprehensively mechanized in a proof assistant. We present a rigorous formalization of the sigmoid function, proving its monotonicity, smoothness, and higher-order derivatives. We provide a constructive proof of the UAT, demonstrating that neural networks with sigmoidal activation functions can approximate any continuous function on a compact interval. Our work identifies and addresses gaps in Isabelle/HOL's formal proof libraries and introduces simpler methods for reasoning about the limits of real functions. By exploiting theorem proving for AI verification, our work enhances trust in neural networks and contributes to the broader goal of verified and trustworthy machine learning.\nCategories: cs.LO, cs.SE\nPublication Date: 2025-12-03\nPaper ID: 2512.03635\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1078, "total_text_length": 1174, "word_count": 164, "sentence_count": 7}, "basic_keywords": ["function", "proof", "sigmoid", "theorem", "higher", "order", "neural", "networks"], "domain_keywords": [], "technical_terms": ["Universal Approximation Theorem", "Isabelle"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03610", "topic": "machine learning", "title": "CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion", "authors": ["Julius Lenz"], "first_author": "Julius Lenz", "abstract": "Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03610", "comments": "15 pages, 4 figures, 8 equations", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion\nResearch Topic: machine learning\nAuthors: Julius Lenz\nAbstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03610\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 608, "total_text_length": 701, "word_count": 94, "sentence_count": 5}, "basic_keywords": ["merging", "methods", "weight", "fisher", "across", "cogram", "loss", "optimization"], "domain_keywords": [], "technical_terms": ["Common", "Fisher", "Merging", "Contextual Granular Merging"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03606", "topic": "machine learning", "title": "Observation-driven correction of numerical weather prediction for marine winds", "authors": ["Matteo Peduto", "Qidong Yang", "Jonathan Giezendanner", "Devis Tuia", "Sherrie Wang"], "first_author": "Matteo Peduto", "abstract": "Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03606", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Observation-driven correction of numerical weather prediction for marine winds\nResearch Topic: machine learning\nAuthors: Matteo Peduto, Qidong Yang, Jonathan Giezendanner, Devis Tuia, Sherrie Wang\nAbstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03606\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1757, "total_text_length": 1836, "word_count": 238, "sentence_count": 10}, "basic_keywords": ["observations", "time", "wind", "ocean", "observation", "model", "forecast", "lead"], "domain_keywords": ["attention mechanisms"], "technical_terms": ["These", "International Comprehensive Ocean", "Atlantic Ocean", "Rather", "Global Forecast System", "Atmosphere Data Set", "Spatial", "Accurate"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03584", "topic": "machine learning", "title": "Federated Learning and Trajectory Compression for Enhanced AIS Coverage", "authors": ["Thomas Gräupl", "Andreas Reisenbauer", "Marcel Hecko", "Anil Rasouli", "Anita Graser", "Melitta Dragaschnig", "Axel Weissenfeld", "Gilles Dejaegere", "Mahmoud Sakr"], "first_author": "Thomas Gräupl", "abstract": "This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03584", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Federated Learning and Trajectory Compression for Enhanced AIS Coverage\nResearch Topic: machine learning\nAuthors: Thomas Gräupl, Andreas Reisenbauer, Marcel Hecko, Anil Rasouli, Anita Graser, Melitta Dragaschnig, Axel Weissenfeld, Gilles Dejaegere, Mahmoud Sakr\nAbstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03584\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 631, "total_text_length": 703, "word_count": 85, "sentence_count": 5}, "basic_keywords": ["vesseledge", "data", "system", "federated", "learning", "bandwidth", "trajectory", "compression"], "domain_keywords": [], "technical_terms": ["Preliminary"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.03579", "topic": "machine learning", "title": "Optimal Transportation and Alignment Between Gaussian Measures", "authors": ["Sanjit Dandapanthula", "Aleksandr Podkopaev", "Shiva Prasad Kasiviswanathan", "Aaditya Ramdas", "Ziv Goldfeld"], "first_author": "Sanjit Dandapanthula", "abstract": "Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "math.PR", "math.ST"], "url": "http://arxiv.org/abs/2512.03579", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.636786", "embedding_text": "Paper Title: Optimal Transportation and Alignment Between Gaussian Measures\nResearch Topic: machine learning\nAuthors: Sanjit Dandapanthula, Aleksandr Podkopaev, Shiva Prasad Kasiviswanathan, Aaditya Ramdas, Ziv Goldfeld\nAbstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.\nCategories: cs.LG, math.PR, math.ST\nPublication Date: 2025-12-03\nPaper ID: 2512.03579\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1375, "total_text_length": 1438, "word_count": 197, "sentence_count": 8}, "basic_keywords": ["gaussian", "quadratic", "alignment", "closed", "form", "provide", "frameworks", "heterogeneous"], "domain_keywords": [], "technical_terms": ["Hilbert", "Wasserstein", "Gaussian", "Because", "First", "Gaussians", "Optimal", "Gromov"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03564", "topic": "machine learning", "title": "Towards Irreversible Machine Unlearning for Diffusion Models", "authors": ["Xun Yuan", "Zilong Zhao", "Jiayu Li", "Aryan Pasikhani", "Prosanta Gope", "Biplab Sikdar"], "first_author": "Xun Yuan", "abstract": "Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.CR"], "url": "http://arxiv.org/abs/2512.03564", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Towards Irreversible Machine Unlearning for Diffusion Models\nResearch Topic: machine learning\nAuthors: Xun Yuan, Zilong Zhao, Jiayu Li, Aryan Pasikhani, Prosanta Gope, Biplab Sikdar\nAbstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.\nCategories: cs.LG, cs.CR\nPublication Date: 2025-12-03\nPaper ID: 2512.03564\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 60, "abstract_length": 1894, "total_text_length": 1955, "word_count": 259, "sentence_count": 11}, "basic_keywords": ["diffusion", "unlearning", "models", "machine", "methods", "data", "model", "dimra"], "domain_keywords": [], "technical_terms": ["Diffusion Model Unlearning", "However", "Diffusion Model Relearning Attack", "Memorization", "Among", "Without", "Unlike", "Current"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03537", "topic": "machine learning", "title": "Parameter-Efficient Augment Plugin for Class-Incremental Learning", "authors": ["Zhiming Xu", "Baile Xu", "Jian Zhao", "Furao Shen", "Suorong Yang"], "first_author": "Zhiming Xu", "abstract": "Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03537", "comments": "10 pages, 6 figures, 2 tables", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Parameter-Efficient Augment Plugin for Class-Incremental Learning\nResearch Topic: machine learning\nAuthors: Zhiming Xu, Baile Xu, Jian Zhao, Furao Shen, Suorong Yang\nAbstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.\nCategories: cs.LG, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03537\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1382, "total_text_length": 1448, "word_count": 190, "sentence_count": 13}, "basic_keywords": ["lora", "base", "model", "task", "approaches", "based", "replay", "knowledge"], "domain_keywords": ["imagenet"], "technical_terms": ["However", "Like", "Existing", "Components", "Deployment", "Moreover", "During", "Remarkably"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03525", "topic": "machine learning", "title": "Adaptive sampling using variational autoencoder and reinforcement learning", "authors": ["Adil Rasheed", "Mikael Aleksander Jansen Shahly", "Muhammad Faisal Aftab"], "first_author": "Adil Rasheed", "abstract": "Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03525", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Adaptive sampling using variational autoencoder and reinforcement learning\nResearch Topic: machine learning\nAuthors: Adil Rasheed, Mikael Aleksander Jansen Shahly, Muhammad Faisal Aftab\nAbstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03525\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 735, "total_text_length": 810, "word_count": 100, "sentence_count": 6}, "basic_keywords": ["sensing", "sparse", "sampling", "measurements", "reconstruction", "generative", "compressed", "bases"], "domain_keywords": [], "technical_terms": ["Optimal", "Experiments", "Generative", "Compressed"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03521", "topic": "machine learning", "title": "Cross-Space Synergy: A Unified Framework for Multimodal Emotion Recognition in Conversation", "authors": ["Xiaosen Lyu", "Jiayu Xiong", "Yuren Chen", "Wanlong Wang", "Xiaoqing Dai", "Jing Wang"], "first_author": "Xiaosen Lyu", "abstract": "Multimodal Emotion Recognition in Conversation (MERC) aims to predict speakers' emotions by integrating textual, acoustic, and visual cues. Existing approaches either struggle to capture complex cross-modal interactions or experience gradient conflicts and unstable training when using deeper architectures. To address these issues, we propose Cross-Space Synergy (CSS), which couples a representation component with an optimization component. Synergistic Polynomial Fusion (SPF) serves the representation role, leveraging low-rank tensor factorization to efficiently capture high-order cross-modal interactions. Pareto Gradient Modulator (PGM) serves the optimization role, steering updates along Pareto-optimal directions across competing objectives to alleviate gradient conflicts and improve stability. Experiments show that CSS outperforms existing representative methods on IEMOCAP and MELD in both accuracy and training stability, demonstrating its effectiveness in complex multimodal scenarios.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.MM", "cs.LG"], "url": "http://arxiv.org/abs/2512.03521", "comments": "Accepted to AAAI 2026", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Cross-Space Synergy: A Unified Framework for Multimodal Emotion Recognition in Conversation\nResearch Topic: machine learning\nAuthors: Xiaosen Lyu, Jiayu Xiong, Yuren Chen, Wanlong Wang, Xiaoqing Dai, Jing Wang\nAbstract: Multimodal Emotion Recognition in Conversation (MERC) aims to predict speakers' emotions by integrating textual, acoustic, and visual cues. Existing approaches either struggle to capture complex cross-modal interactions or experience gradient conflicts and unstable training when using deeper architectures. To address these issues, we propose Cross-Space Synergy (CSS), which couples a representation component with an optimization component. Synergistic Polynomial Fusion (SPF) serves the representation role, leveraging low-rank tensor factorization to efficiently capture high-order cross-modal interactions. Pareto Gradient Modulator (PGM) serves the optimization role, steering updates along Pareto-optimal directions across competing objectives to alleviate gradient conflicts and improve stability. Experiments show that CSS outperforms existing representative methods on IEMOCAP and MELD in both accuracy and training stability, demonstrating its effectiveness in complex multimodal scenarios.\nCategories: cs.MM, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03521\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1002, "total_text_length": 1094, "word_count": 133, "sentence_count": 7}, "basic_keywords": ["cross", "gradient", "multimodal", "existing", "capture", "complex", "modal", "interactions"], "domain_keywords": [], "technical_terms": ["Conversation", "Experiments", "Space Synergy", "Existing", "Cross", "Pareto", "Multimodal Emotion Recognition", "Synergistic Polynomial Fusion"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03494", "topic": "machine learning", "title": "A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention", "authors": ["Di Xiu", "Hongyin Tang", "Bolin Rong", "Lizhi Yan", "Jingang Wang", "Yifan Lu", "Xunliang Cai"], "first_author": "Di Xiu", "abstract": "Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top- Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top- Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top- Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top- Attention operations facilitates the further unlocking of Top- Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top- Attention, we investigate the impact of approximate Top- algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top- Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top- Decoding.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.03494", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention\nResearch Topic: machine learning\nAuthors: Di Xiu, Hongyin Tang, Bolin Rong, Lizhi Yan, Jingang Wang, Yifan Lu, Xunliang Cai\nAbstract: Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.\nCategories: cs.CL, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03494\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1829, "total_text_length": 1915, "word_count": 255, "sentence_count": 12}, "basic_keywords": ["attention", "decoding", "tasks", "downstream", "training", "performance", "entropy", "models"], "domain_keywords": [], "technical_terms": ["Experiments", "Finally", "Keys", "Furthermore", "Large Language Models", "Entropy", "Second", "Experimental"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03491", "topic": "machine learning", "title": "Modal Logical Neural Networks", "authors": ["Antonin Sulc"], "first_author": "Antonin Sulc", "abstract": "We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators and that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure. This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.LO", "cs.MA"], "url": "http://arxiv.org/abs/2512.03491", "comments": "27 pages, 10 figures, 7 tables", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Modal Logical Neural Networks\nResearch Topic: machine learning\nAuthors: Antonin Sulc\nAbstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\\Box$ and $\\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.   This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.\nCategories: cs.LG, cs.LO, cs.MA\nPublication Date: 2025-12-03\nPaper ID: 2512.03491\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 29, "abstract_length": 1506, "total_text_length": 1536, "word_count": 216, "sentence_count": 10}, "basic_keywords": ["logical", "modal", "framework", "learning", "neural", "mlnns", "semantics", "logic"], "domain_keywords": [], "technical_terms": ["Drawing", "Modal Logical Neural Networks", "Kripke", "These"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03485", "topic": "machine learning", "title": "CellScout: Visual Analytics for Mining Biomarkers in Cell State Discovery", "authors": ["Rui Sheng", "Zelin Zang", "Jiachen Wang", "Yan Luo", "Zixin Chen", "Yan Zhou", "Shaolun Ruan", "Huamin Qu"], "first_author": "Rui Sheng", "abstract": "Cell state discovery is crucial for understanding biological systems and enhancing medical outcomes. A key aspect of this process is identifying distinct biomarkers that define specific cell states. However, difficulties arise from the co-discovery process of cell states and biomarkers: biologists often use dimensionality reduction to visualize cells in a two- dimensional space. Then they usually interpret visually clustered cells as distinct states, from which they seek to identify unique biomarkers. However, this assumption is often invalid due to internal inconsistencies in a cluster, making the process trial-and-error and highly uncertain. Therefore, biologists urgently need effective tools to help uncover the hidden association relationships between different cell populations and their potential biomarkers. To address this problem, we first designed a machine-learning algorithm based on the Mixture-of-Experts (MoE) technique to identify meaningful associations between cell populations and biomarkers. We further developed a visual analytics system, CellScout, in collaboration with biologists, to help them explore and refine these association relationships to advance cell state discovery. We validated our system through expert interviews, from which we further selected a representative case to demonstrate its effectiveness in discovering new cell states.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.HC"], "url": "http://arxiv.org/abs/2512.03485", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: CellScout: Visual Analytics for Mining Biomarkers in Cell State Discovery\nResearch Topic: machine learning\nAuthors: Rui Sheng, Zelin Zang, Jiachen Wang, Yan Luo, Zixin Chen, Yan Zhou, Shaolun Ruan, Huamin Qu\nAbstract: Cell state discovery is crucial for understanding biological systems and enhancing medical outcomes. A key aspect of this process is identifying distinct biomarkers that define specific cell states. However, difficulties arise from the co-discovery process of cell states and biomarkers: biologists often use dimensionality reduction to visualize cells in a two- dimensional space. Then they usually interpret visually clustered cells as distinct states, from which they seek to identify unique biomarkers. However, this assumption is often invalid due to internal inconsistencies in a cluster, making the process trial-and-error and highly uncertain. Therefore, biologists urgently need effective tools to help uncover the hidden association relationships between different cell populations and their potential biomarkers. To address this problem, we first designed a machine-learning algorithm based on the Mixture-of-Experts (MoE) technique to identify meaningful associations between cell populations and biomarkers. We further developed a visual analytics system, CellScout, in collaboration with biologists, to help them explore and refine these association relationships to advance cell state discovery. We validated our system through expert interviews, from which we further selected a representative case to demonstrate its effectiveness in discovering new cell states.\nCategories: cs.HC\nPublication Date: 2025-12-03\nPaper ID: 2512.03485\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1379, "total_text_length": 1453, "word_count": 197, "sentence_count": 10}, "basic_keywords": ["cell", "biomarkers", "states", "discovery", "process", "biologists", "state", "distinct"], "domain_keywords": [], "technical_terms": ["Experts", "However", "Then", "Cell", "Mixture", "Therefore"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03477", "topic": "machine learning", "title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis", "authors": ["Zijian Gu", "Yuxi Liu", "Zhenhao Zhang", "Song Wang"], "first_author": "Zijian Gu", "abstract": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.03477", "comments": "10 pages, 3 tables", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis\nResearch Topic: machine learning\nAuthors: Zijian Gu, Yuxi Liu, Zhenhao Zhang, Song Wang\nAbstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03477\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1140, "total_text_length": 1224, "word_count": 143, "sentence_count": 10}, "basic_keywords": ["accuracy", "lora", "medical", "fairness", "optimization", "diagnostic", "disparities", "across"], "domain_keywords": ["medical imaging"], "technical_terms": ["Evaluated", "Vision", "Hybrid", "Ablation", "Rank Adaptation"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03475", "topic": "machine learning", "title": "Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression", "authors": ["Hongtao Hao", "Joseph L. Austerweil"], "first_author": "Hongtao Hao", "abstract": "Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "stat.AP"], "url": "http://arxiv.org/abs/2512.03475", "comments": "49 pages; Machine Learning for Health (ML4H) Symposium 2025", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression\nResearch Topic: machine learning\nAuthors: Hongtao Hao, Joseph L. Austerweil\nAbstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.\nCategories: cs.LG, stat.AP\nPublication Date: 2025-12-03\nPaper ID: 2512.03475\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1395, "total_text_length": 1487, "word_count": 210, "sentence_count": 8}, "basic_keywords": ["disease", "rankings", "progression", "single", "joint", "model", "ebms", "mixed"], "domain_keywords": [], "technical_terms": ["Joint Progression Model", "Terry", "Luce", "Finally", "Bradley", "Event", "Plackett", "Pairwise"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03471", "topic": "machine learning", "title": "SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening", "authors": ["Ian Henriques", "Lynda Elhassar", "Sarvesh Relekar", "Denis Walrave", "Shayan Hassantabar", "Vishu Ghanakota", "Adel Laoui", "Mahmoud Aich", "Rafia Tir", "Mohamed Zerguine", "Samir Louafi", "Moncef Kimouche", "Emmanuel Cosson", "Niraj K Jha"], "first_author": "Ian Henriques", "abstract": "The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "cs.CY"], "url": "http://arxiv.org/abs/2512.03471", "comments": "12 pages, 6 figures. Submitted to the IEEE Journal of Biomedical and Health Informatics", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening\nResearch Topic: machine learning\nAuthors: Ian Henriques, Lynda Elhassar, Sarvesh Relekar, Denis Walrave, Shayan Hassantabar, Vishu Ghanakota, Adel Laoui, Mahmoud Aich, Rafia Tir, Mohamed Zerguine, Samir Louafi, Moncef Kimouche, Emmanuel Cosson, Niraj K Jha\nAbstract: The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.\nCategories: cs.LG, cs.CY\nPublication Date: 2025-12-03\nPaper ID: 2512.03471\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 1305, "total_text_length": 1385, "word_count": 184, "sentence_count": 15}, "basic_keywords": ["type", "diabetes", "detection", "settings", "sweetdeep", "diabetic", "recordings", "patient"], "domain_keywords": [], "technical_terms": ["These", "Despite", "Each", "Samsung Galaxy Watch", "Advances", "Current", "Allowing"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03467", "topic": "machine learning", "title": "Bayesian Event-Based Model for Disease Subtype and Stage Inference", "authors": ["Hongtao Hao", "Joseph L. Austerweil"], "first_author": "Hongtao Hao", "abstract": "Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "stat.ME"], "url": "http://arxiv.org/abs/2512.03467", "comments": "32 pages; machine learning for health symposium (2025); Proceedings of the 5th Machine Learning for Health Symposium in PMLR", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Bayesian Event-Based Model for Disease Subtype and Stage Inference\nResearch Topic: machine learning\nAuthors: Hongtao Hao, Joseph L. Austerweil\nAbstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.\nCategories: cs.LG, stat.ME\nPublication Date: 2025-12-03\nPaper ID: 2512.03467\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1111, "total_text_length": 1178, "word_count": 172, "sentence_count": 10}, "basic_keywords": ["subtype", "sustain", "bebms", "across", "subtypes", "disease", "model", "data"], "domain_keywords": [], "technical_terms": ["Based Model", "Alzheimer", "Rather", "Further", "Subtype", "Bayesian", "Stage Inference Event", "Chronic"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03464", "topic": "machine learning", "title": "Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention", "authors": ["Yujing Liu", "Chen Yang"], "first_author": "Yujing Liu", "abstract": "In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03464", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention\nResearch Topic: machine learning\nAuthors: Yujing Liu, Chen Yang\nAbstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03464\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1529, "total_text_length": 1622, "word_count": 198, "sentence_count": 10}, "basic_keywords": ["financial", "sentiment", "modalities", "opinion", "opinions", "analysis", "market", "risk"], "domain_keywords": [], "technical_terms": ["Head Cross", "However", "Extensive", "These", "While", "Financial Multi", "Chinese", "Transformer"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03458", "topic": "machine learning", "title": "A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses", "authors": ["Maryam Maghsoudi", "Mohsen Rezaeizadeh", "Shihab Shamma"], "first_author": "Maryam Maghsoudi", "abstract": "Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "url": "http://arxiv.org/abs/2512.03458", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses\nResearch Topic: machine learning\nAuthors: Maryam Maghsoudi, Mohsen Rezaeizadeh, Shihab Shamma\nAbstract: Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.\nCategories: eess.SP, cs.LG, cs.SD, eess.AS\nPublication Date: 2025-12-03\nPaper ID: 2512.03458\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1189, "total_text_length": 1279, "word_count": 166, "sentence_count": 8}, "basic_keywords": ["imagined", "responses", "neural", "listened", "speech", "limited", "brain", "specific"], "domain_keywords": [], "technical_terms": ["Using", "Magnetoencephalography", "Decoding"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03450", "topic": "machine learning", "title": "KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models", "authors": ["Rhys Newbury", "Juyan Zhang", "Tin Tran", "Hanna Kurniawati", "Dana Kulić"], "first_author": "Rhys Newbury", "abstract": "Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.03450", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637287", "embedding_text": "Paper Title: KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models\nResearch Topic: machine learning\nAuthors: Rhys Newbury, Juyan Zhang, Tin Tran, Hanna Kurniawati, Dana Kulić\nAbstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03450\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 79, "abstract_length": 956, "total_text_length": 1036, "word_count": 136, "sentence_count": 7}, "basic_keywords": ["unsupervised", "keypoint", "keypoints", "structure", "generative", "point", "across", "object"], "domain_keywords": ["computer vision"], "technical_terms": ["These", "Elucidated Diffusion Model", "Most", "Understanding"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03440", "topic": "machine learning", "title": "An Analysis of LIGO Glitches Using t-SNE During the First Part of the Fourth LIGO-Virgo-KAGRA Observing Run", "authors": ["Tabata Aira Ferreira", "Gabriela González", "Osvaldo Salas"], "first_author": "Tabata Aira Ferreira", "abstract": "This paper presents an analysis of noise transients observed in LIGO data during the first part of the fourth observing run, using the unsupervised machine learning technique t-distributed Stochastic Neighbor Embedding (t-SNE) to examine the behavior of glitch groups. Based on the t-SNE output, we apply Agglomerative Clustering in combination with the Silhouette Score to determine the optimal number of groups. We then track these groups over time and investigate correlations between their occurrence and environmental or instrumental conditions. At the Livingston observatory, the most common glitches during O4a were seasonal and associated with ground motion, whereas at Hanford, the most prevalent glitches were related to instrumental conditions.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["astro-ph.IM", "gr-qc"], "url": "http://arxiv.org/abs/2512.03440", "comments": "30 pages, 26 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: An Analysis of LIGO Glitches Using t-SNE During the First Part of the Fourth LIGO-Virgo-KAGRA Observing Run\nResearch Topic: machine learning\nAuthors: Tabata Aira Ferreira, Gabriela González, Osvaldo Salas\nAbstract: This paper presents an analysis of noise transients observed in LIGO data during the first part of the fourth observing run, using the unsupervised machine learning technique t-distributed Stochastic Neighbor Embedding (t-SNE) to examine the behavior of glitch groups. Based on the t-SNE output, we apply Agglomerative Clustering in combination with the Silhouette Score to determine the optimal number of groups. We then track these groups over time and investigate correlations between their occurrence and environmental or instrumental conditions. At the Livingston observatory, the most common glitches during O4a were seasonal and associated with ground motion, whereas at Hanford, the most prevalent glitches were related to instrumental conditions.\nCategories: astro-ph.IM, gr-qc\nPublication Date: 2025-12-03\nPaper ID: 2512.03440\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 107, "abstract_length": 755, "total_text_length": 863, "word_count": 125, "sentence_count": 5}, "basic_keywords": ["groups", "during", "instrumental", "conditions", "most", "glitches", "paper", "presents"], "domain_keywords": [], "technical_terms": ["Based", "Silhouette Score", "Hanford", "Stochastic Neighbor Embedding", "Agglomerative Clustering", "Livingston"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03437", "topic": "machine learning", "title": "Grokked Models are Better Unlearners", "authors": ["Yuanbang Liang", "Yang Li"], "first_author": "Yuanbang Liang", "abstract": "Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03437", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Grokked Models are Better Unlearners\nResearch Topic: machine learning\nAuthors: Yuanbang Liang, Yang Li\nAbstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03437\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 36, "abstract_length": 1273, "total_text_length": 1310, "word_count": 178, "sentence_count": 10}, "basic_keywords": ["unlearning", "grokking", "more", "after", "model", "training", "data", "without"], "domain_keywords": ["imagenet"], "technical_terms": ["Analyses", "Starting", "Grokking"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03428", "topic": "machine learning", "title": "GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test", "authors": ["Ziyi Ding", "Xiao-Ping Zhang"], "first_author": "Ziyi Ding", "abstract": "We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03428", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test\nResearch Topic: machine learning\nAuthors: Ziyi Ding, Xiao-Ping Zhang\nAbstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.\nCategories: cs.LG, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03428\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 989, "total_text_length": 1065, "word_count": 138, "sentence_count": 6}, "basic_keywords": ["lingam", "gaussianity", "tests", "noise", "independence", "gaussdetect", "causal", "equivalence"], "domain_keywords": [], "technical_terms": ["Gaussianity", "Experimental", "Under"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03419", "topic": "machine learning", "title": "Comparative algorithm performance evaluation and prediction for the maximum clique problem using instance space analysis", "authors": ["Bharat Sharman", "Elkafi Hassini"], "first_author": "Bharat Sharman", "abstract": "The maximum clique problem, a well-known graph-based combinatorial optimization problem, has been addressed through various algorithmic approaches, though systematic analyses of the problem instances remain sparse. This study employs the instance space analysis (ISA) methodology to systematically analyze the instance space of this problem and assess & predict the performance of state-of-the-art (SOTA) algorithms, including exact, heuristic, and graph neural network (GNN)-based methods. A dataset was compiled using graph instances from TWITTER, COLLAB and IMDB-BINARY benchmarks commonly used in graph machine learning research. A set of 33 generic and 2 problem-specific polynomial-time-computable graph-based features, including several spectral properties, was employed for the ISA. A composite performance mea- sure incorporating both solution quality and algorithm runtime was utilized. The comparative analysis demonstrated that the exact algorithm Mixed Order Maximum Clique (MOMC) exhib- ited superior performance across approximately 74.7% of the instance space constituted by the compiled dataset. Gurobi & CliSAT accounted for superior performance in 13.8% and 11% of the instance space, respectively. The ISA-based algorithm performance prediction model run on 34 challenging test instances compiled from the BHOSLIB and DIMACS datasets yielded top-1 and top-2 best performing algorithm prediction accuracies of 88% and 97%, respectively.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.DS", "cs.DM", "cs.LG"], "url": "http://arxiv.org/abs/2512.03419", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Comparative algorithm performance evaluation and prediction for the maximum clique problem using instance space analysis\nResearch Topic: machine learning\nAuthors: Bharat Sharman, Elkafi Hassini\nAbstract: The maximum clique problem, a well-known graph-based combinatorial optimization problem, has been addressed through various algorithmic approaches, though systematic analyses of the problem instances remain sparse. This study employs the instance space analysis (ISA) methodology to systematically analyze the instance space of this problem and assess & predict the performance of state-of-the-art (SOTA) algorithms, including exact, heuristic, and graph neural network (GNN)-based methods. A dataset was compiled using graph instances from TWITTER, COLLAB and IMDB-BINARY benchmarks commonly used in graph machine learning research. A set of 33 generic and 2 problem-specific polynomial-time-computable graph-based features, including several spectral properties, was employed for the ISA. A composite performance mea- sure incorporating both solution quality and algorithm runtime was utilized. The comparative analysis demonstrated that the exact algorithm Mixed Order Maximum Clique (MOMC) exhib- ited superior performance across approximately 74.7% of the instance space constituted by the compiled dataset. Gurobi & CliSAT accounted for superior performance in 13.8% and 11% of the instance space, respectively. The ISA-based algorithm performance prediction model run on 34 challenging test instances compiled from the BHOSLIB and DIMACS datasets yielded top-1 and top-2 best performing algorithm prediction accuracies of 88% and 97%, respectively.\nCategories: cs.DS, cs.DM, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03419\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 120, "abstract_length": 1455, "total_text_length": 1576, "word_count": 210, "sentence_count": 11}, "basic_keywords": ["problem", "graph", "performance", "based", "instance", "space", "algorithm", "instances"], "domain_keywords": [], "technical_terms": ["Gurobi", "Mixed Order Maximum Clique"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03399", "topic": "machine learning", "title": "Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value", "authors": ["Joe Edelman", "Tan Zhi-Xuan", "Ryan Lowe", "Oliver Klingefjord", "Vincent Wang-Mascianica", "Matija Franklin", "Ryan Othniel Kearns", "Ellie Hain", "Atrisha Sarkar", "Michiel Bakker", "Fazl Barez", "David Duvenaud", "Jakob Foerster", "Iason Gabriel", "Joseph Gubbels", "Bryce Goodman", "Andreas Haupt", "Jobst Heitzig", "Julian Jara-Ettinger", "Atoosa Kasirzadeh", "James Ravi Kirkpatrick", "Andrew Koh", "W. Bradley Knox", "Philipp Koralus", "Joel Lehman", "Sydney Levine", "Samuele Marro", "Manon Revel", "Toby Shorin", "Morgan Sutherland", "Michael Henry Tessler", "Ivan Vendrov", "James Wilken-Smith"], "first_author": "Joe Edelman", "abstract": "Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03399", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value\nResearch Topic: machine learning\nAuthors: Joe Edelman, Tan Zhi-Xuan, Ryan Lowe, Oliver Klingefjord, Vincent Wang-Mascianica, Matija Franklin, Ryan Othniel Kearns, Ellie Hain, Atrisha Sarkar, Michiel Bakker, Fazl Barez, David Duvenaud, Jakob Foerster, Iason Gabriel, Joseph Gubbels, Bryce Goodman, Andreas Haupt, Jobst Heitzig, Julian Jara-Ettinger, Atoosa Kasirzadeh, James Ravi Kirkpatrick, Andrew Koh, W. Bradley Knox, Philipp Koralus, Joel Lehman, Sydney Levine, Samuele Marro, Manon Revel, Toby Shorin, Morgan Sutherland, Michael Henry Tessler, Ivan Vendrov, James Wilken-Smith\nAbstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03399\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1404, "total_text_length": 1485, "word_count": 212, "sentence_count": 10}, "basic_keywords": ["values", "systems", "individual", "other", "institutions", "value", "outcomes", "intentions"], "domain_keywords": [], "technical_terms": ["Beneficial", "They", "Even", "These"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03393", "topic": "machine learning", "title": "Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization", "authors": ["Lakshmi Jayalal", "Sheetal Kalyani"], "first_author": "Lakshmi Jayalal", "abstract": "Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a \"momentum-like\" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03393", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization\nResearch Topic: machine learning\nAuthors: Lakshmi Jayalal, Sheetal Kalyani\nAbstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a \"momentum-like\" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.\nCategories: cs.LG, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03393\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1356, "total_text_length": 1457, "word_count": 189, "sentence_count": 8}, "basic_keywords": ["sparse", "multiple", "measurement", "vectors", "tuning", "methods", "like", "prior"], "domain_keywords": [], "technical_terms": ["Implicit Regularization", "Recovering", "Additionally", "Underdetermined System Solver"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03375", "topic": "machine learning", "title": "MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems", "authors": ["Mahdi Arab Loodaricheh", "Mohammad Hossein Manshaei", "Anita Raja"], "first_author": "Mahdi Arab Loodaricheh", "abstract": "Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03375", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems\nResearch Topic: machine learning\nAuthors: Mahdi Arab Loodaricheh, Mohammad Hossein Manshaei, Anita Raja\nAbstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.\nCategories: cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03375\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 968, "total_text_length": 1043, "word_count": 132, "sentence_count": 6}, "basic_keywords": ["detection", "mage", "multimodal", "intrusion", "data", "attack", "generative", "augmentation"], "domain_keywords": [], "technical_terms": ["While", "Intrusion Detection", "Evaluations", "Multimodal Attack Generator", "Transformer", "Modern Intrusion Detection Systems"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03363", "topic": "machine learning", "title": "A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning", "authors": ["Shanika Iroshi Nanayakkara", "Shiva Raj Pokhrel"], "first_author": "Shanika Iroshi Nanayakkara", "abstract": "Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "quant-ph"], "url": "http://arxiv.org/abs/2512.03363", "comments": "8 pages, 4 figures, QCNC 2026", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning\nResearch Topic: machine learning\nAuthors: Shanika Iroshi Nanayakkara, Shiva Raj Pokhrel\nAbstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.\nCategories: cs.LG, quant-ph\nPublication Date: 2025-12-03\nPaper ID: 2512.03363\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1062, "total_text_length": 1137, "word_count": 147, "sentence_count": 6}, "basic_keywords": ["quantum", "classical", "aggregation", "gain", "federated", "heterogeneous", "client", "teleportation"], "domain_keywords": [], "technical_terms": ["Two Gains", "Experiments", "Adaptive Aggregation", "Classical", "Federated"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03358", "topic": "machine learning", "title": "Scaling Trust in Quantum Federated Learning: A Multi-Protocol Privacy Design", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "first_author": "Dev Gurung", "abstract": "Quantum Federated Learning (QFL) promises to revolutionize distributed machine learning by combining the computational power of quantum devices with collaborative model training. Yet, privacy of both data and models remains a critical challenge. In this work, we propose a privacy-preserving QFL framework where a network of quantum devices trains local models and transmits them to a central server under a multi-layered privacy protocol. Our design leverages Singular Value Decomposition (SVD), Quantum Key Distribution (QKD), and Analytic Quantum Gradient Descent (AQGD) to secure data preparation, model sharing, and training stages. Through theoretical analysis and experiments on contemporary quantum platforms and datasets, we demonstrate that the framework robustly safeguards data and model confidentiality while maintaining training efficiency.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CR"], "url": "http://arxiv.org/abs/2512.03358", "comments": "Under Review", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Scaling Trust in Quantum Federated Learning: A Multi-Protocol Privacy Design\nResearch Topic: machine learning\nAuthors: Dev Gurung, Shiva Raj Pokhrel\nAbstract: Quantum Federated Learning (QFL) promises to revolutionize distributed machine learning by combining the computational power of quantum devices with collaborative model training. Yet, privacy of both data and models remains a critical challenge. In this work, we propose a privacy-preserving QFL framework where a network of $n$ quantum devices trains local models and transmits them to a central server under a multi-layered privacy protocol. Our design leverages Singular Value Decomposition (SVD), Quantum Key Distribution (QKD), and Analytic Quantum Gradient Descent (AQGD) to secure data preparation, model sharing, and training stages. Through theoretical analysis and experiments on contemporary quantum platforms and datasets, we demonstrate that the framework robustly safeguards data and model confidentiality while maintaining training efficiency.\nCategories: cs.CR\nPublication Date: 2025-12-03\nPaper ID: 2512.03358\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 854, "total_text_length": 931, "word_count": 125, "sentence_count": 6}, "basic_keywords": ["quantum", "model", "training", "privacy", "data", "learning", "devices", "models"], "domain_keywords": [], "technical_terms": ["Quantum Federated Learning", "Analytic Quantum Gradient Descent", "Singular Value Decomposition", "Through", "Quantum Key Distribution"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03354", "topic": "machine learning", "title": "Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions", "authors": ["Hongseon Yeom", "Jaeyoul Shin", "Soojin Min", "Jeongmin Yoon", "Seunghak Yu", "Dongyeop Kang"], "first_author": "Hongseon Yeom", "abstract": "Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03354", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions\nResearch Topic: machine learning\nAuthors: Hongseon Yeom, Jaeyoul Shin, Soojin Min, Jeongmin Yoon, Seunghak Yu, Dongyeop Kang\nAbstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.\nCategories: cs.LG, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03354\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1723, "total_text_length": 1815, "word_count": 247, "sentence_count": 12}, "basic_keywords": ["online", "auctions", "deterministic", "model", "propensity", "standard", "policies", "evaluation"], "domain_keywords": [], "technical_terms": ["However", "Online", "Policy Evaluation", "Normalized Inverse Propensity Scoring", "Mean Directional Accuracy", "Self"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03335", "topic": "machine learning", "title": "Step-by-step Layered Design Generation", "authors": ["Faizan Farooq Khan", "K J Joseph", "Koustava Goswami", "Mohamed Elhoseiny", "Balaji Vasan Srinivasan"], "first_author": "Faizan Farooq Khan", "abstract": "Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["cs.CV", "cs.LG"], "url": "http://arxiv.org/abs/2512.03335", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Step-by-step Layered Design Generation\nResearch Topic: machine learning\nAuthors: Faizan Farooq Khan, K J Joseph, Koustava Goswami, Mohamed Elhoseiny, Balaji Vasan Srinivasan\nAbstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.\nCategories: cs.CV, cs.LG\nPublication Date: 2025-12-03\nPaper ID: 2512.03335\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 38, "abstract_length": 1177, "total_text_length": 1216, "word_count": 170, "sentence_count": 8}, "basic_keywords": ["step", "design", "generation", "problem", "layered", "process", "work", "approaches"], "domain_keywords": [], "technical_terms": ["Design", "Despite", "Step", "Step Layered Design Generation", "Leveraging"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03333", "topic": "machine learning", "title": "Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State", "authors": ["Xun Tang", "Haoxuan Chen", "Yuehaw Khoo", "Lexing Ying"], "first_author": "Xun Tang", "abstract": "We introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["quant-ph", "math.NA", "math.ST", "physics.comp-ph", "stat.ML"], "url": "http://arxiv.org/abs/2512.03333", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State\nResearch Topic: machine learning\nAuthors: Xun Tang, Haoxuan Chen, Yuehaw Khoo, Lexing Ying\nAbstract: We introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation.\nCategories: quant-ph, math.NA, math.ST, physics.comp-ph, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03333\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1095, "total_text_length": 1168, "word_count": 173, "sentence_count": 8}, "basic_keywords": ["procedure", "quantum", "state", "observable", "estimation", "tomography", "matrix", "show"], "domain_keywords": [], "technical_terms": ["Sketch Tomography"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6000000000000001}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.03325", "topic": "machine learning", "title": "When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling", "authors": ["Garrett G. Wen", "Hong Hu", "Yue M. Lu", "Zhou Fan", "Theodor Misiakiewicz"], "first_author": "Garrett G. Wen", "abstract": "A major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes. We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails. Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["math.ST", "cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03325", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling\nResearch Topic: machine learning\nAuthors: Garrett G. Wen, Hong Hu, Yue M. Lu, Zhou Fan, Theodor Misiakiewicz\nAbstract: A major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes.   We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails.   Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM.\nCategories: math.ST, cs.LG, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03325\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 119, "abstract_length": 1732, "total_text_length": 1852, "word_count": 272, "sentence_count": 11}, "basic_keywords": ["dimensional", "gaussian", "high", "scaling", "models", "analysis", "quadratic", "model"], "domain_keywords": [], "technical_terms": ["Beyond", "Conditional Gaussian Equivalent", "Despite", "Gaussian", "Wiener", "Lindeberg"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03322", "topic": "machine learning", "title": "SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models", "authors": ["Geoffrey J. McLachlan", "Jinran Wu"], "first_author": "Geoffrey J. McLachlan", "abstract": "Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.", "publish_date": "2025-12-03", "update_date": "2025-12-03", "categories": ["stat.CO", "stat.ME", "stat.ML"], "url": "http://arxiv.org/abs/2512.03322", "comments": "15 pages, 1 figure", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models\nResearch Topic: machine learning\nAuthors: Geoffrey J. McLachlan, Jinran Wu\nAbstract: Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \\textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.\nCategories: stat.CO, stat.ME, stat.ML\nPublication Date: 2025-12-03\nPaper ID: 2512.03322\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 109, "abstract_length": 1713, "total_text_length": 1823, "word_count": 258, "sentence_count": 10}, "basic_keywords": ["missingness", "labelled", "missing", "random", "component", "supervised", "learning", "labels"], "domain_keywords": [], "technical_terms": ["Expectation", "Parameters", "Bayes", "Gaussian", "Semi", "Most", "Conditional Maximisation"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03312", "topic": "machine learning", "title": "Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time", "authors": ["Daniel D. Richman", "Jessica Karaguesian", "Carl-Mikael Suomivuori", "Ron O. Dror"], "first_author": "Daniel D. Richman", "abstract": "The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or \"conformations.\" Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models -- whether trained for static structure prediction or conformational generation -- to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["q-bio.BM", "cs.LG"], "url": "http://arxiv.org/abs/2512.03312", "comments": "Project page: https://github.com/drorlab/conformix", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time\nResearch Topic: machine learning\nAuthors: Daniel D. Richman, Jessica Karaguesian, Carl-Mikael Suomivuori, Ron O. Dror\nAbstract: The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or \"conformations.\" Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models -- whether trained for static structure prediction or conformational generation -- to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method.\nCategories: q-bio.BM, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03312\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1288, "total_text_length": 1382, "word_count": 180, "sentence_count": 8}, "basic_keywords": ["static", "structure", "conformix", "conformational", "model", "proteins", "conformations", "distribution"], "domain_keywords": [], "technical_terms": ["Researchers", "Case", "Remarkably", "Boltzmann"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03310", "topic": "machine learning", "title": "Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs", "authors": ["Kunj Joshi", "David A. Smith"], "first_author": "Kunj Joshi", "abstract": "The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CL", "cs.CR", "cs.LG"], "url": "http://arxiv.org/abs/2512.03310", "comments": "To be submitted for ICML 2026", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs\nResearch Topic: machine learning\nAuthors: Kunj Joshi, David A. Smith\nAbstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.\nCategories: cs.CL, cs.CR, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03310\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 880, "total_text_length": 968, "word_count": 130, "sentence_count": 8}, "basic_keywords": ["models", "privacy", "fine", "tuning", "rmft", "memorization", "language", "while"], "domain_keywords": [], "technical_terms": ["Using", "Tuning", "Seen Extraction Rate", "Pareto", "Large Language Models", "Total Extraction Rate", "Enron Email Dataset", "Deduplication"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03296", "topic": "machine learning", "title": "Associating Healthcare Teamwork with Patient Outcomes for Predictive Analysis", "authors": ["Hsiao-Ying Lu", "Kwan-Liu Ma"], "first_author": "Hsiao-Ying Lu", "abstract": "Cancer treatment outcomes are influenced not only by clinical and demographic factors but also by the collaboration of healthcare teams. However, prior work has largely overlooked the potential role of human collaboration in shaping patient survival. This paper presents an applied AI approach to uncovering the impact of healthcare professionals' (HCPs) collaboration-captured through electronic health record (EHR) systems-on cancer patient outcomes. We model EHR-mediated HCP interactions as networks and apply machine learning techniques to detect predictive signals of patient survival embedded in these collaborations. Our models are cross validated to ensure generalizability, and we explain the predictions by identifying key network traits associated with improved outcomes. Importantly, clinical experts and literature validate the relevance of the identified crucial collaboration traits, reinforcing their potential for real-world applications. This work contributes to a practical workflow for leveraging digital traces of collaboration and AI to assess and improve team-based healthcare. The approach is potentially transferable to other domains involving complex collaboration and offers actionable insights to support data-informed interventions in healthcare delivery.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.SI", "cs.CY", "cs.LG"], "url": "http://arxiv.org/abs/2512.03296", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Associating Healthcare Teamwork with Patient Outcomes for Predictive Analysis\nResearch Topic: machine learning\nAuthors: Hsiao-Ying Lu, Kwan-Liu Ma\nAbstract: Cancer treatment outcomes are influenced not only by clinical and demographic factors but also by the collaboration of healthcare teams. However, prior work has largely overlooked the potential role of human collaboration in shaping patient survival. This paper presents an applied AI approach to uncovering the impact of healthcare professionals' (HCPs) collaboration-captured through electronic health record (EHR) systems-on cancer patient outcomes. We model EHR-mediated HCP interactions as networks and apply machine learning techniques to detect predictive signals of patient survival embedded in these collaborations. Our models are cross validated to ensure generalizability, and we explain the predictions by identifying key network traits associated with improved outcomes. Importantly, clinical experts and literature validate the relevance of the identified crucial collaboration traits, reinforcing their potential for real-world applications. This work contributes to a practical workflow for leveraging digital traces of collaboration and AI to assess and improve team-based healthcare. The approach is potentially transferable to other domains involving complex collaboration and offers actionable insights to support data-informed interventions in healthcare delivery.\nCategories: cs.SI, cs.CY, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03296\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1285, "total_text_length": 1363, "word_count": 178, "sentence_count": 9}, "basic_keywords": ["collaboration", "healthcare", "outcomes", "patient", "cancer", "clinical", "work", "potential"], "domain_keywords": [], "technical_terms": ["However", "Importantly", "Cancer"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03290", "topic": "machine learning", "title": "ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics", "authors": ["Julian Evan Chrisnanto", "Nurfauzi Fadillah", "Yulison Herry Chrisnanto"], "first_author": "Julian Evan Chrisnanto", "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "physics.app-ph"], "url": "http://arxiv.org/abs/2512.03290", "comments": "15 pages, 7 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics\nResearch Topic: machine learning\nAuthors: Julian Evan Chrisnanto, Nurfauzi Fadillah, Yulison Herry Chrisnanto\nAbstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.\nCategories: cs.LG, physics.app-ph\nPublication Date: 2025-12-02\nPaper ID: 2512.03290\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1915, "total_text_length": 1996, "word_count": 265, "sentence_count": 13}, "basic_keywords": ["spectral", "aspen", "physics", "standard", "adaptive", "solution", "physical", "pinns"], "domain_keywords": [], "technical_terms": ["However", "Landau", "Enabled Network", "Fourier", "Physics", "Furthermore", "Adaptive Spectral Physics", "Informed Neural Networks"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03287", "topic": "machine learning", "title": "Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors", "authors": ["Dario Fenoglio", "Mohan Li", "Davide Casnici", "Matias Laporte", "Shkurta Gashi", "Silvia Santini", "Martin Gjoreski", "Marc Langheinrich"], "first_author": "Dario Fenoglio", "abstract": "Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.DC"], "url": "http://arxiv.org/abs/2512.03287", "comments": "8 pages, 2024 International Conference on Intelligent Environments (IE), 2024", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors\nResearch Topic: machine learning\nAuthors: Dario Fenoglio, Mohan Li, Davide Casnici, Matias Laporte, Shkurta Gashi, Silvia Santini, Martin Gjoreski, Marc Langheinrich\nAbstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.\nCategories: cs.LG, cs.DC\nPublication Date: 2025-12-02\nPaper ID: 2512.03287\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 867, "total_text_length": 957, "word_count": 125, "sentence_count": 9}, "basic_keywords": ["frequency", "traditional", "centralized", "user", "data", "privacy", "multi", "learning"], "domain_keywords": [], "technical_terms": ["Federated Learning", "Human Activity Recognition", "Results", "Traditional"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03276", "topic": "machine learning", "title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval", "authors": ["Constantin Venhoff", "Ashkan Khakzar", "Sonia Joseph", "Philip Torr", "Neel Nanda"], "first_author": "Constantin Venhoff", "abstract": "Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03276", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval\nResearch Topic: machine learning\nAuthors: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda\nAbstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.\nCategories: cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03276\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1786, "total_text_length": 1871, "word_count": 266, "sentence_count": 10}, "basic_keywords": ["vlms", "factual", "representations", "recall", "models", "entity", "visual", "performance"], "domain_keywords": [], "technical_terms": ["However", "Finally", "Cross", "Training", "More", "Attention", "Native"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03254", "topic": "machine learning", "title": "Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection", "authors": ["Philippe A. Boileau", "Hani Zaki", "Gabriele Lileikyte", "Niklas Nielsen", "Patrick R. Lawler", "Mireille E. Schnitzer"], "first_author": "Philippe A. Boileau", "abstract": "The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["stat.ME"], "url": "http://arxiv.org/abs/2512.03254", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection\nResearch Topic: machine learning\nAuthors: Philippe A. Boileau, Hani Zaki, Gabriele Lileikyte, Niklas Nielsen, Patrick R. Lawler, Mireille E. Schnitzer\nAbstract: The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients.\nCategories: stat.ME\nPublication Date: 2025-12-02\nPaper ID: 2512.03254\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1516, "total_text_length": 1609, "word_count": 208, "sentence_count": 11}, "basic_keywords": ["treatment", "effect", "assumption", "cate", "homogeneous", "under", "inference", "about"], "domain_keywords": [], "technical_terms": ["These", "Numerical", "Under", "Uncovering", "Severe"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.03243", "topic": "machine learning", "title": "Novelty detection on path space", "authors": ["Ioannis Gasteratos", "Antoine Jacquier", "Maud Lemercier", "Terry Lyons", "Cristopher Salvi"], "first_author": "Ioannis Gasteratos", "abstract": "We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type- error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type- error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "url": "http://arxiv.org/abs/2512.03243", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Novelty detection on path space\nResearch Topic: machine learning\nAuthors: Ioannis Gasteratos, Antoine Jacquier, Maud Lemercier, Terry Lyons, Cristopher Salvi\nAbstract: We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.\nCategories: stat.ML, cs.LG, math.PR, math.ST\nPublication Date: 2025-12-02\nPaper ID: 2512.03243\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 31, "abstract_length": 1001, "total_text_length": 1033, "word_count": 145, "sentence_count": 6}, "basic_keywords": ["bounds", "smooth", "signature", "based", "test", "using", "cvar", "type"], "domain_keywords": [], "technical_terms": ["Using", "Jacquier", "Finally", "Exploiting", "Gaussian", "Gasteratos"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.03234", "topic": "machine learning", "title": "Iterative Tilting for Diffusion Fine-Tuning", "authors": ["Jean Pachebat", "Giovanni Conforti", "Alain Durmus", "Yazid Janati"], "first_author": "Jean Pachebat", "abstract": "We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt into sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/abs/2512.03234", "comments": "14 pages", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Iterative Tilting for Diffusion Fine-Tuning\nResearch Topic: machine learning\nAuthors: Jean Pachebat, Giovanni Conforti, Alain Durmus, Yazid Janati\nAbstract: We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt $\\exp(λr)$ into $N$ sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.\nCategories: stat.ML, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03234\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 519, "total_text_length": 563, "word_count": 75, "sentence_count": 5}, "basic_keywords": ["reward", "method", "tilted", "introduce", "iterative", "tilting", "gradient", "free"], "domain_keywords": [], "technical_terms": ["Taylor", "Gaussian"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03225", "topic": "machine learning", "title": "Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both", "authors": ["Christophe Andrieu", "Nicolas Chopin", "Ettore Fincato", "Mathieu Gerber"], "first_author": "Christophe Andrieu", "abstract": "We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["stat.CO", "cs.LG", "stat.ML"], "url": "http://arxiv.org/abs/2512.03225", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both\nResearch Topic: machine learning\nAuthors: Christophe Andrieu, Nicolas Chopin, Ettore Fincato, Mathieu Gerber\nAbstract: We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.\nCategories: stat.CO, cs.LG, stat.ML\nPublication Date: 2025-12-02\nPaper ID: 2512.03225\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 117, "abstract_length": 1018, "total_text_length": 1136, "word_count": 167, "sentence_count": 7}, "basic_keywords": ["convergence", "algorithms", "objective", "function", "smooth", "obtained", "under", "gradient"], "domain_keywords": [], "technical_terms": [], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.4}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04125", "topic": "machine learning", "title": "ASCIIBench: Evaluating Language-Model-Based Understanding of Visually-Oriented Text", "authors": ["Kerry Luo", "Michael Fu", "Joshua Peguero", "Husnain Malik", "Anvay Patil", "Joyce Lin", "Megan Van Overborg", "Ryan Sarmiento", "Kevin Zhu"], "first_author": "Kerry Luo", "abstract": "Large language models (LLMs) have demonstrated several emergent behaviors with scale, including reasoning and fluency in long-form text generation. However, they continue to struggle with tasks requiring precise spatial and positional reasoning. ASCII art, a symbolic medium where characters encode structure and form, provides a unique probe of this limitation. We introduce ASCIIBench, a novel benchmark for evaluating both the generation and classification of ASCII-text images. ASCIIBench consists of a filtered dataset of 5,315 class-labeled ASCII images and is, to our knowledge, the first publicly available benchmark of its kind. Alongside the dataset, we release weights for a fine-tuned CLIP model adapted to capture ASCII structure, enabling the evaluation of LLM-generated ASCII art. Our analysis shows that cosine similarity over CLIP embeddings fails to separate most ASCII categories, yielding chance-level performance even for low-variance classes. In contrast, classes with high internal mean similarity exhibit clear discriminability, revealing that the bottleneck lies in representation rather than generational variance. These findings position ASCII art as a stress test for multimodal representations and motivate the development of new embedding methods or evaluation metrics tailored to symbolic visual modalities. All resources are available at https://github.com/ASCIIBench/ASCIIBench.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.04125", "comments": "Accepted to The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025): LLM Evaluation Workshop & Multimodal Algorithmic Reasoning Workshop", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.637791", "embedding_text": "Paper Title: ASCIIBench: Evaluating Language-Model-Based Understanding of Visually-Oriented Text\nResearch Topic: machine learning\nAuthors: Kerry Luo, Michael Fu, Joshua Peguero, Husnain Malik, Anvay Patil, Joyce Lin, Megan Van Overborg, Ryan Sarmiento, Kevin Zhu\nAbstract: Large language models (LLMs) have demonstrated several emergent behaviors with scale, including reasoning and fluency in long-form text generation. However, they continue to struggle with tasks requiring precise spatial and positional reasoning. ASCII art, a symbolic medium where characters encode structure and form, provides a unique probe of this limitation. We introduce ASCIIBench, a novel benchmark for evaluating both the generation and classification of ASCII-text images. ASCIIBench consists of a filtered dataset of 5,315 class-labeled ASCII images and is, to our knowledge, the first publicly available benchmark of its kind. Alongside the dataset, we release weights for a fine-tuned CLIP model adapted to capture ASCII structure, enabling the evaluation of LLM-generated ASCII art. Our analysis shows that cosine similarity over CLIP embeddings fails to separate most ASCII categories, yielding chance-level performance even for low-variance classes. In contrast, classes with high internal mean similarity exhibit clear discriminability, revealing that the bottleneck lies in representation rather than generational variance. These findings position ASCII art as a stress test for multimodal representations and motivate the development of new embedding methods or evaluation metrics tailored to symbolic visual modalities. All resources are available at https://github.com/ASCIIBench/ASCIIBench.\nCategories: cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.04125\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1411, "total_text_length": 1495, "word_count": 198, "sentence_count": 12}, "basic_keywords": ["ascii", "asciibench", "reasoning", "form", "text", "generation", "symbolic", "structure"], "domain_keywords": [], "technical_terms": ["These", "However", "Alongside", "Large"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03219", "topic": "machine learning", "title": "Perch 2.0 transfers 'whale' to underwater tasks", "authors": ["Andrea Burns", "Lauren Harrell", "Bart van Merriënboer", "Vincent Dumoulin", "Jenny Hamer", "Tom Denton"], "first_author": "Andrea Burns", "abstract": "Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03219", "comments": "8 pages, 3 figures, 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: AI for Non-Human Animal Communication", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Perch 2.0 transfers 'whale' to underwater tasks\nResearch Topic: machine learning\nAuthors: Andrea Burns, Lauren Harrell, Bart van Merriënboer, Vincent Dumoulin, Jenny Hamer, Tom Denton\nAbstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.\nCategories: cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03219\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 47, "abstract_length": 1082, "total_text_length": 1130, "word_count": 157, "sentence_count": 14}, "basic_keywords": ["perch", "performance", "model", "marine", "mammal", "transfer", "learning", "models"], "domain_keywords": ["transfer learning"], "technical_terms": ["Perch", "Birdnet", "Given"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.03216", "topic": "machine learning", "title": "Kaleidoscopic Scintillation Event Imaging", "authors": ["Alex Bocchieri", "John Mamish", "David Appleyard", "Andreas Velten"], "first_author": "Alex Bocchieri", "abstract": "Scintillators are transparent materials that interact with high-energy particles and emit visible light as a result. They are used in state of the art methods of measuring high-energy particles and radiation sources. Most existing methods use fast single-pixel detectors to detect and time scintillation events. Cameras provide spatial resolution but can only capture an average over many events, making it difficult to image the events associated with an individual particle. Emerging single-photon avalanche diode cameras combine speed and spatial resolution to enable capturing images of individual events. This allows us to use machine vision techniques to analyze events, enabling new types of detectors. The main challenge is the very low brightness of the events. Techniques have to work with a very limited number of photons. We propose a kaleidoscopic scintillator to increase light collection in a single-photon camera while preserving the event's spatial information. The kaleidoscopic geometry creates mirror reflections of the event in known locations for a given event location that are captured by the camera. We introduce theory for imaging an event in a kaleidoscopic scintillator and an algorithm to estimate the event's 3D position. We find that the kaleidoscopic scintillator design provides sufficient light collection to perform high-resolution event measurements for advanced radiation imaging techniques using a commercial CMOS single-photon camera. Code and data are available at https://github.com/bocchs/kaleidoscopic_scintillator.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["physics.ins-det", "cs.CV", "eess.IV"], "url": "http://arxiv.org/abs/2512.03216", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Kaleidoscopic Scintillation Event Imaging\nResearch Topic: machine learning\nAuthors: Alex Bocchieri, John Mamish, David Appleyard, Andreas Velten\nAbstract: Scintillators are transparent materials that interact with high-energy particles and emit visible light as a result. They are used in state of the art methods of measuring high-energy particles and radiation sources. Most existing methods use fast single-pixel detectors to detect and time scintillation events. Cameras provide spatial resolution but can only capture an average over many events, making it difficult to image the events associated with an individual particle. Emerging single-photon avalanche diode cameras combine speed and spatial resolution to enable capturing images of individual events. This allows us to use machine vision techniques to analyze events, enabling new types of detectors. The main challenge is the very low brightness of the events. Techniques have to work with a very limited number of photons.   We propose a kaleidoscopic scintillator to increase light collection in a single-photon camera while preserving the event's spatial information. The kaleidoscopic geometry creates mirror reflections of the event in known locations for a given event location that are captured by the camera. We introduce theory for imaging an event in a kaleidoscopic scintillator and an algorithm to estimate the event's 3D position. We find that the kaleidoscopic scintillator design provides sufficient light collection to perform high-resolution event measurements for advanced radiation imaging techniques using a commercial CMOS single-photon camera. Code and data are available at https://github.com/bocchs/kaleidoscopic_scintillator.\nCategories: physics.ins-det, cs.CV, eess.IV\nPublication Date: 2025-12-02\nPaper ID: 2512.03216\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 41, "abstract_length": 1558, "total_text_length": 1600, "word_count": 225, "sentence_count": 15}, "basic_keywords": ["events", "event", "single", "kaleidoscopic", "high", "light", "spatial", "resolution"], "domain_keywords": [], "technical_terms": ["Techniques", "Scintillators", "Cameras", "Code", "Emerging", "Most", "They"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.03211", "topic": "machine learning", "title": "A Multi-Agent, Policy-Gradient approach to Network Routing", "authors": ["Nigel Tao", "Jonathan Baxter", "Lex Weaver"], "first_author": "Nigel Tao", "abstract": "Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG", "cs.NI"], "url": "http://arxiv.org/abs/2512.03211", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: A Multi-Agent, Policy-Gradient approach to Network Routing\nResearch Topic: machine learning\nAuthors: Nigel Tao, Jonathan Baxter, Lex Weaver\nAbstract: Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.\nCategories: cs.LG, cs.NI\nPublication Date: 2025-12-02\nPaper ID: 2512.03211\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 58, "abstract_length": 714, "total_text_length": 773, "word_count": 100, "sentence_count": 5}, "basic_keywords": ["network", "behavior", "routing", "distributed", "performance", "decision", "problem", "naturally"], "domain_keywords": [], "technical_terms": ["Network", "Multiple", "Furthermore"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.03210", "topic": "machine learning", "title": "Flux4D: Flow-based Unsupervised 4D Reconstruction", "authors": ["Jingkang Wang", "Henry Che", "Yun Chen", "Ze Yang", "Lily Goli", "Sivabalan Manivasagam", "Raquel Urtasun"], "first_author": "Jingkang Wang", "abstract": "Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an \"as static as possible\" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CV", "cs.LG", "cs.RO"], "url": "http://arxiv.org/abs/2512.03210", "comments": "NeurIPS 2025. Project page: https://waabi.ai/flux4d/", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Flux4D: Flow-based Unsupervised 4D Reconstruction\nResearch Topic: machine learning\nAuthors: Jingkang Wang, Henry Che, Yun Chen, Ze Yang, Lily Goli, Sivabalan Manivasagam, Raquel Urtasun\nAbstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an \"as static as possible\" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.\nCategories: cs.CV, cs.LG, cs.RO\nPublication Date: 2025-12-02\nPaper ID: 2512.03210\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 49, "abstract_length": 1536, "total_text_length": 1586, "word_count": 203, "sentence_count": 9}, "basic_keywords": ["dynamic", "scenes", "reconstruction", "large", "methods", "motion", "scale", "observations"], "domain_keywords": ["computer vision"], "technical_terms": ["Experiments", "While", "Existing", "Neural Radiance Fields", "Reconstructing", "Gaussians", "Gaussian Splatting"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03208", "topic": "machine learning", "title": "Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback", "authors": ["Pangpang Liu", "Junwei Lu", "Will Wei Sun"], "first_author": "Pangpang Liu", "abstract": "We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of- (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/abs/2512.03208", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback\nResearch Topic: machine learning\nAuthors: Pangpang Liu, Junwei Lu, Will Wei Sun\nAbstract: We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.\nCategories: stat.ML, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03208\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 102, "abstract_length": 1261, "total_text_length": 1364, "word_count": 182, "sentence_count": 10}, "basic_keywords": ["reward", "models", "human", "uncertainty", "statistical", "used", "alignment", "learning"], "domain_keywords": [], "technical_terms": ["These", "However", "Extensive", "Leveraging"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03204", "topic": "machine learning", "title": "Scaling Internal-State Policy-Gradient Methods for POMDPs", "authors": ["Douglas Aberdeen", "Jonathan Baxter"], "first_author": "Douglas Aberdeen", "abstract": "Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03204", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Scaling Internal-State Policy-Gradient Methods for POMDPs\nResearch Topic: machine learning\nAuthors: Douglas Aberdeen, Jonathan Baxter\nAbstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.\nCategories: cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03204\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 582, "total_text_length": 640, "word_count": 89, "sentence_count": 5}, "basic_keywords": ["learning", "problems", "policies", "memory", "algorithms", "policy", "gradient", "methods"], "domain_keywords": [], "technical_terms": ["They", "Policy"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.03200", "topic": "machine learning", "title": "Deteccion de intrusiones en redes mediante algoritmos de aprendizaje automatico: Un estudio multiclase sobre el conjunto de datos NSL-KDD", "authors": ["Luis Miguel Osco Vasquez"], "first_author": "Luis Miguel Osco Vasquez", "abstract": "Intrusion detection is a critical component of cybersecurity, responsible for identifying unauthorized access or anomalous behavior in computer networks. This paper presents a comprehensive study on intrusion detection in networks using classical machine learning algorithms applied to the multiclass version of the NSL-KDD dataset (Normal, DoS, Probe, R2L, and U2R classes). The characteristics of NSL-KDD are described in detail, including its variants and class distribution, and the data preprocessing process (cleaning, coding, and normalization) is documented. Four supervised classification models were implemented: Logistic Regression, Decision Tree, Random Forest, and XGBoost, whose performance is evaluated using standard metrics (accuracy, recall, F1 score, confusion matrix, and area under the ROC curve). Experiments show that models based on tree sets (Random Forest and XGBoost) achieve the best performance, with accuracies approaching 99%, significantly outperforming logistic regression and individual decision trees. The ability of each model to detect each attack category is also analyzed, highlighting the challenges in identifying rare attacks (R2L and U2R). Finally, the implications of the results are discussed, comparing them with the state of the art, and potential avenues for future research are proposed, such as the application of class balancing techniques and deep learning models to improve intrusion detection.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CE"], "url": "http://arxiv.org/abs/2512.03200", "comments": "in Spanish language", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Deteccion de intrusiones en redes mediante algoritmos de aprendizaje automatico: Un estudio multiclase sobre el conjunto de datos NSL-KDD\nResearch Topic: machine learning\nAuthors: Luis Miguel Osco Vasquez\nAbstract: Intrusion detection is a critical component of cybersecurity, responsible for identifying unauthorized access or anomalous behavior in computer networks. This paper presents a comprehensive study on intrusion detection in networks using classical machine learning algorithms applied to the multiclass version of the NSL-KDD dataset (Normal, DoS, Probe, R2L, and U2R classes). The characteristics of NSL-KDD are described in detail, including its variants and class distribution, and the data preprocessing process (cleaning, coding, and normalization) is documented. Four supervised classification models were implemented: Logistic Regression, Decision Tree, Random Forest, and XGBoost, whose performance is evaluated using standard metrics (accuracy, recall, F1 score, confusion matrix, and area under the ROC curve). Experiments show that models based on tree sets (Random Forest and XGBoost) achieve the best performance, with accuracies approaching 99%, significantly outperforming logistic regression and individual decision trees. The ability of each model to detect each attack category is also analyzed, highlighting the challenges in identifying rare attacks (R2L and U2R). Finally, the implications of the results are discussed, comparing them with the state of the art, and potential avenues for future research are proposed, such as the application of class balancing techniques and deep learning models to improve intrusion detection.\nCategories: cs.CE\nPublication Date: 2025-12-02\nPaper ID: 2512.03200\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 137, "abstract_length": 1447, "total_text_length": 1585, "word_count": 219, "sentence_count": 8}, "basic_keywords": ["intrusion", "detection", "models", "identifying", "networks", "using", "learning", "class"], "domain_keywords": [], "technical_terms": ["Random Forest", "Experiments", "Finally", "Decision Tree", "Logistic Regression", "Four", "Intrusion", "Normal"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.03195", "topic": "machine learning", "title": "Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies", "authors": ["Stylianos Saroglou", "Konstantinos Diamantaras", "Francesco Preta", "Marina Delianidi", "Apostolos Benisis", "Christian Johannes Meyer"], "first_author": "Stylianos Saroglou", "abstract": "This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.CL", "cs.LG"], "url": "http://arxiv.org/abs/2512.03195", "comments": "14 pages, 1 figure, Preprint", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies\nResearch Topic: machine learning\nAuthors: Stylianos Saroglou, Konstantinos Diamantaras, Francesco Preta, Marina Delianidi, Apostolos Benisis, Christian Johannes Meyer\nAbstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier\nCategories: cs.CL, cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03195\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1206, "total_text_length": 1307, "word_count": 172, "sentence_count": 8}, "basic_keywords": ["labor", "linking", "european", "qualifications", "language", "models", "classification", "market"], "domain_keywords": [], "technical_terms": ["Qualifications", "European Qualifications Framework", "Additionally", "European Skills", "European", "Sentence Linking", "Entity Linking", "Competences"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03194", "topic": "machine learning", "title": "GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding", "authors": ["Johannes Gaber", "Meshal Alharbi", "Daniele Gammelli", "Gioele Zardini"], "first_author": "Johannes Gaber", "abstract": "Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.RO", "cs.LG", "cs.MA"], "url": "http://arxiv.org/abs/2512.03194", "comments": "", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding\nResearch Topic: machine learning\nAuthors: Johannes Gaber, Meshal Alharbi, Daniele Gammelli, Gioele Zardini\nAbstract: Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.\nCategories: cs.RO, cs.LG, cs.MA\nPublication Date: 2025-12-02\nPaper ID: 2512.03194\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1125, "total_text_length": 1221, "word_count": 167, "sentence_count": 7}, "basic_keywords": ["large", "graph", "robot", "fleets", "small", "into", "scheduling", "learning"], "domain_keywords": [], "technical_terms": ["Multi", "Large", "League", "Robot Runners", "Agent Pickup", "Delivery"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.03193", "topic": "machine learning", "title": "In Situ Quantum Analog Pulse Characterization via Structured Signal Processing", "authors": ["Yulong Dong", "Christopher Kang", "Murphy Yuezhen Niu"], "first_author": "Yulong Dong", "abstract": "Analog quantum simulators can directly emulate time-dependent Hamiltonian dynamics, enabling the exploration of diverse physical phenomena such as phase transitions, quench dynamics, and non-equilibrium processes. Realizing accurate analog simulations requires high-fidelity time-dependent pulse control, yet existing calibration schemes are tailored to digital gate characterization and cannot be readily extended to learn continuous pulse trajectories. We present a characterization algorithm for in situ learning of pulse trajectories by extending the Quantum Signal Processing (QSP) framework to analyze time-dependent pulses. By combining QSP with a logical-level analog-digital mapping paradigm, our method reconstructs a smooth pulse directly from queries of the time-ordered propagator, without requiring mid-circuit measurements or additional evolution. Unlike conventional Trotterization-based methods, our approach avoids unscalable performance degradation arising from accumulated local truncation errors as the logical-level segmentation increases. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate that our method achieves high accuracy with strong efficiency and robustness against SPAM as well as depolarizing errors, providing a lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["quant-ph", "cs.LG", "math.NA"], "url": "http://arxiv.org/abs/2512.03193", "comments": "48 pages, 10 figures", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: In Situ Quantum Analog Pulse Characterization via Structured Signal Processing\nResearch Topic: machine learning\nAuthors: Yulong Dong, Christopher Kang, Murphy Yuezhen Niu\nAbstract: Analog quantum simulators can directly emulate time-dependent Hamiltonian dynamics, enabling the exploration of diverse physical phenomena such as phase transitions, quench dynamics, and non-equilibrium processes. Realizing accurate analog simulations requires high-fidelity time-dependent pulse control, yet existing calibration schemes are tailored to digital gate characterization and cannot be readily extended to learn continuous pulse trajectories. We present a characterization algorithm for in situ learning of pulse trajectories by extending the Quantum Signal Processing (QSP) framework to analyze time-dependent pulses. By combining QSP with a logical-level analog-digital mapping paradigm, our method reconstructs a smooth pulse directly from queries of the time-ordered propagator, without requiring mid-circuit measurements or additional evolution. Unlike conventional Trotterization-based methods, our approach avoids unscalable performance degradation arising from accumulated local truncation errors as the logical-level segmentation increases. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate that our method achieves high accuracy with strong efficiency and robustness against SPAM as well as depolarizing errors, providing a lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults.\nCategories: quant-ph, cs.LG, math.NA\nPublication Date: 2025-12-02\nPaper ID: 2512.03193\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1403, "total_text_length": 1482, "word_count": 183, "sentence_count": 7}, "basic_keywords": ["analog", "time", "pulse", "quantum", "dependent", "simulators", "directly", "dynamics"], "domain_keywords": [], "technical_terms": ["Hamiltonian", "Realizing", "Trotterization", "Unlike", "Quantum Signal Processing", "Analog", "Through"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.03187", "topic": "machine learning", "title": "Neighborhood density estimation using space-partitioning based hashing schemes", "authors": ["Aashi Jindal"], "first_author": "Aashi Jindal", "abstract": "This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.", "publish_date": "2025-12-02", "update_date": "2025-12-02", "categories": ["cs.LG"], "url": "http://arxiv.org/abs/2512.03187", "comments": "arXiv admin note: text overlap with arXiv:2011.03729", "query_keywords": "machine learning", "time_range": "week", "crawl_time": "2025-12-08T01:33:11.638848", "embedding_text": "Paper Title: Neighborhood density estimation using space-partitioning based hashing schemes\nResearch Topic: machine learning\nAuthors: Aashi Jindal\nAbstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.\nCategories: cs.LG\nPublication Date: 2025-12-02\nPaper ID: 2512.03187\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 502, "total_text_length": 581, "word_count": 72, "sentence_count": 5}, "basic_keywords": ["fire", "cell", "data", "drift", "work", "introduces", "novel", "sketching"], "domain_keywords": [], "technical_terms": ["Furthermore", "Enhash"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.05115", "topic": "computer vision", "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control", "authors": ["Tianqi Liu", "Zhaoxi Chen", "Zihao Huang", "Shaocong Xu", "Saining Zhang", "Chongjie Ye", "Bohan Li", "Zhiguo Cao", "Wei Li", "Hao Zhao", "Ziwei Liu"], "first_author": "Tianqi Liu", "abstract": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05115", "comments": "Project Page: https://lightx-ai.github.io/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.879327", "embedding_text": "Paper Title: Light-X: Generative 4D Video Rendering with Camera and Illumination Control\nResearch Topic: computer vision\nAuthors: Tianqi Liu, Zhaoxi Chen, Zihao Huang, Shaocong Xu, Saining Zhang, Chongjie Ye, Bohan Li, Zhiguo Cao, Wei Li, Hao Zhao, Ziwei Liu\nAbstract: Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05115\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1468, "total_text_length": 1544, "word_count": 203, "sentence_count": 9}, "basic_keywords": ["illumination", "control", "geometry", "methods", "video", "lighting", "camera", "both"], "domain_keywords": [], "technical_terms": ["These", "Extensive", "Light", "Recent", "Moving"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05113", "topic": "computer vision", "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting", "authors": ["Hao-Jen Chien", "Yi-Chuan Huang", "Chung-Ho Wu", "Wei-Lun Chao", "Yu-Lun Liu"], "first_author": "Hao-Jen Chien", "abstract": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05113", "comments": "WACV 2025. Project page: https://chien90190.github.io/splannequin/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.879327", "embedding_text": "Paper Title: Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting\nResearch Topic: computer vision\nAuthors: Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu\nAbstract: Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05113\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1481, "total_text_length": 1571, "word_count": 201, "sentence_count": 11}, "basic_keywords": ["states", "scene", "frozen", "dynamic", "user", "gaussian", "temporal", "high"], "domain_keywords": [], "technical_terms": ["Splannequin", "However", "Challenge", "Under", "Project", "Instead", "Gaussian", "Gaussians"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05111", "topic": "computer vision", "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning", "authors": ["Shengyuan Ding", "Xinyu Fang", "Ziyu Liu", "Yuhang Zang", "Yuhang Cao", "Xiangyu Zhao", "Haodong Duan", "Xiaoyi Dong", "Jianze Liang", "Bin Wang", "Conghui He", "Dahua Lin", "Jiaqi Wang"], "first_author": "Shengyuan Ding", "abstract": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05111", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.879327", "embedding_text": "Paper Title: ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning\nResearch Topic: computer vision\nAuthors: Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang\nAbstract: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05111\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 103, "abstract_length": 1394, "total_text_length": 1498, "word_count": 183, "sentence_count": 12}, "basic_keywords": ["reward", "tools", "models", "visual", "multimodal", "reasoning", "thinker", "page"], "domain_keywords": [], "technical_terms": ["Thinker", "Reward", "Reward Model"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05109", "topic": "computer vision", "title": "Global phase diagram of two-dimensional dirty hyperbolic Dirac liquids", "authors": ["Christopher A. Leong", "Daniel J. Salib", "Bitan Roy"], "first_author": "Christopher A. Leong", "abstract": "Within the framework of the canonical nearest-neighbor tight-binding model for spinless fermions, a family of two-dimensional bipartite hyperbolic lattices hosts massless Diraclike excitations near half-filling with the iconic vanishing density of states (DOS) near zero energy. We show that a collection of such ballistic quasiparticles remains stable against sufficiently weak pointlike charge impurities, a feature captured by the vanishing average [] and typical [] DOS at zero energy, computed by employing the kernel polynomial method in sufficiently large hyperbolic lattices (Schl fli symbol) with more than and sites, respectively, with open boundary conditions. However, at moderate disorder the system enters a metallic state via a continuous quantum phase transition where both and become finite. With increasing strength of disorder, ultimately an Anderson insulator sets in, where only . The resulting phase diagram for dirty Dirac fermions living on a hyperbolic space solely stems from the background negative spatial curvature, as confirmed from the vanishing for arbitrarily weak disorder on honeycomb lattices, fostering relativistic fermions on a flatland, as the thermodynamic limit is approached.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.mes-hall", "cond-mat.dis-nn", "cond-mat.stat-mech", "hep-th"], "url": "http://arxiv.org/abs/2512.05109", "comments": "6 Pages and 3 Figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.879327", "embedding_text": "Paper Title: Global phase diagram of two-dimensional dirty hyperbolic Dirac liquids\nResearch Topic: computer vision\nAuthors: Christopher A. Leong, Daniel J. Salib, Bitan Roy\nAbstract: Within the framework of the canonical nearest-neighbor tight-binding model for spinless fermions, a family of two-dimensional bipartite hyperbolic lattices hosts massless Diraclike excitations near half-filling with the iconic vanishing density of states (DOS) near zero energy. We show that a collection of such ballistic quasiparticles remains stable against sufficiently weak pointlike charge impurities, a feature captured by the vanishing average [$ρ_{a}(0)$] and typical [$ρ_{t}(0)$] DOS at zero energy, computed by employing the kernel polynomial method in sufficiently large $\\{ 10, 3\\}$ hyperbolic lattices (Schläfli symbol) with more than $10^8$ and $10^5$ sites, respectively, with open boundary conditions. However, at moderate disorder the system enters a metallic state via a continuous quantum phase transition where both $ρ_{a}(0)$ and $ρ_{t}(0)$ become finite. With increasing strength of disorder, ultimately an Anderson insulator sets in, where only $ρ_{t}(0) \\to 0$. The resulting phase diagram for dirty Dirac fermions living on a hyperbolic space solely stems from the background negative spatial curvature, as confirmed from the vanishing $ρ_{t}(0)$ for arbitrarily weak disorder on honeycomb lattices, fostering relativistic fermions on a flatland, as the thermodynamic limit is approached.\nCategories: cond-mat.mes-hall, cond-mat.dis-nn, cond-mat.stat-mech, hep-th\nPublication Date: 2025-12-04\nPaper ID: 2512.05109\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 1218, "total_text_length": 1289, "word_count": 182, "sentence_count": 6}, "basic_keywords": ["fermions", "hyperbolic", "lattices", "vanishing", "disorder", "near", "zero", "energy"], "domain_keywords": [], "technical_terms": ["However", "Within", "Anderson", "Schl", "Dirac", "Diraclike"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05107", "topic": "computer vision", "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models", "authors": ["Feng Xu", "Guangyao Zhai", "Xin Kong", "Tingzhong Fu", "Daniel F. N. Gordon", "Xueli An", "Benjamin Busam"], "first_author": "Feng Xu", "abstract": "Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.05107", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.879327", "embedding_text": "Paper Title: STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models\nResearch Topic: computer vision\nAuthors: Feng Xu, Guangyao Zhai, Xin Kong, Tingzhong Fu, Daniel F. N. Gordon, Xueli An, Benjamin Busam\nAbstract: Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.05107\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1504, "total_text_length": 1599, "word_count": 193, "sentence_count": 11}, "basic_keywords": ["stage", "action", "optimization", "language", "models", "reinforcement", "fine", "tuning"], "domain_keywords": [], "technical_terms": ["However", "Preference", "Aware", "Further", "Language", "Vision", "Interaction", "Proximal Policy Optimization"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05104", "topic": "computer vision", "title": "EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation", "authors": ["Jiaqi Ma", "Shengkai Hu", "Jun Wan", "Jiaxing Huang", "Lefei Zhang", "Salman Khan"], "first_author": "Jiaqi Ma", "abstract": "All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05104", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation\nResearch Topic: computer vision\nAuthors: Jiaqi Ma, Shengkai Hu, Jun Wan, Jiaxing Huang, Lefei Zhang, Salman Khan\nAbstract: All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05104\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1316, "total_text_length": 1398, "word_count": 166, "sentence_count": 9}, "basic_keywords": ["frequency", "evoir", "evolutionary", "aioir", "degradation", "image", "restoration", "explicit"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "One Image Restoration", "Frequency", "Modulated Module", "Specifically", "Central", "Evolutionary Optimization Strategy"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05096", "topic": "computer vision", "title": "Pump Free Microwave-Optical Quantum Transduction", "authors": ["Fangxin Li", "Jaesung Heo", "Zhaoyou Wang", "Andrew P. Higginbotham", "Alexander A. High", "Liang Jiang"], "first_author": "Fangxin Li", "abstract": "Distributed quantum computing involves superconducting computation nodes operating at microwave frequencies, which are connected by long-distance transmission lines that transmit photons at optical frequencies. Quantum transduction, which coherently converts between microwave and optical (M-O) photons, is a critical component of such an architecture. Current approaches are hindered by the unavoidable problem of device heating due to the optical pump. In this work, we propose a pump-free scheme based on color centers that generates time-bin encoded M-O Bell pairs. Our scheme first creates spin-photon entanglement and then converts the spin state into a time-bin-encoded microwave photon using a strongly coupled Purcell-enhanced resonator. In our protocol, the microwave retrieval is heralded by detecting the microwave signal with a three-level transmon. We have analyzed the resulting Bell state fidelity and generation probability of this protocol. Our simulation shows that by combining a state-of-the-art spin-optical interface with our proposed strongly-coupled spin-microwave design, the pump-free scheme can generate M-O Bell pairs at a heralding rate exceeding one kilohertz with near-unity fidelity, which establishes the scheme as a promising source for M-O Bell pairs.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph"], "url": "http://arxiv.org/abs/2512.05096", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Pump Free Microwave-Optical Quantum Transduction\nResearch Topic: computer vision\nAuthors: Fangxin Li, Jaesung Heo, Zhaoyou Wang, Andrew P. Higginbotham, Alexander A. High, Liang Jiang\nAbstract: Distributed quantum computing involves superconducting computation nodes operating at microwave frequencies, which are connected by long-distance transmission lines that transmit photons at optical frequencies. Quantum transduction, which coherently converts between microwave and optical (M-O) photons, is a critical component of such an architecture. Current approaches are hindered by the unavoidable problem of device heating due to the optical pump. In this work, we propose a pump-free scheme based on color centers that generates time-bin encoded M-O Bell pairs. Our scheme first creates spin-photon entanglement and then converts the spin state into a time-bin-encoded microwave photon using a strongly coupled Purcell-enhanced resonator. In our protocol, the microwave retrieval is heralded by detecting the microwave signal with a three-level transmon. We have analyzed the resulting Bell state fidelity and generation probability of this protocol. Our simulation shows that by combining a state-of-the-art spin-optical interface with our proposed strongly-coupled spin-microwave design, the pump-free scheme can generate M-O Bell pairs at a heralding rate exceeding one kilohertz with near-unity fidelity, which establishes the scheme as a promising source for M-O Bell pairs.\nCategories: quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.05096\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 48, "abstract_length": 1287, "total_text_length": 1336, "word_count": 183, "sentence_count": 9}, "basic_keywords": ["microwave", "optical", "scheme", "bell", "spin", "pump", "pairs", "state"], "domain_keywords": [], "technical_terms": ["Distributed", "Quantum", "Bell", "Current", "Purcell"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05094", "topic": "computer vision", "title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "authors": ["James Ni", "Zekai Wang", "Wei Lin", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik", "Roei Herzig"], "first_author": "James Ni", "abstract": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO", "cs.CV"], "url": "http://arxiv.org/abs/2512.05094", "comments": "For project website, see https://genmimic.github.io", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: From Generated Human Videos to Physically Plausible Robot Trajectories\nResearch Topic: computer vision\nAuthors: James Ni, Zekai Wang, Wei Lin, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik, Roei Herzig\nAbstract: Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.\nCategories: cs.RO, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05094\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 70, "abstract_length": 1480, "total_text_length": 1551, "word_count": 216, "sentence_count": 11}, "basic_keywords": ["video", "human", "actions", "generated", "generation", "models", "potential", "robot"], "domain_keywords": [], "technical_terms": ["Video", "Unitree", "Extensive", "Second", "First"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.05091", "topic": "computer vision", "title": "Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark", "authors": ["Haobo Yuan", "Yueyi Sun", "Yanwei Li", "Tao Zhang", "Xueqing Deng", "Henghui Ding", "Lu Qi", "Anran Wang", "Xiangtai Li", "Ming-Hsuan Yang"], "first_author": "Haobo Yuan", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05091", "comments": "Technical Report; Project Page: https://harboryuan.github.io/visual-reasoning-tracer", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark\nResearch Topic: computer vision\nAuthors: Haobo Yuan, Yueyi Sun, Yanwei Li, Tao Zhang, Xueqing Deng, Henghui Ding, Lu Qi, Anran Wang, Xiangtai Li, Ming-Hsuan Yang\nAbstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05091\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1230, "total_text_length": 1297, "word_count": 181, "sentence_count": 10}, "basic_keywords": ["reasoning", "models", "visual", "intermediate", "large", "output", "only", "final"], "domain_keywords": [], "technical_terms": ["However", "Bench", "Visual Reasoning Tracer", "Recent", "Multimodal Large Language Models"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.05085", "topic": "computer vision", "title": "Performance Analysis of Fluid Reconfigurable Intelligent Surface over Covert Communications", "authors": ["Farshad Rostami Ghadi", "Masoud Kaveh", "Hanjiang Hong", "Kai-Kit Wong", "Riku Jantti", "F. Javier Lopez-Martinez"], "first_author": "Farshad Rostami Ghadi", "abstract": "This paper investigates the impact of the recently proposed concept of fluid reconfigurable intelligent surfaces (FRIS) on covert communications. Specifically, we consider a communication scenario where a legitimate transmitter aims to covertly deliver information to its intended receiver through a planar FRIS, while an adversary attempts to detect whether any transmission is occurring. In this context, we analyze the false alarm (FA) and missed detection (MD) probabilities, and derive a closed-form expression for the covertness outage probability (COP). Furthermore, the success probability is characterized under the optimal detection threshold, providing new insights into the trade-off between covertness and reliable transmission. Numerical results reveal that FRIS provides a clear advantage over fixed-position RIS at low-to-moderate transmit powers by improving reliability and enhancing covertness, while at very high power levels, fixed-position RIS may sustain slightly higher success probability due to reduced leakage toward the adversary.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IT"], "url": "http://arxiv.org/abs/2512.05085", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Performance Analysis of Fluid Reconfigurable Intelligent Surface over Covert Communications\nResearch Topic: computer vision\nAuthors: Farshad Rostami Ghadi, Masoud Kaveh, Hanjiang Hong, Kai-Kit Wong, Riku Jantti, F. Javier Lopez-Martinez\nAbstract: This paper investigates the impact of the recently proposed concept of fluid reconfigurable intelligent surfaces (FRIS) on covert communications. Specifically, we consider a communication scenario where a legitimate transmitter aims to covertly deliver information to its intended receiver through a planar FRIS, while an adversary attempts to detect whether any transmission is occurring. In this context, we analyze the false alarm (FA) and missed detection (MD) probabilities, and derive a closed-form expression for the covertness outage probability (COP). Furthermore, the success probability is characterized under the optimal detection threshold, providing new insights into the trade-off between covertness and reliable transmission. Numerical results reveal that FRIS provides a clear advantage over fixed-position RIS at low-to-moderate transmit powers by improving reliability and enhancing covertness, while at very high power levels, fixed-position RIS may sustain slightly higher success probability due to reduced leakage toward the adversary.\nCategories: cs.IT\nPublication Date: 2025-12-04\nPaper ID: 2512.05085\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1058, "total_text_length": 1150, "word_count": 153, "sentence_count": 6}, "basic_keywords": ["fris", "covertness", "probability", "while", "adversary", "transmission", "detection", "success"], "domain_keywords": [], "technical_terms": ["Specifically", "Furthermore", "Numerical"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.05081", "topic": "computer vision", "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression", "authors": ["Jung Yi", "Wooseok Jang", "Paul Hyunbin Cho", "Jisu Nam", "Heeji Yoon", "Seungryong Kim"], "first_author": "Jung Yi", "abstract": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05081", "comments": "Project Page: https://cvlab-kaist.github.io/DeepForcing/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression\nResearch Topic: computer vision\nAuthors: Jung Yi, Wooseok Jang, Paul Hyunbin Cho, Jisu Nam, Heeji Yoon, Seungryong Kim\nAbstract: Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05081\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1403, "total_text_length": 1498, "word_count": 189, "sentence_count": 10}, "basic_keywords": ["generation", "video", "training", "recent", "diffusion", "real", "time", "streaming"], "domain_keywords": [], "technical_terms": ["Participative Compression", "Deep Sink", "Together", "Specifically", "Recent", "Deep Forcing"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05079", "topic": "computer vision", "title": "Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints", "authors": ["Minghan Zhu", "Zhiyi Wang", "Qihang Sun", "Maani Ghaffari", "Michael Posa"], "first_author": "Minghan Zhu", "abstract": "Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.RO"], "url": "http://arxiv.org/abs/2512.05079", "comments": "Project page: https://contactgen3d.github.io/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints\nResearch Topic: computer vision\nAuthors: Minghan Zhu, Zhiyi Wang, Qihang Sun, Maani Ghaffari, Michael Posa\nAbstract: Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.\nCategories: cs.CV, cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.05079\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 926, "total_text_length": 1019, "word_count": 142, "sentence_count": 9}, "basic_keywords": ["information", "geometry", "contact", "object", "reconstruction", "objects", "sources", "generative"], "domain_keywords": [], "technical_terms": ["Second", "Experiments", "Object", "First"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.05076", "topic": "computer vision", "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation", "authors": ["Yiming Wang", "Qihang Zhang", "Shengqu Cai", "Tong Wu", "Jan Ackermann", "Zhengfei Kuang", "Yang Zheng", "Frano Rajič", "Siyu Tang", "Gordon Wetzstein"], "first_author": "Yiming Wang", "abstract": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05076", "comments": "Project Page: https://19reborn.github.io/Bullet4D/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: BulletTime: Decoupled Control of Time and Camera Pose for Video Generation\nResearch Topic: computer vision\nAuthors: Yiming Wang, Qihang Zhang, Shengqu Cai, Tong Wu, Jan Ackermann, Zhengfei Kuang, Yang Zheng, Frano Rajič, Siyu Tang, Gordon Wetzstein\nAbstract: Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05076\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1085, "total_text_length": 1160, "word_count": 155, "sentence_count": 8}, "basic_keywords": ["camera", "video", "diffusion", "scene", "dynamics", "model", "high", "temporal"], "domain_keywords": [], "technical_terms": ["Experiments", "Emerging"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.6000000000000001}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05071", "topic": "computer vision", "title": "The Evolving Landscape of Interactive Surface Sensing Technologies", "authors": ["David Wang", "Wilson Chen", "Tianju Wang", "Jiale Zhang"], "first_author": "David Wang", "abstract": "Interactive surfaces have evolved from capacitive touch and IR based systems into a diverse ecosystem of sensing technologies that support rich and expressive human computer interaction. This survey traces that progression, beginning with infrared vision based approaches, such as FTIR and diffuse illumination, and the rise of capacitive touch as the dominant technology in modern devices, to focusing on contemporary modalities including vision and acoustic sensing. New technologies under development are also discussed, including mmWave radar, and vibration based techniques. Each sensing technique is examined in terms of its operating principles, resolution, scalability, and applications, along with discussions of multimodal integration. By comparing tradeoffs between sensing modalities, the survey highlights the technical and design factors that shape interactive surface performance and user experience. The review concludes by identifying persistent challenges, including sensing accuracy, power constraints, and privacy concerns, and outlines how emerging sensing modalities can enable future interactive environments to be ubiquitous and intelligent.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SY"], "url": "http://arxiv.org/abs/2512.05071", "comments": "12 pages, 10 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: The Evolving Landscape of Interactive Surface Sensing Technologies\nResearch Topic: computer vision\nAuthors: David Wang, Wilson Chen, Tianju Wang, Jiale Zhang\nAbstract: Interactive surfaces have evolved from capacitive touch and IR based systems into a diverse ecosystem of sensing technologies that support rich and expressive human computer interaction. This survey traces that progression, beginning with infrared vision based approaches, such as FTIR and diffuse illumination, and the rise of capacitive touch as the dominant technology in modern devices, to focusing on contemporary modalities including vision and acoustic sensing. New technologies under development are also discussed, including mmWave radar, and vibration based techniques. Each sensing technique is examined in terms of its operating principles, resolution, scalability, and applications, along with discussions of multimodal integration. By comparing tradeoffs between sensing modalities, the survey highlights the technical and design factors that shape interactive surface performance and user experience. The review concludes by identifying persistent challenges, including sensing accuracy, power constraints, and privacy concerns, and outlines how emerging sensing modalities can enable future interactive environments to be ubiquitous and intelligent.\nCategories: eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.05071\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 66, "abstract_length": 1165, "total_text_length": 1232, "word_count": 162, "sentence_count": 7}, "basic_keywords": ["sensing", "interactive", "based", "modalities", "including", "capacitive", "touch", "technologies"], "domain_keywords": [], "technical_terms": ["Interactive", "Each"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.05067", "topic": "computer vision", "title": "Perceptually-Minimal Color Optimization for Web Accessibility: A Multi-Phase Constrained Approach", "authors": ["Lalitha A R"], "first_author": "Lalitha A R", "abstract": "Web accessibility guidelines require sufficient color contrast between text and backgrounds; yet, manually adjusting colors often necessitates significant visual deviation, compromising vital brand aesthetics. We present a novel, multi-phase optimization approach for automatically generating WCAG-compliant colors while minimizing perceptual change to original design choices. Our method treats this as a constrained, non-linear optimization problem, utilizing the modern perceptually uniform OKLCH color space. Crucially, the optimization is constrained to preserve the original hue () of the color, ensuring that modifications are strictly limited to necessary adjustments in lightness () and chroma (). This is achieved through a three-phase sequence: binary search, gradient descent, and progressive constraint relaxation. Evaluation on a dataset of 10,000 procedurally generated color pairs demonstrates that the algorithm successfully resolves accessibility violations in of cases, with of successful corrections exhibiting imperceptible color difference () as defined by standard perceptibility thresholds. The median perceptual change for successful adjustments is only , and the algorithm achieves this with a median processing time of per color pair. The approach demonstrates that accessibility compliance and visual design integrity can be achieved simultaneously through a computationally efficient, perceptually-aware optimization that respects brand identity. The algorithm is publicly implemented in the open-source cm-colors Python library.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.HC", "math.OC"], "url": "http://arxiv.org/abs/2512.05067", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Perceptually-Minimal Color Optimization for Web Accessibility: A Multi-Phase Constrained Approach\nResearch Topic: computer vision\nAuthors: Lalitha A R\nAbstract: Web accessibility guidelines require sufficient color contrast between text and backgrounds; yet, manually adjusting colors often necessitates significant visual deviation, compromising vital brand aesthetics. We present a novel, multi-phase optimization approach for automatically generating WCAG-compliant colors while minimizing perceptual change to original design choices.   Our method treats this as a constrained, non-linear optimization problem, utilizing the modern perceptually uniform OKLCH color space. Crucially, the optimization is constrained to preserve the original hue ($\\text{H}$) of the color, ensuring that modifications are strictly limited to necessary adjustments in lightness ($\\text{L}$) and chroma ($\\text{C}$). This is achieved through a three-phase sequence: binary search, gradient descent, and progressive constraint relaxation.   Evaluation on a dataset of 10,000 procedurally generated color pairs demonstrates that the algorithm successfully resolves accessibility violations in $77.22\\%$ of cases, with $88.51\\%$ of successful corrections exhibiting imperceptible color difference ($ΔE_{2000} < 2.0$) as defined by standard perceptibility thresholds. The median perceptual change for successful adjustments is only $0.76\\ ΔE_{2000}$, and the algorithm achieves this with a median processing time of $0.876\\text{ms}$ per color pair.   The approach demonstrates that accessibility compliance and visual design integrity can be achieved simultaneously through a computationally efficient, perceptually-aware optimization that respects brand identity. The algorithm is publicly implemented in the open-source cm-colors Python library.\nCategories: cs.HC, math.OC\nPublication Date: 2025-12-04\nPaper ID: 2512.05067\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1558, "total_text_length": 1656, "word_count": 211, "sentence_count": 10}, "basic_keywords": ["color", "optimization", "accessibility", "colors", "algorithm", "visual", "brand", "phase"], "domain_keywords": [], "technical_terms": ["Crucially", "Python", "Evaluation"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05065", "topic": "computer vision", "title": "Personalizing Agent Privacy Decisions via Logical Entailment", "authors": ["James Flemings", "Ren Yi", "Octavian Suciu", "Kassem Fawaz", "Murali Annavaram", "Marco Gruteser"], "first_author": "James Flemings", "abstract": "Personal language model-based agents are becoming more widespread for completing tasks on behalf of users; however, this raises serious privacy questions regarding whether these models will appropriately disclose user data. While prior work has evaluated language models on data-sharing scenarios based on general privacy norms, we focus on personalizing language models' privacy decisions, grounding their judgments directly in prior user privacy decisions. Our findings suggest that general privacy norms are insufficient for effective personalization of privacy decisions. Furthermore, we find that eliciting privacy judgments from the model through In-context Learning (ICL) is unreliable to due misalignment with the user's prior privacy judgments and opaque reasoning traces, which make it difficult for the user to interpret the reasoning behind the model's decisions. To address these limitations, we propose ARIEL (Agentic Reasoning with Individualized Entailment Logic), a framework that jointly leverages a language model and rule-based logic for structured data-sharing reasoning. ARIEL is based on formulating personalization of data sharing as an entailment, whether a prior user judgment on a data-sharing request implies the same judgment for an incoming request. Our experimental evaluations on advanced models and publicly-available datasets demonstrate that ARIEL can reduce the F1 score error by over language model-based reasoning (ICL), demonstrating that ARIEL is effective at correctly judging requests where the user would approve data sharing. Overall, our findings suggest that combining LLMs with strict logical entailment is a highly effective strategy for enabling personalized privacy judgments for agents.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR"], "url": "http://arxiv.org/abs/2512.05065", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Personalizing Agent Privacy Decisions via Logical Entailment\nResearch Topic: computer vision\nAuthors: James Flemings, Ren Yi, Octavian Suciu, Kassem Fawaz, Murali Annavaram, Marco Gruteser\nAbstract: Personal language model-based agents are becoming more widespread for completing tasks on behalf of users; however, this raises serious privacy questions regarding whether these models will appropriately disclose user data. While prior work has evaluated language models on data-sharing scenarios based on general privacy norms, we focus on personalizing language models' privacy decisions, grounding their judgments directly in prior user privacy decisions. Our findings suggest that general privacy norms are insufficient for effective personalization of privacy decisions. Furthermore, we find that eliciting privacy judgments from the model through In-context Learning (ICL) is unreliable to due misalignment with the user's prior privacy judgments and opaque reasoning traces, which make it difficult for the user to interpret the reasoning behind the model's decisions. To address these limitations, we propose ARIEL (Agentic Reasoning with Individualized Entailment Logic), a framework that jointly leverages a language model and rule-based logic for structured data-sharing reasoning. ARIEL is based on formulating personalization of data sharing as an entailment, whether a prior user judgment on a data-sharing request implies the same judgment for an incoming request. Our experimental evaluations on advanced models and publicly-available datasets demonstrate that ARIEL can reduce the F1 score error by $\\textbf{39.1%}$ over language model-based reasoning (ICL), demonstrating that ARIEL is effective at correctly judging requests where the user would approve data sharing. Overall, our findings suggest that combining LLMs with strict logical entailment is a highly effective strategy for enabling personalized privacy judgments for agents.\nCategories: cs.CR\nPublication Date: 2025-12-04\nPaper ID: 2512.05065\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 60, "abstract_length": 1737, "total_text_length": 1798, "word_count": 248, "sentence_count": 9}, "basic_keywords": ["privacy", "user", "data", "language", "model", "based", "sharing", "reasoning"], "domain_keywords": [], "technical_terms": ["Personal", "While", "Furthermore", "Individualized Entailment Logic", "Learning", "Overall", "Agentic Reasoning"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05062", "topic": "computer vision", "title": "Configuration Defects in Kubernetes", "authors": ["Yue Zhang", "Uchswas Paul", "Marcelo d'Amorim", "Akond Rahman"], "first_author": "Yue Zhang", "abstract": "Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SE"], "url": "http://arxiv.org/abs/2512.05062", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.880458", "embedding_text": "Paper Title: Configuration Defects in Kubernetes\nResearch Topic: computer vision\nAuthors: Yue Zhang, Uchswas Paul, Marcelo d'Amorim, Akond Rahman\nAbstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.\nCategories: cs.SE\nPublication Date: 2025-12-04\nPaper ID: 2512.05062\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 35, "abstract_length": 1243, "total_text_length": 1279, "word_count": 193, "sentence_count": 13}, "basic_keywords": ["defects", "kubernetes", "configuration", "paper", "detect", "categories", "tools", "serious"], "domain_keywords": [], "technical_terms": ["Unfortunately", "Using", "Configuration", "Kubernetes"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.05060", "topic": "computer vision", "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer", "authors": ["Xianfeng Wu", "Yajing Bai", "Minghan Li", "Xianzu Wu", "Xueqi Zhao", "Zhongyuan Lai", "Wenyu Liu", "Xinggang Wang"], "first_author": "Xianfeng Wu", "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05060", "comments": "Code: https://github.com/hustvl/4DLangVGGT, Webpage: https://hustvl.github.io/4DLangVGGT", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer\nResearch Topic: computer vision\nAuthors: Xianfeng Wu, Yajing Bai, Minghan Li, Xianzu Wu, Xueqi Zhao, Zhongyuan Lai, Wenyu Liu, Xinggang Wang\nAbstract: Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05060\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 60, "abstract_length": 1785, "total_text_length": 1846, "word_count": 224, "sentence_count": 9}, "basic_keywords": ["scene", "semantic", "language", "dynamic", "understanding", "representations", "open", "vocabulary"], "domain_keywords": [], "technical_terms": ["However", "Experiments", "Visual Geometry Transformer", "Gaussian", "Transformer", "Unlike", "Constructing", "Semantic Bridging Decoder"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.05059", "topic": "computer vision", "title": "On six-loop scaling dimensions of $(φ^2)^n$ operators in $d=3$", "authors": ["A. V. Bednyakov", "M. V. Kompaniets", "A. V. Trenogin"], "first_author": "A. V. Bednyakov", "abstract": "We consider a class of singlet operators in the three-dimensional model with interaction. Recently, the corresponding anomalous dimensions were computed by semiclassical methods and the all-loop result for the leading- corrections in the small limit was found. In this letter, we obtain the six-loop expressions not only for the leading- contribution but also for the subleading one. While the leading correction confirms the predictions of recent semiclassical calculation, the subleading one is a new result and will serve as a future welcome check for the all-loop expressions. As an important by-product of our calculation, we provide a full dependence on of the four-loop in the case.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["hep-th", "cond-mat.stat-mech"], "url": "http://arxiv.org/abs/2512.05059", "comments": "19 pages, 5 figures, results in ancillary file", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: On six-loop scaling dimensions of $(φ^2)^n$ operators in $d=3$\nResearch Topic: computer vision\nAuthors: A. V. Bednyakov, M. V. Kompaniets, A. V. Trenogin\nAbstract: We consider a class of singlet operators $(φ^2)^n$ in the three-dimensional $O(N)$ model with $λ^2 φ^6$ interaction. Recently, the corresponding anomalous dimensions $γ_{2n}$ were computed by semiclassical methods and the all-loop result for the leading-$n$ corrections in the small $λ$ limit was found. In this letter, we obtain the six-loop expressions not only for the leading-$n$ contribution but also for the subleading one. While the leading correction confirms the predictions of recent semiclassical calculation, the subleading one is a new result and will serve as a future welcome check for the all-loop expressions. As an important by-product of our calculation, we provide a full dependence on $n$ of the four-loop $γ_{2n}$ in the $O(N)$ case.\nCategories: hep-th, cond-mat.stat-mech\nPublication Date: 2025-12-04\nPaper ID: 2512.05059\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 689, "total_text_length": 752, "word_count": 115, "sentence_count": 6}, "basic_keywords": ["loop", "leading", "semiclassical", "result", "expressions", "subleading", "calculation", "consider"], "domain_keywords": [], "technical_terms": ["Recently", "While"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.05051", "topic": "computer vision", "title": "Some computations in the heart of the homotopy t-structure on logarithmic motives", "authors": ["Alberto Merici"], "first_author": "Alberto Merici", "abstract": "In this note we will illustrate a method for computing the of the effective log motive of a smooth and proper variety over a perfect field and show that it is -invariant. We will apply this to compute the first homotopy groups of to show that the stripping functor from log motivic sheaves to (usual) Nisnevich sheaves with transfers is fully faithful.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.AG"], "url": "http://arxiv.org/abs/2512.05051", "comments": "This article first appeared as an appendix to 2211.14303, but due to its length and independent interest we decided to separate the two parts", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: Some computations in the heart of the homotopy t-structure on logarithmic motives\nResearch Topic: computer vision\nAuthors: Alberto Merici\nAbstract: In this note we will illustrate a method for computing the $π_0$ of the effective log motive of a smooth and proper variety over a perfect field $k$ and show that it is $\\A^1$-invariant. We will apply this to compute the first homotopy groups of $¶^1$ to show that the stripping functor from log motivic sheaves to (usual) Nisnevich sheaves with transfers is fully faithful.\nCategories: math.AG\nPublication Date: 2025-12-04\nPaper ID: 2512.05051\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 352, "total_text_length": 434, "word_count": 74, "sentence_count": 3}, "basic_keywords": ["show", "sheaves", "note", "illustrate", "method", "computing", "effective", "motive"], "domain_keywords": [], "technical_terms": ["Nisnevich"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.05044", "topic": "computer vision", "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image", "authors": ["Yanran Zhang", "Ziyi Wang", "Wenzhao Zheng", "Zheng Zhu", "Jie Zhou", "Jiwen Lu"], "first_author": "Yanran Zhang", "abstract": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05044", "comments": "18 Pages", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image\nResearch Topic: computer vision\nAuthors: Yanran Zhang, Ziyi Wang, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu\nAbstract: Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05044\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1256, "total_text_length": 1348, "word_count": 173, "sentence_count": 11}, "basic_keywords": ["motion", "generate", "single", "reconstruct", "point", "trajectories", "view", "dynamic"], "domain_keywords": [], "technical_terms": ["Scene Trajectory Generator", "Based", "Experiments", "View Synthesis Module", "Generating", "Motion", "Code", "Synthesis"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05040", "topic": "computer vision", "title": "Geometric Data Science", "authors": ["Olga D Anosova", "Vitaliy A Kurlin"], "first_author": "Olga D Anosova", "abstract": "This book introduces the new research area of Geometric Data Science, where data can represent any real objects through geometric measurements. The first part of the book focuses on finite point sets. The most important result is a complete and continuous classification of all finite clouds of unordered points under rigid motion in any Euclidean space. The key challenge was to avoid the exponential complexity arising from permutations of the given unordered points. For a fixed dimension of the ambient Euclidean space, the times of all algorithms for the resulting invariants and distance metrics depend polynomially on the number of points. The second part of the book advances a similar classification in the much more difficult case of periodic point sets, which model all periodic crystals at the atomic scale. The most significant result is the hierarchy of invariants from the ultra-fast to complete ones. The key challenge was to resolve the discontinuity of crystal representations that break down under almost any noise. Experimental validation on all major materials databases confirmed the Crystal Isometry Principle: any real periodic crystal has a unique location in a common moduli space of all periodic structures under rigid motion. The resulting moduli space contains all known and not yet discovered periodic crystals and hence continuously extends Mendeleev's table to the full crystal universe.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.MG", "cond-mat.mtrl-sci", "cs.CG"], "url": "http://arxiv.org/abs/2512.05040", "comments": "Questions and comments are welcome at vitaliy.kurlin@gmail.com. The latest version is at http://kurlin.org/Geometric-Data-Science-book.pdf", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: Geometric Data Science\nResearch Topic: computer vision\nAuthors: Olga D Anosova, Vitaliy A Kurlin\nAbstract: This book introduces the new research area of Geometric Data Science, where data can represent any real objects through geometric measurements.   The first part of the book focuses on finite point sets. The most important result is a complete and continuous classification of all finite clouds of unordered points under rigid motion in any Euclidean space. The key challenge was to avoid the exponential complexity arising from permutations of the given unordered points. For a fixed dimension of the ambient Euclidean space, the times of all algorithms for the resulting invariants and distance metrics depend polynomially on the number of points.   The second part of the book advances a similar classification in the much more difficult case of periodic point sets, which model all periodic crystals at the atomic scale. The most significant result is the hierarchy of invariants from the ultra-fast to complete ones. The key challenge was to resolve the discontinuity of crystal representations that break down under almost any noise. Experimental validation on all major materials databases confirmed the Crystal Isometry Principle: any real periodic crystal has a unique location in a common moduli space of all periodic structures under rigid motion. The resulting moduli space contains all known and not yet discovered periodic crystals and hence continuously extends Mendeleev's table to the full crystal universe.\nCategories: math.MG, cond-mat.mtrl-sci, cs.CG\nPublication Date: 2025-12-04\nPaper ID: 2512.05040\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 22, "abstract_length": 1419, "total_text_length": 1442, "word_count": 222, "sentence_count": 11}, "basic_keywords": ["periodic", "space", "crystal", "book", "points", "under", "geometric", "data"], "domain_keywords": [], "technical_terms": ["Mendeleev", "Crystal Isometry Principle", "Euclidean", "Experimental", "Geometric Data Science"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.05039", "topic": "computer vision", "title": "Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding", "authors": ["Abhigyan Bhattacharya", "Hiranmoy Roy"], "first_author": "Abhigyan Bhattacharya", "abstract": "Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05039", "comments": "Submitted for review CVPR-2025", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding\nResearch Topic: computer vision\nAuthors: Abhigyan Bhattacharya, Hiranmoy Roy\nAbstract: Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05039\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1727, "total_text_length": 1809, "word_count": 252, "sentence_count": 12}, "basic_keywords": ["facial", "semantic", "image", "inpainting", "face", "images", "methods", "large"], "domain_keywords": [], "technical_terms": ["Modal Texture Generator", "Though", "Experiment", "Facial Image", "Vision Transformers", "Multi"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.05031", "topic": "computer vision", "title": "CNN on `Top': In Search of Scalable & Lightweight Image-based Jet Taggers", "authors": ["Rajneil Baruah", "Subhadeep Mondal", "Sunando Kumar Patra", "Satyajit Roy"], "first_author": "Rajneil Baruah", "abstract": "While Transformer-based and standard Graph Neural Networks (GNNs) have proven to be the best performers in classifying different types of jets, they require substantial computational power. We explore the scope of using a lightweight and scalable version of the EfficientNet architecture, along with global features of the jet. The end product is computationally inexpensive but is capable of competitive performance. We showcase the efficacy of our network for tagging top-quark jets in a sea of other light-quark and gluon jets.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["hep-ph", "physics.comp-ph", "physics.data-an"], "url": "http://arxiv.org/abs/2512.05031", "comments": "12 pages, 3 figures, 2 tables", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: CNN on `Top': In Search of Scalable & Lightweight Image-based Jet Taggers\nResearch Topic: computer vision\nAuthors: Rajneil Baruah, Subhadeep Mondal, Sunando Kumar Patra, Satyajit Roy\nAbstract: While Transformer-based and standard Graph Neural Networks (GNNs) have proven to be the best performers in classifying different types of jets, they require substantial computational power. We explore the scope of using a lightweight and scalable version of the EfficientNet architecture, along with global features of the jet. The end product is computationally inexpensive but is capable of competitive performance. We showcase the efficacy of our network for tagging top-quark jets in a sea of other light-quark and gluon jets.\nCategories: hep-ph, physics.comp-ph, physics.data-an\nPublication Date: 2025-12-04\nPaper ID: 2512.05031\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 530, "total_text_length": 604, "word_count": 92, "sentence_count": 5}, "basic_keywords": ["jets", "quark", "while", "transformer", "based", "standard", "graph", "neural"], "domain_keywords": [], "technical_terms": ["While Transformer", "Graph Neural Networks"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.7749999999999999, "quality_tier": "medium"}
{"paper_id": "2512.05025", "topic": "computer vision", "title": "RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation", "authors": ["Nicolas Houdré", "Diego Marcos", "Hugo Riffaud de Turckheim", "Dino Ienco", "Laurent Wendling", "Camille Kurtz", "Sylvain Lobry"], "first_author": "Nicolas Houdré", "abstract": "Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05025", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation\nResearch Topic: computer vision\nAuthors: Nicolas Houdré, Diego Marcos, Hugo Riffaud de Turckheim, Dino Ienco, Laurent Wendling, Camille Kurtz, Sylvain Lobry\nAbstract: Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05025\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1575, "total_text_length": 1645, "word_count": 209, "sentence_count": 10}, "basic_keywords": ["resolution", "data", "spatial", "resolutions", "sensor", "across", "ramen", "multimodal"], "domain_keywords": [], "technical_terms": ["Earth", "Once", "While"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05017", "topic": "computer vision", "title": "The Magnus expansion in relativistic quantum field theory", "authors": ["Andreas Brandhuber", "Graham R. Brown", "Paolo Pichini", "Gabriele Travaglini", "Pablo Vives Matasan"], "first_author": "Andreas Brandhuber", "abstract": "We investigate the Magnus expansion of the -operator in relativistic quantum field theory, which is related to the -matrix via . We develop direct methods to compute matrix elements of the -operator, which we refer to as Magnus amplitudes, bypassing scattering amplitudes entirely. At tree level, Magnus amplitudes are expressed in terms of retarded and advanced propagators, with each diagram weighted by factors that we identify as Murua coefficients. At loop level this structure is augmented by the Hadamard cut function, and we establish remarkable relations between loop- and tree-level Magnus amplitudes. Among these, we find that -point one-loop Magnus amplitudes are entirely determined by phase-space integrals of forward limits of -point tree-level amplitudes, and hence related to Murua coefficients, and we generalise this to a class of higher-loop contributions. Furthermore, in the case of heavy particles interacting via massless mediators, we conjecture that Magnus diagrams that contribute to the classical limit are always given by forward limits of trees, and we show this explicitly in a one-loop example. We derive these results studying theories of scalar fields with cubic interactions, but our methods are applicable to general theories as well as to integral functions appearing in gravitational-wave computations. Given that Magnus amplitudes are free of hyper-classical terms, and the known relations between Magnus amplitudes and the radial action, our results lay the groundwork for systematic and efficient calculations of classical observables from quantum field theory.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["hep-th", "quant-ph"], "url": "http://arxiv.org/abs/2512.05017", "comments": "74 pages", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: The Magnus expansion in relativistic quantum field theory\nResearch Topic: computer vision\nAuthors: Andreas Brandhuber, Graham R. Brown, Paolo Pichini, Gabriele Travaglini, Pablo Vives Matasan\nAbstract: We investigate the Magnus expansion of the $N$-operator in relativistic quantum field theory, which is related to the $S$-matrix via $S = e^{iN}$. We develop direct methods to compute matrix elements of the $N$-operator, which we refer to as Magnus amplitudes, bypassing scattering amplitudes entirely. At tree level, Magnus amplitudes are expressed in terms of retarded and advanced propagators, with each diagram weighted by factors that we identify as Murua coefficients. At loop level this structure is augmented by the Hadamard cut function, and we establish remarkable relations between loop- and tree-level Magnus amplitudes. Among these, we find that $n$-point one-loop Magnus amplitudes are entirely determined by phase-space integrals of forward limits of $(n{+}2)$-point tree-level amplitudes, and hence related to Murua coefficients, and we generalise this to a class of higher-loop contributions. Furthermore, in the case of heavy particles interacting via massless mediators, we conjecture that Magnus diagrams that contribute to the classical limit are always given by forward limits of trees, and we show this explicitly in a one-loop example. We derive these results studying theories of scalar fields with cubic interactions, but our methods are applicable to general theories as well as to integral functions appearing in gravitational-wave computations. Given that Magnus amplitudes are free of hyper-classical terms, and the known relations between Magnus amplitudes and the radial action, our results lay the groundwork for systematic and efficient calculations of classical observables from quantum field theory.\nCategories: hep-th, quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.05017\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 1602, "total_text_length": 1660, "word_count": 244, "sentence_count": 9}, "basic_keywords": ["magnus", "amplitudes", "loop", "level", "tree", "classical", "operator", "quantum"], "domain_keywords": [], "technical_terms": ["Furthermore", "Murua", "Among", "Magnus", "Given", "Hadamard"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05016", "topic": "computer vision", "title": "Generative Neural Video Compression via Video Diffusion Prior", "authors": ["Qi Mao", "Hao Cheng", "Tinghan Yang", "Libiao Jin", "Siwei Ma"], "first_author": "Qi Mao", "abstract": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05016", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: Generative Neural Video Compression via Video Diffusion Prior\nResearch Topic: computer vision\nAuthors: Qi Mao, Hao Cheng, Tinghan Yang, Libiao Jin, Siwei Ma\nAbstract: We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05016\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 1534, "total_text_length": 1596, "word_count": 199, "sentence_count": 8}, "basic_keywords": ["video", "generative", "compression", "temporal", "gnvc", "perceptual", "generation", "spatio"], "domain_keywords": [], "technical_terms": ["Extensive", "Instead", "Gaussian", "Existing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.05015", "topic": "computer vision", "title": "Plug-and-Play Homeostatic Spark: Zero-Cost Acceleration for SNN Training Across Paradigms", "authors": ["Rui Chen", "Xingyu Chen", "Yaoqing Hu", "Shihan Kong", "Zhiheng Wu", "Junzhi Yu"], "first_author": "Rui Chen", "abstract": "Spiking neural networks offer event driven computation, sparse activation, and hardware efficiency, yet training often converges slowly and lacks stability. We present Adaptive Homeostatic Spiking Activity Regulation (AHSAR), an extremely simple plug in and training paradigm agnostic method that stabilizes optimization and accelerates convergence without changing the model architecture, loss, or gradients. AHSAR introduces no trainable parameters. It maintains a per layer homeostatic state during the forward pass, maps centered firing rate deviations to threshold scales through a bounded nonlinearity, uses lightweight cross layer diffusion to avoid sharp imbalance, and applies a slow across epoch global gain that combines validation progress with activity energy to tune the operating point. The computational cost is negligible. Across diverse training methods, SNN architectures of different depths, widths, and temporal steps, and both RGB and DVS datasets, AHSAR consistently improves strong baselines and enhances out of distribution robustness. These results indicate that keeping layer activity within a moderate band is a simple and effective principle for scalable and efficient SNN training.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.NE"], "url": "http://arxiv.org/abs/2512.05015", "comments": "12 pages, 4 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: Plug-and-Play Homeostatic Spark: Zero-Cost Acceleration for SNN Training Across Paradigms\nResearch Topic: computer vision\nAuthors: Rui Chen, Xingyu Chen, Yaoqing Hu, Shihan Kong, Zhiheng Wu, Junzhi Yu\nAbstract: Spiking neural networks offer event driven computation, sparse activation, and hardware efficiency, yet training often converges slowly and lacks stability. We present Adaptive Homeostatic Spiking Activity Regulation (AHSAR), an extremely simple plug in and training paradigm agnostic method that stabilizes optimization and accelerates convergence without changing the model architecture, loss, or gradients. AHSAR introduces no trainable parameters. It maintains a per layer homeostatic state during the forward pass, maps centered firing rate deviations to threshold scales through a bounded nonlinearity, uses lightweight cross layer diffusion to avoid sharp imbalance, and applies a slow across epoch global gain that combines validation progress with activity energy to tune the operating point. The computational cost is negligible. Across diverse training methods, SNN architectures of different depths, widths, and temporal steps, and both RGB and DVS datasets, AHSAR consistently improves strong baselines and enhances out of distribution robustness. These results indicate that keeping layer activity within a moderate band is a simple and effective principle for scalable and efficient SNN training.\nCategories: cs.NE\nPublication Date: 2025-12-04\nPaper ID: 2512.05015\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 89, "abstract_length": 1211, "total_text_length": 1301, "word_count": 177, "sentence_count": 8}, "basic_keywords": ["training", "activity", "ahsar", "layer", "spiking", "homeostatic", "simple", "across"], "domain_keywords": [], "technical_terms": ["These", "Across", "Spiking", "Adaptive Homeostatic Spiking Activity Regulation"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.05014", "topic": "computer vision", "title": "Hall-like response from anisotropic Fermi surfaces", "authors": ["Abhiram Soori"], "first_author": "Abhiram Soori", "abstract": "We demonstrate that an anisotropic and rotated Fermi surface can generate a finite Hall-like transverse response in electron transport, even in the absence of a magnetic field or Berry curvature. Using a two-dimensional continuum model, we show that broken symmetry inherent to anistropic band structures leads to a nonzero transverse conductivity. We construct a lattice model with direction-dependent nearest- and next-nearest-neighbor hoppings that faithfully reproduces the continuum dispersion and allows controlled rotation of the Fermi contour. Employing a multiterminal geometry and the B ttiker-probe method, we compute the resulting Hall voltage and establish its direct correspondence with the continuum transverse response. The effect increases with the degree of anisotropy and vanishes at rotation angles where mirror symmetry is restored. Unlike the quantum Hall effect, the Hall response predicted here is not quantized but varies continuously with the band-structure parameters. Our results provide a symmetry-based route to engineer Hall-like signals in low-symmetry materials without magnetic fields or topological effects.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.mes-hall", "quant-ph"], "url": "http://arxiv.org/abs/2512.05014", "comments": "6 pages, 4 captioned figures. Comments are welcome", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881060", "embedding_text": "Paper Title: Hall-like response from anisotropic Fermi surfaces\nResearch Topic: computer vision\nAuthors: Abhiram Soori\nAbstract: We demonstrate that an anisotropic and rotated Fermi surface can generate a finite Hall-like transverse response in electron transport, even in the absence of a magnetic field or Berry curvature. Using a two-dimensional continuum model, we show that broken $k_y \\to -k_y$ symmetry inherent to anistropic band structures leads to a nonzero transverse conductivity. We construct a lattice model with direction-dependent nearest- and next-nearest-neighbor hoppings that faithfully reproduces the continuum dispersion and allows controlled rotation of the Fermi contour. Employing a multiterminal geometry and the Büttiker-probe method, we compute the resulting Hall voltage and establish its direct correspondence with the continuum transverse response. The effect increases with the degree of anisotropy and vanishes at rotation angles where mirror symmetry is restored. Unlike the quantum Hall effect, the Hall response predicted here is not quantized but varies continuously with the band-structure parameters. Our results provide a symmetry-based route to engineer Hall-like signals in low-symmetry materials without magnetic fields or topological effects.\nCategories: cond-mat.mes-hall, quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.05014\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 50, "abstract_length": 1142, "total_text_length": 1193, "word_count": 164, "sentence_count": 8}, "basic_keywords": ["hall", "symmetry", "transverse", "response", "continuum", "fermi", "like", "magnetic"], "domain_keywords": [], "technical_terms": ["Using", "Employing", "Hall", "Berry", "Fermi", "Unlike"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.05012", "topic": "computer vision", "title": "Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking", "authors": ["Francielle Vargas", "Daniel Pedronette"], "first_author": "Francielle Vargas", "abstract": "This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.05012", "comments": "This work was presented as a poster at the Applied Social Media Lab during the 2025 Synthesizer & Open Showcase at the Berkman Klein Center for Internet & Society at Harvard University", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking\nResearch Topic: computer vision\nAuthors: Francielle Vargas, Daniel Pedronette\nAbstract: This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.05012\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 876, "total_text_length": 971, "word_count": 120, "sentence_count": 6}, "basic_keywords": ["evidence", "method", "retrieval", "contrastive", "factual", "rationales", "based", "extended"], "domain_keywords": [], "technical_terms": ["Hard", "Self", "Ranking", "Explaining Contrastive Evidence Re"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.05008", "topic": "computer vision", "title": "Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain", "authors": ["Haroon Hublikar"], "first_author": "Haroon Hublikar", "abstract": "This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono's Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.05008", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain\nResearch Topic: computer vision\nAuthors: Haroon Hublikar\nAbstract: This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono's Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.05008\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1402, "total_text_length": 1494, "word_count": 189, "sentence_count": 8}, "basic_keywords": ["rigid", "simulation", "locomotion", "granular", "terrain", "modeling", "sidewinding", "contact"], "domain_keywords": [], "technical_terms": ["Chrono", "Project Chrono", "Simscape", "Together", "Results", "Engine", "Soil Contact Model"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.05006", "topic": "computer vision", "title": "Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects", "authors": ["Xianghui Fan", "Zhaoyu Chen", "Mengyang Pan", "Anping Deng", "Hang Yang"], "first_author": "Xianghui Fan", "abstract": "The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.05006", "comments": "conference", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects\nResearch Topic: computer vision\nAuthors: Xianghui Fan, Zhaoyu Chen, Mengyang Pan, Anping Deng, Hang Yang\nAbstract: The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.05006\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 105, "abstract_length": 1018, "total_text_length": 1124, "word_count": 161, "sentence_count": 8}, "basic_keywords": ["depth", "transparent", "method", "objects", "training", "previous", "maps", "supervision"], "domain_keywords": ["computer vision"], "technical_terms": ["Conventional", "However", "Experiments", "Previous"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04998", "topic": "computer vision", "title": "Introducing V-Soft Pro: a Modular Platform for a Transhumeral Prosthesis with Controllable Stiffness", "authors": ["Giuseppe Milazzo", "Giorgio Grioli", "Antonio Bicchi", "Manuel G. Catalano"], "first_author": "Giuseppe Milazzo", "abstract": "Current upper limb prostheses aim to enhance user independence in daily activities by incorporating basic motor functions. However, they fall short of replicating the natural movement and interaction capabilities of the human arm. In contrast, human limbs leverage intrinsic compliance and actively modulate joint stiffness, enabling adaptive responses to varying tasks, impact absorption, and efficient energy transfer during dynamic actions. Inspired by this adaptability, we developed a transhumeral prosthesis with Variable Stiffness Actuators (VSAs) to replicate the controllable compliance found in biological joints. The proposed prosthesis features a modular design, allowing customization for different residual limb shapes and accommodating a range of independent control signals derived from users' biological cues. Integrated elastic elements passively support more natural movements, facilitate safe interactions with the environment, and adapt to diverse task requirements. This paper presents a comprehensive overview of the platform and its functionalities, highlighting its potential applications in the field of prosthetics.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.04998", "comments": "This article has been accepted for publication in Proceedings of the International Conference On Rehabilitation Robotics (ICORR), 2025. This is the author's version, which has not been fully edited, and content may change prior to final publication. Citation information: DOI 10.1109/ICORR66766.2025.11062964", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Introducing V-Soft Pro: a Modular Platform for a Transhumeral Prosthesis with Controllable Stiffness\nResearch Topic: computer vision\nAuthors: Giuseppe Milazzo, Giorgio Grioli, Antonio Bicchi, Manuel G. Catalano\nAbstract: Current upper limb prostheses aim to enhance user independence in daily activities by incorporating basic motor functions. However, they fall short of replicating the natural movement and interaction capabilities of the human arm. In contrast, human limbs leverage intrinsic compliance and actively modulate joint stiffness, enabling adaptive responses to varying tasks, impact absorption, and efficient energy transfer during dynamic actions. Inspired by this adaptability, we developed a transhumeral prosthesis with Variable Stiffness Actuators (VSAs) to replicate the controllable compliance found in biological joints. The proposed prosthesis features a modular design, allowing customization for different residual limb shapes and accommodating a range of independent control signals derived from users' biological cues. Integrated elastic elements passively support more natural movements, facilitate safe interactions with the environment, and adapt to diverse task requirements. This paper presents a comprehensive overview of the platform and its functionalities, highlighting its potential applications in the field of prosthetics.\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04998\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1142, "total_text_length": 1243, "word_count": 164, "sentence_count": 8}, "basic_keywords": ["limb", "natural", "human", "compliance", "stiffness", "prosthesis", "biological", "current"], "domain_keywords": [], "technical_terms": ["However", "Inspired", "Integrated", "Current", "Variable Stiffness Actuators"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04997", "topic": "computer vision", "title": "LEDDS: Portable LBM-DEM simulations on GPUs", "authors": ["Raphael Maggio-Aprile", "Maxime Rambosson", "Christophe Coreixas", "Jonas Latt"], "first_author": "Raphael Maggio-Aprile", "abstract": "Algorithmic formulations of GPU programs provide a high-level alternative to device-specific code by expressing computations as compositions of well-defined parallel primitives (e.g., map, sort, reduce), rather than through handcrafted GPU kernels. In this work, we demonstrate that this paradigm can be extended to complex and challenging problems in computational physics: the simulation of granular flows and fluid-particle interactions. LEDDS, our open-source framework, performs fully coupled Lattice Boltzmann -- Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives, and runs efficiently on single-GPU platforms. The entire workflow, including neighbor search, collision detection, and fluid-particle coupling, is expressed as a sequence of portable primitives. While the current implementation illustrates these principles primarily through algorithms from the C++ Standard Library, with selective use of Thrust primitives for performance, the underlying concept is compatible with any HPC environment offering a rich set of parallel algorithms and is therefore applicable across a wide range of modern GPU systems and future accelerators. LEDDS is validated through benchmarks spanning both DEM and LBM-DEM configurations, including sphere and ellipsoid collisions, wall friction tests, single-particle settling, Jeffery's orbits, and particle-laden shear flows. Despite its high level of abstraction, LEDDS achieves performances comparable to those of hand-tuned CUDA solvers, while maintaining portability and code clarity. These results show that high-performance LBM-DEM coupling can be achieved without sacrificing generality or readability, establishing LEDDS as a blueprint for portable multiphysics frameworks based on algorithmic primitives.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.comp-ph"], "url": "http://arxiv.org/abs/2512.04997", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: LEDDS: Portable LBM-DEM simulations on GPUs\nResearch Topic: computer vision\nAuthors: Raphael Maggio-Aprile, Maxime Rambosson, Christophe Coreixas, Jonas Latt\nAbstract: Algorithmic formulations of GPU programs provide a high-level alternative to device-specific code by expressing computations as compositions of well-defined parallel primitives (e.g., map, sort, reduce), rather than through handcrafted GPU kernels. In this work, we demonstrate that this paradigm can be extended to complex and challenging problems in computational physics: the simulation of granular flows and fluid-particle interactions.   LEDDS, our open-source framework, performs fully coupled Lattice Boltzmann -- Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives, and runs efficiently on single-GPU platforms. The entire workflow, including neighbor search, collision detection, and fluid-particle coupling, is expressed as a sequence of portable primitives. While the current implementation illustrates these principles primarily through algorithms from the C++ Standard Library, with selective use of Thrust primitives for performance, the underlying concept is compatible with any HPC environment offering a rich set of parallel algorithms and is therefore applicable across a wide range of modern GPU systems and future accelerators.   LEDDS is validated through benchmarks spanning both DEM and LBM-DEM configurations, including sphere and ellipsoid collisions, wall friction tests, single-particle settling, Jeffery's orbits, and particle-laden shear flows. Despite its high level of abstraction, LEDDS achieves performances comparable to those of hand-tuned CUDA solvers, while maintaining portability and code clarity. These results show that high-performance LBM-DEM coupling can be achieved without sacrificing generality or readability, establishing LEDDS as a blueprint for portable multiphysics frameworks based on algorithmic primitives.\nCategories: physics.comp-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04997\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 1785, "total_text_length": 1829, "word_count": 239, "sentence_count": 11}, "basic_keywords": ["primitives", "particle", "ledds", "algorithmic", "high", "through", "level", "code"], "domain_keywords": [], "technical_terms": ["These", "Thrust", "While", "Standard Library", "Despite", "Lattice Boltzmann", "Algorithmic", "Jeffery"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04996", "topic": "computer vision", "title": "A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs", "authors": ["Qiong Chang", "Weimin Wang", "Junpei Zhong", "Jun Miyazaki"], "first_author": "Qiong Chang", "abstract": "This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04996", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs\nResearch Topic: computer vision\nAuthors: Qiong Chang, Weimin Wang, Junpei Zhong, Jun Miyazaki\nAbstract: This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04996\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 1140, "total_text_length": 1227, "word_count": 154, "sentence_count": 10}, "basic_keywords": ["memory", "vanicp", "strategy", "based", "performance", "point", "cloud", "embedded"], "domain_keywords": [], "technical_terms": ["Source", "However", "Furthermore"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04991", "topic": "computer vision", "title": "Parametric disjunctive timed networks", "authors": ["Étienne André", "Swen Jacobs", "Engel Lefaucheux"], "first_author": "Étienne André", "abstract": "We consider distributed systems with an arbitrary number of processes, modelled by timed automata that communicate through location guards: a process can take a guarded transition if at least one other process is in a given location. In this work, we introduce parametric disjunctive timed networks, where each timed automaton may contain timing parameters, i.e. unknown constants. We investigate two problems: deciding the emptiness of the set of parameter valuations for which 1) a given location is reachable for at least one process (local property), and 2) a global state is reachable where all processes are in a given location (global property). Our main positive result is that the first problem is decidable for networks of processes with a single clock and without invariants; this result holds for arbitrarily many timing parameters -- a setting with few known decidability results. However, it becomes undecidable when invariants are allowed, or when considering global properties, even for systems with a single parameter. This highlights the significant expressive power of invariants in these networks. Additionally, we exhibit further decidable subclasses by restraining the syntax of guards and invariants.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LO"], "url": "http://arxiv.org/abs/2512.04991", "comments": "This is the author version of the manuscript of the same name published in the proceedings of the 34th EACSL Annual Conference on Computer Science Logic (CSL 2026)", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Parametric disjunctive timed networks\nResearch Topic: computer vision\nAuthors: Étienne André, Swen Jacobs, Engel Lefaucheux\nAbstract: We consider distributed systems with an arbitrary number of processes, modelled by timed automata that communicate through location guards: a process can take a guarded transition if at least one other process is in a given location. In this work, we introduce parametric disjunctive timed networks, where each timed automaton may contain timing parameters, i.e. unknown constants. We investigate two problems: deciding the emptiness of the set of parameter valuations for which   1) a given location is reachable for at least one process (local property), and   2) a global state is reachable where all processes are in a given location (global property).   Our main positive result is that the first problem is decidable for networks of processes with a single clock and without invariants; this result holds for arbitrarily many timing parameters -- a setting with few known decidability results. However, it becomes undecidable when invariants are allowed, or when considering global properties, even for systems with a single parameter. This highlights the significant expressive power of invariants in these networks. Additionally, we exhibit further decidable subclasses by restraining the syntax of guards and invariants.\nCategories: cs.LO\nPublication Date: 2025-12-04\nPaper ID: 2512.04991\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 37, "abstract_length": 1223, "total_text_length": 1261, "word_count": 189, "sentence_count": 10}, "basic_keywords": ["location", "invariants", "processes", "timed", "process", "given", "networks", "global"], "domain_keywords": [], "technical_terms": ["However", "Additionally"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04987", "topic": "computer vision", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "authors": ["Nex-AGI Team", ":", "Yuxuan Cai", "Lu Chen", "Qiaoling Chen", "Yuyang Ding", "Liwen Fan", "Wenjie Fu", "Yufei Gao", "Honglin Guo", "Pinxue Guo", "Zhenhua Han", "Zhengfu He", "Hanglei Hu", "Kai Hu", "Shengjia Hua", "Tianyu Huai", "Baodai Huang", "Li Ji", "Zhen Jiang", "Zhikai Lei", "Bufan Li", "Jiahang Lin", "Lizhi Lin", "Jinxiu Liu", "Shichun Liu", "Ziming Liu", "Yuchen Ni", "Pengfang Qian", "Yujiong Shen", "Qingyun Shi", "Wentao Shu", "Peng Sun", "Yiran Suo", "Tian Tang", "Boyu Tian", "Guoteng Wang", "Junzhe Wang", "Peixin Wang", "Zhiheng Xi", "Hang Yan", "Jie Yang", "Zhixiong Yang", "Tianchu Yao", "Guangze Ye", "Qianxi Yu", "Shuo Zhang", "Xinyue Zhang", "Yiqi Zhang", "Jiarong Zhao", "Miao Zheng", "Rui Zheng", "Enyu Zhou", "Jiazheng Zhou", "Maosen Zhou", "Yuhao Zhou", "Tao Gui", "Yining Zheng", "Xinchi Chen", "Jie Zhou", "Siyuan Feng", "Qin Chen", "Liang He", "Qi Zhang", "Xuanjing Huang", "Xipeng Qiu"], "first_author": "Nex-AGI Team", "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04987", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction\nResearch Topic: computer vision\nAuthors: Nex-AGI Team, :, Yuxuan Cai, Lu Chen, Qiaoling Chen, Yuyang Ding, Liwen Fan, Wenjie Fu, Yufei Gao, Honglin Guo, Pinxue Guo, Zhenhua Han, Zhengfu He, Hanglei Hu, Kai Hu, Shengjia Hua, Tianyu Huai, Baodai Huang, Li Ji, Zhen Jiang, Zhikai Lei, Bufan Li, Jiahang Lin, Lizhi Lin, Jinxiu Liu, Shichun Liu, Ziming Liu, Yuchen Ni, Pengfang Qian, Yujiong Shen, Qingyun Shi, Wentao Shu, Peng Sun, Yiran Suo, Tian Tang, Boyu Tian, Guoteng Wang, Junzhe Wang, Peixin Wang, Zhiheng Xi, Hang Yan, Jie Yang, Zhixiong Yang, Tianchu Yao, Guangze Ye, Qianxi Yu, Shuo Zhang, Xinyue Zhang, Yiqi Zhang, Jiarong Zhao, Miao Zheng, Rui Zheng, Enyu Zhou, Jiazheng Zhou, Maosen Zhou, Yuhao Zhou, Tao Gui, Yining Zheng, Xinchi Chen, Jie Zhou, Siyuan Feng, Qin Chen, Liang He, Qi Zhang, Xuanjing Huang, Xipeng Qiu\nAbstract: The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04987\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 95, "abstract_length": 1413, "total_text_length": 1509, "word_count": 194, "sentence_count": 8}, "basic_keywords": ["models", "agent", "complex", "language", "learning", "infrastructure", "method", "diversity"], "domain_keywords": [], "technical_terms": ["However", "Empirical", "Large Language Models", "Complexity", "Diversity", "Fidelity"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04986", "topic": "computer vision", "title": "Inflationary relics from an Ultra-Slow-Roll plateau", "authors": ["Albert Escrivà", "Jaume Garriga", "Shi Pi"], "first_author": "Albert Escrivà", "abstract": "We investigate the formation of primordial black holes (PBHs) in inflationary scenarios featuring an ultra-slow-roll (USR) plateau, focusing on two coexisting production channels: PBHs originating from relic vacuum bubbles where the inflaton got trapped on the plateau, and PBHs arising from standard adiabatic density perturbations. From detailed numerical simulations we find that the bubbles are generically surrounded by type-II curvature fluctuations. Special attention is given to the distribution of initial conditions, including the relevant mean profiles and shape dispersion around them. For the adiabatic channel, we extend the logarithmic template formula , which maps the Gaussian curvature perturbation to the fully non-Gaussian one while incorporating mode evolution, and we compare this with numerical results obtained using the formalism. While the template departs from numerical results near its logarithmic divergence, it still provides accurate threshold values for PBH formation in the parameter range relevant to our analysis. Finally, we compute the PBH mass functions for both channels. We find that the adiabatic channel dominates over the bubble-induced channel by a factor , and that both contributions are largely dominated by the mean profiles.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["astro-ph.CO", "gr-qc", "hep-ph", "hep-th"], "url": "http://arxiv.org/abs/2512.04986", "comments": "31 pages and 11 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Inflationary relics from an Ultra-Slow-Roll plateau\nResearch Topic: computer vision\nAuthors: Albert Escrivà, Jaume Garriga, Shi Pi\nAbstract: We investigate the formation of primordial black holes (PBHs) in inflationary scenarios featuring an ultra-slow-roll (USR) plateau, focusing on two coexisting production channels: PBHs originating from relic vacuum bubbles where the inflaton got trapped on the plateau, and PBHs arising from standard adiabatic density perturbations. From detailed numerical simulations we find that the bubbles are generically surrounded by type-II curvature fluctuations. Special attention is given to the distribution of initial conditions, including the relevant mean profiles and shape dispersion around them. For the adiabatic channel, we extend the logarithmic template formula $ζ[ζ_G]$, which maps the Gaussian curvature perturbation to the fully non-Gaussian one while incorporating mode evolution, and we compare this with numerical results obtained using the $δN$ formalism. While the template departs from numerical results near its logarithmic divergence, it still provides accurate threshold values for PBH formation in the parameter range relevant to our analysis. Finally, we compute the PBH mass functions for both channels. We find that the adiabatic channel dominates over the bubble-induced channel by a factor $\\sim \\mathcal{O}(10-10^{2})$, and that both contributions are largely dominated by the mean profiles.\nCategories: astro-ph.CO, gr-qc, hep-ph, hep-th\nPublication Date: 2025-12-04\nPaper ID: 2512.04986\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 1274, "total_text_length": 1326, "word_count": 188, "sentence_count": 8}, "basic_keywords": ["pbhs", "adiabatic", "numerical", "channel", "formation", "plateau", "channels", "bubbles"], "domain_keywords": [], "technical_terms": ["Finally", "Gaussian", "While", "Special"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04984", "topic": "computer vision", "title": "Federated Learning for Terahertz Wireless Communication", "authors": ["O. Tansel Baydas", "Ozgur B. Akan"], "first_author": "O. Tansel Baydas", "abstract": "The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.DC"], "url": "http://arxiv.org/abs/2512.04984", "comments": "10 pages, 4 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: Federated Learning for Terahertz Wireless Communication\nResearch Topic: computer vision\nAuthors: O. Tansel Baydas, Ozgur B. Akan\nAbstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.\nCategories: cs.DC\nPublication Date: 2025-12-04\nPaper ID: 2512.04984\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 55, "abstract_length": 1314, "total_text_length": 1370, "word_count": 182, "sentence_count": 8}, "basic_keywords": ["convergence", "squint", "learning", "impact", "updates", "beam", "critical", "standard"], "domain_keywords": [], "technical_terms": ["Consequently", "Numerical", "Finally", "Federated Learning", "Terahertz"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04983", "topic": "computer vision", "title": "A tangential low-rank ADI method for solving indefinite Lyapunov equations", "authors": ["Rudi Smith", "Steffen W. R. Werner"], "first_author": "Rudi Smith", "abstract": "Continuous-time algebraic Lyapunov equations have become an essential tool in various applications. In the case of large-scale sparse coefficient matrices and indefinite constant terms, indefinite low-rank factorizations have successfully been used to allow methods like the alternating direction implicit (ADI) iteration to efficiently compute accurate approximations to the solution of the Lyapunov equation. However, classical block-type approaches quickly increase in computational costs when the rank of the constant term grows. In this paper, we propose a novel tangential reformulation of the ADI iteration that allows for the efficient construction of low-rank approximations to the solution of Lyapunov equations with indefinite right-hand sides even in the case of constant terms with higher ranks. We provide adaptive methods for the selection of the corresponding ADI parameters, namely shifts and tangential directions, which allow for the automatic application of the method to any relevant problem setting. The effectiveness of the developed algorithms is illustrated by several numerical examples.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.NA", "math.OC"], "url": "http://arxiv.org/abs/2512.04983", "comments": "33 pages, 6 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.881582", "embedding_text": "Paper Title: A tangential low-rank ADI method for solving indefinite Lyapunov equations\nResearch Topic: computer vision\nAuthors: Rudi Smith, Steffen W. R. Werner\nAbstract: Continuous-time algebraic Lyapunov equations have become an essential tool in various applications. In the case of large-scale sparse coefficient matrices and indefinite constant terms, indefinite low-rank factorizations have successfully been used to allow methods like the alternating direction implicit (ADI) iteration to efficiently compute accurate approximations to the solution of the Lyapunov equation. However, classical block-type approaches quickly increase in computational costs when the rank of the constant term grows. In this paper, we propose a novel tangential reformulation of the ADI iteration that allows for the efficient construction of low-rank approximations to the solution of Lyapunov equations with indefinite right-hand sides even in the case of constant terms with higher ranks. We provide adaptive methods for the selection of the corresponding ADI parameters, namely shifts and tangential directions, which allow for the automatic application of the method to any relevant problem setting. The effectiveness of the developed algorithms is illustrated by several numerical examples.\nCategories: math.NA, math.OC\nPublication Date: 2025-12-04\nPaper ID: 2512.04983\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1113, "total_text_length": 1188, "word_count": 165, "sentence_count": 7}, "basic_keywords": ["lyapunov", "indefinite", "constant", "rank", "equations", "case", "terms", "allow"], "domain_keywords": [], "technical_terms": ["However", "Lyapunov", "Continuous"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04976", "topic": "computer vision", "title": "Multipole decomposition of the gravitational field of a point mass at the black hole horizon", "authors": ["João P. B. Brito", "Atsushi Higuchi", "Luís C. B. Crispino"], "first_author": "João P. B. Brito", "abstract": "The portion of the gravitational energy absorbed by the black hole due to the radial infall of a point mass is known to diverge at leading order in perturbation theory. This divergence is an artifact of the point-particle model, where the contribution of each multipole to the total absorbed energy is observed to be roughly constant. We show explicitly that this divergent energy arises from the infinite energy present in the singular static field arbitrarily close to the point mass, which also flows into the black hole when the particle trajectory crosses the horizon. We perform a multipole decomposition of the linearized gravitational field generated by the point mass near its world line at the black hole horizon. By applying the standard field-theoretical approach to the particle field, we compute the corresponding partial energy and find that it matches the constant multipole contribution.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["gr-qc", "hep-th"], "url": "http://arxiv.org/abs/2512.04976", "comments": "9 pages, 2 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Multipole decomposition of the gravitational field of a point mass at the black hole horizon\nResearch Topic: computer vision\nAuthors: João P. B. Brito, Atsushi Higuchi, Luís C. B. Crispino\nAbstract: The portion of the gravitational energy absorbed by the black hole due to the radial infall of a point mass is known to diverge at leading order in perturbation theory. This divergence is an artifact of the point-particle model, where the contribution of each multipole to the total absorbed energy is observed to be roughly constant. We show explicitly that this divergent energy arises from the infinite energy present in the singular static field arbitrarily close to the point mass, which also flows into the black hole when the particle trajectory crosses the horizon. We perform a multipole decomposition of the linearized gravitational field generated by the point mass near its world line at the black hole horizon. By applying the standard field-theoretical approach to the particle field, we compute the corresponding partial energy and find that it matches the constant multipole contribution.\nCategories: gr-qc, hep-th\nPublication Date: 2025-12-04\nPaper ID: 2512.04976\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 904, "total_text_length": 997, "word_count": 158, "sentence_count": 6}, "basic_keywords": ["energy", "point", "field", "black", "hole", "mass", "particle", "multipole"], "domain_keywords": [], "technical_terms": [], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.4}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04973", "topic": "computer vision", "title": "Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist", "authors": ["Giuseppe Milazzo", "Manuel G. Catalano", "Antonio Bicchi", "Giorgio Grioli"], "first_author": "Giuseppe Milazzo", "abstract": "Variable Stiffness Actuators prove invaluable for robotics applications in unstructured environments, fostering safe interactions and enhancing task adaptability. Nevertheless, their mechanical design inevitably results in larger and heavier structures compared to classical rigid actuators. This paper introduces a novel 3 Degrees of Freedom (DoFs) parallel wrist that achieves variable stiffness through redundant elastic actuation. Leveraging its parallel architecture, the device employs only four motors, rendering it compact and lightweight. This characteristic makes it particularly well-suited for applications in prosthetics or humanoid robotics. The manuscript delves into the theoretical model of the device and proposes a sophisticated control strategy for independent regulation of joint position and stiffness. Furthermore, it validates the proposed controller through simulation, utilizing a comprehensive analysis of the system dynamics. The reported results affirm the ability of the device to achieve high accuracy and disturbance rejection in rigid configurations while minimizing interaction forces with its compliant behavior.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.04973", "comments": "This article has been accepted for publication in Springer Proceedings in Advanced Robotics, vol 31. Springer, Cham. This is the author's version, which has not been fully edited, and the content may change prior to final publication. Citation information: DOI https://doi.org/10.1007/978-3-031-64057-5_9", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist\nResearch Topic: computer vision\nAuthors: Giuseppe Milazzo, Manuel G. Catalano, Antonio Bicchi, Giorgio Grioli\nAbstract: Variable Stiffness Actuators prove invaluable for robotics applications in unstructured environments, fostering safe interactions and enhancing task adaptability. Nevertheless, their mechanical design inevitably results in larger and heavier structures compared to classical rigid actuators. This paper introduces a novel 3 Degrees of Freedom (DoFs) parallel wrist that achieves variable stiffness through redundant elastic actuation. Leveraging its parallel architecture, the device employs only four motors, rendering it compact and lightweight. This characteristic makes it particularly well-suited for applications in prosthetics or humanoid robotics. The manuscript delves into the theoretical model of the device and proposes a sophisticated control strategy for independent regulation of joint position and stiffness. Furthermore, it validates the proposed controller through simulation, utilizing a comprehensive analysis of the system dynamics. The reported results affirm the ability of the device to achieve high accuracy and disturbance rejection in rigid configurations while minimizing interaction forces with its compliant behavior.\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04973\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 1147, "total_text_length": 1221, "word_count": 159, "sentence_count": 9}, "basic_keywords": ["stiffness", "device", "variable", "actuators", "robotics", "applications", "results", "rigid"], "domain_keywords": [], "technical_terms": ["Nevertheless", "Furthermore", "Freedom", "Degrees", "Variable Stiffness Actuators", "Leveraging"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04971", "topic": "computer vision", "title": "Exploring YouTube's Political Communication Networks during the 2024 French Elections", "authors": ["Caroline Violot", "Vera Sosnovik", "Mathias Humbert"], "first_author": "Caroline Violot", "abstract": "In 2024, France was shaken by the far-right National Rally's victory in the European elections. In response to this unprecedented result, French President Emmanuel Macron dissolved the National Assembly, triggering legislative elections just two weeks later. A whirlwind campaign followed, partly on social media, as is now the norm, and concluded with the victory of a left-wing coalition. This article examines the YouTube activity of two key actors during this period, news media and politicians, and the commenting behavior they generated. We built a dataset of 35 news media channels, 28 politicians and parties channels, 43.5k videos posted from three months before the European elections to one week after the second round of the legislative elections, and 7.4M associated comments. We examined upload activity and engagement across political orientations and used network analysis methods to uncover the structure of their commenting communities. We also identified politicians' appearances on news media channels and assessed their impact on commenting user bases. Our findings show that, among politicians and parties channels, far-right and left-wing ones were significantly more active and received substantially higher engagement (views, likes, and comments) than other groups, with denser and more clustered commenting communities. About 7% of commenters commented across political orientations and were much more active than in-group commenters. News media channels tended to favor politically aligned guests, while centrist politicians were over-represented. Finally, politicians' presence in the videos of a specific news media channel increased the share of commenters who were active on this channel and political channels, regardless of their orientation.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SI"], "url": "http://arxiv.org/abs/2512.04971", "comments": "12 pages, 8 figures, to be published in ICWSM'2026", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Exploring YouTube's Political Communication Networks during the 2024 French Elections\nResearch Topic: computer vision\nAuthors: Caroline Violot, Vera Sosnovik, Mathias Humbert\nAbstract: In 2024, France was shaken by the far-right National Rally's victory in the European elections. In response to this unprecedented result, French President Emmanuel Macron dissolved the National Assembly, triggering legislative elections just two weeks later. A whirlwind campaign followed, partly on social media, as is now the norm, and concluded with the victory of a left-wing coalition. This article examines the YouTube activity of two key actors during this period, news media and politicians, and the commenting behavior they generated. We built a dataset of 35 news media channels, 28 politicians and parties channels, 43.5k videos posted from three months before the European elections to one week after the second round of the legislative elections, and 7.4M associated comments. We examined upload activity and engagement across political orientations and used network analysis methods to uncover the structure of their commenting communities. We also identified politicians' appearances on news media channels and assessed their impact on commenting user bases. Our findings show that, among politicians and parties channels, far-right and left-wing ones were significantly more active and received substantially higher engagement (views, likes, and comments) than other groups, with denser and more clustered commenting communities. About 7% of commenters commented across political orientations and were much more active than in-group commenters. News media channels tended to favor politically aligned guests, while centrist politicians were over-represented. Finally, politicians' presence in the videos of a specific news media channel increased the share of commenters who were active on this channel and political channels, regardless of their orientation.\nCategories: cs.SI\nPublication Date: 2025-12-04\nPaper ID: 2512.04971\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1775, "total_text_length": 1861, "word_count": 265, "sentence_count": 14}, "basic_keywords": ["media", "politicians", "channels", "news", "elections", "commenting", "political", "more"], "domain_keywords": [], "technical_terms": ["Finally", "French President Emmanuel Macron", "News", "National Assembly", "European", "National Rally", "About", "France"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04970", "topic": "computer vision", "title": "Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks", "authors": ["Leonid Pogorelyuk", "Niels Bracher", "Aaron Verkleeren", "Lars Kühmichel", "Stefan T. Radev"], "first_author": "Leonid Pogorelyuk", "abstract": "We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04970", "comments": "UniReps Workshop 2025, 12 pages, 8 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks\nResearch Topic: computer vision\nAuthors: Leonid Pogorelyuk, Niels Bracher, Aaron Verkleeren, Lars Kühmichel, Stefan T. Radev\nAbstract: We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04970\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 73, "abstract_length": 524, "total_text_length": 598, "word_count": 77, "sentence_count": 5}, "basic_keywords": ["pixel", "representations", "overcomplete", "pilot", "family", "stable", "contrastive", "losses"], "domain_keywords": [], "technical_terms": [], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04962", "topic": "computer vision", "title": "Convergence of sample-based quantum diagonalization on a variable-length cuprate chain", "authors": ["L. Andrew Wray", "Cheng-Ju Lin", "Vincent Su", "Hrant Gharibyan"], "first_author": "L. Andrew Wray", "abstract": "Sample-based quantum diagonalization (SQD) is an algorithm for hybrid quantum-classical molecular simulation that has been of broad interest for application with noisy intermediate scale quantum (NISQ) devices. However, SQD does not always converge on a practical timescale. Here, we explore scaling of the algorithm for a variable-length molecule made up of 2 to 6 copper oxide plaquettes with a minimal molecular orbital basis. The results demonstrate that enabling all-to-all connectivity, instituting a higher expansion order for the SQD algorithm, and adopting a non-Hartree-Fock molecular orbital basis can all play significant roles in overcoming sampling bottlenecks, though with tradeoffs that need to be weighed against the capabilities of quantum and classical hardware. Additionally, we find that noise on a real quantum computer, the Quantinuum H2 trapped ion device, can improve energy convergence beyond expectations based on noise-free statevector simulations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cond-mat.other", "physics.chem-ph", "physics.comp-ph"], "url": "http://arxiv.org/abs/2512.04962", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Convergence of sample-based quantum diagonalization on a variable-length cuprate chain\nResearch Topic: computer vision\nAuthors: L. Andrew Wray, Cheng-Ju Lin, Vincent Su, Hrant Gharibyan\nAbstract: Sample-based quantum diagonalization (SQD) is an algorithm for hybrid quantum-classical molecular simulation that has been of broad interest for application with noisy intermediate scale quantum (NISQ) devices. However, SQD does not always converge on a practical timescale. Here, we explore scaling of the algorithm for a variable-length molecule made up of 2 to 6 copper oxide plaquettes with a minimal molecular orbital basis. The results demonstrate that enabling all-to-all connectivity, instituting a higher expansion order for the SQD algorithm, and adopting a non-Hartree-Fock molecular orbital basis can all play significant roles in overcoming sampling bottlenecks, though with tradeoffs that need to be weighed against the capabilities of quantum and classical hardware. Additionally, we find that noise on a real quantum computer, the Quantinuum H2 trapped ion device, can improve energy convergence beyond expectations based on noise-free statevector simulations.\nCategories: quant-ph, cond-mat.other, physics.chem-ph, physics.comp-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04962\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 976, "total_text_length": 1063, "word_count": 148, "sentence_count": 6}, "basic_keywords": ["quantum", "algorithm", "molecular", "based", "classical", "orbital", "basis", "noise"], "domain_keywords": [], "technical_terms": ["Sample", "However", "Quantinuum", "Here", "Additionally", "Hartree", "Fock"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04960", "topic": "computer vision", "title": "Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies", "authors": ["Jonne Van Haastregt", "Bastian Orthmann", "Michael C. Welle", "Yuchong Zhang", "Danica Kragic"], "first_author": "Jonne Van Haastregt", "abstract": "Despite the fact that visuomotor-based policies obtained via imitation learning demonstrate good performances in complex manipulation tasks, they usually struggle to achieve the same accuracy and speed as traditional control based methods. In this work, we introduce Hybrid-Diffusion models that combine open-loop routines with visuomotor diffusion policies. We develop Teleoperation Augmentation Primitives (TAPs) that allow the operator to perform predefined routines, such as locking specific axes, moving to perching waypoints, or triggering task-specific routines seamlessly during demonstrations. Our Hybrid-Diffusion method learns to trigger such TAPs during inference. We validate the method on challenging real-world tasks: Vial Aspiration, Open-Container Liquid Transfer, and container unscrewing. All experimental videos are available on the project's website: https://hybriddiffusion.github.io/", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.04960", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies\nResearch Topic: computer vision\nAuthors: Jonne Van Haastregt, Bastian Orthmann, Michael C. Welle, Yuchong Zhang, Danica Kragic\nAbstract: Despite the fact that visuomotor-based policies obtained via imitation learning demonstrate good performances in complex manipulation tasks, they usually struggle to achieve the same accuracy and speed as traditional control based methods. In this work, we introduce Hybrid-Diffusion models that combine open-loop routines with visuomotor diffusion policies. We develop Teleoperation Augmentation Primitives (TAPs) that allow the operator to perform predefined routines, such as locking specific axes, moving to perching waypoints, or triggering task-specific routines seamlessly during demonstrations. Our Hybrid-Diffusion method learns to trigger such TAPs during inference. We validate the method on challenging real-world tasks: Vial Aspiration, Open-Container Liquid Transfer, and container unscrewing. All experimental videos are available on the project's website: https://hybriddiffusion.github.io/\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04960\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 906, "total_text_length": 995, "word_count": 122, "sentence_count": 8}, "basic_keywords": ["diffusion", "routines", "visuomotor", "based", "policies", "tasks", "hybrid", "open"], "domain_keywords": [], "technical_terms": ["Our Hybrid", "Teleoperation Augmentation Primitives", "Despite", "Container Liquid Transfer", "Open", "Hybrid", "Vial Aspiration", "Diffusion"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04955", "topic": "computer vision", "title": "Bounds on Maximal Leakage over Bayesian Networks", "authors": ["Anuran Makur", "Japneet Singh"], "first_author": "Anuran Makur", "abstract": "Maximal leakage quantifies the leakage of information from data due to an observation . While fundamental properties of maximal leakage, such as data processing, sub-additivity, and its connection to mutual information, are well-established, its behavior over Bayesian networks is not well-understood and existing bounds are primarily limited to binary . In this paper, we investigate the behavior of maximal leakage over Bayesian networks with finite alphabets. Our bounds on maximal leakage are established by utilizing coupling-based characterizations which exist for channels satisfying certain conditions. Furthermore, we provide more general conditions under which such coupling characterizations hold for . In the course of our analysis, we also present a new simultaneous coupling result on maximal leakage exponents. Finally, we illustrate the effectiveness of the proposed bounds with some examples.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IT", "math.PR", "math.ST"], "url": "http://arxiv.org/abs/2512.04955", "comments": "9 pages, double column format, 2 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Bounds on Maximal Leakage over Bayesian Networks\nResearch Topic: computer vision\nAuthors: Anuran Makur, Japneet Singh\nAbstract: Maximal leakage quantifies the leakage of information from data $X \\in \\mathcal{X}$ due to an observation $Y$. While fundamental properties of maximal leakage, such as data processing, sub-additivity, and its connection to mutual information, are well-established, its behavior over Bayesian networks is not well-understood and existing bounds are primarily limited to binary $\\mathcal{X}$. In this paper, we investigate the behavior of maximal leakage over Bayesian networks with finite alphabets. Our bounds on maximal leakage are established by utilizing coupling-based characterizations which exist for channels satisfying certain conditions. Furthermore, we provide more general conditions under which such coupling characterizations hold for $|\\mathcal{X}| = 4$. In the course of our analysis, we also present a new simultaneous coupling result on maximal leakage exponents. Finally, we illustrate the effectiveness of the proposed bounds with some examples.\nCategories: cs.IT, math.PR, math.ST\nPublication Date: 2025-12-04\nPaper ID: 2512.04955\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 48, "abstract_length": 909, "total_text_length": 958, "word_count": 135, "sentence_count": 8}, "basic_keywords": ["leakage", "maximal", "bounds", "coupling", "information", "data", "well", "established"], "domain_keywords": [], "technical_terms": ["While", "Finally", "Maximal", "Furthermore", "Bayesian"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04952", "topic": "computer vision", "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization", "authors": ["Yicheng Liu", "Shiduo Zhang", "Zibin Dong", "Baijun Ye", "Tianyuan Yuan", "Xiaopeng Yu", "Linqi Yin", "Chenhao Lu", "Junhao Shi", "Luca Jiang-Tao Yu", "Liangtao Zheng", "Tao Jiang", "Jingjing Gong", "Xipeng Qiu", "Hang Zhao"], "first_author": "Yicheng Liu", "abstract": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV", "cs.RO"], "url": "http://arxiv.org/abs/2512.04952", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization\nResearch Topic: computer vision\nAuthors: Yicheng Liu, Shiduo Zhang, Zibin Dong, Baijun Ye, Tianyuan Yuan, Xiaopeng Yu, Linqi Yin, Chenhao Lu, Junhao Shi, Luca Jiang-Tao Yu, Liangtao Zheng, Tao Jiang, Jingjing Gong, Xipeng Qiu, Hang Zhao\nAbstract: Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.\nCategories: cs.CV, cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04952\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 102, "abstract_length": 1101, "total_text_length": 1204, "word_count": 145, "sentence_count": 7}, "basic_keywords": ["action", "autoregressive", "inference", "task", "models", "strong", "reconstruction", "faster"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Autoregressive"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04951", "topic": "computer vision", "title": "MAX BISECTION might be harder to approximate than MAX CUT", "authors": ["Joshua Brakensiek", "Neng Huang", "Aaron Potechin", "Uri Zwick"], "first_author": "Joshua Brakensiek", "abstract": "The MAX BISECTION problem seeks a maximum-size cut that evenly divides the vertices of a given undirected graph. An open problem raised by Austrin, Benabbas, and Georgiou is whether MAX BISECTION can be approximated as well as MAX CUT, i.e., to within , which is the approximation ratio achieved by the celebrated Goemans-Williamson algorithm for MAX CUT, which is best possible assuming the Unique Games Conjecture (UGC). They conjectured that the answer is yes. The current paradigm for obtaining approximation algorithms for MAX BISECTION, due to Raghavendra and Tan and Austrin, Benabbas, and Georgiou, follows a two-phase approach. First, a large number of rounds of the Sum-of-Squares (SoS) hierarchy is used to find a solution to the ``Basic SDP'' relaxation of MAX CUT which is -uncorrelated, for an arbitrarily small . Second, standard SDP rounding techniques (such as ) are used to round this -uncorrelated solution, producing with high probability a cut that is almost balanced, i.e., a cut that has at most fraction of the vertices on each side. This cut is then converted into an exact bisection of the graph with only a small loss. In this paper, we show that this two-stage paradigm cannot be used to obtain an -approximation algorithm for MAX BISECTION if one relies only on the -uncorrelatedness property of the solution produced by the first phase. More precisely, for any , we construct an explicit instance of MAX BISECTION for which the ratio between the value of the optimal integral solution and the value of some -uncorrelated solution of the Basic SDP relaxation is less than . Our instances are also integrality gaps for the Basic SDP relaxation of MAX BISECTION.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CC", "cs.DS"], "url": "http://arxiv.org/abs/2512.04951", "comments": "35 pages, 2 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: MAX BISECTION might be harder to approximate than MAX CUT\nResearch Topic: computer vision\nAuthors: Joshua Brakensiek, Neng Huang, Aaron Potechin, Uri Zwick\nAbstract: The MAX BISECTION problem seeks a maximum-size cut that evenly divides the vertices of a given undirected graph. An open problem raised by Austrin, Benabbas, and Georgiou is whether MAX BISECTION can be approximated as well as MAX CUT, i.e., to within ${α_{GW}}\\approx 0.8785672\\ldots$, which is the approximation ratio achieved by the celebrated Goemans-Williamson algorithm for MAX CUT, which is best possible assuming the Unique Games Conjecture (UGC). They conjectured that the answer is yes.   The current paradigm for obtaining approximation algorithms for MAX BISECTION, due to Raghavendra and Tan and Austrin, Benabbas, and Georgiou, follows a two-phase approach. First, a large number of rounds of the Sum-of-Squares (SoS) hierarchy is used to find a solution to the ``Basic SDP'' relaxation of MAX CUT which is $\\varepsilon$-uncorrelated, for an arbitrarily small $\\varepsilon > 0$. Second, standard SDP rounding techniques (such as ${\\cal THRESH}$) are used to round this $\\varepsilon$-uncorrelated solution, producing with high probability a cut that is almost balanced, i.e., a cut that has at most $\\frac12+\\varepsilon$ fraction of the vertices on each side. This cut is then converted into an exact bisection of the graph with only a small loss.   In this paper, we show that this two-stage paradigm cannot be used to obtain an $α_{GW}$-approximation algorithm for MAX BISECTION if one relies only on the $\\varepsilon$-uncorrelatedness property of the solution produced by the first phase. More precisely, for any $\\varepsilon > 0$, we construct an explicit instance of MAX BISECTION for which the ratio between the value of the optimal integral solution and the value of some $\\varepsilon$-uncorrelated solution of the Basic SDP relaxation is less than $0.87853 < {α_{GW}}$. Our instances are also integrality gaps for the Basic SDP relaxation of MAX BISECTION.\nCategories: cs.CC, cs.DS\nPublication Date: 2025-12-04\nPaper ID: 2512.04951\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 1689, "total_text_length": 1747, "word_count": 288, "sentence_count": 15}, "basic_keywords": ["bisection", "solution", "approximation", "used", "basic", "relaxation", "uncorrelated", "problem"], "domain_keywords": [], "technical_terms": ["Basic", "Austrin", "Benabbas", "Williamson", "Squares", "Unique Games Conjecture", "Goemans", "Second"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04950", "topic": "computer vision", "title": "Opacity problems in multi-energy timed automata", "authors": ["Étienne André", "Lydia Bakiri"], "first_author": "Étienne André", "abstract": "Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR"], "url": "http://arxiv.org/abs/2512.04950", "comments": "This is the author version (extended with all proofs) of the manuscript of the same name published in the proceedings of the 41st ACM/SIGAPP Symposium on Applied Computing (SAC 2026)", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Opacity problems in multi-energy timed automata\nResearch Topic: computer vision\nAuthors: Étienne André, Lydia Bakiri\nAbstract: Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.\nCategories: cs.CR\nPublication Date: 2025-12-04\nPaper ID: 2512.04950\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 47, "abstract_length": 704, "total_text_length": 752, "word_count": 114, "sentence_count": 5}, "basic_keywords": ["energy", "variables", "over", "time", "systems", "information", "timed", "automata"], "domain_keywords": [], "technical_terms": ["Cyber", "Despite"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04948", "topic": "computer vision", "title": "On world-volume supersymmetry of supermembrane action in static gauge", "authors": ["Arkady A. Tseytlin", "Zihan Wang"], "first_author": "Arkady A. Tseytlin", "abstract": "We review and elaborate on the issue of 3d world volume supersymmetry that appears as a residual part of global target space supersymmetry in the BST supermembrane action. While there is no direct ``spinning membrane'' analog of the world-volume supersymmetric spinning string action that could be obtained by coupling copies of 3d scalar multiplet to 3d supergravity, we discuss how one may construct an 3d supersymmetric analog of the derivative expansion of the bosonic membrane action in static gauge. We compare the resulting supersymmetric action for eight 3d scalar multiplets to the 3d supersymmetric action describing the supermembrane in the static gauge. The two actions are not equivalent which is related to the fact that the full supersymmetry of the static-gauge supermembrane action can be realised only if the fermions are described by an spinor rather than vector. The two actions are still directly related in special dimensions and 5. We also compute the one-loop world-volume scattering amplitudes for the two theories, finding that they indeed agree for but disagree for .", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["hep-th"], "url": "http://arxiv.org/abs/2512.04948", "comments": "20 pages", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: On world-volume supersymmetry of supermembrane action in static gauge\nResearch Topic: computer vision\nAuthors: Arkady A. Tseytlin, Zihan Wang\nAbstract: We review and elaborate on the issue of 3d world volume supersymmetry that appears as a residual part of global target space supersymmetry in the BST supermembrane action. While there is no direct ``spinning membrane'' analog of the world-volume supersymmetric spinning string action that could be obtained by coupling $D$ copies of 3d scalar multiplet to 3d supergravity, we discuss how one may construct an $N=1$ 3d supersymmetric analog of the derivative expansion of the bosonic membrane action in static gauge. We compare the resulting $N=1$ supersymmetric action for eight 3d scalar multiplets to the $N=8$ 3d supersymmetric action describing the $D=11$ supermembrane in the static gauge. The two actions are not equivalent which is related to the fact that the full $N=8$ supersymmetry of the static-gauge $D=11$ supermembrane action can be realised only if the fermions are described by an $SO(8)$ spinor rather than vector. The two actions are still directly related in special dimensions $D=4$ and 5. We also compute the one-loop world-volume scattering amplitudes for the two theories, finding that they indeed agree for $D=4,5$ but disagree for $D=11$.\nCategories: hep-th\nPublication Date: 2025-12-04\nPaper ID: 2512.04948\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1094, "total_text_length": 1164, "word_count": 181, "sentence_count": 7}, "basic_keywords": ["action", "supersymmetric", "world", "volume", "supersymmetry", "supermembrane", "static", "gauge"], "domain_keywords": [], "technical_terms": ["While"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.75, "quality_tier": "medium"}
{"paper_id": "2512.04947", "topic": "computer vision", "title": "Crack detection by holomorphic neural networks and transfer-learning-enhanced genetic optimization", "authors": ["Jonas Hund", "Nicolas Cuenca", "Tito Andriollo"], "first_author": "Jonas Hund", "abstract": "A new strategy for detecting cracks in 2D solids based on strain data is introduced. Crack detection is formulated as an inverse problem and solved using genetic optimization. The novelty lies in the evaluation of the model response at each generation. Specifically, the solution to the corresponding plane elasticity problem is expressed via holomorphic potentials, which are determined by training two holomorphic neural networks. As the potentials satisfy equilibrium and traction-free conditions along the crack faces a priori, the training proceeds quickly based solely on boundary information. Training efficiency is further improved by splitting the genetic search into long-range and short-range stages, enabling the use of transfer learning in the latter. The new strategy is tested on three benchmark problems, showing that an optimal number of training epochs exists that provides the best overall performance. A comparison is also made with a popular crack detection approach that uses XFEM to compute the model response. Under the assumption of identical stress-field representation accuracy, the proposed method is found to be between 7 and 23 times faster than the XFEM-based approach. While the strategy is presented here for the simplified case of a single internal crack, generalization is feasible. Overall, the present findings demonstrate that combining genetic optimization with holomorphic neural networks and transfer learning offers a promising avenue for developing crack detection strategies with higher efficiency than those currently available.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CE"], "url": "http://arxiv.org/abs/2512.04947", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Crack detection by holomorphic neural networks and transfer-learning-enhanced genetic optimization\nResearch Topic: computer vision\nAuthors: Jonas Hund, Nicolas Cuenca, Tito Andriollo\nAbstract: A new strategy for detecting cracks in 2D solids based on strain data is introduced. Crack detection is formulated as an inverse problem and solved using genetic optimization. The novelty lies in the evaluation of the model response at each generation. Specifically, the solution to the corresponding plane elasticity problem is expressed via holomorphic potentials, which are determined by training two holomorphic neural networks. As the potentials satisfy equilibrium and traction-free conditions along the crack faces a priori, the training proceeds quickly based solely on boundary information. Training efficiency is further improved by splitting the genetic search into long-range and short-range stages, enabling the use of transfer learning in the latter. The new strategy is tested on three benchmark problems, showing that an optimal number of training epochs exists that provides the best overall performance. A comparison is also made with a popular crack detection approach that uses XFEM to compute the model response. Under the assumption of identical stress-field representation accuracy, the proposed method is found to be between 7 and 23 times faster than the XFEM-based approach. While the strategy is presented here for the simplified case of a single internal crack, generalization is feasible. Overall, the present findings demonstrate that combining genetic optimization with holomorphic neural networks and transfer learning offers a promising avenue for developing crack detection strategies with higher efficiency than those currently available.\nCategories: cs.CE\nPublication Date: 2025-12-04\nPaper ID: 2512.04947\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 98, "abstract_length": 1573, "total_text_length": 1672, "word_count": 239, "sentence_count": 12}, "basic_keywords": ["crack", "training", "strategy", "based", "detection", "genetic", "holomorphic", "problem"], "domain_keywords": ["transfer learning"], "technical_terms": ["While", "Under", "Crack", "Training", "Overall", "Specifically"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04943", "topic": "computer vision", "title": "Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition", "authors": ["Novanto Yudistira"], "first_author": "Novanto Yudistira", "abstract": "This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04943", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition\nResearch Topic: computer vision\nAuthors: Novanto Yudistira\nAbstract: This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04943\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1582, "total_text_length": 1663, "word_count": 206, "sentence_count": 9}, "basic_keywords": ["recognition", "action", "fusion", "human", "across", "information", "gating", "mechanisms"], "domain_keywords": ["optical flow"], "technical_terms": ["Gating", "Employing", "Through"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04939", "topic": "computer vision", "title": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging", "authors": ["Zhijian Shu", "Cheng Lin", "Tao Xie", "Wei Yin", "Ben Li", "Zhiyuan Pu", "Weize Li", "Yao Yao", "Xun Cao", "Xiaoyang Guo", "Xiao-Xiao Long"], "first_author": "Zhijian Shu", "abstract": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04939", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging\nResearch Topic: computer vision\nAuthors: Zhijian Shu, Cheng Lin, Tao Xie, Wei Yin, Ben Li, Zhiyuan Pu, Weize Li, Yao Yao, Xun Cao, Xiaoyang Guo, Xiao-Xiao Long\nAbstract: 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04939\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1312, "total_text_length": 1384, "word_count": 177, "sentence_count": 12}, "basic_keywords": ["token", "geometric", "litevggt", "efficient", "geometry", "vggt", "memory", "scenes"], "domain_keywords": ["3d reconstruction"], "technical_terms": ["Extensive", "However", "Project", "Visual Geometry Grounded Transformer", "Guided"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04927", "topic": "computer vision", "title": "Virtually Unrolling the Herculaneum Papyri by Diffeomorphic Spiral Fitting", "authors": ["Paul Henderson"], "first_author": "Paul Henderson", "abstract": "The Herculaneum Papyri are a collection of rolled papyrus documents that were charred and buried by the famous eruption of Mount Vesuvius. They promise to contain a wealth of previously unseen Greek and Latin texts, but are extremely fragile and thus most cannot be unrolled physically. A solution to access these texts is virtual unrolling, where the papyrus surface is digitally traced out in a CT scan of the scroll, to create a flattened representation. This tracing is very laborious to do manually in gigavoxel-sized scans, so automated approaches are desirable. We present the first top-down method that automatically fits a surface model to a CT scan of a severely damaged scroll. We take a novel approach that globally fits an explicit parametric model of the deformed scroll to existing neural network predictions of where the rolled papyrus likely passes. Our method guarantees the resulting surface is a single continuous 2D sheet, even passing through regions where the surface is not detectable in the CT scan. We conduct comprehensive experiments on high-resolution CT scans of two scrolls, showing that our approach successfully unrolls large regions, and exceeds the performance of the only existing automated unrolling method suitable for this data.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04927", "comments": "Accepted at WACV 2026", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Virtually Unrolling the Herculaneum Papyri by Diffeomorphic Spiral Fitting\nResearch Topic: computer vision\nAuthors: Paul Henderson\nAbstract: The Herculaneum Papyri are a collection of rolled papyrus documents that were charred and buried by the famous eruption of Mount Vesuvius. They promise to contain a wealth of previously unseen Greek and Latin texts, but are extremely fragile and thus most cannot be unrolled physically. A solution to access these texts is virtual unrolling, where the papyrus surface is digitally traced out in a CT scan of the scroll, to create a flattened representation. This tracing is very laborious to do manually in gigavoxel-sized scans, so automated approaches are desirable. We present the first top-down method that automatically fits a surface model to a CT scan of a severely damaged scroll. We take a novel approach that globally fits an explicit parametric model of the deformed scroll to existing neural network predictions of where the rolled papyrus likely passes. Our method guarantees the resulting surface is a single continuous 2D sheet, even passing through regions where the surface is not detectable in the CT scan. We conduct comprehensive experiments on high-resolution CT scans of two scrolls, showing that our approach successfully unrolls large regions, and exceeds the performance of the only existing automated unrolling method suitable for this data.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04927\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1267, "total_text_length": 1342, "word_count": 209, "sentence_count": 9}, "basic_keywords": ["surface", "papyrus", "scan", "scroll", "method", "rolled", "texts", "unrolling"], "domain_keywords": [], "technical_terms": ["Mount Vesuvius", "Greek", "The Herculaneum Papyri", "Latin", "They"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04926", "topic": "computer vision", "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion", "authors": ["Yueming Pan", "Ruoyu Feng", "Qi Dai", "Yuqi Wang", "Wenfeng Lin", "Mingyu Guo", "Chong Luo", "Nanning Zheng"], "first_author": "Yueming Pan", "abstract": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04926", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion\nResearch Topic: computer vision\nAuthors: Yueming Pan, Ruoyu Feng, Qi Dai, Yuqi Wang, Wenfeng Lin, Mingyu Guo, Chong Luo, Nanning Zheng\nAbstract: Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04926\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 100, "abstract_length": 1460, "total_text_length": 1561, "word_count": 201, "sentence_count": 17}, "basic_keywords": ["semantic", "texture", "latent", "diffusion", "fine", "generation", "semantics", "ldms"], "domain_keywords": ["imagenet"], "technical_terms": ["Observing", "Project", "Latent Diffusion Models", "First Diffusion", "Recent", "Semantic"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04924", "topic": "computer vision", "title": "Markov-Renewal Single-Photon LiDAR Simulator", "authors": ["Weijian Zhang", "Prateek Chennuri", "Hashan K. Weerasooriya", "Bole Ma", "Stanley H. Chan"], "first_author": "Weijian Zhang", "abstract": "Single-photon LiDAR (SP-LiDAR) simulators face a dilemma: fast but inaccurate Poisson models or accurate but prohibitively slow sequential models. This paper breaks that compromise. We present a simulator that achieves both fidelity and speed by focusing on the critical, yet overlooked, component of simulation: the photon count statistics. Our key contribution is a Markov-renewal process (MRP) formulation that, for the first time, analytically predicts the mean and variance of registered photon counts under dead time. To make this MRP model computationally tractable, we introduce a spectral truncation rule that efficiently computes the complex covariance statistics. By proving the shift-invariance of the process, we extend this per-pixel model to full histogram cube generation via a precomputed lookup table. Our method generates 3D cubes indistinguishable from the sequential gold-standard, yet is orders of magnitude faster. This finally enables large-scale, physically-faithful data generation for learning-based SP-LiDAR reconstruction.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SP"], "url": "http://arxiv.org/abs/2512.04924", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:17.882092", "embedding_text": "Paper Title: Markov-Renewal Single-Photon LiDAR Simulator\nResearch Topic: computer vision\nAuthors: Weijian Zhang, Prateek Chennuri, Hashan K. Weerasooriya, Bole Ma, Stanley H. Chan\nAbstract: Single-photon LiDAR (SP-LiDAR) simulators face a dilemma: fast but inaccurate Poisson models or accurate but prohibitively slow sequential models. This paper breaks that compromise. We present a simulator that achieves both fidelity and speed by focusing on the critical, yet overlooked, component of simulation: the photon count statistics. Our key contribution is a Markov-renewal process (MRP) formulation that, for the first time, analytically predicts the mean and variance of registered photon counts under dead time. To make this MRP model computationally tractable, we introduce a spectral truncation rule that efficiently computes the complex covariance statistics. By proving the shift-invariance of the process, we extend this per-pixel model to full histogram cube generation via a precomputed lookup table. Our method generates 3D cubes indistinguishable from the sequential gold-standard, yet is orders of magnitude faster. This finally enables large-scale, physically-faithful data generation for learning-based SP-LiDAR reconstruction.\nCategories: eess.SP\nPublication Date: 2025-12-04\nPaper ID: 2512.04924\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 44, "abstract_length": 1051, "total_text_length": 1096, "word_count": 148, "sentence_count": 9}, "basic_keywords": ["photon", "lidar", "models", "sequential", "statistics", "process", "time", "model"], "domain_keywords": [], "technical_terms": ["Markov", "Single", "Poisson"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04917", "topic": "computer vision", "title": "On Disturbance-Aware Minimum-Time Trajectory Planning: Evidence from Tests on a Dynamic Driving Simulator", "authors": ["Matteo Masoni", "Vincenzo Palermo", "Marco Gabiccini", "Martino Gulisano", "Giorgio Previati", "Massimiliano Gobbi", "Francesco Comolli", "Gianpiero Mastinu", "Massimo Guiggiani"], "first_author": "Matteo Masoni", "abstract": "This work investigates how disturbance-aware, robustness-embedded reference trajectories translate into driving performance when executed by professional drivers in a dynamic simulator. Three planned reference trajectories are compared against a free-driving baseline (NOREF) to assess trade-offs between lap time (LT) and steering effort (SE): NOM, the nominal time-optimal trajectory; TLC, a track-limit-robust trajectory obtained by tightening margins to the track edges; and FLC, a friction-limit-robust trajectory obtained by tightening against axle and tire saturation. All trajectories share the same minimum lap-time objective with a small steering-smoothness regularizer and are evaluated by two professional drivers using a high-performance car on a virtual track. The trajectories derive from a disturbance-aware minimum-lap-time framework recently proposed by the authors, where worst-case disturbance growth is propagated over a finite horizon and used to tighten tire-friction and track-limit constraints, preserving performance while providing probabilistic safety margins. LT and SE are used as performance indicators, while RMS lateral deviation, speed error, and drift angle characterize driving style. Results show a Pareto-like LT-SE trade-off: NOM yields the shortest LT but highest SE; TLC minimizes SE at the cost of longer LT; FLC lies near the efficient frontier, substantially reducing SE relative to NOM with only a small LT increase. Removing trajectory guidance (NOREF) increases both LT and SE, confirming that reference trajectories improve pace and control efficiency. Overall, the findings highlight reference-based and disturbance-aware planning, especially FLC, as effective tools for training and for achieving fast yet stable trajectories.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO", "eess.SY"], "url": "http://arxiv.org/abs/2512.04917", "comments": "18 pages, 11 figures, 5 tables", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: On Disturbance-Aware Minimum-Time Trajectory Planning: Evidence from Tests on a Dynamic Driving Simulator\nResearch Topic: computer vision\nAuthors: Matteo Masoni, Vincenzo Palermo, Marco Gabiccini, Martino Gulisano, Giorgio Previati, Massimiliano Gobbi, Francesco Comolli, Gianpiero Mastinu, Massimo Guiggiani\nAbstract: This work investigates how disturbance-aware, robustness-embedded reference trajectories translate into driving performance when executed by professional drivers in a dynamic simulator. Three planned reference trajectories are compared against a free-driving baseline (NOREF) to assess trade-offs between lap time (LT) and steering effort (SE): NOM, the nominal time-optimal trajectory; TLC, a track-limit-robust trajectory obtained by tightening margins to the track edges; and FLC, a friction-limit-robust trajectory obtained by tightening against axle and tire saturation. All trajectories share the same minimum lap-time objective with a small steering-smoothness regularizer and are evaluated by two professional drivers using a high-performance car on a virtual track. The trajectories derive from a disturbance-aware minimum-lap-time framework recently proposed by the authors, where worst-case disturbance growth is propagated over a finite horizon and used to tighten tire-friction and track-limit constraints, preserving performance while providing probabilistic safety margins. LT and SE are used as performance indicators, while RMS lateral deviation, speed error, and drift angle characterize driving style. Results show a Pareto-like LT-SE trade-off: NOM yields the shortest LT but highest SE; TLC minimizes SE at the cost of longer LT; FLC lies near the efficient frontier, substantially reducing SE relative to NOM with only a small LT increase. Removing trajectory guidance (NOREF) increases both LT and SE, confirming that reference trajectories improve pace and control efficiency. Overall, the findings highlight reference-based and disturbance-aware planning, especially FLC, as effective tools for training and for achieving fast yet stable trajectories.\nCategories: cs.RO, eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04917\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 105, "abstract_length": 1776, "total_text_length": 1882, "word_count": 253, "sentence_count": 9}, "basic_keywords": ["trajectories", "disturbance", "reference", "performance", "time", "trajectory", "track", "aware"], "domain_keywords": [], "technical_terms": ["Removing", "Pareto", "Results", "Overall", "Three"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04913", "topic": "computer vision", "title": "Communicating Properties of Quantum States over Classical Noisy Channels", "authors": ["Nikhitha Nunavath", "Jiechen Chen", "Osvaldo Simeone", "Riccardo Bassoli", "Frank H. P. Fitzek"], "first_author": "Nikhitha Nunavath", "abstract": "Transmitting information about quantum states over classical noisy channels is an important problem with applications to science, computing, and sensing. This task, however, poses fundamental challenges due to the exponential scaling of state space with system size. We introduce shadow tomography-based transmission with unequal error protection (STT-UEP), a novel communication protocol that enables efficient transmission of properties of quantum states, allowing decoder-side estimation of arbitrary observables. Unlike conventional approaches requiring the transmission of a number of bits that is exponential in the number of qubits, STT-UEP achieves communication complexity that scales logarithmically with the number of observables, depending on the observable weight. The protocol exploits classical shadow tomography for measurement efficiency, and applies unequal error protection by encoding measurement bases with stronger channel codes than measurement outcomes. We provide theoretical guarantees on estimation accuracy as a function of the bit error probability of the classical channel, and validate the approach against several benchmarks via numerical results.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "eess.SP"], "url": "http://arxiv.org/abs/2512.04913", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Communicating Properties of Quantum States over Classical Noisy Channels\nResearch Topic: computer vision\nAuthors: Nikhitha Nunavath, Jiechen Chen, Osvaldo Simeone, Riccardo Bassoli, Frank H. P. Fitzek\nAbstract: Transmitting information about quantum states over classical noisy channels is an important problem with applications to science, computing, and sensing. This task, however, poses fundamental challenges due to the exponential scaling of state space with system size. We introduce shadow tomography-based transmission with unequal error protection (STT-UEP), a novel communication protocol that enables efficient transmission of properties of quantum states, allowing decoder-side estimation of arbitrary observables. Unlike conventional approaches requiring the transmission of a number of bits that is exponential in the number of qubits, STT-UEP achieves communication complexity that scales logarithmically with the number of observables, depending on the observable weight. The protocol exploits classical shadow tomography for measurement efficiency, and applies unequal error protection by encoding measurement bases with stronger channel codes than measurement outcomes. We provide theoretical guarantees on estimation accuracy as a function of the bit error probability of the classical channel, and validate the approach against several benchmarks via numerical results.\nCategories: quant-ph, eess.SP\nPublication Date: 2025-12-04\nPaper ID: 2512.04913\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1179, "total_text_length": 1252, "word_count": 165, "sentence_count": 7}, "basic_keywords": ["classical", "transmission", "error", "number", "measurement", "quantum", "states", "exponential"], "domain_keywords": [], "technical_terms": ["Transmitting", "Unlike"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04908", "topic": "computer vision", "title": "Logic-Driven Cybersecurity: A Novel Framework for System Log Anomaly Detection using Answer Set Programming", "authors": ["Fang Li", "Fei Zuo", "Gopal Gupta"], "first_author": "Fang Li", "abstract": "This study explores the application of Answer Set Programming (ASP) for detecting anomalies in system logs, addressing the challenges posed by evolving cyber threats. We propose a novel framework that leverages ASP's declarative nature and logical reasoning capabilities to encode complex security rules as logical predicates. Our ASP-based system was applied to a real-world Linux system log dataset, demonstrating its effectiveness in identifying various anomalies such as potential brute-force attacks, privilege escalations, frequent network connections from specific IPs, and various system-level issues. Key findings highlight ASP's strengths in handling structured log data, rule flexibility, and event correlation. The approach shows promise in providing explainable alerts from real-world data. This research contributes to computer forensics by demonstrating a logic-based paradigm for log analysis on a practical dataset, opening avenues for more nuanced and adaptive cyber intelligence systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR", "cs.LO"], "url": "http://arxiv.org/abs/2512.04908", "comments": "Submitted to FLOPS 2026", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Logic-Driven Cybersecurity: A Novel Framework for System Log Anomaly Detection using Answer Set Programming\nResearch Topic: computer vision\nAuthors: Fang Li, Fei Zuo, Gopal Gupta\nAbstract: This study explores the application of Answer Set Programming (ASP) for detecting anomalies in system logs, addressing the challenges posed by evolving cyber threats. We propose a novel framework that leverages ASP's declarative nature and logical reasoning capabilities to encode complex security rules as logical predicates. Our ASP-based system was applied to a real-world Linux system log dataset, demonstrating its effectiveness in identifying various anomalies such as potential brute-force attacks, privilege escalations, frequent network connections from specific IPs, and various system-level issues. Key findings highlight ASP's strengths in handling structured log data, rule flexibility, and event correlation. The approach shows promise in providing explainable alerts from real-world data. This research contributes to computer forensics by demonstrating a logic-based paradigm for log analysis on a practical dataset, opening avenues for more nuanced and adaptive cyber intelligence systems.\nCategories: cs.CR, cs.LO\nPublication Date: 2025-12-04\nPaper ID: 2512.04908\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 107, "abstract_length": 1006, "total_text_length": 1114, "word_count": 150, "sentence_count": 7}, "basic_keywords": ["system", "anomalies", "cyber", "logical", "based", "real", "world", "dataset"], "domain_keywords": [], "technical_terms": ["Answer Set Programming", "Linux"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04900", "topic": "computer vision", "title": "Local mixing length theory with compositional effects:\\ First application to asymptotic giant branch evolution", "authors": ["M. M. Ocampo", "M. M. Miller Bertolami", "A. H. Córsico", "L. G. Althaus"], "first_author": "M. M. Ocampo", "abstract": "During the evolution of stars on the asymptotic giant branch (AGB), thermal pulses lead to the formation of strongly stratified layers in the outer regions of the CO core, which might lead to inversions in the chemical gradient. Such inversions would produce instabilities beyond the ones predicted by the Schwarzschild criterion and the standard use of mixing length theory (MLT). We used a set of MLT equations that consider the impact of the background chemical gradients. This extension of MLT is referred to in this work as MLT, to make a distinction between both prescriptions. We applied MLT in tandem with the more general Ledoux instability criterion. We computed the evolution in the AGB phase and compared the chemical profiles resulting from MLT, MLT and the double diffusive GNA theory. We continued the evolution through a post-AGB thermal pulse and performed a pulsational analysis of the resultant GW Vir models to asses -mode pulsation periods. Finally, we tested our results with pulsation properties of known GW Vir stars derived from recent observations. We find that the much simpler MLT set of equations closely reproduces the results from the GNA theory. As such, MLT offers a simple way to include chemically driven convection in stellar evolution computations. Stellar evolution simulations show that Rayleigh-Taylor and thermohaline instabilities can play an important role during the TP-AGB. We obtained significantly different chemical profiles using a standard MLT approach compared to those resulting from our MLT and GNA computations. Our adiabatic pulsational analysis shows that these differences in the chemical stratification leave clear mode-trapping signatures in the pulsation spectrum of the GW Vir models.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["astro-ph.SR"], "url": "http://arxiv.org/abs/2512.04900", "comments": "11 pages, 9 figures. Accepted for publication in Astronomy & Astrophysics", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Local mixing length theory with compositional effects:\\ First application to asymptotic giant branch evolution\nResearch Topic: computer vision\nAuthors: M. M. Ocampo, M. M. Miller Bertolami, A. H. Córsico, L. G. Althaus\nAbstract: During the evolution of stars on the asymptotic giant branch (AGB), thermal pulses lead to the formation of strongly stratified layers in the outer regions of the CO core, which might lead to inversions in the chemical gradient. Such inversions would produce instabilities beyond the ones predicted by the Schwarzschild criterion and the standard use of mixing length theory (MLT). We used a set of MLT equations that consider the impact of the background chemical gradients. This extension of MLT is referred to in this work as MLT$\\sharp$, to make a distinction between both prescriptions. We applied MLT$\\sharp$ in tandem with the more general Ledoux instability criterion. We computed the evolution in the AGB phase and compared the chemical profiles resulting from MLT, MLT$\\sharp$ and the double diffusive GNA theory. We continued the evolution through a post-AGB thermal pulse and performed a pulsational analysis of the resultant GW Vir models to asses $g$-mode pulsation periods. Finally, we tested our results with pulsation properties of known GW Vir stars derived from recent observations. We find that the much simpler MLT$\\sharp$ set of equations closely reproduces the results from the GNA theory. As such, MLT$\\sharp$ offers a simple way to include chemically driven convection in stellar evolution computations. Stellar evolution simulations show that Rayleigh-Taylor and thermohaline instabilities can play an important role during the TP-AGB. We obtained significantly different chemical profiles using a standard MLT approach compared to those resulting from our MLT$\\sharp$ and GNA computations. Our adiabatic pulsational analysis shows that these differences in the chemical stratification leave clear mode-trapping signatures in the pulsation spectrum of the GW Vir models.\nCategories: astro-ph.SR\nPublication Date: 2025-12-04\nPaper ID: 2512.04900\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 110, "abstract_length": 1745, "total_text_length": 1856, "word_count": 283, "sentence_count": 14}, "basic_keywords": ["evolution", "chemical", "theory", "pulsation", "during", "stars", "thermal", "lead"], "domain_keywords": [], "technical_terms": ["Finally", "Ledoux", "Stellar", "Rayleigh", "Taylor", "During", "Such", "Schwarzschild"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04897", "topic": "computer vision", "title": "In search of the electron-phonon contribution to total energy", "authors": ["Samuel Poncé", "Xavier Gonze"], "first_author": "Samuel Poncé", "abstract": "The total energy is a fundamental characteristic of solids, molecules, and nanostructures. In most first-principles calculations of the total energy, the nuclear kinetic operator is decoupled from the many-body electronic Hamiltonian and the dynamics of the nuclei is reintroduced afterwards. This two-step procedure introduced by Born and Oppenheimer (BO) is approximate. Energies beyond the electronic and vibrational (or phononic) main contributions might be relevant when small energy differences are important, such as when predicting stable polymorphs or describing magnetic energy landscape. We clarify the different flavors of BO decoupling and give an exact formulation for the total energy in the basis of BO electronic wavefunctions. Then, we list contributions, beyond the main ones, that appear in a perturbative expansion in powers of , where is a typical nuclear mass, up to sixth order. Some of these might be grouped and denoted the electron-phonon contribution to total energy, , that first appears at fourth order. The electronic inertial mass contributes at sixth order. We clarify that the sum of the Allen-Heine-Cardona zero-point renormalization of eigenvalues over occupied states is not the electron-phonon contribution to the total energy but a part of the phononic contribution. The computation of the lowest-order is implemented and shown to be small but non-negligible (3.8 meV per atom) in the case of diamond and its hexagonal polymorph. We also estimate the electronic inertial mass contribution and confirm the size-consistency of all computed terms.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04897", "comments": "27 pages and 5 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: In search of the electron-phonon contribution to total energy\nResearch Topic: computer vision\nAuthors: Samuel Poncé, Xavier Gonze\nAbstract: The total energy is a fundamental characteristic of solids, molecules, and nanostructures. In most first-principles calculations of the total energy, the nuclear kinetic operator is decoupled from the many-body electronic Hamiltonian and the dynamics of the nuclei is reintroduced afterwards. This two-step procedure introduced by Born and Oppenheimer (BO) is approximate. Energies beyond the electronic and vibrational (or phononic) main contributions might be relevant when small energy differences are important, such as when predicting stable polymorphs or describing magnetic energy landscape. We clarify the different flavors of BO decoupling and give an exact formulation for the total energy in the basis of BO electronic wavefunctions. Then, we list contributions, beyond the main ones, that appear in a perturbative expansion in powers of $M_0^{-1/4}$, where $M_0$ is a typical nuclear mass, up to sixth order. Some of these might be grouped and denoted the electron-phonon contribution to total energy, $E^{\\textrm{elph}}$, that first appears at fourth order. The electronic inertial mass contributes at sixth order. We clarify that the sum of the Allen-Heine-Cardona zero-point renormalization of eigenvalues over occupied states is not the electron-phonon contribution to the total energy but a part of the phononic contribution. The computation of the lowest-order $E^{\\textrm{elph}}$ is implemented and shown to be small but non-negligible (3.8 meV per atom) in the case of diamond and its hexagonal polymorph. We also estimate the electronic inertial mass contribution and confirm the size-consistency of all computed terms.\nCategories: cond-mat.mtrl-sci\nPublication Date: 2025-12-04\nPaper ID: 2512.04897\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 1583, "total_text_length": 1645, "word_count": 245, "sentence_count": 13}, "basic_keywords": ["energy", "total", "electronic", "order", "contribution", "mass", "first", "nuclear"], "domain_keywords": [], "technical_terms": ["Hamiltonian", "Energies", "Allen", "Cardona", "Born", "Heine", "Oppenheimer", "Some"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04890", "topic": "computer vision", "title": "Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI", "authors": ["Ramya Muthukrishnan", "Borjan Gagoski", "Aryn Lee", "P. Ellen Grant", "Elfar Adalsteinsson", "Polina Golland", "Benjamin Billot"], "first_author": "Ramya Muthukrishnan", "abstract": "We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04890", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI\nResearch Topic: computer vision\nAuthors: Ramya Muthukrishnan, Borjan Gagoski, Aryn Lee, P. Ellen Grant, Elfar Adalsteinsson, Polina Golland, Benjamin Billot\nAbstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04890\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 1111, "total_text_length": 1173, "word_count": 163, "sentence_count": 10}, "basic_keywords": ["pose", "clinical", "fetal", "head", "volumes", "estimation", "method", "equivariance"], "domain_keywords": ["pose estimation"], "technical_terms": ["Pose", "Crucially", "Existing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04888", "topic": "computer vision", "title": "You Only Train Once (YOTO): A Retraining-Free Object Detection Framework", "authors": ["Priyanto Hidayatullah", "Nurjannah Syakrani", "Yudi Widhiyasana", "Muhammad Rizqi Sholahuddin", "Refdinal Tubagus", "Zahri Al Adzani Hidayat", "Hanri Fajar Ramadhan", "Dafa Alfarizki Pratama", "Farhan Muhammad Yasin"], "first_author": "Priyanto Hidayatullah", "abstract": "Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04888", "comments": "under review in the Elsevier Engineering Journal", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: You Only Train Once (YOTO): A Retraining-Free Object Detection Framework\nResearch Topic: computer vision\nAuthors: Priyanto Hidayatullah, Nurjannah Syakrani, Yudi Widhiyasana, Muhammad Rizqi Sholahuddin, Refdinal Tubagus, Zahri Al Adzani Hidayat, Hanri Fajar Ramadhan, Dafa Alfarizki Pratama, Farhan Muhammad Yasin\nAbstract: Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04888\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 72, "abstract_length": 1614, "total_text_length": 1687, "word_count": 240, "sentence_count": 14}, "basic_keywords": ["products", "object", "detection", "training", "time", "numerous", "issue", "catastrophic"], "domain_keywords": ["computer vision", "object detection", "feature extraction"], "technical_terms": ["Furthermore", "Object", "You Only Train Once", "Proxy Anchor Loss", "Nonetheless", "Qdrant"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04887", "topic": "computer vision", "title": "Optical Readout of Reconfigurable Layered Magnetic Domain Structure in CrSBr", "authors": ["Aleksandra Łopion", "Pierre-Maurice Piel", "Manuel Terbeck", "Jan-Hendrik Larusch", "Jakob Henz", "Marie-Christin Heißenbüttel", "Kseniia Mosina", "Thorsten Deilmann", "Michael Rohlfing", "Zdenek Sofer", "Ursula Wurstbauer"], "first_author": "Aleksandra Łopion", "abstract": "The emergence of intelligent matter has sparked significant interest in next generation technologies. We report on the discovery of a reconfigurable magnetic multilayer domain structure in the van der Waals magnet CrSBr, exhibiting a unique combination of magnetic and optical properties. Applying an external magnetic field along the easy axis drives the hysteretic antiferromagnetic-to-ferromagnetic transition that is not universally binary, but instead develops through a cascade of intermediate magnetic configurations whose multiplicity and stability scale systematically with thickness. This material can be considered as a prototypical intelligent matter, capable of encoding, processing, and storing information through its tunable magnetic structure. The directly linked optical properties of CrSBr, modulated by the magnetic structure, provide a readout mechanism for the stored information compatible with modern information distribution using light. With its adaptive properties, CrSBr is an attractive candidate for neuromorphic circuitries, enabling the design of brain-inspired computing architectures that can learn and evolve in response to changing environments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.mes-hall"], "url": "http://arxiv.org/abs/2512.04887", "comments": "13 pages, 3 figures, SI: 4 pages, 5 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Optical Readout of Reconfigurable Layered Magnetic Domain Structure in CrSBr\nResearch Topic: computer vision\nAuthors: Aleksandra Łopion, Pierre-Maurice Piel, Manuel Terbeck, Jan-Hendrik Larusch, Jakob Henz, Marie-Christin Heißenbüttel, Kseniia Mosina, Thorsten Deilmann, Michael Rohlfing, Zdenek Sofer, Ursula Wurstbauer\nAbstract: The emergence of intelligent matter has sparked significant interest in next generation technologies. We report on the discovery of a reconfigurable magnetic multilayer domain structure in the van der Waals magnet CrSBr, exhibiting a unique combination of magnetic and optical properties. Applying an external magnetic field along the easy axis drives the hysteretic antiferromagnetic-to-ferromagnetic transition that is not universally binary, but instead develops through a cascade of intermediate magnetic configurations whose multiplicity and stability scale systematically with thickness. This material can be considered as a prototypical intelligent matter, capable of encoding, processing, and storing information through its tunable magnetic structure. The directly linked optical properties of CrSBr, modulated by the magnetic structure, provide a readout mechanism for the stored information compatible with modern information distribution using light. With its adaptive properties, CrSBr is an attractive candidate for neuromorphic circuitries, enabling the design of brain-inspired computing architectures that can learn and evolve in response to changing environments.\nCategories: cond-mat.mes-hall\nPublication Date: 2025-12-04\nPaper ID: 2512.04887\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1181, "total_text_length": 1258, "word_count": 166, "sentence_count": 7}, "basic_keywords": ["magnetic", "structure", "crsbr", "properties", "information", "intelligent", "matter", "optical"], "domain_keywords": [], "technical_terms": ["Waals", "Applying"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04884", "topic": "computer vision", "title": "Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation", "authors": ["Tim Engelbracht", "René Zurbrügg", "Matteo Wohlrapp", "Martin Büchner", "Abhinav Valada", "Marc Pollefeys", "Hermann Blum", "Zuria Bauer"], "first_author": "Tim Engelbracht", "abstract": "We present a dataset for force-grounded, cross-view articulated manipulation that couples what is seen with what is done and what is felt during real human interaction. The dataset contains 3048 sequences across 381 articulated objects in 38 environments. Each object is operated under four embodiments - (i) human hand, (ii) human hand with a wrist-mounted camera, (iii) handheld UMI gripper, and (iv) a custom Hoi! gripper - where the tool embodiment provides synchronized end-effector forces and tactile sensing. Our dataset offers a holistic view of interaction understanding from video, enabling researchers to evaluate how well methods transfer between human and robotic viewpoints, but also investigate underexplored modalities such as force sensing and prediction.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.04884", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation\nResearch Topic: computer vision\nAuthors: Tim Engelbracht, René Zurbrügg, Matteo Wohlrapp, Martin Büchner, Abhinav Valada, Marc Pollefeys, Hermann Blum, Zuria Bauer\nAbstract: We present a dataset for force-grounded, cross-view articulated manipulation that couples what is seen with what is done and what is felt during real human interaction. The dataset contains 3048 sequences across 381 articulated objects in 38 environments. Each object is operated under four embodiments - (i) human hand, (ii) human hand with a wrist-mounted camera, (iii) handheld UMI gripper, and (iv) a custom Hoi! gripper - where the tool embodiment provides synchronized end-effector forces and tactile sensing. Our dataset offers a holistic view of interaction understanding from video, enabling researchers to evaluate how well methods transfer between human and robotic viewpoints, but also investigate underexplored modalities such as force sensing and prediction.\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04884\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 772, "total_text_length": 857, "word_count": 123, "sentence_count": 7}, "basic_keywords": ["human", "dataset", "force", "view", "articulated", "interaction", "hand", "gripper"], "domain_keywords": [], "technical_terms": ["Each"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04883", "topic": "computer vision", "title": "SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms", "authors": ["Jiawen Wen", "Yu Hu", "Suixuan Qiu", "Jinshan Huang", "Xiaowen Chu"], "first_author": "Jiawen Wen", "abstract": "Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04883", "comments": "https://github.com/Jeffry-wen/SDG-Track", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms\nResearch Topic: computer vision\nAuthors: Jiawen Wen, Yu Hu, Suixuan Qiu, Jinshan Huang, Xiaowen Chu\nAbstract: Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04883\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 109, "abstract_length": 1404, "total_text_length": 1514, "word_count": 189, "sentence_count": 13}, "basic_keywords": ["tracking", "high", "track", "real", "small", "resolution", "conflict", "detector"], "domain_keywords": ["optical flow"], "technical_terms": ["Dual", "Experiments", "Observer", "Jetson Orin Nano", "Real", "The Observer", "Space Recovery", "Jeffry"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04881", "topic": "computer vision", "title": "Beampattern Synthesis for Discrete Phase RIS in Communication and Sensing Systems", "authors": ["Xiao Cai", "Hei Victor Cheng", "Daniel E. Lucani"], "first_author": "Xiao Cai", "abstract": "Extensive research on Reconfigurable Intelligent Surfaces (RIS) has primarily focused on optimizing reflective coefficients for passive beamforming in specific target directions. This optimization typically assumes prior knowledge of the target direction, which is unavailable before the target is detected. To enhance direction estimation, it is critical to develop array pattern synthesis techniques that yield a wider beam by maximizing the received power over the entire target area. Although this challenge has been addressed with active antennas, RIS systems pose a unique challenge due to their inherent phase constraints, which can be continuous or discrete. This work addresses this challenge through a novel array pattern synthesis method tailored for discrete phase constraints in RIS. We introduce a penalty method that pushes these constraints to the boundary of the convex hull. Then, the Minorization-Maximization (MM) method is utilized to reformulate the problem into a convex one. Our numerical results show that our algorithm can generate a wide beam pattern comparable to that achievable with per-power constraints, with both the amplitudes and phases being adjustable. We compare our method with a traditional beam sweeping technique, showing a) several orders of magnitude reduction of the MSE of Angle of Arrival (AOA) at low to medium Signal-to-Noise Ratio (SNR)s; and b) ~dB SNR reduction to achieve a high probability of detection.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SP", "cs.IT"], "url": "http://arxiv.org/abs/2512.04881", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Beampattern Synthesis for Discrete Phase RIS in Communication and Sensing Systems\nResearch Topic: computer vision\nAuthors: Xiao Cai, Hei Victor Cheng, Daniel E. Lucani\nAbstract: Extensive research on Reconfigurable Intelligent Surfaces (RIS) has primarily focused on optimizing reflective coefficients for passive beamforming in specific target directions. This optimization typically assumes prior knowledge of the target direction, which is unavailable before the target is detected. To enhance direction estimation, it is critical to develop array pattern synthesis techniques that yield a wider beam by maximizing the received power over the entire target area. Although this challenge has been addressed with active antennas, RIS systems pose a unique challenge due to their inherent phase constraints, which can be continuous or discrete.   This work addresses this challenge through a novel array pattern synthesis method tailored for discrete phase constraints in RIS. We introduce a penalty method that pushes these constraints to the boundary of the convex hull. Then, the Minorization-Maximization (MM) method is utilized to reformulate the problem into a convex one. Our numerical results show that our algorithm can generate a wide beam pattern comparable to that achievable with per-power constraints, with both the amplitudes and phases being adjustable. We compare our method with a traditional beam sweeping technique, showing a) several orders of magnitude reduction of the MSE of Angle of Arrival (AOA) at low to medium Signal-to-Noise Ratio (SNR)s; and b) $8$~dB SNR reduction to achieve a high probability of detection.\nCategories: eess.SP, cs.IT\nPublication Date: 2025-12-04\nPaper ID: 2512.04881\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 1457, "total_text_length": 1539, "word_count": 227, "sentence_count": 10}, "basic_keywords": ["target", "constraints", "method", "pattern", "beam", "challenge", "direction", "array"], "domain_keywords": [], "technical_terms": ["Extensive", "Minorization", "Signal", "Angle", "Noise Ratio", "Maximization", "Reconfigurable Intelligent Surfaces", "Although"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04879", "topic": "computer vision", "title": "Valley Splittings in Si/SiGe Heterostructures from First Principles", "authors": ["Lukas Cvitkovich", "Tancredi Salamone", "Christoph Wilhelmer", "Biel Martinez", "Tibor Grasser", "Yann-Michel Niquet"], "first_author": "Lukas Cvitkovich", "abstract": "We compute valley splittings in Si/SiGe superlattices using ab initio density functional theory (DFT). This first-principle approach is expected to provide an excellent description of interfaces, strains, and atomistic disorder without empirically fitted parameters. We benchmark atomistic tight- binding (TB) and the ``'' theory within the effective mass (EM) approximation against DFT. We show that DFT supports the main conclusions of the 2 theory, but reveals some limitations of semi-empirical methods such as the EM and TB, in particular about the description of atomistic disorder. The DFT calculations also highlight the effects of strong valley-orbit mixing at large valley splittings. Nevertheless, TB and the 2 theory shall provide reasonable valley splitting statistics in many heterostructures of interest for spin qubit devices.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04879", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Valley Splittings in Si/SiGe Heterostructures from First Principles\nResearch Topic: computer vision\nAuthors: Lukas Cvitkovich, Tancredi Salamone, Christoph Wilhelmer, Biel Martinez, Tibor Grasser, Yann-Michel Niquet\nAbstract: We compute valley splittings in Si/SiGe superlattices using ab initio density functional theory (DFT). This first-principle approach is expected to provide an excellent description of interfaces, strains, and atomistic disorder without empirically fitted parameters. We benchmark atomistic tight- binding (TB) and the ``$2k_0$'' theory within the effective mass (EM) approximation against DFT. We show that DFT supports the main conclusions of the 2$k_0$ theory, but reveals some limitations of semi-empirical methods such as the EM and TB, in particular about the description of atomistic disorder. The DFT calculations also highlight the effects of strong valley-orbit mixing at large valley splittings. Nevertheless, TB and the 2$k_0$ theory shall provide reasonable valley splitting statistics in many heterostructures of interest for spin qubit devices.\nCategories: cond-mat.mtrl-sci\nPublication Date: 2025-12-04\nPaper ID: 2512.04879\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 842, "total_text_length": 910, "word_count": 129, "sentence_count": 7}, "basic_keywords": ["valley", "theory", "atomistic", "splittings", "provide", "description", "disorder", "compute"], "domain_keywords": [], "technical_terms": ["Nevertheless"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04876", "topic": "computer vision", "title": "Optimizations and extensions for fair join pattern matching", "authors": ["Ioannis Karras"], "first_author": "Ioannis Karras", "abstract": "Join patterns are an underexplored approach for the programming of concurrent and distributed systems. When applied to the actor model, join patterns offer the novel capability of matching combinations of messages in the mailbox of an actor. Previous work by Philipp Haller et al. in the paper \"Fair Join Pattern Matching for Actors\" (ECOOP 2024) explored join patterns with conditional guards in an actor-based setting with a specification of fair and deterministic matching semantics. Nevertheless, the question of time efficiency in fair join pattern matching has remained underexplored. The stateful tree-based matching algorithm of Haller et al. performs worse than an implementation that adapts the Rete algorithm to the regular version of a join pattern matching benchmark, while outperforming on a variant with heavy conditional guards, which take longer to evaluate. Nevertheless, conforming Rete to the problem of join pattern matching requires heavy manual adaptation. In this thesis, we enhance and optimize the stateful tree-based matching algorithm of Haller et al. to achieve up to tenfold performance improvements on certain benchmarks, approaching the performance of Rete on regular benchmarks while maintaining the advantages of versatility and performance with heavy guards. We also enhance the benchmark suite, adding new features and enhancing its extensibility and user-friendliness. We extend the join pattern implementation with a less ambiguous syntax as well as dynamic pattern switching. Finally, we present a new complex model use case for join patterns, showing their applicability in a microservice web architecture.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.PL", "cs.DS"], "url": "http://arxiv.org/abs/2512.04876", "comments": "This is a Master's thesis for the Master's in Computer Science and Engineering at DTU (Technical University of Denmark)", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Optimizations and extensions for fair join pattern matching\nResearch Topic: computer vision\nAuthors: Ioannis Karras\nAbstract: Join patterns are an underexplored approach for the programming of concurrent and distributed systems. When applied to the actor model, join patterns offer the novel capability of matching combinations of messages in the mailbox of an actor. Previous work by Philipp Haller et al. in the paper \"Fair Join Pattern Matching for Actors\" (ECOOP 2024) explored join patterns with conditional guards in an actor-based setting with a specification of fair and deterministic matching semantics. Nevertheless, the question of time efficiency in fair join pattern matching has remained underexplored. The stateful tree-based matching algorithm of Haller et al. performs worse than an implementation that adapts the Rete algorithm to the regular version of a join pattern matching benchmark, while outperforming on a variant with heavy conditional guards, which take longer to evaluate. Nevertheless, conforming Rete to the problem of join pattern matching requires heavy manual adaptation.   In this thesis, we enhance and optimize the stateful tree-based matching algorithm of Haller et al. to achieve up to tenfold performance improvements on certain benchmarks, approaching the performance of Rete on regular benchmarks while maintaining the advantages of versatility and performance with heavy guards. We also enhance the benchmark suite, adding new features and enhancing its extensibility and user-friendliness. We extend the join pattern implementation with a less ambiguous syntax as well as dynamic pattern switching. Finally, we present a new complex model use case for join patterns, showing their applicability in a microservice web architecture.\nCategories: cs.PL, cs.DS\nPublication Date: 2025-12-04\nPaper ID: 2512.04876\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 59, "abstract_length": 1646, "total_text_length": 1706, "word_count": 250, "sentence_count": 14}, "basic_keywords": ["join", "matching", "pattern", "patterns", "actor", "haller", "fair", "guards"], "domain_keywords": [], "technical_terms": ["Rete", "Nevertheless", "Finally", "Philipp Haller", "Fair Join Pattern Matching", "Previous", "When", "Actors"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04875", "topic": "computer vision", "title": "SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection", "authors": ["Qing Xu", "Yanqian Wang", "Xiangjian Hea", "Yue Li", "Yixuan Zhang", "Rong Qu", "Wenting Duan", "Zhen Chen"], "first_author": "Qing Xu", "abstract": "Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04875", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection\nResearch Topic: computer vision\nAuthors: Qing Xu, Yanqian Wang, Xiangjian Hea, Yue Li, Yixuan Zhang, Rong Qu, Wenting Duan, Zhen Chen\nAbstract: Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04875\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1413, "total_text_length": 1497, "word_count": 175, "sentence_count": 7}, "basic_keywords": ["detection", "prompts", "disease", "context", "expert", "lesion", "chest", "clinical"], "domain_keywords": [], "technical_terms": ["Extensive", "While", "Automated", "Specifically", "Moreover"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04873", "topic": "computer vision", "title": "VNS Tokamak OpenMC-Serpent Validation for Medical Isotope Studies", "authors": ["Christopher Ehrich", "Christian Bachmann", "Pavel Pereslavtsev", "Christian Reiter"], "first_author": "Christopher Ehrich", "abstract": "The Volumetric Neutron Source (VNS) tokamak is a proposed fusion reactor for testing and qualification of reactor components for future use in a fusion power facility, and has potential use for radioisotope production. The VNS geometry is modeled in the Serpent and OpenMC neutronics codes. Analog neutron-photon coupled simulations are carried out to compare the model's vacuum vessel and blanket components across codes. In the vacuum vessel, neutron and photon flux maps are calculated, while in the blanket region, neutron and photon spectra, (n,T), and (n,2n) reaction rates are calculated and compared between models. The detector response comparisons found the following: neutron flux and (n,T) reactions achieved excellent agreement, the (n,2n) detector response had good agreement, and photon flux had regional discrepancies depending on Serpent tracking used. Hybrid tracking lead to a relative difference of about 20% in the outboard side blanket, where as employment of delta tracking resulted in less than 1% relative difference. On an HPC cluster, Serpent was found to have shorter computation time than OpenMC in neutron photon coupled simulations using both hybrid tracking and delta tracking, but longer in neutron only simulations. An exemplary radioisotope production case is presented for the demonstration of additional VNS capabilities.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.comp-ph", "cs.CE"], "url": "http://arxiv.org/abs/2512.04873", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: VNS Tokamak OpenMC-Serpent Validation for Medical Isotope Studies\nResearch Topic: computer vision\nAuthors: Christopher Ehrich, Christian Bachmann, Pavel Pereslavtsev, Christian Reiter\nAbstract: The Volumetric Neutron Source (VNS) tokamak is a proposed fusion reactor for testing and qualification of reactor components for future use in a fusion power facility, and has potential use for radioisotope production. The VNS geometry is modeled in the Serpent and OpenMC neutronics codes. Analog neutron-photon coupled simulations are carried out to compare the model's vacuum vessel and blanket components across codes. In the vacuum vessel, neutron and photon flux maps are calculated, while in the blanket region, neutron and photon spectra, (n,T), and (n,2n) reaction rates are calculated and compared between models. The detector response comparisons found the following: neutron flux and (n,T) reactions achieved excellent agreement, the (n,2n) detector response had good agreement, and photon flux had regional discrepancies depending on Serpent tracking used. Hybrid tracking lead to a relative difference of about 20% in the outboard side blanket, where as employment of delta tracking resulted in less than 1% relative difference. On an HPC cluster, Serpent was found to have shorter computation time than OpenMC in neutron photon coupled simulations using both hybrid tracking and delta tracking, but longer in neutron only simulations. An exemplary radioisotope production case is presented for the demonstration of additional VNS capabilities.\nCategories: physics.comp-ph, cs.CE\nPublication Date: 2025-12-04\nPaper ID: 2512.04873\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1358, "total_text_length": 1424, "word_count": 209, "sentence_count": 9}, "basic_keywords": ["neutron", "photon", "tracking", "serpent", "simulations", "blanket", "flux", "fusion"], "domain_keywords": [], "technical_terms": ["Hybrid", "Serpent", "Analog", "The Volumetric Neutron Source"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04867", "topic": "computer vision", "title": "Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project", "authors": ["Bychkov Oleksii", "Senysh Taras"], "first_author": "Bychkov Oleksii", "abstract": "This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.AR", "cs.NE"], "url": "http://arxiv.org/abs/2512.04867", "comments": "14 pages", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.628308", "embedding_text": "Paper Title: Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project\nResearch Topic: computer vision\nAuthors: Bychkov Oleksii, Senysh Taras\nAbstract: This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.\nCategories: cs.AR, cs.NE\nPublication Date: 2025-12-04\nPaper ID: 2512.04867\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 491, "total_text_length": 585, "word_count": 76, "sentence_count": 4}, "basic_keywords": ["hardware", "individual", "neuron", "during", "system", "paper", "presents", "innovative"], "domain_keywords": [], "technical_terms": ["Dropout", "Unlike", "Each"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04862", "topic": "computer vision", "title": "Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing", "authors": ["Maria-Paola Forte", "Nikos Athanasiou", "Giulia Ballardini", "Jan Ulrich Bartels", "Katherine J. Kuchenbecker", "Michael J. Black"], "first_author": "Maria-Paola Forte", "abstract": "Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04862", "comments": "* Equal contribution. Minor figure corrections compared to the ICCV 2025 version", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing\nResearch Topic: computer vision\nAuthors: Maria-Paola Forte, Nikos Athanasiou, Giulia Ballardini, Jan Ulrich Bartels, Katherine J. Kuchenbecker, Michael J. Black\nAbstract: Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04862\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1349, "total_text_length": 1433, "word_count": 191, "sentence_count": 14}, "basic_keywords": ["pose", "contact", "bioimpedance", "data", "estimation", "self", "biotuch", "using"], "domain_keywords": ["pose estimation"], "technical_terms": ["Consequently", "While", "Capturing", "Code", "Testing"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04860", "topic": "computer vision", "title": "Stochastic Density Functional Theory Through the Lens of Multilevel Monte Carlo Method", "authors": ["Xue Quan", "Huajie Chen"], "first_author": "Xue Quan", "abstract": "The stochastic density functional theory (sDFT) has exhibited advantages over the standard Kohn-Sham DFT method and has become an attractive approach for large-scale electronic structure calculations. The sDFT method avoids the expensive matrix diagonalization by introducing a set of random orbitals and approximating the density matrix via Chebyshev expansion of a matrix-valued function. In this work, we study the sDFT with a plane-wave discretization, and discuss variance reduction algorithms in the framework of multilevel Monte Carlo (MLMC) methods. In particular, we show that the density matrix evaluation in sDFT can be decomposed into many levels by increasing the plane-wave cutoffs or the Chebyshev polynomial orders. This decomposition renders the computational cost independent of the discretization size or temperature. To demonstrate the efficiency of the algorithm, we provide rigorous analysis of the statistical errors and present numerical experiments on some material systems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.comp-ph"], "url": "http://arxiv.org/abs/2512.04860", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Stochastic Density Functional Theory Through the Lens of Multilevel Monte Carlo Method\nResearch Topic: computer vision\nAuthors: Xue Quan, Huajie Chen\nAbstract: The stochastic density functional theory (sDFT) has exhibited advantages over the standard Kohn-Sham DFT method and has become an attractive approach for large-scale electronic structure calculations. The sDFT method avoids the expensive matrix diagonalization by introducing a set of random orbitals and approximating the density matrix via Chebyshev expansion of a matrix-valued function. In this work, we study the sDFT with a plane-wave discretization, and discuss variance reduction algorithms in the framework of multilevel Monte Carlo (MLMC) methods. In particular, we show that the density matrix evaluation in sDFT can be decomposed into many levels by increasing the plane-wave cutoffs or the Chebyshev polynomial orders. This decomposition renders the computational cost independent of the discretization size or temperature. To demonstrate the efficiency of the algorithm, we provide rigorous analysis of the statistical errors and present numerical experiments on some material systems.\nCategories: physics.comp-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04860\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 86, "abstract_length": 999, "total_text_length": 1086, "word_count": 153, "sentence_count": 7}, "basic_keywords": ["sdft", "matrix", "density", "method", "chebyshev", "plane", "wave", "discretization"], "domain_keywords": [], "technical_terms": ["Chebyshev", "Sham", "Kohn", "Monte Carlo"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04859", "topic": "computer vision", "title": "High-Performance DBMSs with io_uring: When and How to use it", "authors": ["Matthias Jasny", "Muhammad El-Hindi", "Tobias Ziegler", "Viktor Leis", "Carsten Binnig"], "first_author": "Matthias Jasny", "abstract": "We study how modern database systems can leverage the Linux io_uring interface for efficient, low-overhead I/O. io_uring is an asynchronous system call batching interface that unifies storage and network operations, addressing limitations of existing Linux I/O interfaces. However, naively replacing traditional I/O interfaces with io_uring does not necessarily yield performance benefits. To demonstrate when io_uring delivers the greatest benefits and how to use it effectively in modern database systems, we evaluate it in two use cases: Integrating io_uring into a storage-bound buffer manager and using it for high-throughput data shuffling in network-bound analytical workloads. We further analyze how advanced io_uring features, such as registered buffers and passthrough I/O, affect end-to-end performance. Our study shows when low-level optimizations translate into tangible system-wide gains and how architectural choices influence these benefits. Building on these insights, we derive practical guidelines for designing I/O-intensive systems using io_uring and validate their effectiveness in a case study of PostgreSQL's recent io_uring integration, where applying our guidelines yields a performance improvement of 14%.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.DB"], "url": "http://arxiv.org/abs/2512.04859", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: High-Performance DBMSs with io_uring: When and How to use it\nResearch Topic: computer vision\nAuthors: Matthias Jasny, Muhammad El-Hindi, Tobias Ziegler, Viktor Leis, Carsten Binnig\nAbstract: We study how modern database systems can leverage the Linux io_uring interface for efficient, low-overhead I/O. io_uring is an asynchronous system call batching interface that unifies storage and network operations, addressing limitations of existing Linux I/O interfaces. However, naively replacing traditional I/O interfaces with io_uring does not necessarily yield performance benefits. To demonstrate when io_uring delivers the greatest benefits and how to use it effectively in modern database systems, we evaluate it in two use cases: Integrating io_uring into a storage-bound buffer manager and using it for high-throughput data shuffling in network-bound analytical workloads. We further analyze how advanced io_uring features, such as registered buffers and passthrough I/O, affect end-to-end performance. Our study shows when low-level optimizations translate into tangible system-wide gains and how architectural choices influence these benefits. Building on these insights, we derive practical guidelines for designing I/O-intensive systems using io_uring and validate their effectiveness in a case study of PostgreSQL's recent io_uring integration, where applying our guidelines yields a performance improvement of 14%.\nCategories: cs.DB\nPublication Date: 2025-12-04\nPaper ID: 2512.04859\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 60, "abstract_length": 1232, "total_text_length": 1293, "word_count": 176, "sentence_count": 8}, "basic_keywords": ["study", "systems", "performance", "benefits", "modern", "database", "linux", "interface"], "domain_keywords": [], "technical_terms": ["However", "Integrating", "Linux", "Building"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04858", "topic": "computer vision", "title": "Exact 3-D Channel Impulse Response for Spherical Receivers with Arbitrary Drift Directions", "authors": ["Yen-Chi Lee", "Ping-Cheng Yeh", "Chia-Han Lee"], "first_author": "Yen-Chi Lee", "abstract": "Accurate channel modeling for spherical absorbing receivers is fundamental to the design of realistic molecular multiple-input multiple-output (MIMO) systems. While advanced modulation schemes have been proposed to mitigate interference, determining the channel impulse response (CIR) under arbitrary flow directions remains a challenge; existing exact solutions are restricted to either 1-D/no-drift scenarios or planar receiver geometries. Addressing this gap, we derive the first exact analytical CIR for a spherical receiver in a 3-D molecular communication system with uniform drift in an arbitrary direction. Unlike prior approximations that ignore the angle between the drift and the transmission axis, our approach utilizes the Girsanov theorem to analytically transform the hitting-time distribution from a stationary medium to a drifted one. The proposed closed-form expression not only eliminates modeling errors inherent in previous approximations for off-axis receivers but also enables efficient parameter-space exploration of critical system metrics (e.g., peak time and amplitude), a task that would be computationally costly with pure simulation-based approaches.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IT", "eess.SP", "math.PR"], "url": "http://arxiv.org/abs/2512.04858", "comments": "5 pages, 4 figures. Preprint prepared for journal submission", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Exact 3-D Channel Impulse Response for Spherical Receivers with Arbitrary Drift Directions\nResearch Topic: computer vision\nAuthors: Yen-Chi Lee, Ping-Cheng Yeh, Chia-Han Lee\nAbstract: Accurate channel modeling for spherical absorbing receivers is fundamental to the design of realistic molecular multiple-input multiple-output (MIMO) systems. While advanced modulation schemes have been proposed to mitigate interference, determining the channel impulse response (CIR) under arbitrary flow directions remains a challenge; existing exact solutions are restricted to either 1-D/no-drift scenarios or planar receiver geometries. Addressing this gap, we derive the first exact analytical CIR for a spherical receiver in a 3-D molecular communication system with uniform drift in an arbitrary direction. Unlike prior approximations that ignore the angle between the drift and the transmission axis, our approach utilizes the Girsanov theorem to analytically transform the hitting-time distribution from a stationary medium to a drifted one. The proposed closed-form expression not only eliminates modeling errors inherent in previous approximations for off-axis receivers but also enables efficient parameter-space exploration of critical system metrics (e.g., peak time and amplitude), a task that would be computationally costly with pure simulation-based approaches.\nCategories: cs.IT, eess.SP, math.PR\nPublication Date: 2025-12-04\nPaper ID: 2512.04858\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1180, "total_text_length": 1271, "word_count": 170, "sentence_count": 8}, "basic_keywords": ["drift", "channel", "modeling", "spherical", "receivers", "molecular", "multiple", "proposed"], "domain_keywords": [], "technical_terms": ["Addressing", "While", "Unlike", "Girsanov", "Accurate"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04857", "topic": "computer vision", "title": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens", "authors": ["Ziran Qin", "Youru Lv", "Mingbao Lin", "Zeren Zhang", "Chanfan Gan", "Tieyuan Chen", "Weiyao Lin"], "first_author": "Ziran Qin", "abstract": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce , a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04857", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens\nResearch Topic: computer vision\nAuthors: Ziran Qin, Youru Lv, Mingbao Lin, Zeren Zhang, Chanfan Gan, Tieyuan Chen, Weiyao Lin\nAbstract: Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04857\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1626, "total_text_length": 1698, "word_count": 236, "sentence_count": 18}, "basic_keywords": ["generation", "image", "cache", "memory", "autoregressive", "visual", "linear", "throughput"], "domain_keywords": ["imagenet"], "technical_terms": ["Extensive", "However", "Lumina", "Additionally", "Janus", "Autoregressive"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04855", "topic": "computer vision", "title": "A Novel Trust-Based DDoS Cyberattack Detection Model for Smart Business Environments", "authors": ["Oghenetejiri Okporokpo", "Funminiyi Olajide", "Nemitari Ajienka", "Xiaoqi Ma"], "first_author": "Oghenetejiri Okporokpo", "abstract": "As the frequency and complexity of Distributed Denial-of-Service (DDoS) attacks continue to increase, the level of threats posed to Smart Internet of Things (SIoT) business environments have also increased. These environments generally have several interconnected SIoT systems and devices that are integral to daily operations, usually depending on cloud infrastructure and real-time data analytics, which require continuous availability and secure data exchange. Conventional detection mechanisms, while useful in static or traditional network environments, often are inadequate in responding to the needs of these dynamic and diverse SIoT networks. In this paper, we introduce a novel trust-based DDoS detection model tailored to meet the unique requirements of smart business environments. The proposed model incorporates a trust evaluation engine that continuously monitors node behaviour, calculating trust scores based on packet delivery ratio, response time, and anomaly detection. These trust metrics are then aggregated by a central trust-based repository that uses inherent trust values to identify traffic patterns indicative of DDoS attacks. By integrating both trust scores and central trust-based outputs, the trust calculation is enhanced, ensuring that threats are accurately identified and addressed in real-time. The model demonstrated a significant improvement in detection accuracy, and a low false-positive rate with enhanced scalability and adaptability under TCP SYN, Ping Flood, and UDP Flood attacks. The results show that a trust-based approach provides an effective, lightweight alternative for securing resource-constrained business IoT environments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR"], "url": "http://arxiv.org/abs/2512.04855", "comments": "22 Pages", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: A Novel Trust-Based DDoS Cyberattack Detection Model for Smart Business Environments\nResearch Topic: computer vision\nAuthors: Oghenetejiri Okporokpo, Funminiyi Olajide, Nemitari Ajienka, Xiaoqi Ma\nAbstract: As the frequency and complexity of Distributed Denial-of-Service (DDoS) attacks continue to increase, the level of threats posed to Smart Internet of Things (SIoT) business environments have also increased. These environments generally have several interconnected SIoT systems and devices that are integral to daily operations, usually depending on cloud infrastructure and real-time data analytics, which require continuous availability and secure data exchange. Conventional detection mechanisms, while useful in static or traditional network environments, often are inadequate in responding to the needs of these dynamic and diverse SIoT networks. In this paper, we introduce a novel trust-based DDoS detection model tailored to meet the unique requirements of smart business environments. The proposed model incorporates a trust evaluation engine that continuously monitors node behaviour, calculating trust scores based on packet delivery ratio, response time, and anomaly detection. These trust metrics are then aggregated by a central trust-based repository that uses inherent trust values to identify traffic patterns indicative of DDoS attacks. By integrating both trust scores and central trust-based outputs, the trust calculation is enhanced, ensuring that threats are accurately identified and addressed in real-time. The model demonstrated a significant improvement in detection accuracy, and a low false-positive rate with enhanced scalability and adaptability under TCP SYN, Ping Flood, and UDP Flood attacks. The results show that a trust-based approach provides an effective, lightweight alternative for securing resource-constrained business IoT environments.\nCategories: cs.CR\nPublication Date: 2025-12-04\nPaper ID: 2512.04855\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1678, "total_text_length": 1763, "word_count": 240, "sentence_count": 10}, "basic_keywords": ["trust", "environments", "based", "detection", "ddos", "attacks", "siot", "business"], "domain_keywords": [], "technical_terms": ["These", "Flood", "Service", "Distributed Denial", "Smart Internet", "Conventional", "Things", "Ping Flood"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04852", "topic": "computer vision", "title": "Ask Safely: Privacy-Aware LLM Query Generation for Knowledge Graphs", "authors": ["Mauro Dalle Lucca Tosi", "Jordi Cabot"], "first_author": "Mauro Dalle Lucca Tosi", "abstract": "Large Language Models (LLMs) are increasingly used to query knowledge graphs (KGs) due to their strong semantic understanding and extrapolation capabilities compared to traditional approaches. However, these methods cannot be applied when the KG contains sensitive data and the user lacks the resources to deploy a local generative LLM. To address this issue, we propose a privacy-aware query generation approach for KGs. Our method identifies sensitive information in the graph based on its structure and omits such values before requesting the LLM to translate natural language questions into Cypher queries. Experimental results show that our approach preserves the quality of the generated queries while preventing sensitive data from being transmitted to third-party services.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IR"], "url": "http://arxiv.org/abs/2512.04852", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Ask Safely: Privacy-Aware LLM Query Generation for Knowledge Graphs\nResearch Topic: computer vision\nAuthors: Mauro Dalle Lucca Tosi, Jordi Cabot\nAbstract: Large Language Models (LLMs) are increasingly used to query knowledge graphs (KGs) due to their strong semantic understanding and extrapolation capabilities compared to traditional approaches. However, these methods cannot be applied when the KG contains sensitive data and the user lacks the resources to deploy a local generative LLM. To address this issue, we propose a privacy-aware query generation approach for KGs. Our method identifies sensitive information in the graph based on its structure and omits such values before requesting the LLM to translate natural language questions into Cypher queries. Experimental results show that our approach preserves the quality of the generated queries while preventing sensitive data from being transmitted to third-party services.\nCategories: cs.IR\nPublication Date: 2025-12-04\nPaper ID: 2512.04852\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 67, "abstract_length": 781, "total_text_length": 849, "word_count": 122, "sentence_count": 6}, "basic_keywords": ["sensitive", "language", "query", "data", "approach", "queries", "large", "models"], "domain_keywords": [], "technical_terms": ["However", "Experimental", "Large Language Models", "Cypher"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04850", "topic": "computer vision", "title": "Side-by-side first-price auctions with imperfect bidders", "authors": ["Benjamin Heymann"], "first_author": "Benjamin Heymann", "abstract": "We model a procurement scenario in which two bidders act simultaneously on behalf of a single buyer, a configuration common in display advertising and referred to as but largely unexplored in theory. We prove that the iterated best response algorithm converges to an equilibrium under standard distributional assumptions and provide sufficient condition for uniqueness. Beyond establishing existence and convergence, our analysis provides a tractable numerical method for quantitative studies of side-by-side procurement.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.GT", "econ.TH", "math.OC"], "url": "http://arxiv.org/abs/2512.04850", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Side-by-side first-price auctions with imperfect bidders\nResearch Topic: computer vision\nAuthors: Benjamin Heymann\nAbstract: We model a procurement scenario in which two \\textit{imperfect} bidders act simultaneously on behalf of a single buyer, a configuration common in display advertising and referred to as \\textit{side-by-side bidding} but largely unexplored in theory. We prove that the iterated best response algorithm converges to an equilibrium under standard distributional assumptions and provide sufficient condition for uniqueness. Beyond establishing existence and convergence, our analysis provides a tractable numerical method for quantitative studies of side-by-side procurement.\nCategories: cs.GT, econ.TH, math.OC\nPublication Date: 2025-12-04\nPaper ID: 2512.04850\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 56, "abstract_length": 521, "total_text_length": 578, "word_count": 78, "sentence_count": 4}, "basic_keywords": ["procurement", "side", "model", "scenario", "bidders", "simultaneously", "behalf", "single"], "domain_keywords": [], "technical_terms": ["Beyond"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04848", "topic": "computer vision", "title": "Extreme-Mass-Ratio Inspirals Embedded in Dark Matter Halo II: Chaotic Imprints in Gravitational Waves", "authors": ["Surajit Das", "Surojit Dalui", "Bum-Hoon Lee", "Yi-Fu Cai"], "first_author": "Surajit Das", "abstract": "We investigate the imprints of chaos in gravitational waves from extreme-mass-ratio inspirals configuration, where a stellar massive object, confined in a harmonic potential, orbits a supermassive Schwarzschild-like black hole embedded in a Dehnen-type dark matter halo. In our first paper [1], we demonstrated the system's transition from non-chaotic to chaotic dynamics by analyzing Poincar sections, orbital evolution, and Lyapunov exponents across different energies and dark matter halo parameters. In this work, we compute the gravitational waveforms of the small celestial object along different chaotic and non-chaotic orbits by implementing the numerical kludge scheme. We further perform a spectral analysis of the gravitational waveforms from such orbits. In particular, we show that when the system is in a chaotic state, the gravitational wave signals are characterized by broader frequency spectra with finite widths, enhanced amplitude and energy emission rate, distinctly differentiating them from the signals generated during the system's non-chaotic state. Through recurrence analysis we also show that the time series of gravitational waveforms strain carry unique information on the motion of chaotic dynamics, which can be used to distinctly differentiate from non-chaotic to chaotic motion of the source. Furthermore, we discuss the potential detectability of these orbits for upcoming observatories like LISA, TianQin, and Taiji, emphasizing the significant potential for detecting chaotic imprints in gravitational waves to substantially enhance our understanding of chaotic dynamics in black hole physics and the dark matter environments of galactic nuclei.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["gr-qc", "astro-ph.CO", "hep-th", "nlin.CD"], "url": "http://arxiv.org/abs/2512.04848", "comments": "28 pages, 16 figures, 1 table", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Extreme-Mass-Ratio Inspirals Embedded in Dark Matter Halo II: Chaotic Imprints in Gravitational Waves\nResearch Topic: computer vision\nAuthors: Surajit Das, Surojit Dalui, Bum-Hoon Lee, Yi-Fu Cai\nAbstract: We investigate the imprints of chaos in gravitational waves from extreme-mass-ratio inspirals configuration, where a stellar massive object, confined in a harmonic potential, orbits a supermassive Schwarzschild-like black hole embedded in a Dehnen-type dark matter halo. In our first paper [1], we demonstrated the system's transition from non-chaotic to chaotic dynamics by analyzing Poincaré sections, orbital evolution, and Lyapunov exponents across different energies and dark matter halo parameters. In this work, we compute the gravitational waveforms of the small celestial object along different chaotic and non-chaotic orbits by implementing the numerical kludge scheme. We further perform a spectral analysis of the gravitational waveforms from such orbits. In particular, we show that when the system is in a chaotic state, the gravitational wave signals are characterized by broader frequency spectra with finite widths, enhanced amplitude and energy emission rate, distinctly differentiating them from the signals generated during the system's non-chaotic state. Through recurrence analysis we also show that the time series of gravitational waveforms strain carry unique information on the motion of chaotic dynamics, which can be used to distinctly differentiate from non-chaotic to chaotic motion of the source. Furthermore, we discuss the potential detectability of these orbits for upcoming observatories like LISA, TianQin, and Taiji, emphasizing the significant potential for detecting chaotic imprints in gravitational waves to substantially enhance our understanding of chaotic dynamics in black hole physics and the dark matter environments of galactic nuclei.\nCategories: gr-qc, astro-ph.CO, hep-th, nlin.CD\nPublication Date: 2025-12-04\nPaper ID: 2512.04848\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1682, "total_text_length": 1784, "word_count": 249, "sentence_count": 8}, "basic_keywords": ["chaotic", "gravitational", "orbits", "potential", "dark", "matter", "system", "dynamics"], "domain_keywords": [], "technical_terms": ["Poincar", "Furthermore", "Through", "Lyapunov", "Dehnen", "Taiji", "Schwarzschild"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04845", "topic": "computer vision", "title": "A High-Order Discretization Scheme for Surface Integral Equations for Analyzing the Electroencephalography Forward Problem", "authors": ["Rui Chen", "Viviana Giunzioni", "Adrien Merlini", "Francesco P. Andriulli"], "first_author": "Rui Chen", "abstract": "A Nystrom-based high-order (HO) discretization scheme for surface integral equations (SIEs) for analyzing the electroencephalography (EEG) forward problem is proposed in this work. We use HO surface elements and interpolation functions for the discretization of the interfaces of the head volume and the unknowns on the elements, respectively. The advantage of this work over existing isoparametric HO discretization schemes resides in the fact that the interpolation points are different from the mesh nodes, allowing for the flexible manipulation of the order of the basis functions without regenerating the mesh of the interfaces. Moreover, the interpolation points are chosen from the quadrature rules with the same number of points on the elements simplifying the numerical computation of the surface integrals for the far-interaction case. In this contribution, we extend the implementation of the HO discretization scheme to the double-layer and the adjoint double-layer formulations, as well as to the isolated-skull-approach for the double-layer formulation and to the indirect adjoint double-layer formulation, employed to improve the solution accuracy in case of high conductivity contrast models, which requires the development of different techniques for the singularity treatment. Numerical experiments are presented to demonstrate the accuracy, flexibility, and efficiency of the proposed scheme for the four SIEs for analyzing the EEG forward problem.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.NA", "math-ph", "physics.bio-ph"], "url": "http://arxiv.org/abs/2512.04845", "comments": "9 pages, 7 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: A High-Order Discretization Scheme for Surface Integral Equations for Analyzing the Electroencephalography Forward Problem\nResearch Topic: computer vision\nAuthors: Rui Chen, Viviana Giunzioni, Adrien Merlini, Francesco P. Andriulli\nAbstract: A Nystrom-based high-order (HO) discretization scheme for surface integral equations (SIEs) for analyzing the electroencephalography (EEG) forward problem is proposed in this work. We use HO surface elements and interpolation functions for the discretization of the interfaces of the head volume and the unknowns on the elements, respectively. The advantage of this work over existing isoparametric HO discretization schemes resides in the fact that the interpolation points are different from the mesh nodes, allowing for the flexible manipulation of the order of the basis functions without regenerating the mesh of the interfaces. Moreover, the interpolation points are chosen from the quadrature rules with the same number of points on the elements simplifying the numerical computation of the surface integrals for the far-interaction case. In this contribution, we extend the implementation of the HO discretization scheme to the double-layer and the adjoint double-layer formulations, as well as to the isolated-skull-approach for the double-layer formulation and to the indirect adjoint double-layer formulation, employed to improve the solution accuracy in case of high conductivity contrast models, which requires the development of different techniques for the singularity treatment. Numerical experiments are presented to demonstrate the accuracy, flexibility, and efficiency of the proposed scheme for the four SIEs for analyzing the EEG forward problem.\nCategories: math.NA, math-ph, physics.bio-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04845\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 122, "abstract_length": 1467, "total_text_length": 1590, "word_count": 223, "sentence_count": 7}, "basic_keywords": ["discretization", "double", "layer", "scheme", "surface", "elements", "interpolation", "points"], "domain_keywords": [], "technical_terms": ["Moreover", "Numerical", "Nystrom"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04838", "topic": "computer vision", "title": "DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution", "authors": ["L. D. M. S. Sai Teja", "N. Siva Gopala Krishna", "Ufaq Khan", "Muhammad Haris Khan", "Partha Pakray", "Atul Mishra"], "first_author": "L. D. M. S. Sai Teja", "abstract": "In the age of advanced large language models (LLMs), the boundaries between human and AI-generated text are becoming increasingly blurred. We address the challenge of segmenting mixed-authorship text, that is identifying transition points in text where authorship shifts from human to AI or vice-versa, a problem with critical implications for authenticity, trust, and human oversight. We introduce a novel framework, called Info-Mask for mixed authorship detection that integrates stylometric cues, perplexity-driven signals, and structured boundary modeling to accurately segment collaborative human-AI content. To evaluate the robustness of our system against adversarial perturbations, we construct and release an adversarial benchmark dataset Mixed-text Adversarial setting for Segmentation (MAS), designed to probe the limits of existing detectors. Beyond segmentation accuracy, we introduce Human-Interpretable Attribution (HIA overlays that highlight how stylometric features inform boundary predictions, and we conduct a small-scale human study assessing their usefulness. Across multiple architectures, Info-Mask significantly improves span-level robustness under adversarial conditions, establishing new baselines while revealing remaining challenges. Our findings highlight both the promise and limitations of adversarially robust, interpretable mixed-authorship detection, with implications for trust and oversight in human-AI co-authorship.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04838", "comments": "18 pages, 10 Figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution\nResearch Topic: computer vision\nAuthors: L. D. M. S. Sai Teja, N. Siva Gopala Krishna, Ufaq Khan, Muhammad Haris Khan, Partha Pakray, Atul Mishra\nAbstract: In the age of advanced large language models (LLMs), the boundaries between human and AI-generated text are becoming increasingly blurred. We address the challenge of segmenting mixed-authorship text, that is identifying transition points in text where authorship shifts from human to AI or vice-versa, a problem with critical implications for authenticity, trust, and human oversight. We introduce a novel framework, called Info-Mask for mixed authorship detection that integrates stylometric cues, perplexity-driven signals, and structured boundary modeling to accurately segment collaborative human-AI content. To evaluate the robustness of our system against adversarial perturbations, we construct and release an adversarial benchmark dataset Mixed-text Adversarial setting for Segmentation (MAS), designed to probe the limits of existing detectors. Beyond segmentation accuracy, we introduce Human-Interpretable Attribution (HIA overlays that highlight how stylometric features inform boundary predictions, and we conduct a small-scale human study assessing their usefulness. Across multiple architectures, Info-Mask significantly improves span-level robustness under adversarial conditions, establishing new baselines while revealing remaining challenges. Our findings highlight both the promise and limitations of adversarially robust, interpretable mixed-authorship detection, with implications for trust and oversight in human-AI co-authorship.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04838\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 102, "abstract_length": 1454, "total_text_length": 1557, "word_count": 194, "sentence_count": 8}, "basic_keywords": ["human", "authorship", "text", "mixed", "adversarial", "implications", "trust", "oversight"], "domain_keywords": [], "technical_terms": ["Info", "Beyond", "Human", "Interpretable Attribution", "Across", "Adversarial", "Mixed", "Mask"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04837", "topic": "computer vision", "title": "A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World", "authors": ["Jikang Cheng", "Renye Yan", "Zhiyuan Yan", "Yaozhong Gan", "Xueyi Zhang", "Zhongyuan Wang", "Wei Peng", "Ling Liang"], "first_author": "Jikang Cheng", "abstract": "Existing methods for deepfake detection aim to develop generalizable detectors. Although \"generalizable\" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04837", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World\nResearch Topic: computer vision\nAuthors: Jikang Cheng, Renye Yan, Zhiyuan Yan, Yaozhong Gan, Xueyi Zhang, Zhongyuan Wang, Wei Peng, Ling Liang\nAbstract: Existing methods for deepfake detection aim to develop generalizable detectors. Although \"generalizable\" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04837\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1673, "total_text_length": 1749, "word_count": 233, "sentence_count": 15}, "basic_keywords": ["real", "domain", "fake", "detection", "training", "domains", "world", "multi"], "domain_keywords": [], "technical_terms": ["However", "Then", "Experiments", "Tuning", "Existing", "Meanwhile", "Dose", "Developer"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04833", "topic": "computer vision", "title": "Counterfactual Explanations for Power System Optimisation", "authors": ["Benjamin Fritz", "Waqquas Bukhsh"], "first_author": "Benjamin Fritz", "abstract": "Enhanced computational capabilities of modern decision-making software have allowed us to solve increasingly sophisticated optimisation problems. But in complex socio-economic, technical environments such as electricity markets, transparent operation is key to ensure a fair treatment of all parties involved, particularly regarding dispatch decisions. We address this issue by building on the concept of counterfactual explanations, answering questions such as \"Why was this generator not dispatched?\" by identifying minimum changes in the input parameters that would have changed the optimal solution. Both DC Optimal Power Flow and Unit Commitment problems are considered, wherein the variable parameters are the spatial and temporal demand profiles, respectively. The thereby obtained explanations allow users to identify the most important differences between the real and expected market outcomes and observe which constraints have led to the solution. The framework uses a bilevel optimisation problem to find the counterfactual demand scenarios. State-of-the-art methods are compared with data-driven heuristics on the basis of computational efficiency and explanation accuracy. Results show that leveraging historical data from previously solved instances can provide significant speed benefits and allows us to derive explanations in cases where conventional methods would not be tractable.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SY"], "url": "http://arxiv.org/abs/2512.04833", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Counterfactual Explanations for Power System Optimisation\nResearch Topic: computer vision\nAuthors: Benjamin Fritz, Waqquas Bukhsh\nAbstract: Enhanced computational capabilities of modern decision-making software have allowed us to solve increasingly sophisticated optimisation problems. But in complex socio-economic, technical environments such as electricity markets, transparent operation is key to ensure a fair treatment of all parties involved, particularly regarding dispatch decisions. We address this issue by building on the concept of counterfactual explanations, answering questions such as \"Why was this generator not dispatched?\" by identifying minimum changes in the input parameters that would have changed the optimal solution. Both DC Optimal Power Flow and Unit Commitment problems are considered, wherein the variable parameters are the spatial and temporal demand profiles, respectively. The thereby obtained explanations allow users to identify the most important differences between the real and expected market outcomes and observe which constraints have led to the solution. The framework uses a bilevel optimisation problem to find the counterfactual demand scenarios. State-of-the-art methods are compared with data-driven heuristics on the basis of computational efficiency and explanation accuracy. Results show that leveraging historical data from previously solved instances can provide significant speed benefits and allows us to derive explanations in cases where conventional methods would not be tractable.\nCategories: eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04833\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 1400, "total_text_length": 1458, "word_count": 196, "sentence_count": 10}, "basic_keywords": ["explanations", "computational", "optimisation", "problems", "counterfactual", "parameters", "would", "optimal"], "domain_keywords": [], "technical_terms": ["Unit Commitment", "Optimal Power Flow", "Results", "Both", "Enhanced", "State"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04830", "topic": "computer vision", "title": "FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis", "authors": ["Shijie Chen", "Peixi Peng"], "first_author": "Shijie Chen", "abstract": "Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04830", "comments": "Novel View Synthesis, Driving Scene, Free Trajectory, Image Generation", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis\nResearch Topic: computer vision\nAuthors: Shijie Chen, Peixi Peng\nAbstract: Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04830\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 102, "abstract_length": 1135, "total_text_length": 1238, "word_count": 137, "sentence_count": 8}, "basic_keywords": ["training", "driving", "free", "viewpoint", "generative", "realism", "scene", "reconstruction"], "domain_keywords": [], "technical_terms": ["However", "Experiments", "While", "Through", "Closed"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04828", "topic": "computer vision", "title": "The Stagnant Persistence Paradox: Survival Analysis and Temporal Efficiency in Exact Sciences and Engineering Education", "authors": ["H. R. Paz"], "first_author": "H. R. Paz", "abstract": "Research on student progression in higher education has traditionally focused on vertical outcomes such as persistence and dropout, often reducing complex academic histories to binary indicators. While the structural component of horizontal mobility (major switching, plan changes, re-entries) has recently been recognised as a core feature of contemporary university systems, the temporal cost and efficiency of these pathways remain largely unquantified. Using forty years of administrative records from a large faculty of engineering and exact sciences in Argentina (N = 24,016), this study applies a dual-outcome survival analysis framework to two key outcomes: definitive dropout and first major switch. We reconstruct academic trajectories as sequences of enrolment spells and typed transitions under the CAPIRE protocol, and then deploy non-parametric Kaplan-Meier estimators to model time-to-event under right-censoring. Results uncover a critical systemic inefficiency: a global median survival time of 4.33 years prior to definitive dropout, with a pronounced long tail of extended enrolment. This pattern reveals a phenomenon of stagnant persistence, where students remain formally enrolled for long periods without commensurate curricular progression. In contrast, major switching follows an early-event regime, with a median time of 1.0 year among switchers and most switches concentrated within the first academic year. We argue that academic failure in rigid engineering curricula is not a sudden outcome but a long-tail process that generates high opportunity costs, and that institutional indicators should shift from static retention metrics towards measures of curricular velocity based on time-to-event analysis.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CY"], "url": "http://arxiv.org/abs/2512.04828", "comments": "18 pages , 5 figures, 3 tables", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: The Stagnant Persistence Paradox: Survival Analysis and Temporal Efficiency in Exact Sciences and Engineering Education\nResearch Topic: computer vision\nAuthors: H. R. Paz\nAbstract: Research on student progression in higher education has traditionally focused on vertical outcomes such as persistence and dropout, often reducing complex academic histories to binary indicators. While the structural component of horizontal mobility (major switching, plan changes, re-entries) has recently been recognised as a core feature of contemporary university systems, the temporal cost and efficiency of these pathways remain largely unquantified. Using forty years of administrative records from a large faculty of engineering and exact sciences in Argentina (N = 24,016), this study applies a dual-outcome survival analysis framework to two key outcomes: definitive dropout and first major switch. We reconstruct academic trajectories as sequences of enrolment spells and typed transitions under the CAPIRE protocol, and then deploy non-parametric Kaplan-Meier estimators to model time-to-event under right-censoring. Results uncover a critical systemic inefficiency: a global median survival time of 4.33 years prior to definitive dropout, with a pronounced long tail of extended enrolment. This pattern reveals a phenomenon of stagnant persistence, where students remain formally enrolled for long periods without commensurate curricular progression. In contrast, major switching follows an early-event regime, with a median time of 1.0 year among switchers and most switches concentrated within the first academic year. We argue that academic failure in rigid engineering curricula is not a sudden outcome but a long-tail process that generates high opportunity costs, and that institutional indicators should shift from static retention metrics towards measures of curricular velocity based on time-to-event analysis.\nCategories: cs.CY\nPublication Date: 2025-12-04\nPaper ID: 2512.04828\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 119, "abstract_length": 1732, "total_text_length": 1852, "word_count": 255, "sentence_count": 11}, "basic_keywords": ["academic", "time", "dropout", "major", "event", "long", "progression", "outcomes"], "domain_keywords": [], "technical_terms": ["Using", "Meier", "While", "Research", "Results", "Argentina", "Kaplan"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04824", "topic": "computer vision", "title": "Hierarchical matrix approximability of inverse of convection dominated finite element matrices", "authors": ["Arthur Saunier", "Leo Agelas", "Ani Anciaux Sedrakian", "Ibtihel Ben Gharbia", "Xavier Claeys"], "first_author": "Arthur Saunier", "abstract": "Several researchers have developed a rich toolbox of matrix compression techniques that exploit structure and redundancy in large matrices. Classical methods such as the block low-rank format and the Fast Multipole Method make it possible to manipulate very large systems by representing them in a reduced form. Among the most sophisticated tools in this area are hierarchical matrices (H-matrices), which exploit local properties of the underlying kernel or operator to approximate matrix blocks by low-rank factors, organized in a recursive hierarchy. H-matrices offer a flexible and scalable framework, yielding nearly linear complexity in both storage and computation. Hierarchical matrix techniques, originally developed for boundary integral equations, have recently been applied to matrices stemming from the discretization of advection-dominated problems. However, their effectiveness is limited by the loss of coercivity induced by convection phenomena, where traditional methods fail. Initial work by Le Borne addressed this by modifying the admissibility criterion for structured grids with constant convection, but challenges remain for more general grids and advection fields. In this work, we propose a novel partitioning strategy based on \"convection tubes\", clusters aligned with the convection vector field. This method does not require a structured grid or constant convection, overcoming the limitations of previous approaches. We present both theoretical analyses and numerical experiments, that demonstrate the efficiency and robustness of our method for convection-dominated PDEs on unstructured grids. The approach builds on a P clet-robust Caccioppoli inequality, crucial for handling convection-dominated problems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.NA", "math.AP"], "url": "http://arxiv.org/abs/2512.04824", "comments": "32 pages, 22 figures. Submited to the Journal of Computational Physics", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Hierarchical matrix approximability of inverse of convection dominated finite element matrices\nResearch Topic: computer vision\nAuthors: Arthur Saunier, Leo Agelas, Ani Anciaux Sedrakian, Ibtihel Ben Gharbia, Xavier Claeys\nAbstract: Several researchers have developed a rich toolbox of matrix compression techniques that exploit structure and redundancy in large matrices. Classical methods such as the block low-rank format and the Fast Multipole Method make it possible to manipulate very large systems by representing them in a reduced form. Among the most sophisticated tools in this area are hierarchical matrices (H-matrices), which exploit local properties of the underlying kernel or operator to approximate matrix blocks by low-rank factors, organized in a recursive hierarchy. H-matrices offer a flexible and scalable framework, yielding nearly linear complexity in both storage and computation. Hierarchical matrix techniques, originally developed for boundary integral equations, have recently been applied to matrices stemming from the discretization of advection-dominated problems. However, their effectiveness is limited by the loss of coercivity induced by convection phenomena, where traditional methods fail. Initial work by Le Borne addressed this by modifying the admissibility criterion for structured grids with constant convection, but challenges remain for more general grids and advection fields. In this work, we propose a novel partitioning strategy based on \"convection tubes\", clusters aligned with the convection vector field. This method does not require a structured grid or constant convection, overcoming the limitations of previous approaches. We present both theoretical analyses and numerical experiments, that demonstrate the efficiency and robustness of our method for convection-dominated PDEs on unstructured grids. The approach builds on a Péclet-robust Caccioppoli inequality, crucial for handling convection-dominated problems.\nCategories: math.NA, math.AP\nPublication Date: 2025-12-04\nPaper ID: 2512.04824\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1739, "total_text_length": 1834, "word_count": 250, "sentence_count": 12}, "basic_keywords": ["convection", "matrices", "matrix", "method", "dominated", "grids", "developed", "techniques"], "domain_keywords": [], "technical_terms": ["Several", "However", "Caccioppoli", "Initial", "Fast Multipole Method", "Among", "Hierarchical", "Le Borne"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04821", "topic": "computer vision", "title": "LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation", "authors": ["Huynh Trinh Ngoc", "Hoang Anh Nguyen Kim", "Toan Nguyen Hai", "Long Tran Quoc"], "first_author": "Huynh Trinh Ngoc", "abstract": "Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04821", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation\nResearch Topic: computer vision\nAuthors: Huynh Trinh Ngoc, Hoang Anh Nguyen Kim, Toan Nguyen Hai, Long Tran Quoc\nAbstract: Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04821\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1427, "total_text_length": 1511, "word_count": 199, "sentence_count": 10}, "basic_keywords": ["flow", "latent", "both", "generative", "based", "data", "model", "space"], "domain_keywords": [], "technical_terms": ["Furthermore", "Motivated", "Through", "Clinic", "Generative"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04815", "topic": "computer vision", "title": "RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS", "authors": ["Chuanyu Fu", "Guanying Chen", "Yuqi Zhang", "Kunbin Yao", "Yuan Xiong", "Chuan Huang", "Shuguang Cui", "Yasuyuki Matsushita", "Xiaochun Cao"], "first_author": "Chuanyu Fu", "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04815", "comments": "arXiv admin note: substantial text overlap with arXiv:2506.02751", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS\nResearch Topic: computer vision\nAuthors: Chuanyu Fu, Guanying Chen, Yuqi Zhang, Kunbin Yao, Yuan Xiong, Chuan Huang, Shuguang Cui, Yasuyuki Matsushita, Xiaochun Cao\nAbstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04815\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1571, "total_text_length": 1660, "word_count": 206, "sentence_count": 9}, "basic_keywords": ["gaussian", "transient", "mask", "modeling", "existing", "methods", "wild", "scenes"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Third", "Gaussian", "Second", "First", "Gaussians", "Gaussian Splatting"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04814", "topic": "computer vision", "title": "Shared Multi-modal Embedding Space for Face-Voice Association", "authors": ["Christopher Simic", "Korbinian Riedhammer", "Tobias Bocklet"], "first_author": "Christopher Simic", "abstract": "The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained. Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction. The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss. Our approach achieved first place in the FAME 2026 challenge, with an average Equal-Error Rate (EER) of 23.99%.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD", "cs.CV"], "url": "http://arxiv.org/abs/2512.04814", "comments": "Ranked 1st in Fame 2026 Challenge, ICASSP", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Shared Multi-modal Embedding Space for Face-Voice Association\nResearch Topic: computer vision\nAuthors: Christopher Simic, Korbinian Riedhammer, Tobias Bocklet\nAbstract: The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained. Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction. The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss. Our approach achieved first place in the FAME 2026 challenge, with an average Equal-Error Rate (EER) of 23.99%.\nCategories: cs.SD, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04814\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 61, "abstract_length": 636, "total_text_length": 698, "word_count": 97, "sentence_count": 6}, "basic_keywords": ["fame", "challenge", "face", "voice", "trained", "approach", "modal", "feature"], "domain_keywords": ["feature extraction"], "technical_terms": ["Error Rate", "Equal", "Adaptive Angular Margin"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04813", "topic": "computer vision", "title": "MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation", "authors": ["Huanqian Wang", "Chi Bene Chen", "Yang Yue", "Danhua Tao", "Tong Guo", "Shaoxuan Xie", "Denghang Huang", "Shiji Song", "Guocai Yao", "Gao Huang"], "first_author": "Huanqian Wang", "abstract": "Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose (), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, achieves an average success rate of 39.1\\%, a 76.1\\% relative improvement over the static data collection paradigm (22.2\\%), and yields up to 2--5 gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.04813", "comments": "9 pages, 9 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation\nResearch Topic: computer vision\nAuthors: Huanqian Wang, Chi Bene Chen, Yang Yue, Danhua Tao, Tong Guo, Shaoxuan Xie, Denghang Huang, Shiji Song, Guocai Yao, Gao Huang\nAbstract: Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a \\emph{single, static spatial configuration} of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose \\textbf{MOtion-Based Variability Enhancement} (\\emph{MOVE}), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, \\emph{MOVE} achieves an average success rate of 39.1\\%, a 76.1\\% relative improvement over the static data collection paradigm (22.2\\%), and yields up to 2--5$\\times$ gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04813\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 103, "abstract_length": 1499, "total_text_length": 1603, "word_count": 219, "sentence_count": 15}, "basic_keywords": ["spatial", "data", "learning", "generalization", "environment", "information", "available", "efficiency"], "domain_keywords": [], "technical_terms": ["Imitation", "Despite"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04812", "topic": "computer vision", "title": "Construction of the Nearest Nonnegative Hankel Matrix for a Prescribed Eigenpair", "authors": ["Prince Kanhya", "Udit Raj"], "first_author": "Prince Kanhya", "abstract": "We study the problem of determining whether a prescribed eigenpair can be made an exact eigenpair of a nonnegative Hankel matrix through the smallest possible structured perturbation. The task reduces to check the feasibility of a set of linear constraints that encode both the Hankel structure and entrywise nonnegativity. When the feasibility set is nonempty, we compute the minimum-norm perturbation such that . When no such perturbation exists, we compute the nearest nonnegative Hankel matrix in a residual sense by minimizing subject to the imposed constraints. Because closed-form formulas for the structured backward error are generally unavailable, our method provides a fully numerical and optimization-based framework for evaluating eigenpair sensitivity under nonnegativity-preserving Hankel perturbations. Numerical examples illustrate both feasible and infeasible cases.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.NA"], "url": "http://arxiv.org/abs/2512.04812", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Construction of the Nearest Nonnegative Hankel Matrix for a Prescribed Eigenpair\nResearch Topic: computer vision\nAuthors: Prince Kanhya, Udit Raj\nAbstract: We study the problem of determining whether a prescribed eigenpair $(λ,x)$   can be made an exact eigenpair of a nonnegative Hankel matrix through the smallest   possible structured perturbation. The task reduces to check the feasibility of a   set of linear constraints that encode both the Hankel structure and entrywise   nonnegativity. When the feasibility set is nonempty, we compute the minimum-norm   perturbation $ΔH$ such that $(H+ΔH)x=λx$. When no such perturbation   exists, we compute the nearest nonnegative Hankel matrix in a residual sense by   minimizing $\\|(H+ΔH)x-λx\\|_{2}$ subject to the imposed constraints.   Because closed-form formulas for the structured backward error are generally   unavailable, our method provides a fully numerical and optimization-based   framework for evaluating eigenpair sensitivity under nonnegativity-preserving   Hankel perturbations. Numerical examples illustrate both feasible and infeasible cases.\nCategories: math.NA\nPublication Date: 2025-12-04\nPaper ID: 2512.04812\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 884, "total_text_length": 965, "word_count": 133, "sentence_count": 7}, "basic_keywords": ["hankel", "eigenpair", "perturbation", "nonnegative", "matrix", "structured", "feasibility", "constraints"], "domain_keywords": [], "technical_terms": ["Because", "Numerical", "When", "Hankel"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04810", "topic": "computer vision", "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture", "authors": ["Xin He", "Longhui Wei", "Jianbo Ouyang", "Lingxi Xie", "Qi Tian"], "first_author": "Xin He", "abstract": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04810", "comments": "Project Page: https://emma-umm.github.io/emma/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture\nResearch Topic: computer vision\nAuthors: Xin He, Longhui Wei, Jianbo Ouyang, Lingxi Xie, Qi Tian\nAbstract: We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04810\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 93, "abstract_length": 1316, "total_text_length": 1410, "word_count": 178, "sentence_count": 13}, "basic_keywords": ["understanding", "generation", "emma", "unified", "multimodal", "tokens", "visual", "efficient"], "domain_keywords": [], "technical_terms": ["Extensive", "Channel", "Image", "Specifically", "Qwen"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04806", "topic": "computer vision", "title": "Inspiraling binary charged black holes in an external magnetic field: Application of post-Newtonian dynamics in Einstein-Maxwell theory", "authors": ["RunDong Tang", "Lang Liu", "Wen-Biao Han"], "first_author": "RunDong Tang", "abstract": "We present a systematic post-Newtonian treatment of binary charged black holes immersed in external magnetic fields within the framework of Einstein-Maxwell theory. By incorporating a uniform external magnetic field into the two-body Lagrangian expanded to first post-Newtonian order, we derive the complete equations of motion that capture both gravitational and electromagnetic interactions. The magnetic Lorentz force fundamentally alters the orbital dynamics, breaking the conservation of linear and angular momentum and inducing transitions from planar to three-dimensional trajectories. Through numerical integration of these equations, we compute the resulting gravitational waveforms and quantify the magnetic field imprints using matched filtering techniques. Our results demonstrate that strong background magnetic fields can substantially modify the orbital evolution and leave distinctive signatures in the gravitational wave signals. These findings provide a promising avenue for detecting charged black holes and probing magnetic field environments through gravitational wave observations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["gr-qc"], "url": "http://arxiv.org/abs/2512.04806", "comments": "17 pages, 9 figures; Comments are welcome!", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Inspiraling binary charged black holes in an external magnetic field: Application of post-Newtonian dynamics in Einstein-Maxwell theory\nResearch Topic: computer vision\nAuthors: RunDong Tang, Lang Liu, Wen-Biao Han\nAbstract: We present a systematic post-Newtonian treatment of binary charged black holes immersed in external magnetic fields within the framework of Einstein-Maxwell theory. By incorporating a uniform external magnetic field into the two-body Lagrangian expanded to first post-Newtonian order, we derive the complete equations of motion that capture both gravitational and electromagnetic interactions. The magnetic Lorentz force fundamentally alters the orbital dynamics, breaking the conservation of linear and angular momentum and inducing transitions from planar to three-dimensional trajectories. Through numerical integration of these equations, we compute the resulting gravitational waveforms and quantify the magnetic field imprints using matched filtering techniques. Our results demonstrate that strong background magnetic fields can substantially modify the orbital evolution and leave distinctive signatures in the gravitational wave signals. These findings provide a promising avenue for detecting charged black holes and probing magnetic field environments through gravitational wave observations.\nCategories: gr-qc\nPublication Date: 2025-12-04\nPaper ID: 2512.04806\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 135, "abstract_length": 1103, "total_text_length": 1239, "word_count": 159, "sentence_count": 7}, "basic_keywords": ["magnetic", "gravitational", "field", "post", "newtonian", "charged", "black", "holes"], "domain_keywords": [], "technical_terms": ["These", "Maxwell", "Newtonian", "Einstein", "Through", "Lagrangian", "Lorentz"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04801", "topic": "computer vision", "title": "Hybrid VQE-CVQE algorithm using diabatic state preparation", "authors": ["John P. T. Stenger", "C. Stephen Hellberg", "Daniel Gunlycke"], "first_author": "John P. T. Stenger", "abstract": "We propose a hybrid variational quantum algorithm that has variational parameters used by both the quantum circuit and the subsequent classical optimization. Similar to the Variational Quantum Eigensolver (VQE), this algorithm applies a parameterized unitary operator to the qubit register. We generate this operator using diabatic state preparation. The quantum measurement results then inform the classical optimization procedure used by the Cascaded Variational Quantum Eigensolver (CVQE). We demonstrate the algorithm on a system of interacting electrons and show how it can be used on long-term error-corrected as well as short-term intermediate-scale quantum computers. Our simulations performed on IBM Brisbane produced energies well within chemical accuracy.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph"], "url": "http://arxiv.org/abs/2512.04801", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: Hybrid VQE-CVQE algorithm using diabatic state preparation\nResearch Topic: computer vision\nAuthors: John P. T. Stenger, C. Stephen Hellberg, Daniel Gunlycke\nAbstract: We propose a hybrid variational quantum algorithm that has variational parameters used by both the quantum circuit and the subsequent classical optimization. Similar to the Variational Quantum Eigensolver (VQE), this algorithm applies a parameterized unitary operator to the qubit register. We generate this operator using diabatic state preparation. The quantum measurement results then inform the classical optimization procedure used by the Cascaded Variational Quantum Eigensolver (CVQE). We demonstrate the algorithm on a system of interacting electrons and show how it can be used on long-term error-corrected as well as short-term intermediate-scale quantum computers. Our simulations performed on IBM Brisbane produced energies well within chemical accuracy.\nCategories: quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04801\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 58, "abstract_length": 766, "total_text_length": 825, "word_count": 112, "sentence_count": 7}, "basic_keywords": ["quantum", "variational", "algorithm", "used", "classical", "optimization", "eigensolver", "operator"], "domain_keywords": [], "technical_terms": ["Brisbane", "Variational Quantum Eigensolver", "Similar", "Cascaded Variational Quantum Eigensolver"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04799", "topic": "computer vision", "title": "DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors", "authors": ["Gianluca Barmina", "Nathalie Carmen Hau Norman", "Peter Schneider-Kamp", "Lukas Galke"], "first_author": "Gianluca Barmina", "abstract": "We present an enhanced benchmark for evaluating linguistic acceptability in Danish. We first analyze the most common errors found in written Danish. Based on this analysis, we introduce a set of fourteen corruption functions that generate incorrect sentences by systematically introducing errors into existing correct Danish sentences. To ensure the accuracy of these corruptions, we assess their validity using both manual and automatic methods. The results are then used as a benchmark for evaluating Large Language Models on a linguistic acceptability judgement task. Our findings demonstrate that this extension is both broader and more comprehensive than the current state of the art. By incorporating a greater variety of corruption types, our benchmark provides a more rigorous assessment of linguistic acceptability, increasing task difficulty, as evidenced by the lower performance of LLMs on our benchmark compared to existing ones. Our results also suggest that our benchmark has a higher discriminatory power which allows to better distinguish well-performing models from low-performing ones.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04799", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.629360", "embedding_text": "Paper Title: DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors\nResearch Topic: computer vision\nAuthors: Gianluca Barmina, Nathalie Carmen Hau Norman, Peter Schneider-Kamp, Lukas Galke\nAbstract: We present an enhanced benchmark for evaluating linguistic acceptability in Danish. We first analyze the most common errors found in written Danish. Based on this analysis, we introduce a set of fourteen corruption functions that generate incorrect sentences by systematically introducing errors into existing correct Danish sentences. To ensure the accuracy of these corruptions, we assess their validity using both manual and automatic methods. The results are then used as a benchmark for evaluating Large Language Models on a linguistic acceptability judgement task. Our findings demonstrate that this extension is both broader and more comprehensive than the current state of the art. By incorporating a greater variety of corruption types, our benchmark provides a more rigorous assessment of linguistic acceptability, increasing task difficulty, as evidenced by the lower performance of LLMs on our benchmark compared to existing ones. Our results also suggest that our benchmark has a higher discriminatory power which allows to better distinguish well-performing models from low-performing ones.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04799\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1104, "total_text_length": 1181, "word_count": 170, "sentence_count": 9}, "basic_keywords": ["benchmark", "linguistic", "acceptability", "danish", "evaluating", "errors", "corruption", "sentences"], "domain_keywords": [], "technical_terms": ["Based", "Danish", "Large Language Models"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04795", "topic": "computer vision", "title": "Unavoidable patterns and plane paths in dense topological graphs", "authors": ["Balázs Keszegh", "Andrew Suk", "Gábor Tardos", "Ji Zeng"], "first_author": "Balázs Keszegh", "abstract": "Let be the complete bipartite geometric graph, with and vertices on two distinct parallel lines respectively, and all straight-line edges drawn between them. In this paper, we show that every complete bipartite simple topological graph, with parts of size and , contains a topological subgraph weakly isomorphic to . As a corollary, every -vertex simple topological graph not containing a plane path of length has at most edges. When , we obtain a stronger bound by showing that every -vertex simple topological graph not containing a plane path of length 3 has at most edges. We also prove that -monotone simple topological graphs not containing a plane path of length 3 have at most a linear number of edges.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.CO", "cs.CG"], "url": "http://arxiv.org/abs/2512.04795", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630422", "embedding_text": "Paper Title: Unavoidable patterns and plane paths in dense topological graphs\nResearch Topic: computer vision\nAuthors: Balázs Keszegh, Andrew Suk, Gábor Tardos, Ji Zeng\nAbstract: Let $C_{s,t}$ be the complete bipartite geometric graph, with $s$ and $t$ vertices on two distinct parallel lines respectively, and all $s t$ straight-line edges drawn between them. In this paper, we show that every complete bipartite simple topological graph, with parts of size $2(k-1)^4 + 1$ and $2^{k^{5k}}$, contains a topological subgraph weakly isomorphic to $C_{k,k}$. As a corollary, every $n$-vertex simple topological graph not containing a plane path of length $k$ has at most $O_k(n^{2 - 8/k^4})$ edges. When $k = 3$, we obtain a stronger bound by showing that every $n$-vertex simple topological graph not containing a plane path of length 3 has at most $O(n^{4/3})$ edges. We also prove that $x$-monotone simple topological graphs not containing a plane path of length 3 have at most a linear number of edges.\nCategories: math.CO, cs.CG\nPublication Date: 2025-12-04\nPaper ID: 2512.04795\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 64, "abstract_length": 710, "total_text_length": 775, "word_count": 128, "sentence_count": 6}, "basic_keywords": ["topological", "graph", "edges", "simple", "every", "containing", "plane", "path"], "domain_keywords": [], "technical_terms": ["When"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.75, "quality_tier": "medium"}
{"paper_id": "2512.04786", "topic": "computer vision", "title": "LaFiTe: A Generative Latent Field for 3D Native Texturing", "authors": ["Chia-Hao Chen", "Zi-Xin Zou", "Yan-Pei Cao", "Ze Yuan", "Guan Luo", "Xiaojuan Qi", "Ding Liang", "Song-Hai Zhang", "Yuan-Chen Guo"], "first_author": "Chia-Hao Chen", "abstract": "Generating high-fidelity, seamless textures directly on 3D surfaces, what we term 3D-native texturing, remains a fundamental open challenge, with the potential to overcome long-standing limitations of UV-based and multi-view projection methods. However, existing native approaches are constrained by the absence of a powerful and versatile latent representation, which severely limits the fidelity and generality of their generated textures. We identify this representation gap as the principal barrier to further progress. We introduce LaFiTe, a framework that addresses this challenge by learning to generate textures as a 3D generative sparse latent color field. At its core, LaFiTe employs a variational autoencoder (VAE) to encode complex surface appearance into a sparse, structured latent space, which is subsequently decoded into a continuous color field. This representation achieves unprecedented fidelity, exceeding state-of-the-art methods by >10 dB PSNR in reconstruction, by effectively disentangling texture appearance from mesh topology and UV parameterization. Building upon this strong representation, a conditional rectified-flow model synthesizes high-quality, coherent textures across diverse styles and geometries. Extensive experiments demonstrate that LaFiTe not only sets a new benchmark for 3D-native texturing but also enables flexible downstream applications such as material synthesis and texture super-resolution, paving the way for the next generation of 3D content creation workflows.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04786", "comments": "Project page: https://vast-ai-research.github.io/LaFiTe/", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630422", "embedding_text": "Paper Title: LaFiTe: A Generative Latent Field for 3D Native Texturing\nResearch Topic: computer vision\nAuthors: Chia-Hao Chen, Zi-Xin Zou, Yan-Pei Cao, Ze Yuan, Guan Luo, Xiaojuan Qi, Ding Liang, Song-Hai Zhang, Yuan-Chen Guo\nAbstract: Generating high-fidelity, seamless textures directly on 3D surfaces, what we term 3D-native texturing, remains a fundamental open challenge, with the potential to overcome long-standing limitations of UV-based and multi-view projection methods. However, existing native approaches are constrained by the absence of a powerful and versatile latent representation, which severely limits the fidelity and generality of their generated textures. We identify this representation gap as the principal barrier to further progress. We introduce LaFiTe, a framework that addresses this challenge by learning to generate textures as a 3D generative sparse latent color field. At its core, LaFiTe employs a variational autoencoder (VAE) to encode complex surface appearance into a sparse, structured latent space, which is subsequently decoded into a continuous color field. This representation achieves unprecedented fidelity, exceeding state-of-the-art methods by >10 dB PSNR in reconstruction, by effectively disentangling texture appearance from mesh topology and UV parameterization. Building upon this strong representation, a conditional rectified-flow model synthesizes high-quality, coherent textures across diverse styles and geometries. Extensive experiments demonstrate that LaFiTe not only sets a new benchmark for 3D-native texturing but also enables flexible downstream applications such as material synthesis and texture super-resolution, paving the way for the next generation of 3D content creation workflows.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04786\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 1516, "total_text_length": 1574, "word_count": 212, "sentence_count": 9}, "basic_keywords": ["textures", "representation", "fidelity", "native", "latent", "lafite", "high", "texturing"], "domain_keywords": [], "technical_terms": ["Extensive", "Generating", "Building", "However"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04776", "topic": "computer vision", "title": "Customer Identification for Electricity Retailers Based on Monthly Demand Profiles by Activity Sectors and Locations", "authors": ["Joaquin Luque", "Alejandro Carrasco", "Enrique Personal", "Francisco Perez", "Carlos Leon"], "first_author": "Joaquin Luque", "abstract": "The increasing competition in the electric sector is challenging retail companies as they must assign its commercial efforts to attract the most profitable customers. Those are whose energy demand best fit certain target profiles, which usually depend on generation or cost policies. But, even when the demand profile is available, it is in an anonymous way, preventing its association to a particular client. In this paper, we explore a large dataset containing several millions of monthly demand profiles in Spain and use the available information about the associated economic sector and location for an indirect identification of the customers. The distance of the demand profile from the target is used to define a key performance indicator (KPI) which is used as the main driver of the proposed marketing strategy. The combined use of activity and location has been revealed as a powerful tool for indirect identification of customers, as 100,000 customers are uniquely identified, while about 300,000 clients are identifiable in small sets containing 10 or less consumers. To assess the proposed marketing strategy, it has been compared to the random attraction of new clients, showing a reduction of distance from the target of 40% for 10,000 new customers.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CE"], "url": "http://arxiv.org/abs/2512.04776", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630422", "embedding_text": "Paper Title: Customer Identification for Electricity Retailers Based on Monthly Demand Profiles by Activity Sectors and Locations\nResearch Topic: computer vision\nAuthors: Joaquin Luque, Alejandro Carrasco, Enrique Personal, Francisco Perez, Carlos Leon\nAbstract: The increasing competition in the electric sector is challenging retail companies as they must assign its commercial efforts to attract the most profitable customers. Those are whose energy demand best fit certain target profiles, which usually depend on generation or cost policies. But, even when the demand profile is available, it is in an anonymous way, preventing its association to a particular client. In this paper, we explore a large dataset containing several millions of monthly demand profiles in Spain and use the available information about the associated economic sector and location for an indirect identification of the customers. The distance of the demand profile from the target is used to define a key performance indicator (KPI) which is used as the main driver of the proposed marketing strategy. The combined use of activity and location has been revealed as a powerful tool for indirect identification of customers, as 100,000 customers are uniquely identified, while about 300,000 clients are identifiable in small sets containing 10 or less consumers. To assess the proposed marketing strategy, it has been compared to the random attraction of new clients, showing a reduction of distance from the target of 40% for 10,000 new customers.\nCategories: cs.CE\nPublication Date: 2025-12-04\nPaper ID: 2512.04776\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 116, "abstract_length": 1265, "total_text_length": 1382, "word_count": 214, "sentence_count": 8}, "basic_keywords": ["customers", "demand", "target", "sector", "profiles", "profile", "available", "containing"], "domain_keywords": [], "technical_terms": ["Spain", "Those"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04772", "topic": "computer vision", "title": "TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards", "authors": ["Mauro Martini", "Marco Ambrosio", "Judith Vilella-Cantos", "Alessandro Navone", "Marcello Chiaberge"], "first_author": "Mauro Martini", "abstract": "In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation. However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials. The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions. Vineyards pose significant challenges due to their dynamic nature, and they are increasingly drawing attention from both academic and industrial stakeholders interested in automation. In this context, we introduce the TEMPO-VINE dataset, a large-scale multi-temporal dataset specifically designed for evaluating sensor fusion, simultaneous localization and mapping (SLAM), and place recognition techniques within operational vineyard environments. TEMPO-VINE is the first multi-modal public dataset that brings together data from heterogeneous LiDARs of different price levels, AHRS, RTK-GPS, and cameras in real trellis and pergola vineyards, with multiple rows exceeding 100 m in length. In this work, we address a critical gap in the landscape of agricultural datasets by providing researchers with a comprehensive data collection and ground truth trajectories in different seasons, vegetation growth stages, terrain and weather conditions. The sequence paths with multiple runs and revisits will foster the development of sensor fusion, localization, mapping and place recognition solutions for agricultural fields. The dataset, the processing tools and the benchmarking results will be available at the dedicated webpage upon acceptance.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO", "eess.SY"], "url": "http://arxiv.org/abs/2512.04772", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630422", "embedding_text": "Paper Title: TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards\nResearch Topic: computer vision\nAuthors: Mauro Martini, Marco Ambrosio, Judith Vilella-Cantos, Alessandro Navone, Marcello Chiaberge\nAbstract: In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation. However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials. The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions. Vineyards pose significant challenges due to their dynamic nature, and they are increasingly drawing attention from both academic and industrial stakeholders interested in automation. In this context, we introduce the TEMPO-VINE dataset, a large-scale multi-temporal dataset specifically designed for evaluating sensor fusion, simultaneous localization and mapping (SLAM), and place recognition techniques within operational vineyard environments. TEMPO-VINE is the first multi-modal public dataset that brings together data from heterogeneous LiDARs of different price levels, AHRS, RTK-GPS, and cameras in real trellis and pergola vineyards, with multiple rows exceeding 100 m in length. In this work, we address a critical gap in the landscape of agricultural datasets by providing researchers with a comprehensive data collection and ground truth trajectories in different seasons, vegetation growth stages, terrain and weather conditions. The sequence paths with multiple runs and revisits will foster the development of sensor fusion, localization, mapping and place recognition solutions for agricultural fields. The dataset, the processing tools and the benchmarking results will be available at the dedicated webpage upon acceptance.\nCategories: cs.RO, eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04772\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1676, "total_text_length": 1769, "word_count": 240, "sentence_count": 10}, "basic_keywords": ["dataset", "agricultural", "field", "automation", "autonomous", "significant", "real", "conditions"], "domain_keywords": [], "technical_terms": ["However", "Vineyards"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04762", "topic": "computer vision", "title": "Maehara Interpolation in Extensions of R-mingle", "authors": ["Wesley Fussner", "Krzysztof Krawczyk"], "first_author": "Wesley Fussner", "abstract": "We show that there are exactly five quasivarieties of Sugihara algebras with the amalgamation property, and that all of these have the relative congruence extension property. As a consequence, we obtain that the amalgamation property and transferable injections property coincide for arbitrary quasivarieties of Sugihara algebras. These results provide a complete description of arbitrary (not merely axiomatic) extensions of the logic R-mingle that have the Maehara interpolation property, and further demonstrates that the Robinson property and Maehara interpolation property coincide for arbitrary extensions of R-mingle. Further, we show that the question of whether a given finitely based extension of R-mingle has the Maehara interpolation property is decidable.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.LO", "cs.LO"], "url": "http://arxiv.org/abs/2512.04762", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630422", "embedding_text": "Paper Title: Maehara Interpolation in Extensions of R-mingle\nResearch Topic: computer vision\nAuthors: Wesley Fussner, Krzysztof Krawczyk\nAbstract: We show that there are exactly five quasivarieties of Sugihara algebras with the amalgamation property, and that all of these have the relative congruence extension property. As a consequence, we obtain that the amalgamation property and transferable injections property coincide for arbitrary quasivarieties of Sugihara algebras. These results provide a complete description of arbitrary (not merely axiomatic) extensions of the logic R-mingle that have the Maehara interpolation property, and further demonstrates that the Robinson property and Maehara interpolation property coincide for arbitrary extensions of R-mingle. Further, we show that the question of whether a given finitely based extension of R-mingle has the Maehara interpolation property is decidable.\nCategories: math.LO, cs.LO\nPublication Date: 2025-12-04\nPaper ID: 2512.04762\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 47, "abstract_length": 768, "total_text_length": 816, "word_count": 113, "sentence_count": 5}, "basic_keywords": ["property", "arbitrary", "mingle", "maehara", "interpolation", "show", "quasivarieties", "sugihara"], "domain_keywords": [], "technical_terms": ["These", "Further", "Maehara", "Robinson", "Sugihara"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04761", "topic": "computer vision", "title": "Order Matters: 3D Shape Generation from Sequential VR Sketches", "authors": ["Yizi Chen", "Sidi Wu", "Tianyi Xiao", "Nina Wiedemann", "Loic Landrieu"], "first_author": "Yizi Chen", "abstract": "VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04761", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630422", "embedding_text": "Paper Title: Order Matters: 3D Shape Generation from Sequential VR Sketches\nResearch Topic: computer vision\nAuthors: Yizi Chen, Sidi Wu, Tianyi Xiao, Nina Wiedemann, Loic Landrieu\nAbstract: VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04761\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1014, "total_text_length": 1077, "word_count": 146, "sentence_count": 9}, "basic_keywords": ["sketches", "sketch", "shape", "models", "dataset", "shapes", "sequential", "synthetic"], "domain_keywords": [], "technical_terms": ["However"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04759", "topic": "computer vision", "title": "Challenging the Abilities of Large Language Models in Italian: a Community Initiative", "authors": ["Malvina Nissim", "Danilo Croce", "Viviana Patti", "Pierpaolo Basile", "Giuseppe Attanasio", "Elio Musacchio", "Matteo Rinaldi", "Federico Borazio", "Maria Francis", "Jacopo Gili", "Daniel Scalena", "Begoña Altuna", "Ekhi Azurmendi", "Valerio Basile", "Luisa Bentivogli", "Arianna Bisazza", "Marianna Bolognesi", "Dominique Brunato", "Tommaso Caselli", "Silvia Casola", "Maria Cassese", "Mauro Cettolo", "Claudia Collacciani", "Leonardo De Cosmo", "Maria Pia Di Buono", "Andrea Esuli", "Julen Etxaniz", "Chiara Ferrando", "Alessia Fidelangeli", "Simona Frenda", "Achille Fusco", "Marco Gaido", "Andrea Galassi", "Federico Galli", "Luca Giordano", "Mattia Goffetti", "Itziar Gonzalez-Dios", "Lorenzo Gregori", "Giulia Grundler", "Sandro Iannaccone", "Chunyang Jiang", "Moreno La Quatra", "Francesca Lagioia", "Soda Marem Lo", "Marco Madeddu", "Bernardo Magnini", "Raffaele Manna", "Fabio Mercorio", "Paola Merlo", "Arianna Muti", "Vivi Nastase", "Matteo Negri", "Dario Onorati", "Elena Palmieri", "Sara Papi", "Lucia Passaro", "Giulia Pensa", "Andrea Piergentili", "Daniele Potertì", "Giovanni Puccetti", "Federico Ranaldi", "Leonardo Ranaldi", "Andrea Amelio Ravelli", "Martina Rosola", "Elena Sofia Ruzzetti", "Giuseppe Samo", "Andrea Santilli", "Piera Santin", "Gabriele Sarti", "Giovanni Sartor", "Beatrice Savoldi", "Antonio Serino", "Andrea Seveso", "Lucia Siciliani", "Paolo Torroni", "Rossella Varvara", "Andrea Zaninello", "Asya Zanollo", "Fabio Massimo Zanzotto", "Kamyar Zeinalipour", "Andrea Zugarini"], "first_author": "Malvina Nissim", "abstract": "The rapid progress of Large Language Models (LLMs) has transformed natural language processing and broadened its impact across research and society. Yet, systematic evaluation of these models, especially for languages beyond English, remains limited. \"Challenging the Abilities of LAnguage Models in ITAlian\" (CALAMITA) is a large-scale collaborative benchmarking initiative for Italian, coordinated under the Italian Association for Computational Linguistics. Unlike existing efforts that focus on leaderboards, CALAMITA foregrounds methodology: it federates more than 80 contributors from academia, industry, and the public sector to design, document, and evaluate a diverse collection of tasks, covering linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. Through this process, we not only assembled a benchmark of over 20 tasks and almost 100 subtasks, but also established a centralized evaluation pipeline that supports heterogeneous datasets and metrics. We report results for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities, as well as challenges in task-specific evaluation. Beyond quantitative results, CALAMITA exposes methodological lessons: the necessity of fine-grained, task-representative metrics, the importance of harmonized pipelines, and the benefits and limitations of broad community engagement. CALAMITA is conceived as a rolling benchmark, enabling continuous integration of new tasks and models. This makes it both a resource -- the most comprehensive and diverse benchmark for Italian to date -- and a framework for sustainable, community-driven evaluation. We argue that this combination offers a blueprint for other languages and communities seeking inclusive and rigorous LLM evaluation practices.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04759", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: Challenging the Abilities of Large Language Models in Italian: a Community Initiative\nResearch Topic: computer vision\nAuthors: Malvina Nissim, Danilo Croce, Viviana Patti, Pierpaolo Basile, Giuseppe Attanasio, Elio Musacchio, Matteo Rinaldi, Federico Borazio, Maria Francis, Jacopo Gili, Daniel Scalena, Begoña Altuna, Ekhi Azurmendi, Valerio Basile, Luisa Bentivogli, Arianna Bisazza, Marianna Bolognesi, Dominique Brunato, Tommaso Caselli, Silvia Casola, Maria Cassese, Mauro Cettolo, Claudia Collacciani, Leonardo De Cosmo, Maria Pia Di Buono, Andrea Esuli, Julen Etxaniz, Chiara Ferrando, Alessia Fidelangeli, Simona Frenda, Achille Fusco, Marco Gaido, Andrea Galassi, Federico Galli, Luca Giordano, Mattia Goffetti, Itziar Gonzalez-Dios, Lorenzo Gregori, Giulia Grundler, Sandro Iannaccone, Chunyang Jiang, Moreno La Quatra, Francesca Lagioia, Soda Marem Lo, Marco Madeddu, Bernardo Magnini, Raffaele Manna, Fabio Mercorio, Paola Merlo, Arianna Muti, Vivi Nastase, Matteo Negri, Dario Onorati, Elena Palmieri, Sara Papi, Lucia Passaro, Giulia Pensa, Andrea Piergentili, Daniele Potertì, Giovanni Puccetti, Federico Ranaldi, Leonardo Ranaldi, Andrea Amelio Ravelli, Martina Rosola, Elena Sofia Ruzzetti, Giuseppe Samo, Andrea Santilli, Piera Santin, Gabriele Sarti, Giovanni Sartor, Beatrice Savoldi, Antonio Serino, Andrea Seveso, Lucia Siciliani, Paolo Torroni, Rossella Varvara, Andrea Zaninello, Asya Zanollo, Fabio Massimo Zanzotto, Kamyar Zeinalipour, Andrea Zugarini\nAbstract: The rapid progress of Large Language Models (LLMs) has transformed natural language processing and broadened its impact across research and society. Yet, systematic evaluation of these models, especially for languages beyond English, remains limited. \"Challenging the Abilities of LAnguage Models in ITAlian\" (CALAMITA) is a large-scale collaborative benchmarking initiative for Italian, coordinated under the Italian Association for Computational Linguistics. Unlike existing efforts that focus on leaderboards, CALAMITA foregrounds methodology: it federates more than 80 contributors from academia, industry, and the public sector to design, document, and evaluate a diverse collection of tasks, covering linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. Through this process, we not only assembled a benchmark of over 20 tasks and almost 100 subtasks, but also established a centralized evaluation pipeline that supports heterogeneous datasets and metrics. We report results for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities, as well as challenges in task-specific evaluation. Beyond quantitative results, CALAMITA exposes methodological lessons: the necessity of fine-grained, task-representative metrics, the importance of harmonized pipelines, and the benefits and limitations of broad community engagement. CALAMITA is conceived as a rolling benchmark, enabling continuous integration of new tasks and models. This makes it both a resource -- the most comprehensive and diverse benchmark for Italian to date -- and a framework for sustainable, community-driven evaluation. We argue that this combination offers a blueprint for other languages and communities seeking inclusive and rigorous LLM evaluation practices.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04759\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 85, "abstract_length": 1841, "total_text_length": 1927, "word_count": 255, "sentence_count": 11}, "basic_keywords": ["evaluation", "models", "italian", "calamita", "language", "tasks", "benchmark", "large"], "domain_keywords": [], "technical_terms": ["Abilities", "Beyond", "English", "Italian", "Large Language Models", "Challenging", "Italian Association", "Unlike"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04758", "topic": "computer vision", "title": "The next-to-next-to-leading-order QCD corrections to $e^+e^-\\to η_c/χ_{cJ}+γ$ at B factories", "authors": ["Cong Li", "Wen-Long Sang", "Hong-Fei Zhang"], "first_author": "Cong Li", "abstract": "We investigate the processes and at B factories within the NRQCD factorization framework, computing the corresponding helicity amplitudes through . The short-distance coefficients are obtained as series expansions in around , using the method of differential equations. By combining the expansions from all four points, we construct composite asymptotic expressions that reproduce the exact results accurately over the full range , with relative errors below over most of the domain and remaining under elsewhere. Analytic expressions for the leading and next-to-leading logarithmic terms are extracted in the limit . Using these results, we compute the unpolarized cross sections and observe that the perturbative corrections are small for , moderate for , and substantial for and . Theoretical prediction for is consistent with the {Belle} measurement within , showing good agreement between theory and experiment. We also predict the angular distribution parameters , which are insensitive to NRQCD matrix elements and exhibit small theoretical uncertainties. These parameters further display good stability across different perturbative orders. With the high luminosity anticipated at {Belle 2}, future experimental measurements will thus provide a clear test of NRQCD factorization.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["hep-ph", "hep-ex"], "url": "http://arxiv.org/abs/2512.04758", "comments": "35 pages, 7 figures, 3 tables, 1 attached file", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: The next-to-next-to-leading-order QCD corrections to $e^+e^-\\to η_c/χ_{cJ}+γ$ at B factories\nResearch Topic: computer vision\nAuthors: Cong Li, Wen-Long Sang, Hong-Fei Zhang\nAbstract: We investigate the processes $e^+e^-\\to η_c+γ$ and $e^+e^-\\to χ_{cJ}+γ$ at B factories within the NRQCD factorization framework, computing the corresponding helicity amplitudes through $\\mathcal{O}(α_s^2)$. The short-distance coefficients are obtained as series expansions in $r=\\frac{4m_c^2}{s}$ around $r=0, 1/3, 2/3, 1$, using the method of differential equations. By combining the expansions from all four points, we construct composite asymptotic expressions that reproduce the exact results accurately over the full range $0 \\leq r\\leq 1$, with relative errors below $0.1\\%$ over most of the domain and remaining under $1\\%$ elsewhere. Analytic expressions for the leading and next-to-leading logarithmic terms are extracted in the limit $r\\to 0$. Using these results, we compute the unpolarized cross sections and observe that the perturbative corrections are small for $χ_{c0}+γ$, moderate for $χ_{c1}+γ$, and substantial for $η_c+γ$ and $χ_{c2}+γ$. Theoretical prediction for $χ_{c1}+γ$ is consistent with the {\\tt Belle} measurement within $2σ$, showing good agreement between theory and experiment. We also predict the angular distribution parameters $α^H_θ$, which are insensitive to NRQCD matrix elements and exhibit small theoretical uncertainties. These parameters further display good stability across different perturbative orders. With the high luminosity anticipated at {\\tt Belle 2}, future experimental measurements will thus provide a clear test of NRQCD factorization.\nCategories: hep-ph, hep-ex\nPublication Date: 2025-12-04\nPaper ID: 2512.04758\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1287, "total_text_length": 1380, "word_count": 194, "sentence_count": 10}, "basic_keywords": ["nrqcd", "within", "factorization", "expansions", "using", "expressions", "results", "over"], "domain_keywords": [], "technical_terms": ["These", "Using", "Analytic", "Belle", "Theoretical"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04755", "topic": "computer vision", "title": "Typing Fallback Functions: A Semantic Approach to Type Safe Smart Contracts", "authors": ["Stian Lybech", "Daniele Gorla", "Luca Aceto"], "first_author": "Stian Lybech", "abstract": "This paper develops semantic typing in a smart-contract setting to ensure type safety of code that uses statically untypable language constructs, such as the fallback function. The idea is that the creator of a contract on the blockchain equips code containing such constructs with a formal proof of its type safety, given in terms of the semantics of types. Then, a user of the contract only needs to check the validity of the provided `proof certificate' of type safety. This is a form of proof-carrying code, which naturally fits with the immutable nature of the blockchain environment. As a concrete application of our approach, we focus on ensuring information flow control and non-interference for the language TINYSOL, a distilled version of the Solidity language, through security types. We provide the semantics of types in terms of a typed operational semantics of TINYSOL, and a way for expressing the proofs of safety as coinductively-defined typing interpretations and for representing them compactly via up-to techniques, similar to those used for bisimilarity. We also show how our machinery can be used to type the typical pointer-to-implementation pattern based on the fallback function. However, our main contribution is not the safety theorem per se (and so security properties different from non-interference can be considered as well), but rather the presentation of the theoretical developments necessary to make this approach work in a blockchain/smart-contract setting.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.PL"], "url": "http://arxiv.org/abs/2512.04755", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: Typing Fallback Functions: A Semantic Approach to Type Safe Smart Contracts\nResearch Topic: computer vision\nAuthors: Stian Lybech, Daniele Gorla, Luca Aceto\nAbstract: This paper develops semantic typing in a smart-contract setting to ensure type safety of code that uses statically untypable language constructs, such as the fallback function. The idea is that the creator of a contract on the blockchain equips code containing such constructs with a formal proof of its type safety, given in terms of the semantics of types. Then, a user of the contract only needs to check the validity of the provided `proof certificate' of type safety. This is a form of proof-carrying code, which naturally fits with the immutable nature of the blockchain environment.   As a concrete application of our approach, we focus on ensuring information flow control and non-interference for the language TINYSOL, a distilled version of the Solidity language, through security types. We provide the semantics of types in terms of a typed operational semantics of TINYSOL, and a way for expressing the proofs of safety as coinductively-defined typing interpretations and for representing them compactly via up-to techniques, similar to those used for bisimilarity. We also show how our machinery can be used to type the typical pointer-to-implementation pattern based on the fallback function. However, our main contribution is not the safety theorem per se (and so security properties different from non-interference can be considered as well), but rather the presentation of the theoretical developments necessary to make this approach work in a blockchain/smart-contract setting.\nCategories: cs.PL\nPublication Date: 2025-12-04\nPaper ID: 2512.04755\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 75, "abstract_length": 1493, "total_text_length": 1569, "word_count": 241, "sentence_count": 9}, "basic_keywords": ["safety", "contract", "type", "code", "language", "blockchain", "proof", "semantics"], "domain_keywords": [], "technical_terms": ["However", "Solidity", "Then"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04754", "topic": "computer vision", "title": "Evolution of Correlated Electrons in ${\\rm La_3Ni_2O_7}$ at Ambient Pressure: a Study of Double-Counting Effect", "authors": ["Zhong-Yi Xie", "Zhihui Luo", "Wéi Wú", "Dao-Xin Yao"], "first_author": "Zhong-Yi Xie", "abstract": "We employ cluster extension of dynamical mean-field theory (CDMFT) to systematically investigate the impact of double counting corrections on the correlated electronic structure of under ambient pressure. By adjusting double-counting parameters, while maintaining a fixed Fermi surface, we observe a pronounced orbital-selective density of states change: the orbital undergoes significant variation near the Fermi level with increasing , while the orbital remains essentially unchanged throughout the entire range. Analysis of renormalization factor show the monotonic dependence with double counting in both and orbital, and it also identifies an optimal double counting window in orbital aligns with experimental values. We also find the interlayer Matsubara self energy exhibits non-monotonic dependence on , deviating from theoretical predictions. This anomaly is attributed to the metallization of oxygen-bridged pathways, which disrupts the prerequisite for charge transfer via apical oxygen. Our results establish as a critical control parameter for correlated electronic structure in and provide a computational framework for resolving orbital-dependent correlation effects in layered materials.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.supr-con", "cond-mat.str-el"], "url": "http://arxiv.org/abs/2512.04754", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: Evolution of Correlated Electrons in ${\\rm La_3Ni_2O_7}$ at Ambient Pressure: a Study of Double-Counting Effect\nResearch Topic: computer vision\nAuthors: Zhong-Yi Xie, Zhihui Luo, Wéi Wú, Dao-Xin Yao\nAbstract: We employ cluster extension of dynamical mean-field theory (CDMFT) to systematically investigate the impact of double counting corrections on the correlated electronic structure of ${\\rm La_3Ni_2O_7}$ under ambient pressure. By adjusting double-counting parameters, while maintaining a fixed Fermi surface, we observe a pronounced orbital-selective density of states change: the $d_{z^2}$ orbital undergoes significant variation near the Fermi level with increasing $E_{dc}^z$, while the $d_{x^2-y^2}$ orbital remains essentially unchanged throughout the entire range. Analysis of renormalization factor show the monotonic dependence with double counting in both $d_{z^2}$ and $d_{x^2-y^2}$ orbital, and it also identifies an optimal double counting window in $d_{z^2}$ orbital aligns with experimental values. We also find the interlayer Matsubara self energy exhibits non-monotonic dependence on $E_{dc}^z$, deviating from theoretical predictions. This anomaly is attributed to the metallization of oxygen-bridged pathways, which disrupts the prerequisite for charge transfer via apical oxygen. Our results establish $E_{dc}$ as a critical control parameter for correlated electronic structure in ${\\rm La_3Ni_2O_7}$ and provide a computational framework for resolving orbital-dependent correlation effects in layered materials.\nCategories: cond-mat.supr-con, cond-mat.str-el\nPublication Date: 2025-12-04\nPaper ID: 2512.04754\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 111, "abstract_length": 1203, "total_text_length": 1315, "word_count": 176, "sentence_count": 7}, "basic_keywords": ["orbital", "double", "counting", "correlated", "electronic", "structure", "while", "fermi"], "domain_keywords": [], "technical_terms": ["Fermi", "Matsubara", "Analysis"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04753", "topic": "computer vision", "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing", "authors": ["Ruilin Li", "Yibin Wang", "Wenhong Zhu", "Chenglin Li", "Jinghao Zhang", "Chenliang Li", "Junchi Yan", "Jiaqi Wang"], "first_author": "Ruilin Li", "abstract": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04753", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: EtCon: Edit-then-Consolidate for Reliable Knowledge Editing\nResearch Topic: computer vision\nAuthors: Ruilin Li, Yibin Wang, Wenhong Zhu, Chenglin Li, Jinghao Zhang, Chenliang Li, Junchi Yan, Jiaqi Wang\nAbstract: Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04753\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 59, "abstract_length": 1704, "total_text_length": 1764, "word_count": 226, "sentence_count": 8}, "basic_keywords": ["knowledge", "editing", "llms", "between", "real", "world", "behavior", "under"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Prior", "Tuning", "Group Relative Policy Optimization", "Specifically", "Edit", "Knowledge"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04750", "topic": "computer vision", "title": "Robust Precoding Designs of RSMA for Multiuser MIMO Systems", "authors": ["Wentao Zhou", "Yijie Mao", "Di Zhang", "Mérouane Debbah", "Inkyu Lee"], "first_author": "Wentao Zhou", "abstract": "Rate-splitting multiple access (RSMA) has been studied for multiuser multiple-input multiple-output (MUMIMO) systems especially in the presence of imperfect channel state information (CSI) at the transmitter. However, its precoding designs that maximize the sum rate normally have high computational complexity. To implement an efficient RSMA scheme for the MU-MIMO system, in this work, we propose a novel robust precoding design, which can handle imperfect CSI. Specifically, we first adopt the generalized mutual information to construct a lower bound of the objective function in the sum rate maximization problem. Then, we apply a smooth lower bound of the non-smooth sum rate objective function to construct a new optimization problem. By revealing the relationship between the generalized signal-to-interference-plus-noise ratio and the minimum mean square error matrices, we transform the constructed problem into a tractable one. After decomposing the transformed problem into three subproblems, we investigate a new alternating precoding design based on sequential solutions. Simulation results demonstrate that the proposed precoding scheme achieves comparable performance to conventional methods, while significantly reducing the computational complexity.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IT"], "url": "http://arxiv.org/abs/2512.04750", "comments": "This work has been accepted for publication in IEEE Transactions on Wireless Communications", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: Robust Precoding Designs of RSMA for Multiuser MIMO Systems\nResearch Topic: computer vision\nAuthors: Wentao Zhou, Yijie Mao, Di Zhang, Mérouane Debbah, Inkyu Lee\nAbstract: Rate-splitting multiple access (RSMA) has been studied for multiuser multiple-input multiple-output (MUMIMO) systems especially in the presence of imperfect channel state information (CSI) at the transmitter. However, its precoding designs that maximize the sum rate normally have high computational complexity. To implement an efficient RSMA scheme for the MU-MIMO system, in this work, we propose a novel robust precoding design, which can handle imperfect CSI. Specifically, we first adopt the generalized mutual information to construct a lower bound of the objective function in the sum rate maximization problem. Then, we apply a smooth lower bound of the non-smooth sum rate objective function to construct a new optimization problem. By revealing the relationship between the generalized signal-to-interference-plus-noise ratio and the minimum mean square error matrices, we transform the constructed problem into a tractable one. After decomposing the transformed problem into three subproblems, we investigate a new alternating precoding design based on sequential solutions. Simulation results demonstrate that the proposed precoding scheme achieves comparable performance to conventional methods, while significantly reducing the computational complexity.\nCategories: cs.IT\nPublication Date: 2025-12-04\nPaper ID: 2512.04750\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 59, "abstract_length": 1267, "total_text_length": 1327, "word_count": 181, "sentence_count": 9}, "basic_keywords": ["rate", "precoding", "problem", "multiple", "rsma", "imperfect", "information", "computational"], "domain_keywords": [], "technical_terms": ["However", "Simulation", "Rate", "After", "Specifically", "Then"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04748", "topic": "computer vision", "title": "Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time", "authors": ["Xinyue Kang", "Diwei Shi", "Li Chen"], "first_author": "Xinyue Kang", "abstract": "It is a critical challenge to efficiently unlock the powerful reasoning potential of Large Language Models (LLMs) for specific tasks or new distributions. Existing test-time adaptation methods often require tuning model parameters, which is not only computationally expensive but also risks degrading the model's pre-existing abilities.To address this, we introduce a lightweight component, Test-Time Steering Vectors (TTSV), which is prepended to the input while keeping the LLM's parameters entirely frozen. By optimizing the TTSV on test data to minimize the model's output entropy, we steer the model towards an internal state of higher confidence, activating its inherent abilities most relevant to the current task. TTSV is both lightweight and highly efficient to optimize, making it a true plug-and-play enhancement. Extensive experiments validate our approach's effectiveness on both base models and reasoning-enhanced models. For instance, on the MATH500 task, TTSV achieves a 45.88% relative performance gain on the Qwen2.5-Math-7B model and a 16.22% relative gain on the Qwen3-4B model. Furthermore, our approach exhibits robust generalization, with its steering vectors proving highly transferable across diverse tasks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04748", "comments": "accepted to aaai2026", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time\nResearch Topic: computer vision\nAuthors: Xinyue Kang, Diwei Shi, Li Chen\nAbstract: It is a critical challenge to efficiently unlock the powerful reasoning potential of Large Language Models (LLMs) for specific tasks or new distributions. Existing test-time adaptation methods often require tuning model parameters, which is not only computationally expensive but also risks degrading the model's pre-existing abilities.To address this, we introduce a lightweight component, Test-Time Steering Vectors (TTSV), which is prepended to the input while keeping the LLM's parameters entirely frozen. By optimizing the TTSV on test data to minimize the model's output entropy, we steer the model towards an internal state of higher confidence, activating its inherent abilities most relevant to the current task. TTSV is both lightweight and highly efficient to optimize, making it a true plug-and-play enhancement. Extensive experiments validate our approach's effectiveness on both base models and reasoning-enhanced models. For instance, on the MATH500 task, TTSV achieves a 45.88% relative performance gain on the Qwen2.5-Math-7B model and a 16.22% relative gain on the Qwen3-4B model. Furthermore, our approach exhibits robust generalization, with its steering vectors proving highly transferable across diverse tasks.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04748\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1232, "total_text_length": 1317, "word_count": 185, "sentence_count": 12}, "basic_keywords": ["model", "ttsv", "models", "test", "reasoning", "tasks", "existing", "time"], "domain_keywords": [], "technical_terms": ["Extensive", "Existing", "Furthermore", "Large Language Models", "Math", "Time Steering Vectors", "Test"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04742", "topic": "computer vision", "title": "Rotatable Antenna-Enhanced Cell-Free Communication", "authors": ["Kecheng Pan", "Beixiong Zheng", "Yanhua Tan", "Emil Björnson", "Robert Schober", "Rui Zhang"], "first_author": "Kecheng Pan", "abstract": "Rotatable antenna (RA) is a promising technology that can exploit new spatial degrees-of-freedom (DoFs) by flexibly adjusting the three-dimensional (3D) boresight direction of antennas. In this letter, we investigate an RA-enhanced cell-free system for downlink transmission, where multiple RA-equipped access points (APs) cooperatively serve multiple single-antenna users over the same time-frequency resource. Specifically, we aim to maximize the sum rate of all users by jointly optimizing the AP-user associations and the RA boresight directions. Accordingly, we propose a two-stage strategy to solve the AP-user association problem, and then employ fractional programming (FP) and successive convex approximation (SCA) techniques to optimize the RA boresight directions. Numerical results demonstrate that the proposed RA-enhanced cell-free system significantly outperforms various benchmark schemes.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IT"], "url": "http://arxiv.org/abs/2512.04742", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: Rotatable Antenna-Enhanced Cell-Free Communication\nResearch Topic: computer vision\nAuthors: Kecheng Pan, Beixiong Zheng, Yanhua Tan, Emil Björnson, Robert Schober, Rui Zhang\nAbstract: Rotatable antenna (RA) is a promising technology that can exploit new spatial degrees-of-freedom (DoFs) by flexibly adjusting the three-dimensional (3D) boresight direction of antennas. In this letter, we investigate an RA-enhanced cell-free system for downlink transmission, where multiple RA-equipped access points (APs) cooperatively serve multiple single-antenna users over the same time-frequency resource. Specifically, we aim to maximize the sum rate of all users by jointly optimizing the AP-user associations and the RA boresight directions. Accordingly, we propose a two-stage strategy to solve the AP-user association problem, and then employ fractional programming (FP) and successive convex approximation (SCA) techniques to optimize the RA boresight directions. Numerical results demonstrate that the proposed RA-enhanced cell-free system significantly outperforms various benchmark schemes.\nCategories: cs.IT\nPublication Date: 2025-12-04\nPaper ID: 2512.04742\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 50, "abstract_length": 905, "total_text_length": 956, "word_count": 122, "sentence_count": 6}, "basic_keywords": ["boresight", "antenna", "enhanced", "cell", "free", "system", "multiple", "users"], "domain_keywords": [], "technical_terms": ["Specifically", "Accordingly", "Numerical", "Rotatable"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04735", "topic": "computer vision", "title": "A Fast Ethereum-Compatible Forkless Database", "authors": ["Herbert Jordan", "Kamil Jezek", "Pavle Subotic", "Bernhard Scholz"], "first_author": "Herbert Jordan", "abstract": "The State Database of a blockchain stores account data and enables authentication. Modern blockchains use fast consensus protocols to avoid forking, improving throughput and finality. However, Ethereum's StateDB was designed for a forking chain that maintains multiple state versions. While newer blockchains adopt Ethereum's standard for DApp compatibility, they do not require multiple state versions, making legacy Ethereum databases inefficient for fast, non-forking blockchains. Moreover, existing StateDB implementations have been built on key-value stores (e.g., LevelDB), which make them less efficient. This paper introduces a novel state database that is a native database implementation and maintains Ethereum compatibility while being specialized for non-forking blockchains. Our database delivers ten times speedups and 99% space reductions for validators, and a threefold decrease in storage requirements for archive nodes.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.DB"], "url": "http://arxiv.org/abs/2512.04735", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:23.630938", "embedding_text": "Paper Title: A Fast Ethereum-Compatible Forkless Database\nResearch Topic: computer vision\nAuthors: Herbert Jordan, Kamil Jezek, Pavle Subotic, Bernhard Scholz\nAbstract: The State Database of a blockchain stores account data and enables authentication. Modern blockchains use fast consensus protocols to avoid forking, improving throughput and finality. However, Ethereum's StateDB was designed for a forking chain that maintains multiple state versions. While newer blockchains adopt Ethereum's standard for DApp compatibility, they do not require multiple state versions, making legacy Ethereum databases inefficient for fast, non-forking blockchains. Moreover, existing StateDB implementations have been built on key-value stores (e.g., LevelDB), which make them less efficient.   This paper introduces a novel state database that is a native database implementation and maintains Ethereum compatibility while being specialized for non-forking blockchains. Our database delivers ten times speedups and 99% space reductions for validators, and a threefold decrease in storage requirements for archive nodes.\nCategories: cs.DB\nPublication Date: 2025-12-04\nPaper ID: 2512.04735\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 44, "abstract_length": 937, "total_text_length": 982, "word_count": 131, "sentence_count": 10}, "basic_keywords": ["state", "database", "blockchains", "forking", "ethereum", "stores", "fast", "statedb"], "domain_keywords": [], "technical_terms": ["However", "While", "Ethereum", "The State Database", "Modern", "Moreover"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04734", "topic": "computer vision", "title": "MT-Depth: Multi-task Instance feature analysis for the Depth Completion", "authors": ["Abdul Haseeb Nizamani", "Dandi Zhou", "Xinhai Sun"], "first_author": "Abdul Haseeb Nizamani", "abstract": "Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04734", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: MT-Depth: Multi-task Instance feature analysis for the Depth Completion\nResearch Topic: computer vision\nAuthors: Abdul Haseeb Nizamani, Dandi Zhou, Xinhai Sun\nAbstract: Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04734\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1447, "total_text_length": 1519, "word_count": 205, "sentence_count": 9}, "basic_keywords": ["depth", "completion", "instance", "semantic", "segmentation", "object", "branch", "attention"], "domain_keywords": ["semantic segmentation", "instance segmentation", "augmented reality"], "technical_terms": ["Depth", "Qualitative", "While", "Virtual"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04731", "topic": "computer vision", "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting", "authors": ["Jian Tang", "Pu Pang", "Haowen Sun", "Chengzhong Ma", "Xingyu Chen", "Hua Huang", "Xuguang Lan"], "first_author": "Jian Tang", "abstract": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.RO"], "url": "http://arxiv.org/abs/2512.04731", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting\nResearch Topic: computer vision\nAuthors: Jian Tang, Pu Pang, Haowen Sun, Chengzhong Ma, Xingyu Chen, Hua Huang, Xuguang Lan\nAbstract: Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.\nCategories: cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04731\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1407, "total_text_length": 1498, "word_count": 195, "sentence_count": 9}, "basic_keywords": ["domain", "real", "policy", "world", "features", "semantic", "scenarios", "invariant"], "domain_keywords": [], "technical_terms": ["Existing", "Cross", "Gaussian", "Accordingly", "Results", "Gaussian Splatting", "Diffusion Policy", "Semantic"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04720", "topic": "computer vision", "title": "M3-TTS: Multi-modal DiT Alignment & Mel-latent for Zero-shot High-fidelity Speech Synthesis", "authors": ["Xiaopeng Wang", "Chunyu Qiang", "Ruibo Fu", "Zhengqi Wen", "Xuefei Liu", "Yukun Liu", "Yuzhe Liang", "Kang Yin", "Yuankun Xie", "Heng Xie", "Chenxing Li", "Chen Zhang", "Changsheng Li"], "first_author": "Xiaopeng Wang", "abstract": "Non-autoregressive (NAR) text-to-speech synthesis relies on length alignment between text sequences and audio representations, constraining naturalness and expressiveness. Existing methods depend on duration modeling or pseudo-alignment strategies that severely limit naturalness and computational efficiency. We propose M3-TTS, a concise and efficient NAR TTS paradigm based on multi-modal diffusion transformer (MM-DiT) architecture. M3-TTS employs joint diffusion transformer layers for cross-modal alignment, achieving stable monotonic alignment between variable-length text-speech sequences without pseudo-alignment requirements. Single diffusion transformer layers further enhance acoustic detail modeling. The framework integrates a mel-vae codec that provides 3* training acceleration. Experimental results on Seed-TTS and AISHELL-3 benchmarks demonstrate that M3-TTS achieves state-of-the-art NAR performance with the lowest word error rates (1.36\\% English, 1.31\\% Chinese) while maintaining competitive naturalness scores. Code and demos will be available at https://wwwwxp.github.io/M3-TTS.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SD"], "url": "http://arxiv.org/abs/2512.04720", "comments": "Submitted to ICASSP 2026", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: M3-TTS: Multi-modal DiT Alignment & Mel-latent for Zero-shot High-fidelity Speech Synthesis\nResearch Topic: computer vision\nAuthors: Xiaopeng Wang, Chunyu Qiang, Ruibo Fu, Zhengqi Wen, Xuefei Liu, Yukun Liu, Yuzhe Liang, Kang Yin, Yuankun Xie, Heng Xie, Chenxing Li, Chen Zhang, Changsheng Li\nAbstract: Non-autoregressive (NAR) text-to-speech synthesis relies on length alignment between text sequences and audio representations, constraining naturalness and expressiveness. Existing methods depend on duration modeling or pseudo-alignment strategies that severely limit naturalness and computational efficiency. We propose M3-TTS, a concise and efficient NAR TTS paradigm based on multi-modal diffusion transformer (MM-DiT) architecture. M3-TTS employs joint diffusion transformer layers for cross-modal alignment, achieving stable monotonic alignment between variable-length text-speech sequences without pseudo-alignment requirements. Single diffusion transformer layers further enhance acoustic detail modeling. The framework integrates a mel-vae codec that provides 3* training acceleration. Experimental results on Seed-TTS and AISHELL-3 benchmarks demonstrate that M3-TTS achieves state-of-the-art NAR performance with the lowest word error rates (1.36\\% English, 1.31\\% Chinese) while maintaining competitive naturalness scores. Code and demos will be available at https://wwwwxp.github.io/M3-TTS.\nCategories: cs.SD\nPublication Date: 2025-12-04\nPaper ID: 2512.04720\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 91, "abstract_length": 1102, "total_text_length": 1194, "word_count": 139, "sentence_count": 13}, "basic_keywords": ["alignment", "text", "naturalness", "diffusion", "transformer", "speech", "length", "between"], "domain_keywords": [], "technical_terms": ["English", "Existing", "Chinese", "Code", "Seed", "Single", "Experimental"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04719", "topic": "computer vision", "title": "Pinching-Antenna System Design under Random LoS and NLoS Channels", "authors": ["Yanqing Xu", "Yang Lu", "Zhiguo Ding", "Tsung-Hui Chang"], "first_author": "Yanqing Xu", "abstract": "Pinching antennas, realized through position-adjustable radiating elements along dielectric waveguides, have emerged as a promising flexible-antenna technology thanks to their ability to dynamically reshape large-scale channel conditions. However, most existing studies focus on idealized LoS-dominated environments, overlooking the stochastic nature of realistic wireless propagation. This paper investigates a more practical multiuser pinching-antenna system under a composite probabilistic channel model that captures distance-dependent LoS blockage and NLoS scattering. To account for both efficiency and reliability aspects of communication, two complementary design metrics are considered: an average signal-to-noise ratio (SNR) metric characterizing long-term throughput and fairness, and an outage-constrained metric ensuring a prescribed reliability level. Based on these metrics, we formulate two optimization problems: the first maximizes the max-min average SNR across users, while the second maximizes a guaranteed SNR threshold under per-user outage constraints. Although both problems are inherently nonconvex, we exploit their underlying monotonic structures and develop low-complexity, bisection-based algorithms that achieve globally optimal solutions using only simple scalar evaluations. Extensive simulations validate the effectiveness of the proposed methods and demonstrate that pinching-antenna systems significantly outperform conventional fixed-antenna designs even under random LoS and NLoS channels.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SP", "cs.IT"], "url": "http://arxiv.org/abs/2512.04719", "comments": "13 pages, 8 figures, 2 tables", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Pinching-Antenna System Design under Random LoS and NLoS Channels\nResearch Topic: computer vision\nAuthors: Yanqing Xu, Yang Lu, Zhiguo Ding, Tsung-Hui Chang\nAbstract: Pinching antennas, realized through position-adjustable radiating elements along dielectric waveguides, have emerged as a promising flexible-antenna technology thanks to their ability to dynamically reshape large-scale channel conditions. However, most existing studies focus on idealized LoS-dominated environments, overlooking the stochastic nature of realistic wireless propagation. This paper investigates a more practical multiuser pinching-antenna system under a composite probabilistic channel model that captures distance-dependent LoS blockage and NLoS scattering. To account for both efficiency and reliability aspects of communication, two complementary design metrics are considered: an average signal-to-noise ratio (SNR) metric characterizing long-term throughput and fairness, and an outage-constrained metric ensuring a prescribed reliability level. Based on these metrics, we formulate two optimization problems: the first maximizes the max-min average SNR across users, while the second maximizes a guaranteed SNR threshold under per-user outage constraints. Although both problems are inherently nonconvex, we exploit their underlying monotonic structures and develop low-complexity, bisection-based algorithms that achieve globally optimal solutions using only simple scalar evaluations. Extensive simulations validate the effectiveness of the proposed methods and demonstrate that pinching-antenna systems significantly outperform conventional fixed-antenna designs even under random LoS and NLoS channels.\nCategories: eess.SP, cs.IT\nPublication Date: 2025-12-04\nPaper ID: 2512.04719\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1527, "total_text_length": 1593, "word_count": 195, "sentence_count": 8}, "basic_keywords": ["antenna", "pinching", "under", "channel", "nlos", "both", "reliability", "metrics"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Based", "Pinching", "Although"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04712", "topic": "computer vision", "title": "Demonstration of surface-engineered oxidation-resistant Nb-Nb thermocompression bonding toward scalable superconducting quantum computing architectures", "authors": ["Harsh Mishra", "Yusuke Kozuka", "Sathish Bonam", "Jun Uzuhashi", "Praveenkumar Suggisetti", "Tadakatsu Ohkubo", "Shiv Govind Singh"], "first_author": "Harsh Mishra", "abstract": "Scalable quantum computing currently requires a large array of qubit integration, but present two-dimensional interconnects face challenges such as wiring congestion, electromagnetic interference, and limited cryogenic space. To overcome this challenge, implementing three-dimensional (3D) vertical architectures becomes crucial. Niobium (Nb), due to its excellent superconducting characteristics and strong fabrication process compatibility, stands out as a prime material choice. The main challenge in Nb-Nb bonding is the presence of an oxide layer at the interface, even after post-bonding annealing across various bonding methods. The native Nb oxide forms rapidly in air, creating a resistive barrier to supercurrent flow and introducing two-level system losses that degrade qubit coherence while increasing the overall thermal budget. These issues show the need for effective surface engineering to suppress oxidation during bonding. This study introduces an ultrathin gold (Au) capping layer as a passivation strategy to prevent oxygen incorporation at the Nb surface. This approach enables low-temperature Nb-Nb thermocompression bonding at 350 C under a reduced bonding pressure of 0.495 MPa. Detailed microstructural and interfacial analyses confirm that Au passivation effectively suppresses oxide formation and hence enhances bonding uniformity and strength with keeping the superconductivity, establishing a robust route toward low-temperature, low-pressure Nb-Nb bonding for scalable 3D superconducting quantum computing architectures.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.supr-con", "cond-mat.mtrl-sci"], "url": "http://arxiv.org/abs/2512.04712", "comments": "8 pages, 6 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Demonstration of surface-engineered oxidation-resistant Nb-Nb thermocompression bonding toward scalable superconducting quantum computing architectures\nResearch Topic: computer vision\nAuthors: Harsh Mishra, Yusuke Kozuka, Sathish Bonam, Jun Uzuhashi, Praveenkumar Suggisetti, Tadakatsu Ohkubo, Shiv Govind Singh\nAbstract: Scalable quantum computing currently requires a large array of qubit integration, but present two-dimensional interconnects face challenges such as wiring congestion, electromagnetic interference, and limited cryogenic space. To overcome this challenge, implementing three-dimensional (3D) vertical architectures becomes crucial. Niobium (Nb), due to its excellent superconducting characteristics and strong fabrication process compatibility, stands out as a prime material choice. The main challenge in Nb-Nb bonding is the presence of an oxide layer at the interface, even after post-bonding annealing across various bonding methods. The native Nb oxide forms rapidly in air, creating a resistive barrier to supercurrent flow and introducing two-level system losses that degrade qubit coherence while increasing the overall thermal budget. These issues show the need for effective surface engineering to suppress oxidation during bonding. This study introduces an ultrathin gold (Au) capping layer as a passivation strategy to prevent oxygen incorporation at the Nb surface. This approach enables low-temperature Nb-Nb thermocompression bonding at 350 °C under a reduced bonding pressure of 0.495 MPa. Detailed microstructural and interfacial analyses confirm that Au passivation effectively suppresses oxide formation and hence enhances bonding uniformity and strength with keeping the superconductivity, establishing a robust route toward low-temperature, low-pressure Nb-Nb bonding for scalable 3D superconducting quantum computing architectures.\nCategories: cond-mat.supr-con, cond-mat.mtrl-sci\nPublication Date: 2025-12-04\nPaper ID: 2512.04712\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 151, "abstract_length": 1550, "total_text_length": 1702, "word_count": 218, "sentence_count": 11}, "basic_keywords": ["bonding", "oxide", "scalable", "quantum", "computing", "qubit", "dimensional", "challenge"], "domain_keywords": [], "technical_terms": ["These", "Niobium", "Scalable", "Detailed"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04710", "topic": "computer vision", "title": "Quantum-Inspired Optimization through Qudit-Based Imaginary Time Evolution", "authors": ["Erik M. Åsgrim", "Ahsan Javed Awan"], "first_author": "Erik M. Åsgrim", "abstract": "Imaginary-time evolution has been shown to be a promising framework for tackling combinatorial optimization problems on quantum hardware. In this work, we propose a classical quantum-inspired strategy for solving combinatorial optimization problems with integer-valued decision variables by encoding decision variables into multi-level quantum states known as qudits. This method results in a reduced number of decision variables compared to binary formulations while inherently incorporating single-association constraints. Efficient classical simulation is enabled by constraining the system to remain in a product state throughout optimization. The qudit states are optimized by applying a sequence of unitary operators that iteratively approximate the dynamics of imaginary time evolution. Unlike previous studies, we propose a gradient-based method of adaptively choosing the Hermitian operators used to generate the state evolution at each optimization step, as a means to improve the convergence properties of the algorithm. The proposed algorithm demonstrates promising results on Min-d-Cut problem with constraints, outperforming Gurobi on penalized constraint formulation, particularly for larger values of d.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cs.ET"], "url": "http://arxiv.org/abs/2512.04710", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Quantum-Inspired Optimization through Qudit-Based Imaginary Time Evolution\nResearch Topic: computer vision\nAuthors: Erik M. Åsgrim, Ahsan Javed Awan\nAbstract: Imaginary-time evolution has been shown to be a promising framework for tackling combinatorial optimization problems on quantum hardware. In this work, we propose a classical quantum-inspired strategy for solving combinatorial optimization problems with integer-valued decision variables by encoding decision variables into multi-level quantum states known as qudits. This method results in a reduced number of decision variables compared to binary formulations while inherently incorporating single-association constraints. Efficient classical simulation is enabled by constraining the system to remain in a product state throughout optimization. The qudit states are optimized by applying a sequence of unitary operators that iteratively approximate the dynamics of imaginary time evolution. Unlike previous studies, we propose a gradient-based method of adaptively choosing the Hermitian operators used to generate the state evolution at each optimization step, as a means to improve the convergence properties of the algorithm. The proposed algorithm demonstrates promising results on Min-d-Cut problem with constraints, outperforming Gurobi on penalized constraint formulation, particularly for larger values of d.\nCategories: quant-ph, cs.ET\nPublication Date: 2025-12-04\nPaper ID: 2512.04710\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 1219, "total_text_length": 1294, "word_count": 169, "sentence_count": 8}, "basic_keywords": ["optimization", "evolution", "quantum", "decision", "variables", "imaginary", "time", "promising"], "domain_keywords": [], "technical_terms": ["Imaginary", "Efficient", "Unlike", "Hermitian", "Gurobi"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04705", "topic": "computer vision", "title": "Hardware-aware Neural Architecture Search of Early Exiting Networks on Edge Accelerators", "authors": ["Alaa Zniber", "Arne Symons", "Ouassim Karrakchou", "Marian Verhelst", "Mounir Ghogho"], "first_author": "Alaa Zniber", "abstract": "Advancements in high-performance computing and cloud technologies have enabled the development of increasingly sophisticated Deep Learning (DL) models. However, the growing demand for embedded intelligence at the edge imposes stringent computational and energy constraints, challenging the deployment of these large-scale models. Early Exiting Neural Networks (EENN) have emerged as a promising solution, allowing dynamic termination of inference based on input complexity to enhance efficiency. Despite their potential, EENN performance is highly influenced by the heterogeneity of edge accelerators and the constraints imposed by quantization, affecting accuracy, energy efficiency, and latency. Yet, research on the automatic optimization of EENN design for edge hardware remains limited. To bridge this gap, we propose a hardware-aware Neural Architecture Search (NAS) framework that systematically integrates the effects of quantization and hardware resource allocation to optimize the placement of early exit points within a network backbone. Experimental results on the CIFAR-10 dataset demonstrate that our NAS framework can discover architectures that achieve over a 50\\% reduction in computational costs compared to conventional static networks, making them more suitable for deployment in resource-constrained edge environments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CC", "cs.AR", "cs.CV"], "url": "http://arxiv.org/abs/2512.04705", "comments": "Submitted to IEEE Transactions on Emerging Topics in Computing", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Hardware-aware Neural Architecture Search of Early Exiting Networks on Edge Accelerators\nResearch Topic: computer vision\nAuthors: Alaa Zniber, Arne Symons, Ouassim Karrakchou, Marian Verhelst, Mounir Ghogho\nAbstract: Advancements in high-performance computing and cloud technologies have enabled the development of increasingly sophisticated Deep Learning (DL) models. However, the growing demand for embedded intelligence at the edge imposes stringent computational and energy constraints, challenging the deployment of these large-scale models. Early Exiting Neural Networks (EENN) have emerged as a promising solution, allowing dynamic termination of inference based on input complexity to enhance efficiency. Despite their potential, EENN performance is highly influenced by the heterogeneity of edge accelerators and the constraints imposed by quantization, affecting accuracy, energy efficiency, and latency. Yet, research on the automatic optimization of EENN design for edge hardware remains limited. To bridge this gap, we propose a hardware-aware Neural Architecture Search (NAS) framework that systematically integrates the effects of quantization and hardware resource allocation to optimize the placement of early exit points within a network backbone. Experimental results on the CIFAR-10 dataset demonstrate that our NAS framework can discover architectures that achieve over a 50\\% reduction in computational costs compared to conventional static networks, making them more suitable for deployment in resource-constrained edge environments.\nCategories: cs.CC, cs.AR, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04705\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1339, "total_text_length": 1428, "word_count": 189, "sentence_count": 8}, "basic_keywords": ["edge", "eenn", "hardware", "performance", "models", "computational", "energy", "constraints"], "domain_keywords": [], "technical_terms": ["Deep Learning", "However", "Despite", "Neural Architecture Search", "Advancements", "Early Exiting Neural Networks", "Experimental"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04702", "topic": "computer vision", "title": "POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?", "authors": ["Divyansh Pandey", "Vyakhya Gupta", "Prakhar Singhal", "Karthik Vaidhyanathan"], "first_author": "Divyansh Pandey", "abstract": "The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SE"], "url": "http://arxiv.org/abs/2512.04702", "comments": "Accepted as a short paper at SEAMS 2026", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?\nResearch Topic: computer vision\nAuthors: Divyansh Pandey, Vyakhya Gupta, Prakhar Singhal, Karthik Vaidhyanathan\nAbstract: The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.\nCategories: cs.SE\nPublication Date: 2025-12-04\nPaper ID: 2512.04702\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 1694, "total_text_length": 1782, "word_count": 224, "sentence_count": 13}, "basic_keywords": ["adaptation", "self", "polaris", "layer", "adaptive", "systems", "software", "introduce"], "domain_keywords": [], "technical_terms": ["Adapter", "Existing", "Preliminary", "Software", "Reasoning", "Through", "Meta", "Recent"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04699", "topic": "computer vision", "title": "OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution", "authors": ["Xinning Chai", "Zhengxue Cheng", "Yuhong Zhang", "Hengsheng Zhang", "Yingsheng Qin", "Yucai Yang", "Rong Xie", "Li Song"], "first_author": "Xinning Chai", "abstract": "Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04699", "comments": "Accepted as TCSVT, 15 pages", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution\nResearch Topic: computer vision\nAuthors: Xinning Chai, Zhengxue Cheng, Yuhong Zhang, Hengsheng Zhang, Yingsheng Qin, Yucai Yang, Rong Xie, Li Song\nAbstract: Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04699\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 122, "abstract_length": 1752, "total_text_length": 1875, "word_count": 228, "sentence_count": 14}, "basic_keywords": ["scale", "diffusion", "realism", "high", "arbitrary", "super", "resolution", "assr"], "domain_keywords": ["feature extraction"], "technical_terms": ["Extensive", "However", "Real", "Code", "Arbitrary", "Recent", "Most"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04692", "topic": "computer vision", "title": "Interactive Communication -- cross-disciplinary perspectives from psychology, acoustics, and media technology", "authors": ["Mareike Daeglau", "Stephan Getzmann", "Moritz Bender", "Janina Fels", "Rainer Martin", "Alexander Raake", "Isabel S. Schiller", "Sabine J. Schlittmeier", "Katrin Schoenenberg", "Felix Stärz", "Leon O. H. Kroczek"], "first_author": "Mareike Daeglau", "abstract": "Interactive communication (IC), i.e., the reciprocal exchange of information between two or more interactive partners, is a fundamental part of human nature. As such, it has been studied across multiple scientific disciplines with different goals and methods. This article provides a cross-disciplinary primer on contemporary IC that integrates psychological mechanisms with acoustic and media-technological constraints across theory, measurement, and applications. First, we outline theoretical frameworks that account for verbal, nonverbal and multimodal aspects of IC, including distinctions between face-to-face and computer-mediated communication. Second, we summarize key methodological approaches, including behavioral, cognitive, and experiential measures of communicative synchrony and acoustic signal quality. Third, we discuss selected applications, i.e. assistive listening technologies, conversational agents, alongside ethical considerations. Taken together, this review highlights how human capacities and technical systems jointly shape IC, consolidating concepts, findings, and challenges that have often been discussed in separate lines of research.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.HC"], "url": "http://arxiv.org/abs/2512.04692", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Interactive Communication -- cross-disciplinary perspectives from psychology, acoustics, and media technology\nResearch Topic: computer vision\nAuthors: Mareike Daeglau, Stephan Getzmann, Moritz Bender, Janina Fels, Rainer Martin, Alexander Raake, Isabel S. Schiller, Sabine J. Schlittmeier, Katrin Schoenenberg, Felix Stärz, Leon O. H. Kroczek\nAbstract: Interactive communication (IC), i.e., the reciprocal exchange of information between two or more interactive partners, is a fundamental part of human nature. As such, it has been studied across multiple scientific disciplines with different goals and methods. This article provides a cross-disciplinary primer on contemporary IC that integrates psychological mechanisms with acoustic and media-technological constraints across theory, measurement, and applications. First, we outline theoretical frameworks that account for verbal, nonverbal and multimodal aspects of IC, including distinctions between face-to-face and computer-mediated communication. Second, we summarize key methodological approaches, including behavioral, cognitive, and experiential measures of communicative synchrony and acoustic signal quality. Third, we discuss selected applications, i.e. assistive listening technologies, conversational agents, alongside ethical considerations. Taken together, this review highlights how human capacities and technical systems jointly shape IC, consolidating concepts, findings, and challenges that have often been discussed in separate lines of research.\nCategories: cs.HC\nPublication Date: 2025-12-04\nPaper ID: 2512.04692\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 109, "abstract_length": 1167, "total_text_length": 1277, "word_count": 155, "sentence_count": 12}, "basic_keywords": ["interactive", "communication", "between", "human", "across", "acoustic", "applications", "including"], "domain_keywords": [], "technical_terms": ["Third", "Second", "First", "Interactive", "Taken"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04688", "topic": "computer vision", "title": "Flux-controlled wall model for large eddy simulation integrating the compressible law of the wall", "authors": ["Youjie Xu", "Steffen J. Schmidt", "Nikolaus A. Adams"], "first_author": "Youjie Xu", "abstract": "Recent advances in velocity and temperature transformations have enabled recovery of the law of the wall in compressible wall-bounded turbulent flows. Building on this foundation, a flux-controlled wall model (FCWM) for Large Eddy Simulation (LES) is proposed. Unlike conventional wall-stress models that solve the turbulent boundary layer equations, FCWM formulates the near-wall modeling as a control problem applied directly to the outer LES solution. It consists of three components: (1) the compressible law of the wall, (2) a feedback flux-control strategy, and (3) a shifted boundary condition. The model adjusts the wall shear stress and heat flux based on discrepancies between the computed and target transformed velocity and temperature, respectively, at the matching location. The proposed wall model is evaluated using LES of turbulent channel flows across a broad range of conditions, including quasi-incompressible cases with bulk Mach number \\(M_b = 0.1\\) and friction Reynolds number \\(Re_ = 180 10{,}000\\), and compressible cases with \\(M_b = 0.74 4.0\\) and bulk Reynolds number \\(Re_b = 7667 34{,}000\\). The wall-modelled LES reproduce mean velocity and temperature profiles in agreement with direct numerical simulation data. For all tested cases with \\(M_b 3\\), the wall model achieves relative errors of \\(| _{C_f}| < 4.1\\%\\), \\(| _{B_q}| < 2.7\\%\\), and \\(| _{T_c}| < 2.7\\%\\) in friction coefficient, non-dimensional heat flux, and centerline temperature, respectively. In the quasi-incompressible regime, the wall model achieves \\(| _{C_f}| < 1\\%\\). Compared to the conventional equilibrium wall model, the proposed FCWM achieves higher accuracy in compressible turbulent channel flows without solving the boundary layer equations, thereby reducing computational cost.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.flu-dyn"], "url": "http://arxiv.org/abs/2512.04688", "comments": "The manuscript has been accepted by Physics of Fluids. After it is published, it will be found at the Physics of Fluids website", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Flux-controlled wall model for large eddy simulation integrating the compressible law of the wall\nResearch Topic: computer vision\nAuthors: Youjie Xu, Steffen J. Schmidt, Nikolaus A. Adams\nAbstract: Recent advances in velocity and temperature transformations have enabled recovery of the law of the wall in compressible wall-bounded turbulent flows. Building on this foundation, a flux-controlled wall model (FCWM) for Large Eddy Simulation (LES) is proposed. Unlike conventional wall-stress models that solve the turbulent boundary layer equations, FCWM formulates the near-wall modeling as a control problem applied directly to the outer LES solution. It consists of three components: (1) the compressible law of the wall, (2) a feedback flux-control strategy, and (3) a shifted boundary condition. The model adjusts the wall shear stress and heat flux based on discrepancies between the computed and target transformed velocity and temperature, respectively, at the matching location. The proposed wall model is evaluated using LES of turbulent channel flows across a broad range of conditions, including quasi-incompressible cases with bulk Mach number \\(M_b = 0.1\\) and friction Reynolds number \\(Re_τ= 180 \\sim 10{,}000\\), and compressible cases with \\(M_b = 0.74 \\sim 4.0\\) and bulk Reynolds number \\(Re_b = 7667 \\sim 34{,}000\\). The wall-modelled LES reproduce mean velocity and temperature profiles in agreement with direct numerical simulation data. For all tested cases with \\(M_b \\leq 3\\), the wall model achieves relative errors of \\(|ε_{C_f}| < 4.1\\%\\), \\(|ε_{B_q}| < 2.7\\%\\), and \\(|ε_{T_c}| < 2.7\\%\\) in friction coefficient, non-dimensional heat flux, and centerline temperature, respectively. In the quasi-incompressible regime, the wall model achieves \\(|ε_{C_f}| < 1\\%\\). Compared to the conventional equilibrium wall model, the proposed FCWM achieves higher accuracy in compressible turbulent channel flows without solving the boundary layer equations, thereby reducing computational cost.\nCategories: physics.flu-dyn\nPublication Date: 2025-12-04\nPaper ID: 2512.04688\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1791, "total_text_length": 1889, "word_count": 273, "sentence_count": 17}, "basic_keywords": ["wall", "model", "temperature", "compressible", "turbulent", "flux", "velocity", "flows"], "domain_keywords": [], "technical_terms": ["Compared", "Large Eddy Simulation", "Reynolds", "Unlike", "Mach", "Recent", "Building"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04687", "topic": "computer vision", "title": "Intuitionistic modal logic LIK4 is decidable", "authors": ["Philippe Balbiani", "Çigdem Gencer", "Tinko Tinchev"], "first_author": "Philippe Balbiani", "abstract": "In this note, we prove that intuitionistic modal logic LIK4 is decidable.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LO"], "url": "http://arxiv.org/abs/2512.04687", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Intuitionistic modal logic LIK4 is decidable\nResearch Topic: computer vision\nAuthors: Philippe Balbiani, Çigdem Gencer, Tinko Tinchev\nAbstract: In this note, we prove that intuitionistic modal logic LIK4 is decidable.\nCategories: cs.LO\nPublication Date: 2025-12-04\nPaper ID: 2512.04687\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 44, "abstract_length": 73, "total_text_length": 118, "word_count": 18, "sentence_count": 2}, "basic_keywords": ["note", "prove", "intuitionistic", "modal", "logic", "decidable"], "domain_keywords": [], "technical_terms": [], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.5, "technical_substance": 0.0}, "overall_quality_score": 0.625, "quality_tier": "medium"}
{"paper_id": "2512.04686", "topic": "computer vision", "title": "Towards Cross-View Point Correspondence in Vision-Language Models", "authors": ["Yipu Wang", "Yuheng Ji", "Yuyang Liu", "Enshen Zhou", "Ziqiang Yang", "Yuxuan Tian", "Ziheng Qin", "Yue Liu", "Huajie Tan", "Cheng Chi", "Zhiyuan Ma", "Daniel Dajun Zeng", "Xiaolong Zheng"], "first_author": "Yipu Wang", "abstract": "Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of \"perceive\", \"reason\", and \"correspond\". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04686", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Towards Cross-View Point Correspondence in Vision-Language Models\nResearch Topic: computer vision\nAuthors: Yipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Cheng Chi, Zhiyuan Ma, Daniel Dajun Zeng, Xiaolong Zheng\nAbstract: Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of \"perceive\", \"reason\", and \"correspond\". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04686\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 65, "abstract_length": 1373, "total_text_length": 1439, "word_count": 178, "sentence_count": 16}, "basic_keywords": ["crosspoint", "correspondence", "cross", "view", "dataset", "still", "models", "precise"], "domain_keywords": [], "technical_terms": ["However", "Cross", "Furthermore", "Vision", "Bench", "Gemini", "Language Models", "View Point Correspondence"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04683", "topic": "computer vision", "title": "Geschlechtsübergreifende Maskulina im Sprachgebrauch Eine korpusbasierte Untersuchung zu lexemspezifischen Unterschieden", "authors": ["Carolin Mueller-Spitzer", "Samira Ochs", "Jan Oliver Ruediger", "Sascha Wolfer"], "first_author": "Carolin Mueller-Spitzer", "abstract": "This study examines the distribution and linguistic characteristics of generic masculines (GM) in contemporary German press texts. The use of masculine personal nouns to refer to mixed-gender groups or unspecified individuals has been widely debated in academia and the public, with con-flicting perspectives on its gender-neutrality. While psycholinguistic studies suggest that GM is more readily associated with male referents, corpus-based analyses of its actual use remain scarce. We investigate GM in a large corpus of press texts, focusing on lexeme-specific differences across dif-ferent types of personal nouns. We conducted manual annotations of the whole inflectional para-digm of 21 personal nouns, resulting in 6,195 annotated tokens. Our findings reveal considerable differences between lexical items, especially between passive role nouns and prestige-related per-sonal nouns. On a grammatical level, we find that GM occurs predominantly in the plural and in indefinite noun phrases. Furthermore, our data shows that GM is not primarily used to denote entire classes of people, as has been previously claimed. By providing an empirical insight into the use of GM in authentic written language, we contribute to a more nuanced understanding of its forms and manifestations. These findings provide a solid basis for aligning linguistic stimuli in psy-cholinguistic studies more closely with real-world language use.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04683", "comments": "32 pages, 8 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Geschlechtsübergreifende Maskulina im Sprachgebrauch Eine korpusbasierte Untersuchung zu lexemspezifischen Unterschieden\nResearch Topic: computer vision\nAuthors: Carolin Mueller-Spitzer, Samira Ochs, Jan Oliver Ruediger, Sascha Wolfer\nAbstract: This study examines the distribution and linguistic characteristics of generic masculines (GM) in contemporary German press texts. The use of masculine personal nouns to refer to mixed-gender groups or unspecified individuals has been widely debated in academia and the public, with con-flicting perspectives on its gender-neutrality. While psycholinguistic studies suggest that GM is more readily associated with male referents, corpus-based analyses of its actual use remain scarce. We investigate GM in a large corpus of press texts, focusing on lexeme-specific differences across dif-ferent types of personal nouns. We conducted manual annotations of the whole inflectional para-digm of 21 personal nouns, resulting in 6,195 annotated tokens. Our findings reveal considerable differences between lexical items, especially between passive role nouns and prestige-related per-sonal nouns. On a grammatical level, we find that GM occurs predominantly in the plural and in indefinite noun phrases. Furthermore, our data shows that GM is not primarily used to denote entire classes of people, as has been previously claimed. By providing an empirical insight into the use of GM in authentic written language, we contribute to a more nuanced understanding of its forms and manifestations. These findings provide a solid basis for aligning linguistic stimuli in psy-cholinguistic studies more closely with real-world language use.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04683\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 120, "abstract_length": 1427, "total_text_length": 1548, "word_count": 216, "sentence_count": 11}, "basic_keywords": ["nouns", "personal", "more", "linguistic", "press", "texts", "gender", "studies"], "domain_keywords": [], "technical_terms": ["These", "German", "Furthermore", "While"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.825, "quality_tier": "high"}
{"paper_id": "2512.04679", "topic": "computer vision", "title": "Timely Information for Strategic Persuasion", "authors": ["Ahmet Bugra Gundogan", "Melih Bastopcu"], "first_author": "Ahmet Bugra Gundogan", "abstract": "This work investigates a dynamic variant of Bayesian persuasion, in which a strategic sender seeks to influence a receiver's belief over time through controlling the timing of the information disclosure, under resource constraints. We consider a binary information source (i.e., taking values 0 or 1), where the source's state evolve according to a continuous-time Markov chain (CTMC). In this setting, the receiver aims to estimate the source's state as accurately as possible. In contrast, the sender seeks to persuade the receiver to estimate the state to be 1, regardless of whether this estimate reflects the true state. This misalignment between their objectives naturally leads to a Stackelberg game formulation where the sender, acting as the leader, chooses an information-revelation policy, and the receiver, as the follower, decides whether to follow the sender's messages. As a result, the sender's objective is to maximize the long-term average time that the receiver's estimate equals 1, subject to a total sampling constraint and a constraint for the receiver to follow the sender's messages called incentive compatibility (IC) constraint. We first consider the single-source problem and show that the sender's optimal policy is to allocate a minimal sampling rate to the undesired state 0 (just enough to satisfy the IC constraint) and assign the remaining sampling rate to the desired state 1. Next, we extend the analysis to the multi-source case, where each source has a different minimal sampling rate. Our results show that the sender can leverage the timeliness of the revealed information to influence the receiver, thereby achieving a higher utility.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IT", "cs.GT", "cs.NI", "eess.SP", "eess.SY"], "url": "http://arxiv.org/abs/2512.04679", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Timely Information for Strategic Persuasion\nResearch Topic: computer vision\nAuthors: Ahmet Bugra Gundogan, Melih Bastopcu\nAbstract: This work investigates a dynamic variant of Bayesian persuasion, in which a strategic sender seeks to influence a receiver's belief over time through controlling the timing of the information disclosure, under resource constraints. We consider a binary information source (i.e., taking values 0 or 1), where the source's state evolve according to a continuous-time Markov chain (CTMC). In this setting, the receiver aims to estimate the source's state as accurately as possible. In contrast, the sender seeks to persuade the receiver to estimate the state to be 1, regardless of whether this estimate reflects the true state. This misalignment between their objectives naturally leads to a Stackelberg game formulation where the sender, acting as the leader, chooses an information-revelation policy, and the receiver, as the follower, decides whether to follow the sender's messages. As a result, the sender's objective is to maximize the long-term average time that the receiver's estimate equals 1, subject to a total sampling constraint and a constraint for the receiver to follow the sender's messages called incentive compatibility (IC) constraint. We first consider the single-source problem and show that the sender's optimal policy is to allocate a minimal sampling rate to the undesired state 0 (just enough to satisfy the IC constraint) and assign the remaining sampling rate to the desired state 1. Next, we extend the analysis to the multi-source case, where each source has a different minimal sampling rate. Our results show that the sender can leverage the timeliness of the revealed information to influence the receiver, thereby achieving a higher utility.\nCategories: cs.IT, cs.GT, cs.NI, eess.SP, eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04679\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 43, "abstract_length": 1674, "total_text_length": 1718, "word_count": 263, "sentence_count": 12}, "basic_keywords": ["sender", "receiver", "source", "state", "information", "estimate", "sampling", "constraint"], "domain_keywords": [], "technical_terms": ["Bayesian", "Stackelberg", "Markov", "Next"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.3}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04678", "topic": "computer vision", "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "authors": ["Yunhong Lu", "Yanhong Zeng", "Haobo Li", "Hao Ouyang", "Qiuyu Wang", "Ka Leong Cheng", "Jiapeng Zhu", "Hengyuan Cao", "Zhipeng Zhang", "Xing Zhu", "Yujun Shen", "Min Zhang"], "first_author": "Yunhong Lu", "abstract": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04678", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation\nResearch Topic: computer vision\nAuthors: Yunhong Lu, Yanhong Zeng, Haobo Li, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jiapeng Zhu, Hengyuan Cao, Zhipeng Zhang, Xing Zhu, Yujun Shen, Min Zhang\nAbstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04678\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 101, "abstract_length": 1635, "total_text_length": 1737, "word_count": 227, "sentence_count": 13}, "basic_keywords": ["tokens", "video", "initial", "frames", "dynamics", "sink", "motion", "reward"], "domain_keywords": [], "technical_terms": ["However", "Reward Forcing", "Existing", "Efficient", "Instead", "Without", "Second", "First"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04677", "topic": "computer vision", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "authors": ["Yubo Huang", "Hailong Guo", "Fangtai Wu", "Shifeng Zhang", "Shijie Huang", "Qijun Gan", "Lin Liu", "Sirui Zhao", "Enhong Chen", "Jiaming Liu", "Steven Hoi"], "first_author": "Yubo Huang", "abstract": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04677", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length\nResearch Topic: computer vision\nAuthors: Yubo Huang, Hailong Guo, Fangtai Wu, Shifeng Zhang, Shijie Huang, Qijun Gan, Lin Liu, Sirui Zhao, Enhong Chen, Jiaming Liu, Steven Hoi\nAbstract: Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04677\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 84, "abstract_length": 1465, "total_text_length": 1550, "word_count": 187, "sentence_count": 8}, "basic_keywords": ["avatar", "generation", "diffusion", "real", "time", "fidelity", "video", "long"], "domain_keywords": [], "technical_terms": ["Existing", "Pipeline Parallelism", "Additionally", "Timestep", "Live Avatar", "Forcing Distribution Matching Distillation", "Rolling Sink Frame Mechanism", "Self"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04676", "topic": "computer vision", "title": "A Unified Low-rank ADI Framework with Shared Linear Solves for Simultaneously Solving Multiple Lyapunov, Sylvester, and Riccati Equations", "authors": ["Umair Zulfiqar", "Zhong-Yi Huang"], "first_author": "Umair Zulfiqar", "abstract": "It is known in the literature that the low-rank ADI method for Lyapunov equations is a Petrov-Galerkin projection algorithm that implicitly performs model order reduction. In this paper, we show that the low-rank ADI methods for Sylvester and Riccati equations are also Petrov-Galerkin projection algorithms that implicitly perform model order reduction. By observing that the ADI methods for Lyapunov, Sylvester, and Riccati equations differ only in pole placement and not in their interpolatory nature, we show that the shifted linear solves-which constitute the bulk of the computational cost-can be shared. The pole-placement step involves only small-scale operations and is therefore inexpensive. We propose a unified ADI framework that requires only two shifted linear solves per iteration to simultaneously solve six Lyapunov equations, one Sylvester equation, and ten Riccati equations, thus substantially increasing the return on investment for the computational cost spent on the linear solves. All operations needed to extract the individual solutions from these shared linear solves are small-scale and inexpensive. Since all ADI methods implicitly perform model order reduction when solving these linear matrix equations, we show that the resulting reduced-order models can be obtained as an additional byproduct. These models not only interpolate the original transfer function at the mirror images of the ADI shifts but also preserve important system properties such as stability, minimum-phase property, positive-realness, bounded-realness, and passivity. Consequently, the proposed unified ADI framework also serves as a recursive, interpolation-based model order reduction method, which can preserve several important properties of the original model in the reduced-order model.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SY", "math.NA"], "url": "http://arxiv.org/abs/2512.04676", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: A Unified Low-rank ADI Framework with Shared Linear Solves for Simultaneously Solving Multiple Lyapunov, Sylvester, and Riccati Equations\nResearch Topic: computer vision\nAuthors: Umair Zulfiqar, Zhong-Yi Huang\nAbstract: It is known in the literature that the low-rank ADI method for Lyapunov equations is a Petrov-Galerkin projection algorithm that implicitly performs model order reduction. In this paper, we show that the low-rank ADI methods for Sylvester and Riccati equations are also Petrov-Galerkin projection algorithms that implicitly perform model order reduction. By observing that the ADI methods for Lyapunov, Sylvester, and Riccati equations differ only in pole placement and not in their interpolatory nature, we show that the shifted linear solves-which constitute the bulk of the computational cost-can be shared. The pole-placement step involves only small-scale operations and is therefore inexpensive. We propose a unified ADI framework that requires only two shifted linear solves per iteration to simultaneously solve six Lyapunov equations, one Sylvester equation, and ten Riccati equations, thus substantially increasing the return on investment for the computational cost spent on the linear solves. All operations needed to extract the individual solutions from these shared linear solves are small-scale and inexpensive.   Since all ADI methods implicitly perform model order reduction when solving these linear matrix equations, we show that the resulting reduced-order models can be obtained as an additional byproduct. These models not only interpolate the original transfer function at the mirror images of the ADI shifts but also preserve important system properties such as stability, minimum-phase property, positive-realness, bounded-realness, and passivity. Consequently, the proposed unified ADI framework also serves as a recursive, interpolation-based model order reduction method, which can preserve several important properties of the original model in the reduced-order model.\nCategories: eess.SY, math.NA\nPublication Date: 2025-12-04\nPaper ID: 2512.04676\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 137, "abstract_length": 1796, "total_text_length": 1934, "word_count": 271, "sentence_count": 10}, "basic_keywords": ["equations", "model", "order", "linear", "reduction", "only", "solves", "lyapunov"], "domain_keywords": [], "technical_terms": ["These", "Consequently", "Sylvester", "Petrov", "Riccati", "Since", "Lyapunov", "Galerkin"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04675", "topic": "computer vision", "title": "Cryptanalysis of Gleeok-128", "authors": ["Siwei Chen", "Peipei Xie", "Shengyuan Xu", "Xiutao Feng", "Zejun Xiang", "Xiangyong Zeng"], "first_author": "Siwei Chen", "abstract": "Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR"], "url": "http://arxiv.org/abs/2512.04675", "comments": "44 pages, 5 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Cryptanalysis of Gleeok-128\nResearch Topic: computer vision\nAuthors: Siwei Chen, Peipei Xie, Shengyuan Xu, Xiutao Feng, Zejun Xiang, Xiangyong Zeng\nAbstract: Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.\nCategories: cs.CR\nPublication Date: 2025-12-04\nPaper ID: 2512.04675\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 27, "abstract_length": 1892, "total_text_length": 1920, "word_count": 301, "sentence_count": 16}, "basic_keywords": ["branch", "gleeok", "full", "round", "power", "linear", "minus", "based"], "domain_keywords": [], "technical_terms": ["These", "Gleeok", "Both Gleeok", "Branch", "Owing"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04673", "topic": "computer vision", "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models", "authors": ["Gunjan Das", "Paheli Bhattacharya", "Rishabh Gupta"], "first_author": "Gunjan Das", "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.SE"], "url": "http://arxiv.org/abs/2512.04673", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.141973", "embedding_text": "Paper Title: Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models\nResearch Topic: computer vision\nAuthors: Gunjan Das, Paheli Bhattacharya, Rishabh Gupta\nAbstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.\nCategories: cs.SE\nPublication Date: 2025-12-04\nPaper ID: 2512.04673\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 97, "abstract_length": 1018, "total_text_length": 1116, "word_count": 137, "sentence_count": 8}, "basic_keywords": ["code", "reasoning", "language", "models", "llms", "general", "natural", "domain"], "domain_keywords": [], "technical_terms": ["However", "Additionally", "Large Language Models", "Llama", "Mistral"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04671", "topic": "computer vision", "title": "Evolutionary Dynamics Based on Reputation in Networked Populations with Game Transitions", "authors": ["Yuji Zhang", "Minyu Feng", "Jürgen Kurths", "Attila Szolnoki"], "first_author": "Yuji Zhang", "abstract": "The environment undergoes perpetual changes that are influenced by a combination of endogenous and exogenous factors. Consequently, it exerts a substantial influence on an individual's physical and psychological state, directly or indirectly affecting the evolutionary dynamics of a population described by a network, which in turn can also alter the environment. Furthermore, the evolution of strategies, shaped by reputation, can diverge due to variations in multiple factors. To explore the potential consequences of the mentioned situations, this paper studies how game and reputation dynamics alter the evolution of cooperation. Concretely, game transitions are determined by individuals' behaviors and external uncontrollable factors. The cooperation level of its neighbors reflects individuals' reputation, and further, a general fitness function regarding payoff and reputation is provided. Within the context of the donation game, we investigate the relevant outcomes associated with the aforementioned evolutionary process, considering various topologies for distinct interactions. Additionally, a biased mutation is introduced to gain a deeper insight into the strategy evolution. We detect a substantial increase in the cooperation level through intensive simulations, and some important phenomena are observed, e.g., the unilateral increase of the value of prosocial behavior limits promotion in cooperative behavior in square-lattice networks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.soc-ph", "cs.SI"], "url": "http://arxiv.org/abs/2512.04671", "comments": "13 pages", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Evolutionary Dynamics Based on Reputation in Networked Populations with Game Transitions\nResearch Topic: computer vision\nAuthors: Yuji Zhang, Minyu Feng, Jürgen Kurths, Attila Szolnoki\nAbstract: The environment undergoes perpetual changes that are influenced by a combination of endogenous and exogenous factors. Consequently, it exerts a substantial influence on an individual's physical and psychological state, directly or indirectly affecting the evolutionary dynamics of a population described by a network, which in turn can also alter the environment. Furthermore, the evolution of strategies, shaped by reputation, can diverge due to variations in multiple factors. To explore the potential consequences of the mentioned situations, this paper studies how game and reputation dynamics alter the evolution of cooperation. Concretely, game transitions are determined by individuals' behaviors and external uncontrollable factors. The cooperation level of its neighbors reflects individuals' reputation, and further, a general fitness function regarding payoff and reputation is provided. Within the context of the donation game, we investigate the relevant outcomes associated with the aforementioned evolutionary process, considering various topologies for distinct interactions. Additionally, a biased mutation is introduced to gain a deeper insight into the strategy evolution. We detect a substantial increase in the cooperation level through intensive simulations, and some important phenomena are observed, e.g., the unilateral increase of the value of prosocial behavior limits promotion in cooperative behavior in square-lattice networks.\nCategories: physics.soc-ph, cs.SI\nPublication Date: 2025-12-04\nPaper ID: 2512.04671\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1457, "total_text_length": 1546, "word_count": 208, "sentence_count": 12}, "basic_keywords": ["reputation", "factors", "evolution", "game", "cooperation", "environment", "substantial", "evolutionary"], "domain_keywords": [], "technical_terms": ["Consequently", "Within", "Additionally", "Furthermore", "Concretely"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04662", "topic": "computer vision", "title": "Spectral micro-CT for quantitative analysis of calcification in fibrocartilage", "authors": ["Vittoria Mazzini", "Paolo Cardarelli", "Andrew L. Coathup", "Eleonora Olivotto", "Francesco Grassi", "Enrico Tassinari", "Simone Velardita", "Angelo Taibi", "Luca Brombal"], "first_author": "Vittoria Mazzini", "abstract": "This work introduces a quantitative method for assessing calcification in fibrocartilage using spectral micro-computed tomography (CT). Tissue samples of hip acetabular labrum from patients with osteoarthritis and femoroacetabular impingement were imaged with a laboratory-based spectral CT system equipped with a small-pixel photon-counting detector. The detector operated with two energy thresholds, allowing the simultaneous acquisition of two CT datasets at different X-ray energies. A material decomposition algorithm accounting for the system's spectral response was applied to separate calcium- and water-like components, yielding three-dimensional visualization and quantification of calcified regions within intact paraffin-embedded samples. Unlike the conventional method for calcification assessment based on histology, this spectral CT approach offers volumetric quantification of calcium structures without physical sectioning or staining. The method achieved a voxel size of 20 m for samples up to ~3 cm, with a calcium detection threshold of ~0.3 g/cm for structures down to 50 m. Quantification accuracy was estimated to be 5% by using a calibration phantom. Further comparison with histology demonstrated the correct localization of calcium spatial distributions and a match in the calcium crystal deposition score by providing non-destructive, quantitative 3D calcium maps of preserved tissue samples. This technique complements histology and could enhance the characterization of pathological fibrocartilage calcification in hip joint disorders.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["physics.med-ph"], "url": "http://arxiv.org/abs/2512.04662", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Spectral micro-CT for quantitative analysis of calcification in fibrocartilage\nResearch Topic: computer vision\nAuthors: Vittoria Mazzini, Paolo Cardarelli, Andrew L. Coathup, Eleonora Olivotto, Francesco Grassi, Enrico Tassinari, Simone Velardita, Angelo Taibi, Luca Brombal\nAbstract: This work introduces a quantitative method for assessing calcification in fibrocartilage using spectral micro-computed tomography ($μ$CT). Tissue samples of hip acetabular labrum from patients with osteoarthritis and femoroacetabular impingement were imaged with a laboratory-based spectral $μ$CT system equipped with a small-pixel photon-counting detector. The detector operated with two energy thresholds, allowing the simultaneous acquisition of two CT datasets at different X-ray energies. A material decomposition algorithm accounting for the system's spectral response was applied to separate calcium- and water-like components, yielding three-dimensional visualization and quantification of calcified regions within intact paraffin-embedded samples. Unlike the conventional method for calcification assessment based on histology, this spectral μCT approach offers volumetric quantification of calcium structures without physical sectioning or staining. The method achieved a voxel size of 20 $μ$m for samples up to ~3 cm, with a calcium detection threshold of ~0.3 g/cm$^3$ for structures down to 50 $μ$m. Quantification accuracy was estimated to be 5% by using a calibration phantom. Further comparison with histology demonstrated the correct localization of calcium spatial distributions and a match in the calcium crystal deposition score by providing non-destructive, quantitative 3D calcium maps of preserved tissue samples. This technique complements histology and could enhance the characterization of pathological fibrocartilage calcification in hip joint disorders.\nCategories: physics.med-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04662\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 78, "abstract_length": 1564, "total_text_length": 1643, "word_count": 216, "sentence_count": 11}, "basic_keywords": ["calcium", "spectral", "samples", "method", "calcification", "quantification", "histology", "quantitative"], "domain_keywords": [], "technical_terms": ["Quantification", "Tissue", "Further", "Unlike"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04661", "topic": "computer vision", "title": "Velocity Dispersion Functions of Pressure-supported Galaxies in EAGLE Simulations with Varying AGN Feedback", "authors": ["Jungwon Choi", "Jubee Sohn"], "first_author": "Jungwon Choi", "abstract": "We investigate the stellar velocity dispersion functions (VDFs) of pressure-supported galaxies in the EAGLE cosmological simulations. The central stellar velocity dispersion is one of the fundamental dynamical tracers of the total mass of galaxy subhalos, alongside luminosity and stellar mass. Because it reflects the gravitational potential, the stellar velocity dispersion is expected to be relatively insensitive to feedback from Active Galactic Nuclei (AGN), a critical process that regulates the connection between other galaxy observables and subhalo mass. To examine the impact of AGN feedback, we analyze the VDFs from five EAGLE simulation runs, each adopting a different AGN feedback model: one \"standard\", two \"enhanced\", one \"reduced\", and one with no AGN feedback. We compute the stellar velocity dispersions of pressure-supported galaxies using member stellar particles, mimicking fiber spectroscopy. The VDFs from the standard and enhanced AGN feedback models show little difference. However, contrary to our initial expectation that the VDF shape would be largely insensitive to AGN feedback, the simulations with reduced and no AGN feedback show a significant excess of high velocity dispersion galaxies ( > 200 km s) and a deficit of low dispersion galaxies (100 < (km s) < 200), compared to those with standard or enhanced AGN feedback. The presence of high velocity dispersion galaxies in the no-AGN model arises from enhanced central star formation, due to the absence of AGN-driven gas heating or expulsion. Our results demonstrate that the shape of the theoretical VDF is sensitive to the strength of AGN feedback. These predictions offer a theoretical benchmark for future observational studies of the galaxy velocity dispersion function using large-scale spectroscopic surveys.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["astro-ph.GA"], "url": "http://arxiv.org/abs/2512.04661", "comments": "13 pages, 13 figures, A&A accepted", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Velocity Dispersion Functions of Pressure-supported Galaxies in EAGLE Simulations with Varying AGN Feedback\nResearch Topic: computer vision\nAuthors: Jungwon Choi, Jubee Sohn\nAbstract: We investigate the stellar velocity dispersion functions (VDFs) of pressure-supported galaxies in the EAGLE cosmological simulations. The central stellar velocity dispersion is one of the fundamental dynamical tracers of the total mass of galaxy subhalos, alongside luminosity and stellar mass. Because it reflects the gravitational potential, the stellar velocity dispersion is expected to be relatively insensitive to feedback from Active Galactic Nuclei (AGN), a critical process that regulates the connection between other galaxy observables and subhalo mass. To examine the impact of AGN feedback, we analyze the VDFs from five EAGLE simulation runs, each adopting a different AGN feedback model: one \"standard\", two \"enhanced\", one \"reduced\", and one with no AGN feedback. We compute the stellar velocity dispersions of pressure-supported galaxies using member stellar particles, mimicking fiber spectroscopy. The VDFs from the standard and enhanced AGN feedback models show little difference. However, contrary to our initial expectation that the VDF shape would be largely insensitive to AGN feedback, the simulations with reduced and no AGN feedback show a significant excess of high velocity dispersion galaxies ($σ_{*}$ > 200 km s$^{-1}$) and a deficit of low dispersion galaxies (100 < $σ_{*}$ (km s$^{-1}$) < 200), compared to those with standard or enhanced AGN feedback. The presence of high velocity dispersion galaxies in the no-AGN model arises from enhanced central star formation, due to the absence of AGN-driven gas heating or expulsion. Our results demonstrate that the shape of the theoretical VDF is sensitive to the strength of AGN feedback. These predictions offer a theoretical benchmark for future observational studies of the galaxy velocity dispersion function using large-scale spectroscopic surveys.\nCategories: astro-ph.GA\nPublication Date: 2025-12-04\nPaper ID: 2512.04661\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 107, "abstract_length": 1803, "total_text_length": 1911, "word_count": 280, "sentence_count": 11}, "basic_keywords": ["feedback", "velocity", "dispersion", "stellar", "galaxies", "enhanced", "vdfs", "mass"], "domain_keywords": [], "technical_terms": ["These", "However", "Because", "Active Galactic Nuclei"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.6}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04660", "topic": "computer vision", "title": "I2I-Bench: A Comprehensive Benchmark Suite for Image-to-Image Editing Models", "authors": ["Juntong Wang", "Jiarui Wang", "Huiyu Duan", "Jiaxiang Kang", "Guangtao Zhai", "Xiongkuo Min"], "first_author": "Juntong Wang", "abstract": "Image editing models are advancing rapidly, yet comprehensive evaluation remains a significant challenge. Existing image editing benchmarks generally suffer from limited task scopes, insufficient evaluation dimensions, and heavy reliance on manual annotations, which significantly constrain their scalability and practical applicability. To address this, we propose , a comprehensive benchmark for image-to-image editing models, which features (i) diverse tasks, encompassing 10 task categories across both single-image and multi-image editing tasks, (ii) comprehensive evaluation dimensions, including 30 decoupled and fine-grained evaluation dimensions with automated hybrid evaluation methods that integrate specialized tools and large multimodal models (LMMs), and (iii) rigorous alignment validation, justifying the consistency between our benchmark evaluations and human preferences. Using I2I-Bench, we benchmark numerous mainstream image editing models, investigating the gaps and trade-offs between editing models across various dimensions. We will open-source all components of I2I-Bench to facilitate future research.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04660", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: I2I-Bench: A Comprehensive Benchmark Suite for Image-to-Image Editing Models\nResearch Topic: computer vision\nAuthors: Juntong Wang, Jiarui Wang, Huiyu Duan, Jiaxiang Kang, Guangtao Zhai, Xiongkuo Min\nAbstract: Image editing models are advancing rapidly, yet comprehensive evaluation remains a significant challenge. Existing image editing benchmarks generally suffer from limited task scopes, insufficient evaluation dimensions, and heavy reliance on manual annotations, which significantly constrain their scalability and practical applicability. To address this, we propose \\textbf{I2I-Bench}, a comprehensive benchmark for image-to-image editing models, which features (i) diverse tasks, encompassing 10 task categories across both single-image and multi-image editing tasks, (ii) comprehensive evaluation dimensions, including 30 decoupled and fine-grained evaluation dimensions with automated hybrid evaluation methods that integrate specialized tools and large multimodal models (LMMs), and (iii) rigorous alignment validation, justifying the consistency between our benchmark evaluations and human preferences. Using I2I-Bench, we benchmark numerous mainstream image editing models, investigating the gaps and trade-offs between editing models across various dimensions. We will open-source all components of I2I-Bench to facilitate future research.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04660\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 76, "abstract_length": 1128, "total_text_length": 1205, "word_count": 149, "sentence_count": 6}, "basic_keywords": ["image", "editing", "models", "evaluation", "dimensions", "comprehensive", "benchmark", "task"], "domain_keywords": [], "technical_terms": ["Using", "Image", "Bench", "Existing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04652", "topic": "computer vision", "title": "Quantised Academic Mobility: Network and Cluster Analysis of Degree Switching, Plan Changes, and Re-entries in an Engineering Faculty (1980-2019)", "authors": ["H. R. Paz"], "first_author": "H. R. Paz", "abstract": "This study challenges the traditional binary view of student progression (retention versus dropout) by conceptualising academic trajectories as complex, quantised pathways. Utilising a 40-year longitudinal dataset from an Argentine engineering faculty (N = 24,016), we introduce CAPIRE, an analytical framework that differentiates between degree major switches, curriculum plan changes, and same-plan re-entries. While 73.3 per cent of students follow linear trajectories (Estables), a significant 26.7 per cent exhibit complex mobility patterns. By applying Principal Component Analysis (PCA) and DBSCAN clustering, we reveal that these trajectories are not continuous but structurally quantised, occupying discrete bands of complexity. The analysis identifies six distinct student archetypes, including 'Switchers' (10.7 per cent) who reorient vocationally, and 'Stable Re-entrants' (6.9 per cent) who exhibit stop-out behaviours without changing discipline. Furthermore, network analysis highlights specific 'hub majors' - such as electronics and computing - that act as systemic attractors. These findings suggest that student flux is an organised ecosystemic feature rather than random noise, offering institutions a new lens for curriculum analytics and predictive modelling.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CY"], "url": "http://arxiv.org/abs/2512.04652", "comments": "23 pages, 4 tables , 5 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Quantised Academic Mobility: Network and Cluster Analysis of Degree Switching, Plan Changes, and Re-entries in an Engineering Faculty (1980-2019)\nResearch Topic: computer vision\nAuthors: H. R. Paz\nAbstract: This study challenges the traditional binary view of student progression (retention versus dropout) by conceptualising academic trajectories as complex, quantised pathways. Utilising a 40-year longitudinal dataset from an Argentine engineering faculty (N = 24,016), we introduce CAPIRE, an analytical framework that differentiates between degree major switches, curriculum plan changes, and same-plan re-entries. While 73.3 per cent of students follow linear trajectories (Estables), a significant 26.7 per cent exhibit complex mobility patterns. By applying Principal Component Analysis (PCA) and DBSCAN clustering, we reveal that these trajectories are not continuous but structurally quantised, occupying discrete bands of complexity. The analysis identifies six distinct student archetypes, including 'Switchers' (10.7 per cent) who reorient vocationally, and 'Stable Re-entrants' (6.9 per cent) who exhibit stop-out behaviours without changing discipline. Furthermore, network analysis highlights specific 'hub majors' - such as electronics and computing - that act as systemic attractors. These findings suggest that student flux is an organised ecosystemic feature rather than random noise, offering institutions a new lens for curriculum analytics and predictive modelling.\nCategories: cs.CY\nPublication Date: 2025-12-04\nPaper ID: 2512.04652\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 145, "abstract_length": 1281, "total_text_length": 1427, "word_count": 188, "sentence_count": 12}, "basic_keywords": ["cent", "student", "trajectories", "analysis", "complex", "quantised", "curriculum", "plan"], "domain_keywords": [], "technical_terms": ["These", "While", "Stable Re", "Argentine", "Furthermore", "Utilising", "Principal Component Analysis", "Switchers"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.875, "quality_tier": "high"}
{"paper_id": "2512.04649", "topic": "computer vision", "title": "Probing chiral topological states with permutation defects", "authors": ["Yarden Sheffer", "Ruihua Fan", "Ady Stern", "Erez Berg", "Shinsei Ryu"], "first_author": "Yarden Sheffer", "abstract": "The hallmark of two-dimensional chiral topological phases is the existence of anomalous gapless modes at the spatial boundary. Yet, the manifestation of this edge anomaly within the bulk ground-state wavefunction itself remains only partially understood. In this work, we introduce a family of multipartite entanglement measures that probe chirality directly from the bulk wavefunction. Our construction involves applying different permutations between replicas of the ground state wavefunction in neighboring spatial regions, creating \"permutation defects\" at the boundaries between these regions. We provide general arguments for the robustness of these measures and develop a field-theoretical framework to compute them systematically. While the standard topological field theory prescription misses the chiral contribution, our method correctly identifies it as the chiral conformal field theory partition function on high-genus Riemann surfaces. This feature is a consequence of the bulk-edge correspondence, which dictates that any regularization of the theory at the permutation defects must introduce gapless boundary modes. We numerically verify our results with both free-fermion and strongly-interacting chiral topological states and find excellent agreement. Our results enable the extraction of the chiral central charge and the Hall conductance using a finite number of wavefunction replicas, making these quantities accessible to Monte-Carlo numerical techniques and noisy intermediate-scale quantum devices.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cond-mat.str-el", "hep-th"], "url": "http://arxiv.org/abs/2512.04649", "comments": "16+14 pages, 20 figures, comments welcome!", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Probing chiral topological states with permutation defects\nResearch Topic: computer vision\nAuthors: Yarden Sheffer, Ruihua Fan, Ady Stern, Erez Berg, Shinsei Ryu\nAbstract: The hallmark of two-dimensional chiral topological phases is the existence of anomalous gapless modes at the spatial boundary. Yet, the manifestation of this edge anomaly within the bulk ground-state wavefunction itself remains only partially understood. In this work, we introduce a family of multipartite entanglement measures that probe chirality directly from the bulk wavefunction. Our construction involves applying different permutations between replicas of the ground state wavefunction in neighboring spatial regions, creating \"permutation defects\" at the boundaries between these regions. We provide general arguments for the robustness of these measures and develop a field-theoretical framework to compute them systematically. While the standard topological field theory prescription misses the chiral contribution, our method correctly identifies it as the chiral conformal field theory partition function on high-genus Riemann surfaces. This feature is a consequence of the bulk-edge correspondence, which dictates that any regularization of the theory at the permutation defects must introduce gapless boundary modes. We numerically verify our results with both free-fermion and strongly-interacting chiral topological states and find excellent agreement. Our results enable the extraction of the chiral central charge and the Hall conductance using a finite number of wavefunction replicas, making these quantities accessible to Monte-Carlo numerical techniques and noisy intermediate-scale quantum devices.\nCategories: quant-ph, cond-mat.str-el, hep-th\nPublication Date: 2025-12-04\nPaper ID: 2512.04649\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 58, "abstract_length": 1523, "total_text_length": 1582, "word_count": 211, "sentence_count": 10}, "basic_keywords": ["chiral", "wavefunction", "topological", "bulk", "field", "theory", "gapless", "modes"], "domain_keywords": [], "technical_terms": ["Hall", "While", "Riemann", "Carlo", "Monte"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04642", "topic": "computer vision", "title": "Limit cycles for speech", "authors": ["Adamantios I. Gafos", "Stephan R. Kuberski"], "first_author": "Adamantios I. Gafos", "abstract": "Rhythmic fluctuations in acoustic energy and accompanying neuronal excitations in cortical oscillations are characteristic of human speech, yet whether a corresponding rhythmicity inheres in the articulatory movements that generate speech remains unclear. The received understanding of speech movements as discrete, goal-oriented actions struggles to make contact with the rhythmicity findings. In this work, we demonstrate that an unintuitive -- but no less principled than the conventional -- representation for discrete movements reveals a pervasive limit cycle organization and unlocks the recovery of previously inaccessible rhythmic structure underlying the motor activity of speech. These results help resolve a time-honored tension between the ubiquity of biological rhythmicity and discreteness in speech, the quintessential human higher function, by revealing a rhythmic organization at the most fundamental level of individual articulatory actions.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["q-bio.NC", "cs.CL"], "url": "http://arxiv.org/abs/2512.04642", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Limit cycles for speech\nResearch Topic: computer vision\nAuthors: Adamantios I. Gafos, Stephan R. Kuberski\nAbstract: Rhythmic fluctuations in acoustic energy and accompanying neuronal excitations in cortical oscillations are characteristic of human speech, yet whether a corresponding rhythmicity inheres in the articulatory movements that generate speech remains unclear. The received understanding of speech movements as discrete, goal-oriented actions struggles to make contact with the rhythmicity findings. In this work, we demonstrate that an unintuitive -- but no less principled than the conventional -- representation for discrete movements reveals a pervasive limit cycle organization and unlocks the recovery of previously inaccessible rhythmic structure underlying the motor activity of speech. These results help resolve a time-honored tension between the ubiquity of biological rhythmicity and discreteness in speech, the quintessential human higher function, by revealing a rhythmic organization at the most fundamental level of individual articulatory actions.\nCategories: q-bio.NC, cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04642\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 23, "abstract_length": 959, "total_text_length": 983, "word_count": 132, "sentence_count": 5}, "basic_keywords": ["speech", "rhythmic", "rhythmicity", "movements", "human", "articulatory", "discrete", "actions"], "domain_keywords": [], "technical_terms": ["Rhythmic", "These"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04630", "topic": "computer vision", "title": "Reflection-Satisfaction Tradeoff: Investigating Impact of Reflection on Student Engagement with AI-Generated Programming Hints", "authors": ["Heeryung Choi", "Tung Phung", "Mengyan Wu", "Adish Singla", "Christopher Brooks"], "first_author": "Heeryung Choi", "abstract": "Generative AI tools, such as AI-generated hints, are increasingly integrated into programming education to offer timely, personalized support. However, little is known about how to effectively leverage these hints while ensuring autonomous and meaningful learning. One promising approach involves pairing AI-generated hints with reflection prompts, asking students to review and analyze their learning, when they request hints. This study investigates the interplay between AI-generated hints and different designs of reflection prompts in an online introductory programming course. We conducted a two-trial field experiment. In Trial 1, students were randomly assigned to receive prompts either before or after receiving hints, or no prompt at all. Each prompt also targeted one of three SRL phases: planning, monitoring, and evaluation. In Trial 2, we examined two types of prompt guidance: directed (offering more explicit and structured guidance) and open (offering more general and less constrained guidance). Findings show that students in the before-hint (RQ1), planning (RQ2), and directed (RQ3) prompt groups produced higher-quality reflections but reported lower satisfaction with AI-generated hints than those in other conditions. Immediate performance did not differ across conditions. This negative relationship between reflection quality and hint satisfaction aligns with previous work on student mental effort and satisfaction. Our results highlight the need to reconsider how AI models are trained and evaluated for education, as prioritizing user satisfaction can undermine deeper learning.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CY"], "url": "http://arxiv.org/abs/2512.04630", "comments": "Preprint", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Reflection-Satisfaction Tradeoff: Investigating Impact of Reflection on Student Engagement with AI-Generated Programming Hints\nResearch Topic: computer vision\nAuthors: Heeryung Choi, Tung Phung, Mengyan Wu, Adish Singla, Christopher Brooks\nAbstract: Generative AI tools, such as AI-generated hints, are increasingly integrated into programming education to offer timely, personalized support. However, little is known about how to effectively leverage these hints while ensuring autonomous and meaningful learning. One promising approach involves pairing AI-generated hints with reflection prompts, asking students to review and analyze their learning, when they request hints. This study investigates the interplay between AI-generated hints and different designs of reflection prompts in an online introductory programming course. We conducted a two-trial field experiment. In Trial 1, students were randomly assigned to receive prompts either before or after receiving hints, or no prompt at all. Each prompt also targeted one of three SRL phases: planning, monitoring, and evaluation. In Trial 2, we examined two types of prompt guidance: directed (offering more explicit and structured guidance) and open (offering more general and less constrained guidance). Findings show that students in the before-hint (RQ1), planning (RQ2), and directed (RQ3) prompt groups produced higher-quality reflections but reported lower satisfaction with AI-generated hints than those in other conditions. Immediate performance did not differ across conditions. This negative relationship between reflection quality and hint satisfaction aligns with previous work on student mental effort and satisfaction. Our results highlight the need to reconsider how AI models are trained and evaluated for education, as prioritizing user satisfaction can undermine deeper learning.\nCategories: cs.CY\nPublication Date: 2025-12-04\nPaper ID: 2512.04630\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 126, "abstract_length": 1607, "total_text_length": 1734, "word_count": 236, "sentence_count": 13}, "basic_keywords": ["hints", "generated", "prompt", "satisfaction", "learning", "reflection", "prompts", "students"], "domain_keywords": [], "technical_terms": ["However", "Findings", "In Trial", "Immediate", "Each", "Generative"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04622", "topic": "computer vision", "title": "Temporal and Spatial Decomposition for Prospective Studies in Energy Systems under Uncertainty", "authors": ["Camila Martinez Parra", "Michel de Lara", "Jean-Philippe Chancelier", "Pierre Carpentier", "Jean-Marc Janin"], "first_author": "Camila Martinez Parra", "abstract": "The increasing penetration of renewable energy requires greater use of storage resources to manage system intermittency. As a result, there is growing interest in evaluating the opportunity cost of stored energy, or usage values, which can be derived by solving a multistage stochastic optimization problem. Stochasticity arises from net demand (the aggregation of demand and non-dispatchable generation), the availability of dispatchable generation, and inflows when the storage facilities considered are hydroelectric dams. We aim to compute these usage values for each market zone of the interconnected European electricity system, in the context of prospective studies currently conducted by RTE, the French TSO. The energy system is mathematically modelled as a directed graph, where nodes represent market zones and arcs represent interconnection links. In large energy systems, spatial complexity (thirty nodes in the system, each with at most one aggregated storage unit) compounds temporal complexity (a one-year horizon modelled with two timescales: weekly subproblems with hourly time steps). This work addresses three main sources of complexity: temporal, spatial, and stochastic. We tackle the multinode multistage stochastic optimisation problem by incorporating a spatio-temporal decomposition scheme. To efficiently compute usage values, we apply Dual Approximate Dynamic Programming (DADP), which enables tractable decomposition across both time and space. This approach yields nodal usage values that depend solely on the local state of each node, independently of the others. We conduct numerical studies on a realistic system composed of thirty nodes (modelling part of Europe) and show that DADP obtains competitive results when comparing with traditional methods like Stochastic Dual Dynamic Programming (SDDP).", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.OC"], "url": "http://arxiv.org/abs/2512.04622", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Temporal and Spatial Decomposition for Prospective Studies in Energy Systems under Uncertainty\nResearch Topic: computer vision\nAuthors: Camila Martinez Parra, Michel de Lara, Jean-Philippe Chancelier, Pierre Carpentier, Jean-Marc Janin\nAbstract: The increasing penetration of renewable energy requires greater use of storage resources to manage system intermittency. As a result, there is growing interest in evaluating the opportunity cost of stored energy, or usage values, which can be derived by solving a multistage stochastic optimization problem. Stochasticity arises from net demand (the aggregation of demand and non-dispatchable generation), the availability of dispatchable generation, and inflows when the storage facilities considered are hydroelectric dams. We aim to compute these usage values for each market zone of the interconnected European electricity system, in the context of prospective studies currently conducted by RTE, the French TSO. The energy system is mathematically modelled as a directed graph, where nodes represent market zones and arcs represent interconnection links. In large energy systems, spatial complexity (thirty nodes in the system, each with at most one aggregated storage unit) compounds temporal complexity (a one-year horizon modelled with two timescales: weekly subproblems with hourly time steps). This work addresses three main sources of complexity: temporal, spatial, and stochastic. We tackle the multinode multistage stochastic optimisation problem by incorporating a spatio-temporal decomposition scheme. To efficiently compute usage values, we apply Dual Approximate Dynamic Programming (DADP), which enables tractable decomposition across both time and space. This approach yields nodal usage values that depend solely on the local state of each node, independently of the others. We conduct numerical studies on a realistic system composed of thirty nodes (modelling part of Europe) and show that DADP obtains competitive results when comparing with traditional methods like Stochastic Dual Dynamic Programming (SDDP).\nCategories: math.OC\nPublication Date: 2025-12-04\nPaper ID: 2512.04622\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 94, "abstract_length": 1833, "total_text_length": 1928, "word_count": 270, "sentence_count": 12}, "basic_keywords": ["system", "energy", "usage", "values", "stochastic", "storage", "each", "nodes"], "domain_keywords": [], "technical_terms": ["Stochastic Dual Dynamic Programming", "Dual Approximate Dynamic Programming", "Stochasticity", "European", "French", "Europe"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04620", "topic": "computer vision", "title": "Metric dimension of Cartesian product of stars", "authors": ["Akbar Davoodi", "Mohsen Jannesari"], "first_author": "Akbar Davoodi", "abstract": "The metric dimension of a graph is the minimum number of landmark vertices required so that every vertex can be uniquely identified by its distances to the landmarks. This parameter captures the fundamental tradeoff between compact information encoding and unambiguous identification in networked systems. In this work, we determine exact value for the metric dimension of the Cartesian product , also known as hub-and-spoke grids, across all values of and . In addition, we present a constructive linear-time algorithm that builds a minimum resolving set, providing both theoretical guarantees and practical feasibility. We complement our results with visualization of parameter regimes that illustrate the design space. The findings establish design rules for minimizing landmark sensors and support applications in graph-based localization, monitoring networks, and intelligent information systems. Our results extend the theory of metric dimension and contribute efficient methods of direct relevance to information science and computational graph theory.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.CO"], "url": "http://arxiv.org/abs/2512.04620", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Metric dimension of Cartesian product of stars\nResearch Topic: computer vision\nAuthors: Akbar Davoodi, Mohsen Jannesari\nAbstract: The metric dimension of a graph is the minimum number of landmark vertices required so that every vertex can be uniquely identified by its distances to the landmarks. This parameter captures the fundamental tradeoff between compact information encoding and unambiguous identification in networked systems. In this work, we determine exact value for the metric dimension of the Cartesian product $K_{1,m} \\square K_{1,n}$, also known as hub-and-spoke grids, across all values of $m$ and $n$. In addition, we present a constructive linear-time algorithm that builds a minimum resolving set, providing both theoretical guarantees and practical feasibility. We complement our results with visualization of parameter regimes that illustrate the design space. The findings establish design rules for minimizing landmark sensors and support applications in graph-based localization, monitoring networks, and intelligent information systems. Our results extend the theory of metric dimension and contribute efficient methods of direct relevance to information science and computational graph theory.\nCategories: math.CO\nPublication Date: 2025-12-04\nPaper ID: 2512.04620\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 46, "abstract_length": 1059, "total_text_length": 1106, "word_count": 156, "sentence_count": 8}, "basic_keywords": ["metric", "dimension", "graph", "information", "minimum", "landmark", "parameter", "systems"], "domain_keywords": [], "technical_terms": ["Cartesian"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04619", "topic": "computer vision", "title": "Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence", "authors": ["Tianyu Yuan", "Yuanbo Yang", "Lin-Zhuo Chen", "Yao Yao", "Zhuzhong Qian"], "first_author": "Tianyu Yuan", "abstract": "In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04619", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence\nResearch Topic: computer vision\nAuthors: Tianyu Yuan, Yuanbo Yang, Lin-Zhuo Chen, Yao Yao, Zhuzhong Qian\nAbstract: In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04619\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 77, "abstract_length": 1514, "total_text_length": 1592, "word_count": 202, "sentence_count": 9}, "basic_keywords": ["frequency", "models", "head", "tracking", "video", "diffusion", "components", "work"], "domain_keywords": [], "technical_terms": ["Extensive", "Specifically", "Additionally", "Head", "Video Diffusion Transformer", "Frequency Tracker", "Building"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04615", "topic": "computer vision", "title": "Ground state energy and phase transitions of Long-range XXZ using VQE", "authors": ["Mrinal Dev", "Shraddha Sharma"], "first_author": "Mrinal Dev", "abstract": "The variational quantum eigen solver (VQE), has been widely used to find the ground state energy of different Hamiltonians with no analytical solutions and are classically difficult to compute. In our work, we have used VQE to identify the phase transition boundary for an infinite order phase transition. We use long-range XXZ (LRXXZ) chain for our study. In order to probe infinite order phase transition, we propose to utilise the ground state energy obtained from VQE. The idea rests on the argument that VQE requires an ansatz circuit; therefore, the accuracy of the VQE will rely on this ansatz circuit. We have designed this circuit such that the estimated ground state energy is sensitive to the phase it is evaluated in. It is achieved by applying the constraint that the net spin remains constant throughout the optimisation process. Consequently, the ansatz works in a certain phase where it gives relatively small random error, as it should, when compared to the error in ground state energy calculations of the other phases, where the ansatz fails. By identifying these changes in the behaviour of the error in ground state energy using VQE, we were able to determine the phase boundaries. Using exact diagonalisation, we also compare the behaviour of the energy gradient and energy gap across both the phase transition boundaries for this model. Further, by increasing the depth of the optimisation circuit, we also accurately evaluate the ground energy of the LRXXZ chain for the value of coupling constant, J equal to -1", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "cond-mat.str-el"], "url": "http://arxiv.org/abs/2512.04615", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Ground state energy and phase transitions of Long-range XXZ using VQE\nResearch Topic: computer vision\nAuthors: Mrinal Dev, Shraddha Sharma\nAbstract: The variational quantum eigen solver (VQE), has been widely used to find the ground state energy of different Hamiltonians with no analytical solutions and are classically difficult to compute. In our work, we have used VQE to identify the phase transition boundary for an infinite order phase transition. We use long-range XXZ (LRXXZ) chain for our study. In order to probe infinite order phase transition, we propose to utilise the ground state energy obtained from VQE. The idea rests on the argument that VQE requires an ansatz circuit; therefore, the accuracy of the VQE will rely on this ansatz circuit. We have designed this circuit such that the estimated ground state energy is sensitive to the phase it is evaluated in. It is achieved by applying the constraint that the net spin remains constant throughout the optimisation process. Consequently, the ansatz works in a certain phase where it gives relatively small random error, as it should, when compared to the error in ground state energy calculations of the other phases, where the ansatz fails. By identifying these changes in the behaviour of the error in ground state energy using VQE, we were able to determine the phase boundaries. Using exact diagonalisation, we also compare the behaviour of the energy gradient and energy gap across both the phase transition boundaries for this model. Further, by increasing the depth of the optimisation circuit, we also accurately evaluate the ground energy of the LRXXZ chain for the value of coupling constant, J equal to -1\nCategories: quant-ph, cond-mat.str-el\nPublication Date: 2025-12-04\nPaper ID: 2512.04615\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 69, "abstract_length": 1536, "total_text_length": 1606, "word_count": 263, "sentence_count": 11}, "basic_keywords": ["energy", "phase", "ground", "state", "transition", "ansatz", "circuit", "order"], "domain_keywords": [], "technical_terms": ["Hamiltonians", "Consequently", "Using", "Further"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04614", "topic": "computer vision", "title": "On Tight FPT Time Approximation Algorithms for k-Clustering Problems", "authors": ["Han Dai", "Shi Li", "Sijin Peng"], "first_author": "Han Dai", "abstract": "Following recent advances in combining approximation algorithms with fixed-parameter tractability (FPT), we study FPT-time approximation algorithms for minimum-norm -clustering problems, parameterized by the number of open facilities. For the capacitated setting, we give a tight -approximation for the general-norm capacitated -clustering problem in FPT-time parameterized by and . Prior to our work, such a result was only known for the capacitated -median problem [CL, ICALP, 2019]. As a special case, our result yields an FPT-time -approximation for capacitated -center. The problem has not been studied in the FPT-time setting, with the previous best known polynomial-time approximation ratio being 9 [ABCG, MP, 2015]. In the uncapacitated setting, we consider the - norm -clustering problem, where the goal of the problem is to minimize the - norm of the connection distance vector. Our main result is a tight -approximation algorithm for the problem with . (For the case , there is a simple tight -approximation.) Our framework can be easily extended to give a tight -bicriteria approximation for the (-center, -median) problem in FPT time, improving the previous best polynomial-time guarantee [AB, WAOA, 2017]. All results are based on a unified framework: computing a -approximate solution using facilities via LP rounding, sampling a few client representatives based on the solution , guessing a few pivots from and some radius information on the pivots, and solving the problem using the guesses. We believe this framework can lead to further results on -clustering problems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.DS"], "url": "http://arxiv.org/abs/2512.04614", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: On Tight FPT Time Approximation Algorithms for k-Clustering Problems\nResearch Topic: computer vision\nAuthors: Han Dai, Shi Li, Sijin Peng\nAbstract: Following recent advances in combining approximation algorithms with fixed-parameter tractability (FPT), we study FPT-time approximation algorithms for minimum-norm $k$-clustering problems, parameterized by the number $k$ of open facilities.   For the capacitated setting, we give a tight $(3+ε)$-approximation for the general-norm capacitated $k$-clustering problem in FPT-time parameterized by $k$ and $ε$. Prior to our work, such a result was only known for the capacitated $k$-median problem [CL, ICALP, 2019]. As a special case, our result yields an FPT-time $3$-approximation for capacitated $k$-center. The problem has not been studied in the FPT-time setting, with the previous best known polynomial-time approximation ratio being 9 [ABCG, MP, 2015].   In the uncapacitated setting, we consider the $top$-$cn$ norm $k$-clustering problem, where the goal of the problem is to minimize the $top$-$cn$ norm of the connection distance vector. Our main result is a tight $\\big(1 + \\frac 2{ec} + ε\\big)$-approximation algorithm for the problem with $c \\in \\big(\\frac1e, 1\\big]$. (For the case $c \\leq \\frac1e$, there is a simple tight $(3+ε)$-approximation.) Our framework can be easily extended to give a tight $\\left(3, 1+\\frac2e + ε\\right)$-bicriteria approximation for the ($k$-center, $k$-median) problem in FPT time, improving the previous best polynomial-time $(4, 8)$ guarantee [AB, WAOA, 2017].   All results are based on a unified framework: computing a $(1+ε)$-approximate solution using $O\\left(\\frac{k\\log n}ε\\right)$ facilities $S$ via LP rounding, sampling a few client representatives $R$ based on the solution $S$, guessing a few pivots from $S \\cup R$ and some radius information on the pivots, and solving the problem using the guesses. We believe this framework can lead to further results on $k$-clustering problems.\nCategories: cs.DS\nPublication Date: 2025-12-04\nPaper ID: 2512.04614\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 68, "abstract_length": 1587, "total_text_length": 1656, "word_count": 249, "sentence_count": 12}, "basic_keywords": ["approximation", "problem", "time", "norm", "clustering", "capacitated", "tight", "setting"], "domain_keywords": [], "technical_terms": ["Prior", "Following"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.30000000000000004}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04611", "topic": "computer vision", "title": "PBFuzz: Agentic Directed Fuzzing for PoV Generation", "authors": ["Haochen Zeng", "Andrew Bao", "Jiajun Cheng", "Chengyu Song"], "first_author": "Haochen Zeng", "abstract": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CR", "cs.SE"], "url": "http://arxiv.org/abs/2512.04611", "comments": "24 pages, 8 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: PBFuzz: Agentic Directed Fuzzing for PoV Generation\nResearch Topic: computer vision\nAuthors: Haochen Zeng, Andrew Bao, Jiajun Cheng, Chengyu Song\nAbstract: Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.\nCategories: cs.CR, cs.SE\nPublication Date: 2025-12-04\nPaper ID: 2512.04611\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 51, "abstract_length": 1671, "total_text_length": 1723, "word_count": 228, "sentence_count": 14}, "basic_keywords": ["constraints", "pbfuzz", "vulnerability", "input", "generation", "code", "triggering", "fuzzing"], "domain_keywords": [], "technical_terms": ["Experiments", "Human", "Existing", "Vulnerability", "Generating", "Median", "Magma", "Proof"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04610", "topic": "computer vision", "title": "Weakly-sparse and strongly flip-flat classes of graphs are uniformly almost-wide", "authors": ["Fatemeh Ghasemi", "Julien Grange", "Mamadou Moustapha Kanté", "Florent Madelaine"], "first_author": "Fatemeh Ghasemi", "abstract": "In this work we take a step towards characterising strongly flip-flat classes of graphs. Strong flip-flatness appears to be the analogue of uniform almost-wideness in the setting of dense classes of graphs. We prove that strongly flip-flat classes of graphs that are weakly sparse are indeed uniformly almost-wide.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.DM", "math.CO"], "url": "http://arxiv.org/abs/2512.04610", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Weakly-sparse and strongly flip-flat classes of graphs are uniformly almost-wide\nResearch Topic: computer vision\nAuthors: Fatemeh Ghasemi, Julien Grange, Mamadou Moustapha Kanté, Florent Madelaine\nAbstract: In this work we take a step towards characterising strongly flip-flat classes of graphs. Strong flip-flatness appears to be the analogue of uniform almost-wideness in the setting of dense classes of graphs. We prove that strongly flip-flat classes of graphs that are weakly sparse are indeed uniformly almost-wide.\nCategories: cs.DM, math.CO\nPublication Date: 2025-12-04\nPaper ID: 2512.04610\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 314, "total_text_length": 395, "word_count": 58, "sentence_count": 4}, "basic_keywords": ["flip", "classes", "graphs", "strongly", "flat", "almost", "work", "take"], "domain_keywords": [], "technical_terms": ["Strong"], "paper_type": "research", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.1}, "overall_quality_score": 0.75, "quality_tier": "medium"}
{"paper_id": "2512.04599", "topic": "computer vision", "title": "Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot", "authors": ["Sheng Hang", "Chaoxiang He", "Hongsheng Hu", "Hanqing Hu", "Bin Benjamin Zhu", "Shi-Feng Sun", "Dawu Gu", "Shuo Wang"], "first_author": "Sheng Hang", "abstract": "Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04599", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot\nResearch Topic: computer vision\nAuthors: Sheng Hang, Chaoxiang He, Hongsheng Hu, Hanqing Hu, Bin Benjamin Zhu, Shi-Feng Sun, Dawu Gu, Shuo Wang\nAbstract: Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04599\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 110, "abstract_length": 1522, "total_text_length": 1633, "word_count": 225, "sentence_count": 13}, "basic_keywords": ["image", "content", "pipeline", "malicious", "against", "method", "recall", "precision"], "domain_keywords": [], "technical_terms": ["Against", "Each", "Evaluated", "Detecting"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04592", "topic": "computer vision", "title": "Stable self-adaptive timestepping for Reduced Order Models for incompressible flows", "authors": ["Josep Plana-Riu", "Henrik Rosenberger", "Benjamin Sanderse", "F. Xavier Trias"], "first_author": "Josep Plana-Riu", "abstract": "This work introduces RedEigCD, the first self-adaptive timestepping technique specifically tailored for reduced-order models (ROMs) of the incompressible Navier-Stokes equations. Building upon linear stability concepts, the method adapts the timestep by directly bounding the stability function of the employed time integration scheme using exact spectral information of matrices related to the reduced operators. Unlike traditional error-based adaptive methods, RedEigCD relies on the eigenbounds of the convective and diffusive ROM operators, whose computation is feasible at reduced scale and fully preserves the online efficiency of the ROM. A central theoretical contribution of this work is the proof, based on the combined theorems of Bendixson and Rao, that, under linearized assumptions, the maximum stable timestep for projection-based ROMs is shown to be larger than or equal to that of their corresponding full-order models (FOMs). Numerical experiments for both periodic and non-homogeneous boundary conditions demonstrate that RedEigCD yields stable timestep increases up to a factor 40 compared to the FOM, without compromising accuracy. The methodology thus establishes a new link between linear stability theory and reduced-order modeling, offering a systematic path towards efficient, self-regulating ROM integration in incompressible flow simulations.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.NA", "physics.comp-ph", "physics.flu-dyn"], "url": "http://arxiv.org/abs/2512.04592", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Stable self-adaptive timestepping for Reduced Order Models for incompressible flows\nResearch Topic: computer vision\nAuthors: Josep Plana-Riu, Henrik Rosenberger, Benjamin Sanderse, F. Xavier Trias\nAbstract: This work introduces RedEigCD, the first self-adaptive timestepping technique specifically tailored for reduced-order models (ROMs) of the incompressible Navier-Stokes equations. Building upon linear stability concepts, the method adapts the timestep by directly bounding the stability function of the employed time integration scheme using exact spectral information of matrices related to the reduced operators. Unlike traditional error-based adaptive methods, RedEigCD relies on the eigenbounds of the convective and diffusive ROM operators, whose computation is feasible at reduced scale and fully preserves the online efficiency of the ROM. A central theoretical contribution of this work is the proof, based on the combined theorems of Bendixson and Rao, that, under linearized assumptions, the maximum stable timestep for projection-based ROMs is shown to be larger than or equal to that of their corresponding full-order models (FOMs). Numerical experiments for both periodic and non-homogeneous boundary conditions demonstrate that RedEigCD yields stable timestep increases up to a factor 40 compared to the FOM, without compromising accuracy. The methodology thus establishes a new link between linear stability theory and reduced-order modeling, offering a systematic path towards efficient, self-regulating ROM integration in incompressible flow simulations.\nCategories: math.NA, physics.comp-ph, physics.flu-dyn\nPublication Date: 2025-12-04\nPaper ID: 2512.04592\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 83, "abstract_length": 1370, "total_text_length": 1454, "word_count": 197, "sentence_count": 7}, "basic_keywords": ["reduced", "redeigcd", "order", "stability", "timestep", "based", "work", "self"], "domain_keywords": [], "technical_terms": ["Numerical", "Bendixson", "Unlike", "Stokes", "Navier", "Building"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04588", "topic": "computer vision", "title": "UserSimCRS v2: Simulation-Based Evaluation for Conversational Recommender Systems", "authors": ["Nolwenn Bernard", "Krisztian Balog"], "first_author": "Nolwenn Bernard", "abstract": "Resources for simulation-based evaluation of conversational recommender systems (CRSs) are scarce. The UserSimCRS toolkit was introduced to address this gap. In this work, we present UserSimCRS v2, a significant upgrade aligning the toolkit with state-of-the-art research. Key extensions include an enhanced agenda-based user simulator, introduction of large language model-based simulators, integration for a wider range of CRSs and datasets, and new LLM-as-a-judge evaluation utilities. We demonstrate these extensions in a case study.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.IR"], "url": "http://arxiv.org/abs/2512.04588", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: UserSimCRS v2: Simulation-Based Evaluation for Conversational Recommender Systems\nResearch Topic: computer vision\nAuthors: Nolwenn Bernard, Krisztian Balog\nAbstract: Resources for simulation-based evaluation of conversational recommender systems (CRSs) are scarce. The UserSimCRS toolkit was introduced to address this gap. In this work, we present UserSimCRS v2, a significant upgrade aligning the toolkit with state-of-the-art research. Key extensions include an enhanced agenda-based user simulator, introduction of large language model-based simulators, integration for a wider range of CRSs and datasets, and new LLM-as-a-judge evaluation utilities. We demonstrate these extensions in a case study.\nCategories: cs.IR\nPublication Date: 2025-12-04\nPaper ID: 2512.04588\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 537, "total_text_length": 619, "word_count": 80, "sentence_count": 6}, "basic_keywords": ["based", "evaluation", "crss", "usersimcrs", "toolkit", "extensions", "resources", "simulation"], "domain_keywords": [], "technical_terms": ["Resources"], "paper_type": "tutorial", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04587", "topic": "computer vision", "title": "Supramolecular approach-based intermolecular interaction energy calculations using quantum phase estimation algorithm", "authors": ["Yuhei Tachi", "Akihiko Arakawa", "Taisei Osawa", "Masayoshi Terabe", "Kenji Sugisaki"], "first_author": "Yuhei Tachi", "abstract": "Accurate computation of non-covalent, intermolecular interaction energies is important to understand various chemical phenomena, and quantum computers are anticipated to accelerate it. Although the state-of-the-art quantum computers are still noisy and intermediate-scale ones, development of theoretical frameworks those are expected to work on a fault-tolerant quantum computer is an urgent issue. In this work, we explore resource-efficient implementation of the quantum phase estimation-based complete active space configuration interaction (QPE-CASCI) calculations, with the aid of the second-order M ller--Plesset perturbation theory (MP2)-based active space selection with Boys localized orbitals. We performed numerical simulations of QPE for the supramolecular approach-based intermolecular interaction energy calculations of the hydrogen-bonded water dimer, using 6 system and 6 ancilla qubits. With the aid of algorithmic error mitigation, the QPE-CASCI simulations achieved interaction energy predictions with an error of 0.02 kcal mol relative to the CASCI result, demonstrating the accuracy and efficiency of the proposed methodology. Preliminary results on quantum circuit compression for QPE are also presented to reduce the number of two-qubit gates and depth.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph", "physics.chem-ph"], "url": "http://arxiv.org/abs/2512.04587", "comments": "13 pages, 12 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Supramolecular approach-based intermolecular interaction energy calculations using quantum phase estimation algorithm\nResearch Topic: computer vision\nAuthors: Yuhei Tachi, Akihiko Arakawa, Taisei Osawa, Masayoshi Terabe, Kenji Sugisaki\nAbstract: Accurate computation of non-covalent, intermolecular interaction energies is important to understand various chemical phenomena, and quantum computers are anticipated to accelerate it. Although the state-of-the-art quantum computers are still noisy and intermediate-scale ones, development of theoretical frameworks those are expected to work on a fault-tolerant quantum computer is an urgent issue. In this work, we explore resource-efficient implementation of the quantum phase estimation-based complete active space configuration interaction (QPE-CASCI) calculations, with the aid of the second-order Møller--Plesset perturbation theory (MP2)-based active space selection with Boys localized orbitals. We performed numerical simulations of QPE for the supramolecular approach-based intermolecular interaction energy calculations of the hydrogen-bonded water dimer, using 6 system and 6 ancilla qubits. With the aid of algorithmic error mitigation, the QPE-CASCI simulations achieved interaction energy predictions with an error of 0.02 kcal mol$^{-1}$ relative to the CASCI result, demonstrating the accuracy and efficiency of the proposed methodology. Preliminary results on quantum circuit compression for QPE are also presented to reduce the number of two-qubit gates and depth.\nCategories: quant-ph, physics.chem-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04587\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 117, "abstract_length": 1277, "total_text_length": 1395, "word_count": 180, "sentence_count": 8}, "basic_keywords": ["quantum", "interaction", "based", "casci", "intermolecular", "computers", "work", "active"], "domain_keywords": [], "technical_terms": ["Preliminary", "Plesset", "Although", "Accurate", "Boys"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04585", "topic": "computer vision", "title": "SAM3-I: Segment Anything with Instructions", "authors": ["Jingjing Li", "Yue Feng", "Yuchen Guo", "Jincai Huang", "Yongri Piao", "Qi Bi", "Miao Zhang", "Xiaoqi Zhao", "Qiang Chen", "Shihao Zou", "Wei Ji", "Huchuan Lu", "Li Cheng"], "first_author": "Jingjing Li", "abstract": "Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04585", "comments": "Preliminary results; work in progress", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: SAM3-I: Segment Anything with Instructions\nResearch Topic: computer vision\nAuthors: Jingjing Li, Yue Feng, Yuchen Guo, Jincai Huang, Yongri Piao, Qi Bi, Miao Zhang, Xiaoqi Zhao, Qiang Chen, Shihao Zou, Wei Ji, Huchuan Lu, Li Cheng\nAbstract: Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04585\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 42, "abstract_length": 1778, "total_text_length": 1821, "word_count": 225, "sentence_count": 11}, "basic_keywords": ["concept", "instruction", "level", "segmentation", "language", "segment", "open", "instances"], "domain_keywords": [], "technical_terms": ["However", "Experiments", "While", "Segment Anything Model", "Furthermore", "Currently"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04581", "topic": "computer vision", "title": "Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation", "authors": ["Houzhang Fang", "Chenxing Wu", "Kun Bai", "Tianqi Chen", "Xiaolin Wang", "Xiyang Liu", "Yi Chang", "Luxin Yan"], "first_author": "Houzhang Fang", "abstract": "Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04581", "comments": "Accepted by IEEE TMM", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation\nResearch Topic: computer vision\nAuthors: Houzhang Fang, Chenxing Wu, Kun Bai, Tianqi Chen, Xiaolin Wang, Xiyang Liu, Yi Chang, Luxin Yan\nAbstract: Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04581\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 115, "abstract_length": 1913, "total_text_length": 2029, "word_count": 268, "sentence_count": 11}, "basic_keywords": ["target", "network", "tracking", "feature", "template", "infrared", "features", "attention"], "domain_keywords": ["feature extraction"], "technical_terms": ["Extensive", "However", "Siamese", "Furthermore", "Unmanned"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04579", "topic": "computer vision", "title": "Gauss-Newton accelerated MPPI Control", "authors": ["Hannes Homburger", "Katrin Baumgärtner", "Moritz Diehl", "Johannes Reuter"], "first_author": "Hannes Homburger", "abstract": "Model Predictive Path Integral (MPPI) control is a sampling-based optimization method that has recently attracted attention, particularly in the robotics and reinforcement learning communities. MPPI has been widely applied as a GPU-accelerated random search method to deterministic direct single-shooting optimal control problems arising in model predictive control (MPC) formulations. MPPI offers several key advantages, including flexibility, robustness, ease of implementation, and inherent parallelizability. However, its performance can deteriorate in high-dimensional settings since the optimal control problem is solved via Monte Carlo sampling. To address this limitation, this paper proposes an enhanced MPPI method that incorporates a Jacobian reconstruction technique and the second-order Generalized Gauss-Newton method. This novel approach is called . The numerical results show that the Gauss-Newton accelerated MPPI approach substantially improves MPPI scalability and computational efficiency while preserving the key benefits of the classical MPPI framework, making it a promising approach even for high-dimensional problems.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SY", "cs.RO"], "url": "http://arxiv.org/abs/2512.04579", "comments": "6 pages, 3 figures, submitted to the IFAC World Congress 2026", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.143006", "embedding_text": "Paper Title: Gauss-Newton accelerated MPPI Control\nResearch Topic: computer vision\nAuthors: Hannes Homburger, Katrin Baumgärtner, Moritz Diehl, Johannes Reuter\nAbstract: Model Predictive Path Integral (MPPI) control is a sampling-based optimization method that has recently attracted attention, particularly in the robotics and reinforcement learning communities. MPPI has been widely applied as a GPU-accelerated random search method to deterministic direct single-shooting optimal control problems arising in model predictive control (MPC) formulations. MPPI offers several key advantages, including flexibility, robustness, ease of implementation, and inherent parallelizability. However, its performance can deteriorate in high-dimensional settings since the optimal control problem is solved via Monte Carlo sampling. To address this limitation, this paper proposes an enhanced MPPI method that incorporates a Jacobian reconstruction technique and the second-order Generalized Gauss-Newton method. This novel approach is called \\textit{Gauss-Newton accelerated MPPI}. The numerical results show that the Gauss-Newton accelerated MPPI approach substantially improves MPPI scalability and computational efficiency while preserving the key benefits of the classical MPPI framework, making it a promising approach even for high-dimensional problems.\nCategories: eess.SY, cs.RO\nPublication Date: 2025-12-04\nPaper ID: 2512.04579\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 37, "abstract_length": 1142, "total_text_length": 1180, "word_count": 151, "sentence_count": 8}, "basic_keywords": ["mppi", "control", "method", "approach", "model", "predictive", "sampling", "accelerated"], "domain_keywords": [], "technical_terms": ["However", "Monte Carlo", "Model Predictive Path Integral", "Generalized Gauss", "Newton", "Jacobian", "Gauss"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 1.0}, "overall_quality_score": 1.0, "quality_tier": "high"}
{"paper_id": "2512.04576", "topic": "computer vision", "title": "TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification", "authors": ["Zishuo Wan", "Qinqin Kang", "Yi Huang", "Yun Bian", "Dawei Ding", "Ke Yan"], "first_author": "Zishuo Wan", "abstract": "Tumor segmentation and diagnosis in contrast-enhanced Computed Tomography (CT) rely heavily on the physiological dynamics of contrast agents. However, obtaining a complete multi-phase series is often clinically unfeasible due to radiation concerns or scanning limitations, leading to the \"missing modality\" problem. Existing deep learning approaches typically treat missing phases as absent independent channels, ignoring the inherent temporal continuity of hemodynamics. In this work, we propose Time Attenuated Representation Disentanglement (TARDis), a novel physics-aware framework that redefines missing modalities as missing sample points on a continuous Time-Attenuation Curve. TARDis explicitly disentangles the latent feature space into a time-invariant static component (anatomy) and a time-dependent dynamic component (perfusion). We achieve this via a dual-path architecture: a quantization-based path using a learnable embedding dictionary to extract consistent anatomical structures, and a probabilistic path using a Conditional Variational Autoencoder to model dynamic enhancement conditioned on the estimated scan time. This design allows the network to hallucinate missing hemodynamic features by sampling from the learned latent distribution. Extensive experiments on a large-scale private abdominal CT dataset (2,282 cases) and two public datasets demonstrate that TARDis significantly outperforms state-of-the-art incomplete modality frameworks. Notably, our method maintains robust diagnostic performance even in extreme data-sparsity scenarios, highlighting its potential for reducing radiation exposure while maintaining diagnostic precision.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04576", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification\nResearch Topic: computer vision\nAuthors: Zishuo Wan, Qinqin Kang, Yi Huang, Yun Bian, Dawei Ding, Ke Yan\nAbstract: Tumor segmentation and diagnosis in contrast-enhanced Computed Tomography (CT) rely heavily on the physiological dynamics of contrast agents. However, obtaining a complete multi-phase series is often clinically unfeasible due to radiation concerns or scanning limitations, leading to the \"missing modality\" problem. Existing deep learning approaches typically treat missing phases as absent independent channels, ignoring the inherent temporal continuity of hemodynamics. In this work, we propose Time Attenuated Representation Disentanglement (TARDis), a novel physics-aware framework that redefines missing modalities as missing sample points on a continuous Time-Attenuation Curve. TARDis explicitly disentangles the latent feature space into a time-invariant static component (anatomy) and a time-dependent dynamic component (perfusion). We achieve this via a dual-path architecture: a quantization-based path using a learnable embedding dictionary to extract consistent anatomical structures, and a probabilistic path using a Conditional Variational Autoencoder to model dynamic enhancement conditioned on the estimated scan time. This design allows the network to hallucinate missing hemodynamic features by sampling from the learned latent distribution. Extensive experiments on a large-scale private abdominal CT dataset (2,282 cases) and two public datasets demonstrate that TARDis significantly outperforms state-of-the-art incomplete modality frameworks. Notably, our method maintains robust diagnostic performance even in extreme data-sparsity scenarios, highlighting its potential for reducing radiation exposure while maintaining diagnostic precision.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04576\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 119, "abstract_length": 1665, "total_text_length": 1785, "word_count": 221, "sentence_count": 10}, "basic_keywords": ["missing", "time", "tardis", "path", "contrast", "radiation", "modality", "latent"], "domain_keywords": [], "technical_terms": ["Extensive", "However", "Attenuation Curve", "Conditional Variational Autoencoder", "Existing", "Notably", "Time", "Computed Tomography"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04573", "topic": "computer vision", "title": "A Rocq Formalization of Monomial and Graded Orders", "authors": ["Sylvie Boldo", "François Clément", "Vincent Martin", "Micaela Mayero"], "first_author": "Sylvie Boldo", "abstract": "Even if binary relations and orders are a common formalization topic, we need to formalize specific orders (namely monomial and graded) in the process of formalizing in Rocq the finite element method. This article is therefore definitions, operators, and proofs of properties about relations and orders, thus providing a comprehensive Rocq library. We especially focus on monomial orders, that are total orders compatible with the monoid operation. More than its definition and proved properties, we define several of them, among them the lexicographic and grevlex orders. For the sake of genericity, we formalize the grading of an order, a high-level operator that transforms a binary relation into another one, and we prove that grading an order preserves many of its properties, such as the monomial order property. This leads us to the definition and properties of four different graded orders, with very factorized proofs. We therefore provide a comprehensive and user-friendly library in Rocq about orders, including monomial and graded orders, that contains more than 700 lemmas.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.LO"], "url": "http://arxiv.org/abs/2512.04573", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: A Rocq Formalization of Monomial and Graded Orders\nResearch Topic: computer vision\nAuthors: Sylvie Boldo, François Clément, Vincent Martin, Micaela Mayero\nAbstract: Even if binary relations and orders are a common formalization topic, we need to formalize specific orders (namely monomial and graded) in the process of formalizing in Rocq the finite element method. This article is therefore definitions, operators, and proofs of properties about relations and orders, thus providing a comprehensive Rocq library. We especially focus on monomial orders, that are total orders compatible with the monoid operation. More than its definition and proved properties, we define several of them, among them the lexicographic and grevlex orders. For the sake of genericity, we formalize the grading of an order, a high-level operator that transforms a binary relation into another one, and we prove that grading an order preserves many of its properties, such as the monomial order property. This leads us to the definition and properties of four different graded orders, with very factorized proofs. We therefore provide a comprehensive and user-friendly library in Rocq about orders, including monomial and graded orders, that contains more than 700 lemmas.\nCategories: cs.LO\nPublication Date: 2025-12-04\nPaper ID: 2512.04573\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 50, "abstract_length": 1086, "total_text_length": 1137, "word_count": 175, "sentence_count": 8}, "basic_keywords": ["orders", "monomial", "properties", "graded", "rocq", "order", "binary", "relations"], "domain_keywords": [], "technical_terms": ["More", "Rocq", "Even"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04568", "topic": "computer vision", "title": "Prompt2Craft: Generating Functional Craft Assemblies with LLMs", "authors": ["Vitor Hideyo Isume", "Takuya Kiyokawa", "Natsuki Yamanobe", "Yukiyasu Domae", "Weiwei Wan", "Kensuke Harada"], "first_author": "Vitor Hideyo Isume", "abstract": "Inspired by traditional handmade crafts, where a person improvises assemblies based on the available objects, we formally introduce the Craft Assembly Task. It is a robotic assembly task that involves building an accurate representation of a given target object using the available objects, which do not directly correspond to its parts. In this work, we focus on selecting the subset of available objects for the final craft, when the given input is an RGB image of the target in the wild. We use a mask segmentation neural network to identify visible parts, followed by retrieving labeled template meshes. These meshes undergo pose optimization to determine the most suitable template. Then, we propose to simplify the parts of the transformed template mesh to primitive shapes like cuboids or cylinders. Finally, we design a search algorithm to find correspondences in the scene based on local and global proportions. We develop baselines for comparison that consider all possible combinations, and choose the highest scoring combination for common metrics used in foreground maps and mask accuracy. Our approach achieves comparable results to the baselines for two different scenes, and we show qualitative results for an implementation in a real-world scenario.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04568", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Prompt2Craft: Generating Functional Craft Assemblies with LLMs\nResearch Topic: computer vision\nAuthors: Vitor Hideyo Isume, Takuya Kiyokawa, Natsuki Yamanobe, Yukiyasu Domae, Weiwei Wan, Kensuke Harada\nAbstract: Inspired by traditional handmade crafts, where a person improvises assemblies based on the available objects, we formally introduce the Craft Assembly Task. It is a robotic assembly task that involves building an accurate representation of a given target object using the available objects, which do not directly correspond to its parts. In this work, we focus on selecting the subset of available objects for the final craft, when the given input is an RGB image of the target in the wild. We use a mask segmentation neural network to identify visible parts, followed by retrieving labeled template meshes. These meshes undergo pose optimization to determine the most suitable template. Then, we propose to simplify the parts of the transformed template mesh to primitive shapes like cuboids or cylinders. Finally, we design a search algorithm to find correspondences in the scene based on local and global proportions. We develop baselines for comparison that consider all possible combinations, and choose the highest scoring combination for common metrics used in foreground maps and mask accuracy. Our approach achieves comparable results to the baselines for two different scenes, and we show qualitative results for an implementation in a real-world scenario.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04568\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 62, "abstract_length": 1266, "total_text_length": 1329, "word_count": 203, "sentence_count": 10}, "basic_keywords": ["available", "objects", "parts", "template", "based", "craft", "assembly", "task"], "domain_keywords": [], "technical_terms": ["These", "Inspired", "Finally", "Craft Assembly Task", "Then"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04564", "topic": "computer vision", "title": "Dataset creation for supervised deep learning-based analysis of microscopic images -- review of important considerations and recommendations", "authors": ["Christof A. Bertram", "Viktoria Weiss", "Jonas Ammeling", "F. Maria Schabel", "Taryn A. Donovan", "Frauke Wilm", "Christian Marzahl", "Katharina Breininger", "Marc Aubreville"], "first_author": "Christof A. Bertram", "abstract": "Supervised deep learning (DL) receives great interest for automated analysis of microscopic images with an increasing body of literature supporting its potential. The development and validation of those DL models relies heavily on the availability of high-quality, large-scale datasets. However, creating such datasets is a complex and resource-intensive process, often hindered by challenges such as time constraints, domain variability, and risks of bias in image collection and label creation. This review provides a comprehensive guide to the critical steps in dataset creation, including: 1) image acquisition, 2) selection of annotation software, and 3) annotation creation. In addition to ensuring a sufficiently large number of images, it is crucial to address sources of image variability (domain shifts) - such as those related to slide preparation and digitization - that could lead to algorithmic errors if not adequately represented in the training data. Key quality criteria for annotations are the three \"C\"s: correctness, completeness, and consistency. This review explores methods to enhance annotation quality through the use of advanced techniques that mitigate the limitations of single annotators. To support dataset creators, a standard operating procedure (SOP) is provided as supplemental material, outlining best practices for dataset development. Furthermore, the article underscores the importance of open datasets in driving innovation and enhancing reproducibility of DL research. By addressing the challenges and offering practical recommendations, this review aims to advance the creation of and availability to high-quality, large-scale datasets, ultimately contributing to the development of generalizable and robust DL models for pathology applications.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04564", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Dataset creation for supervised deep learning-based analysis of microscopic images -- review of important considerations and recommendations\nResearch Topic: computer vision\nAuthors: Christof A. Bertram, Viktoria Weiss, Jonas Ammeling, F. Maria Schabel, Taryn A. Donovan, Frauke Wilm, Christian Marzahl, Katharina Breininger, Marc Aubreville\nAbstract: Supervised deep learning (DL) receives great interest for automated analysis of microscopic images with an increasing body of literature supporting its potential. The development and validation of those DL models relies heavily on the availability of high-quality, large-scale datasets. However, creating such datasets is a complex and resource-intensive process, often hindered by challenges such as time constraints, domain variability, and risks of bias in image collection and label creation. This review provides a comprehensive guide to the critical steps in dataset creation, including: 1) image acquisition, 2) selection of annotation software, and 3) annotation creation. In addition to ensuring a sufficiently large number of images, it is crucial to address sources of image variability (domain shifts) - such as those related to slide preparation and digitization - that could lead to algorithmic errors if not adequately represented in the training data. Key quality criteria for annotations are the three \"C\"s: correctness, completeness, and consistency. This review explores methods to enhance annotation quality through the use of advanced techniques that mitigate the limitations of single annotators. To support dataset creators, a standard operating procedure (SOP) is provided as supplemental material, outlining best practices for dataset development. Furthermore, the article underscores the importance of open datasets in driving innovation and enhancing reproducibility of DL research. By addressing the challenges and offering practical recommendations, this review aims to advance the creation of and availability to high-quality, large-scale datasets, ultimately contributing to the development of generalizable and robust DL models for pathology applications.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04564\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 140, "abstract_length": 1787, "total_text_length": 1928, "word_count": 266, "sentence_count": 11}, "basic_keywords": ["quality", "datasets", "creation", "development", "large", "image", "review", "dataset"], "domain_keywords": [], "technical_terms": ["However", "Furthermore", "Supervised"], "paper_type": "survey", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04563", "topic": "computer vision", "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence", "authors": ["Zefeng Zhang", "Xiangzhao Hao", "Hengzhu Tang", "Zhenyu Zhang", "Jiawei Sheng", "Xiaodong Li", "Zhenyang Li", "Li Gao", "Daiting Shi", "Dawei Yin", "Tingwen Liu"], "first_author": "Zefeng Zhang", "abstract": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose , a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04563", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence\nResearch Topic: computer vision\nAuthors: Zefeng Zhang, Xiangzhao Hao, Hengzhu Tang, Zhenyu Zhang, Jiawei Sheng, Xiaodong Li, Zhenyang Li, Li Gao, Daiting Shi, Dawei Yin, Tingwen Liu\nAbstract: Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91\\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92\\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04563\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 88, "abstract_length": 1241, "total_text_length": 1330, "word_count": 179, "sentence_count": 7}, "basic_keywords": ["spatial", "reasoning", "auxiliary", "modalities", "models", "enhance", "perception", "depth"], "domain_keywords": [], "technical_terms": ["Moreover", "Visual Spatial Reasoning", "Multimodal Large Language Models", "Existing"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04561", "topic": "computer vision", "title": "Omniscient Attacker in Stochastic Security Games with Interdependent Nodes", "authors": ["Yuksel Arslantas", "Ahmed Said Donmez", "Ege Yuceel", "Muhammed O. Sayin"], "first_author": "Yuksel Arslantas", "abstract": "The adoption of reinforcement learning for critical infrastructure defense introduces a vulnerability where sophisticated attackers can strategically exploit the defense algorithm's learning dynamics. While prior work addresses this vulnerability in the context of repeated normal-form games, its extension to the stochastic games remains an open research gap. We close this gap by examining stochastic security games between an RL defender and an omniscient attacker, utilizing a tractable linear influence network model. To overcome the structural limitations of prior methods, we propose and apply neuro-dynamic programming. Our experimental results demonstrate that the omniscient attacker can significantly outperform a naive defender, highlighting the critical vulnerability introduced by the learning dynamics and the effectiveness of the proposed strategy.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.GT", "eess.SY"], "url": "http://arxiv.org/abs/2512.04561", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Omniscient Attacker in Stochastic Security Games with Interdependent Nodes\nResearch Topic: computer vision\nAuthors: Yuksel Arslantas, Ahmed Said Donmez, Ege Yuceel, Muhammed O. Sayin\nAbstract: The adoption of reinforcement learning for critical infrastructure defense introduces a vulnerability where sophisticated attackers can strategically exploit the defense algorithm's learning dynamics. While prior work addresses this vulnerability in the context of repeated normal-form games, its extension to the stochastic games remains an open research gap. We close this gap by examining stochastic security games between an RL defender and an omniscient attacker, utilizing a tractable linear influence network model. To overcome the structural limitations of prior methods, we propose and apply neuro-dynamic programming. Our experimental results demonstrate that the omniscient attacker can significantly outperform a naive defender, highlighting the critical vulnerability introduced by the learning dynamics and the effectiveness of the proposed strategy.\nCategories: cs.GT, eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04561\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 74, "abstract_length": 864, "total_text_length": 939, "word_count": 124, "sentence_count": 6}, "basic_keywords": ["learning", "vulnerability", "games", "critical", "defense", "dynamics", "prior", "stochastic"], "domain_keywords": [], "technical_terms": ["While"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04560", "topic": "computer vision", "title": "Reflection of Nichols Algebras over Coquasi-Hopf Algebras", "authors": ["Bowen Li", "Gongxiang Liu"], "first_author": "Bowen Li", "abstract": "This paper extends the foundational reflection theory of Nichols algebras to the setting of some certain coquasi-Hopf algebras. Our primary motivation arises from the classification of pointed finite-dimensional coquasi-Hopf algebras. We develop a reflection theory for tuples of simple Yetter-Drinfeld modules in the category , where is a finite group and is a 3-cocycle on . We prove that such a tuple gives rise to a semi-Cartan graph if admitting all reflections. Consequently, its Weyl groupoid is well-defined. We further establish several criteria for the finite-dimensionality of Nichols algebras in terms of the associated semi-Cartan graph. As an application, we provide a new proof for the infinite-dimensionality of a specific class of Nichols algebras previously studied in , bypassing extensive computational arguments.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["math.QA"], "url": "http://arxiv.org/abs/2512.04560", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Reflection of Nichols Algebras over Coquasi-Hopf Algebras\nResearch Topic: computer vision\nAuthors: Bowen Li, Gongxiang Liu\nAbstract: This paper extends the foundational reflection theory of Nichols algebras to the setting of some certain coquasi-Hopf algebras. Our primary motivation arises from the classification of pointed finite-dimensional coquasi-Hopf algebras. We develop a reflection theory for tuples of simple Yetter-Drinfeld modules in the category $\\GG$, where $G$ is a finite group and $Φ$ is a 3-cocycle on $G$. We prove that such a tuple gives rise to a semi-Cartan graph if admitting all reflections. Consequently, its Weyl groupoid is well-defined. We further establish several criteria for the finite-dimensionality of Nichols algebras in terms of the associated semi-Cartan graph. As an application, we provide a new proof for the infinite-dimensionality of a specific class of Nichols algebras previously studied in \\cite{huang2024classification}, bypassing extensive computational arguments.\nCategories: math.QA\nPublication Date: 2025-12-04\nPaper ID: 2512.04560\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 57, "abstract_length": 833, "total_text_length": 891, "word_count": 129, "sentence_count": 8}, "basic_keywords": ["algebras", "nichols", "finite", "reflection", "theory", "coquasi", "hopf", "semi"], "domain_keywords": [], "technical_terms": ["Consequently", "Hopf", "Weyl", "Nichols", "Yetter", "Cartan", "Drinfeld"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
{"paper_id": "2512.04557", "topic": "computer vision", "title": "Efficient Safety Verification of Autonomous Vehicles with Neural Network Operator", "authors": ["Lingxiang Fan", "Linxuan He", "Haoyuan Ji", "Shuo Feng"], "first_author": "Lingxiang Fan", "abstract": "When autonomous vehicles encounter untrained scenarios, ensuring safety hinges on effective safety verification to prevent accidents stemming from unexpected model decisions. Reachability analysis, a method of safety verification, offers relatively high precision but at the cost of significant computational complexity. Our method leverages end-to-end neural network operators to compute reachable sets, replacing traditional mathematical set operators, thereby achieving higher efficiency in safety verification without substantially compromising accuracy or increasing conservativeness. We define vehicle dynamics on discrete time series and detail the safety verification process and safety standard based on reachable sets. Experimental evaluations conducted in several typical road driving scenarios demonstrate the superior efficiency performance of our proposed operator over classical methods.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["eess.SY"], "url": "http://arxiv.org/abs/2512.04557", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Efficient Safety Verification of Autonomous Vehicles with Neural Network Operator\nResearch Topic: computer vision\nAuthors: Lingxiang Fan, Linxuan He, Haoyuan Ji, Shuo Feng\nAbstract: When autonomous vehicles encounter untrained scenarios, ensuring safety hinges on effective safety verification to prevent accidents stemming from unexpected model decisions. Reachability analysis, a method of safety verification, offers relatively high precision but at the cost of significant computational complexity. Our method leverages end-to-end neural network operators to compute reachable sets, replacing traditional mathematical set operators, thereby achieving higher efficiency in safety verification without substantially compromising accuracy or increasing conservativeness. We define vehicle dynamics on discrete time series and detail the safety verification process and safety standard based on reachable sets. Experimental evaluations conducted in several typical road driving scenarios demonstrate the superior efficiency performance of our proposed operator over classical methods.\nCategories: eess.SY\nPublication Date: 2025-12-04\nPaper ID: 2512.04557\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 81, "abstract_length": 902, "total_text_length": 984, "word_count": 122, "sentence_count": 6}, "basic_keywords": ["safety", "verification", "scenarios", "method", "operators", "reachable", "sets", "efficiency"], "domain_keywords": ["autonomous vehicles"], "technical_terms": ["Reachability", "Experimental", "When"], "paper_type": "theoretical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04556", "topic": "computer vision", "title": "Efficient Spatially-Variant Convolution via Differentiable Sparse Kernel Complex", "authors": ["Zhizhen Wu", "Zhe Cao", "Yuchi Huo"], "first_author": "Zhizhen Wu", "abstract": "Image convolution with complex kernels is a fundamental operation in photography, scientific imaging, and animation effects, yet direct dense convolution is computationally prohibitive on resource-limited devices. Existing approximations, such as simulated annealing or low-rank decompositions, either lack efficiency or fail to capture non-convex kernels. We introduce a differentiable kernel decomposition framework that represents a target spatially-variant, dense, complex kernel using a set of sparse kernel samples. Our approach features (i) a decomposition that enables differentiable optimization of sparse kernels, (ii) a dedicated initialization strategy for non-convex shapes to avoid poor local minima, and (iii) a kernel-space interpolation scheme that extends single-kernel filtering to spatially varying filtering without retraining and additional runtime overhead. Experiments on Gaussian and non-convex kernels show that our method achieves higher fidelity than simulated annealing and significantly lower cost than low-rank decompositions. Our approach provides a practical solution for mobile imaging and real-time rendering, while remaining fully differentiable for integration into broader learning pipelines.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.GR", "cs.CV"], "url": "http://arxiv.org/abs/2512.04556", "comments": "10 pages, 7 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Efficient Spatially-Variant Convolution via Differentiable Sparse Kernel Complex\nResearch Topic: computer vision\nAuthors: Zhizhen Wu, Zhe Cao, Yuchi Huo\nAbstract: Image convolution with complex kernels is a fundamental operation in photography, scientific imaging, and animation effects, yet direct dense convolution is computationally prohibitive on resource-limited devices. Existing approximations, such as simulated annealing or low-rank decompositions, either lack efficiency or fail to capture non-convex kernels. We introduce a differentiable kernel decomposition framework that represents a target spatially-variant, dense, complex kernel using a set of sparse kernel samples. Our approach features (i) a decomposition that enables differentiable optimization of sparse kernels, (ii) a dedicated initialization strategy for non-convex shapes to avoid poor local minima, and (iii) a kernel-space interpolation scheme that extends single-kernel filtering to spatially varying filtering without retraining and additional runtime overhead. Experiments on Gaussian and non-convex kernels show that our method achieves higher fidelity than simulated annealing and significantly lower cost than low-rank decompositions. Our approach provides a practical solution for mobile imaging and real-time rendering, while remaining fully differentiable for integration into broader learning pipelines.\nCategories: cs.GR, cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04556\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 80, "abstract_length": 1230, "total_text_length": 1311, "word_count": 165, "sentence_count": 7}, "basic_keywords": ["kernel", "kernels", "convex", "differentiable", "convolution", "complex", "imaging", "dense"], "domain_keywords": [], "technical_terms": ["Experiments", "Image", "Gaussian", "Existing"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.9249999999999999, "quality_tier": "high"}
{"paper_id": "2512.04555", "topic": "computer vision", "title": "ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning", "authors": ["Pritam Kadasi", "Abhishek Upperwal", "Mayank SIngh"], "first_author": "Pritam Kadasi", "abstract": "We propose ADAPT, a meta-learning algorithm that task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three 1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of , , and of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04555", "comments": "Under Review", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning\nResearch Topic: computer vision\nAuthors: Pritam Kadasi, Abhishek Upperwal, Mayank SIngh\nAbstract: We propose ADAPT, a meta-learning algorithm that \\emph{learns} task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, \\adapt{} maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three $\\sim$1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of $1\\%$, $5\\%$, and $10\\%$ of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04555\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 71, "abstract_length": 1081, "total_text_length": 1153, "word_count": 152, "sentence_count": 7}, "basic_keywords": ["task", "adapt", "tasks", "tokens", "meta", "under", "budget", "instruction"], "domain_keywords": [], "technical_terms": ["Gemma", "Qwen", "Instead", "Natural Instructions"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.8}, "overall_quality_score": 0.95, "quality_tier": "high"}
{"paper_id": "2512.04554", "topic": "computer vision", "title": "Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering", "authors": ["Marco Pintore", "Maura Pintor", "Dimosthenis Karatzas", "Battista Biggio"], "first_author": "Marco Pintore", "abstract": "Document Visual Question Answering (DocVQA) enables end-to-end reasoning grounded on information present in a document input. While recent models have shown impressive capabilities, they remain vulnerable to adversarial attacks. In this work, we introduce a novel attack scenario that aims to forge document content in a visually imperceptible yet semantically targeted manner, allowing an adversary to induce specific or generally incorrect answers from a DocVQA model. We develop specialized attack algorithms that can produce adversarially forged documents tailored to different attackers' goals, ranging from targeted misinformation to systematic model failure scenarios. We demonstrate the effectiveness of our approach against two end-to-end state-of-the-art models: Pix2Struct, a vision-language transformer that jointly processes image and text through sequence-to-sequence modeling, and Donut, a transformer-based model that directly extracts text and answers questions from document images. Our findings highlight critical vulnerabilities in current DocVQA systems and call for the development of more robust defenses.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CV"], "url": "http://arxiv.org/abs/2512.04554", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering\nResearch Topic: computer vision\nAuthors: Marco Pintore, Maura Pintor, Dimosthenis Karatzas, Battista Biggio\nAbstract: Document Visual Question Answering (DocVQA) enables end-to-end reasoning grounded on information present in a document input. While recent models have shown impressive capabilities, they remain vulnerable to adversarial attacks. In this work, we introduce a novel attack scenario that aims to forge document content in a visually imperceptible yet semantically targeted manner, allowing an adversary to induce specific or generally incorrect answers from a DocVQA model. We develop specialized attack algorithms that can produce adversarially forged documents tailored to different attackers' goals, ranging from targeted misinformation to systematic model failure scenarios. We demonstrate the effectiveness of our approach against two end-to-end state-of-the-art models: Pix2Struct, a vision-language transformer that jointly processes image and text through sequence-to-sequence modeling, and Donut, a transformer-based model that directly extracts text and answers questions from document images. Our findings highlight critical vulnerabilities in current DocVQA systems and call for the development of more robust defenses.\nCategories: cs.CV\nPublication Date: 2025-12-04\nPaper ID: 2512.04554\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 92, "abstract_length": 1128, "total_text_length": 1221, "word_count": 159, "sentence_count": 7}, "basic_keywords": ["document", "docvqa", "model", "models", "attack", "targeted", "answers", "transformer"], "domain_keywords": [], "technical_terms": ["Document Visual Question Answering", "Donut", "While"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.7}, "overall_quality_score": 0.8999999999999999, "quality_tier": "high"}
{"paper_id": "2512.04553", "topic": "computer vision", "title": "Probing Hard Scattering Processes via Multiple Weak Gauge Boson Production at the Future Colliders", "authors": ["Ijaz Ahmed", "M. S. Amjad", "Jamil Muhammad"], "first_author": "Ijaz Ahmed", "abstract": "One of the possible ways to detect the new physics phenomena particles is to investigate the weak gauge boson production as a result of hadron-hadron scattering. This study comprises the production of multiple weak gauge bosons as a result of hard scattering between the proton-proton beams at multi-TeV energies and integrated luminosity 3000 . The effective production cross-sections for pair, triple, and quartic scattering mechanisms have been computed as a function of . The center of mass energy has been varied from 8 TeV to 100 TeV to encompass the future collider capabilities. Out of all the studied processes, the triple scattering process has been chosen as the signal process based on the dominant cross-section. The background channels ZZZ, ZZZZ, , , , , , having comparatively lower cross-sections, have been selected from possible scattering mechanisms to investigate the effect of higher luminosity on the low production cross-section processes. We have investigated the different decay modes. For both lepton and hadron-specific decays of W and Z, the cumulative efficiencies for each signal and background process have been computed. In this study, we have successfully demonstrated an effective methodology for background suppression by systematically optimizing the signal-to-background ratio. The results indicate that, despite lower cross-sections for higher-order scattering, the distinct kinematic features enable effective signal isolation at future colliders.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["hep-ph"], "url": "http://arxiv.org/abs/2512.04553", "comments": "18 Pages, 7 figures", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Probing Hard Scattering Processes via Multiple Weak Gauge Boson Production at the Future Colliders\nResearch Topic: computer vision\nAuthors: Ijaz Ahmed, M. S. Amjad, Jamil Muhammad\nAbstract: One of the possible ways to detect the new physics phenomena particles is to investigate the weak gauge boson production as a result of hadron-hadron scattering. This study comprises the production of multiple weak gauge bosons as a result of hard scattering between the proton-proton beams at multi-TeV energies and integrated luminosity $\\mathcal L =$ 3000 $fb ^{-1}$. The effective production cross-sections for pair, triple, and quartic scattering mechanisms have been computed as a function of $\\sqrt s$. The center of mass energy has been varied from 8 TeV to 100 TeV to encompass the future collider capabilities. Out of all the studied processes, the triple scattering process $W^+W^-W^+$ has been chosen as the signal process based on the dominant cross-section. The background channels ZZZ, ZZZZ, $W^-ZZ$, $W^+ZZ$, $W^+W^-Z$, $W^+W^-ZZ$, $W^+W^-W^+W^-$, having comparatively lower cross-sections, have been selected from possible scattering mechanisms to investigate the effect of higher luminosity on the low production cross-section processes. We have investigated the different decay modes. For both lepton and hadron-specific decays of W and Z, the cumulative efficiencies for each signal and background process have been computed. In this study, we have successfully demonstrated an effective methodology for background suppression by systematically optimizing the signal-to-background ratio. The results indicate that, despite lower cross-sections for higher-order scattering, the distinct kinematic features enable effective signal isolation at future colliders.\nCategories: hep-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04553\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 98, "abstract_length": 1486, "total_text_length": 1585, "word_count": 232, "sentence_count": 11}, "basic_keywords": ["scattering", "cross", "production", "signal", "background", "hadron", "effective", "sections"], "domain_keywords": [], "technical_terms": [], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.9999999999999999, "technical_substance": 0.2}, "overall_quality_score": 0.7999999999999999, "quality_tier": "medium"}
{"paper_id": "2512.04546", "topic": "computer vision", "title": "A hybrid Green-Kubo (hGK) framework for calculating viscosity from short MD simulations", "authors": ["Akash K. Meel", "Santosh Mogurampelly"], "first_author": "Akash K. Meel", "abstract": "Viscosity calculation from equilibrium molecular dynamics (MD) simulations relies on the traditional Green-Kubo (GK) framework, which integrates the stress autocorrelation function (SACF) over time. While the formalism is exact in the linear response regime, the traditional approach often suffers from poor convergence and requires extensive phase space sampling, which is computationally demanding for soft matter and polymer systems. In this Letter, we introduce a hybrid Green-Kubo (hGK) framework that alleviates these limitations by partitioning the SACF into two physically meaningful regimes: (i) a short time ballistic component extracted directly from short MD simulations, and (ii) a long time relaxation tail represented using analytically motivated functions, , fitted only to short trajectories. This strategy bypasses the need for extensive sampling while preserving physical rigor. Benchmarking against SPC/E water confirms excellent agreement with established results, and we further demonstrate the efficacy of the method for challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI), for which the GK framework fails to converge. The computational savings are substantial, with reductions of several orders of magnitude in required sampling, achieved without compromising predictive accuracy. We also discuss the limitations of the hGK framework and outline clear avenues for refinement, including optimal tail selection and robust identification of relaxation regimes in noisy stress data. The hGK framework presented in this Letter provides a conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cond-mat.soft", "physics.comp-ph"], "url": "http://arxiv.org/abs/2512.04546", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: A hybrid Green-Kubo (hGK) framework for calculating viscosity from short MD simulations\nResearch Topic: computer vision\nAuthors: Akash K. Meel, Santosh Mogurampelly\nAbstract: Viscosity calculation from equilibrium molecular dynamics (MD) simulations relies on the traditional Green-Kubo (GK) framework, which integrates the stress autocorrelation function (SACF) over time. While the formalism is exact in the linear response regime, the traditional approach often suffers from poor convergence and requires extensive phase space sampling, which is computationally demanding for soft matter and polymer systems. In this Letter, we introduce a hybrid Green-Kubo (hGK) framework that alleviates these limitations by partitioning the SACF into two physically meaningful regimes: (i) a short time ballistic component extracted directly from short MD simulations, and (ii) a long time relaxation tail represented using analytically motivated functions, $φ(τ)$, fitted only to short trajectories. This strategy bypasses the need for extensive sampling while preserving physical rigor. Benchmarking against SPC/E water confirms excellent agreement with established results, and we further demonstrate the efficacy of the method for challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI), for which the GK framework fails to converge. The computational savings are substantial, with reductions of several orders of magnitude in required sampling, achieved without compromising predictive accuracy. We also discuss the limitations of the hGK framework and outline clear avenues for refinement, including optimal tail selection and robust identification of relaxation regimes in noisy stress data. The hGK framework presented in this Letter provides a conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials.\nCategories: cond-mat.soft, physics.comp-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04546\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 87, "abstract_length": 1744, "total_text_length": 1832, "word_count": 248, "sentence_count": 9}, "basic_keywords": ["framework", "time", "sampling", "short", "viscosity", "molecular", "simulations", "traditional"], "domain_keywords": [], "technical_terms": ["While", "Kubo", "Green", "Letter", "Viscosity", "Benchmarking"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04545", "topic": "computer vision", "title": "EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion", "authors": ["Pengfei Cao", "Zeao Ji", "Daojian Zeng", "Jun Zhao", "Kang Liu"], "first_author": "Pengfei Cao", "abstract": "Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch. However, existing methods suffer from two limitations. First, they depend on structured triplets that are misaligned with the free-text nature of LLM pretraining and fail to capture the nuanced relationships among facts. Second, they typically support one-time knowledge updates, with relatively limited research on the problem of sequential or lifelong editing. To address these gaps, we propose a new task, Lifelong Free-text Knowledge Editing (LF-Edit), which enables models to incorporate updates expressed in natural language and supports continual editing over time. Despite its promise, LF-Edit faces the dual challenge of integrating new knowledge while mitigating the forgetting of prior information. To foster research on this new task, we construct a large-scale benchmark, Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench), containing 16,835 free-text edit requests. We further design a cognitively inspired multi-rank evaluation framework encompassing four levels: memorization, understanding, constrained comprehension, and reasoning. To tackle the challenges inherent in LF-Edit, we introduce a novel approach named EvoEdit that enhances knowledge injection through Latent Perturbation Augmentation and preserves prior information via Knowledge-driven Parameter Fusion. Experimental results demonstrate that EvoEdit substantially outperforms existing knowledge editing methods on the proposed LF-Edit task.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["cs.CL"], "url": "http://arxiv.org/abs/2512.04545", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion\nResearch Topic: computer vision\nAuthors: Pengfei Cao, Zeao Ji, Daojian Zeng, Jun Zhao, Kang Liu\nAbstract: Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch. However, existing methods suffer from two limitations. First, they depend on structured triplets that are misaligned with the free-text nature of LLM pretraining and fail to capture the nuanced relationships among facts. Second, they typically support one-time knowledge updates, with relatively limited research on the problem of sequential or lifelong editing. To address these gaps, we propose a new task, Lifelong Free-text Knowledge Editing (LF-Edit), which enables models to incorporate updates expressed in natural language and supports continual editing over time. Despite its promise, LF-Edit faces the dual challenge of integrating new knowledge while mitigating the forgetting of prior information. To foster research on this new task, we construct a large-scale benchmark, Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench), containing 16,835 free-text edit requests. We further design a cognitively inspired multi-rank evaluation framework encompassing four levels: memorization, understanding, constrained comprehension, and reasoning. To tackle the challenges inherent in LF-Edit, we introduce a novel approach named EvoEdit that enhances knowledge injection through Latent Perturbation Augmentation and preserves prior information via Knowledge-driven Parameter Fusion. Experimental results demonstrate that EvoEdit substantially outperforms existing knowledge editing methods on the proposed LF-Edit task.\nCategories: cs.CL\nPublication Date: 2025-12-04\nPaper ID: 2512.04545\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 124, "abstract_length": 1734, "total_text_length": 1859, "word_count": 241, "sentence_count": 12}, "basic_keywords": ["knowledge", "editing", "edit", "free", "text", "lifelong", "task", "large"], "domain_keywords": [], "technical_terms": ["However", "Lifelong Free", "Despite", "Knowledge Editing", "Editing Benchmark", "Adjusting", "Bench", "Second"], "paper_type": "empirical", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 1.0}, "overall_quality_score": 0.975, "quality_tier": "high"}
{"paper_id": "2512.04543", "topic": "computer vision", "title": "Efficient Identification the Inequivalence of Mutually Unbiased Bases via Finite Operators", "authors": ["Jianxin Song", "Zhen-Peng Xu", "Changliang Ren"], "first_author": "Jianxin Song", "abstract": "The structural characterization of high-dimensional mutually unbiased bases (MUBs) by classifying MUBs subsets remains a major open problem. The existing methods not only fail to conclude on the exact classification, but also are severely limited by computational resources and suffer from the numerical precision problem. Here we introduce an operational approach to identify the inequivalence of MUBs subsets, which has less time complexity and entirely avoids the computational precision issues. For arbitrary MUBs subsets of elements in any prime dimension, this method yields a universal analytical upper bound for the amount of MUBs equivalence classes. By applying this method through simple iterations, we further obtain tighter classification upper bounds for any prime dimension . Crucially, the comparison of these upper bounds with existing lower bounds successfully determines the exact classification for all MUBs subsets in any dimension . We further extend this method to the case that the dimension is a power of prime number. This general and scalable framework for the classification of MUBs subsets sheds new light on related applications.", "publish_date": "2025-12-04", "update_date": "2025-12-04", "categories": ["quant-ph"], "url": "http://arxiv.org/abs/2512.04543", "comments": "", "query_keywords": "computer vision", "time_range": "week", "crawl_time": "2025-12-08T01:33:31.144033", "embedding_text": "Paper Title: Efficient Identification the Inequivalence of Mutually Unbiased Bases via Finite Operators\nResearch Topic: computer vision\nAuthors: Jianxin Song, Zhen-Peng Xu, Changliang Ren\nAbstract: The structural characterization of high-dimensional mutually unbiased bases (MUBs) by classifying MUBs subsets remains a major open problem. The existing methods not only fail to conclude on the exact classification, but also are severely limited by computational resources and suffer from the numerical precision problem. Here we introduce an operational approach to identify the inequivalence of MUBs subsets, which has less time complexity and entirely avoids the computational precision issues. For arbitrary MUBs subsets of $k$ elements in any prime dimension, this method yields a universal analytical upper bound for the amount of MUBs equivalence classes. By applying this method through simple iterations, we further obtain tighter classification upper bounds for any prime dimension $d\\leq 37$. Crucially, the comparison of these upper bounds with existing lower bounds successfully determines the exact classification for all MUBs subsets in any dimension $d \\leq 17$. We further extend this method to the case that the dimension is a power of prime number. This general and scalable framework for the classification of MUBs subsets sheds new light on related applications.\nCategories: quant-ph\nPublication Date: 2025-12-04\nPaper ID: 2512.04543\nSource: arXiv", "abstract_cleaned": true, "text_statistics": {"title_length": 90, "abstract_length": 1159, "total_text_length": 1250, "word_count": 183, "sentence_count": 9}, "basic_keywords": ["mubs", "subsets", "classification", "dimension", "prime", "method", "upper", "bounds"], "domain_keywords": [], "technical_terms": ["Here", "Crucially"], "paper_type": "methodology", "quality_scores": {"completeness": 0.9999999999999999, "recency": 1.0, "content_quality": 0.8999999999999999, "technical_substance": 0.5}, "overall_quality_score": 0.85, "quality_tier": "high"}
